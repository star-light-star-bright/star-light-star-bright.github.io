<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Spark学习笔记, ZWHBlog">
    <meta name="description" content="SparkSpark是什么？Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎，Spark是用Scala编写的
Spark和mr的区别？1、mr读取数据应该是多次,如果运算比较复杂应该是多个mr联合在一起使用的，处理的">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    

    <title>Spark学习笔记 | ZWHBlog</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 6.2.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">ZWHBlog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">ZWHBlog</div>
        <div class="logo-desc">
            
            这是我的博客网站
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/13.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Spark学习笔记</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/CDH/">
                                <span class="chip bg-color">CDH</span>
                            </a>
                        
                            <a href="/tags/Spark/">
                                <span class="chip bg-color">Spark</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/CDH/" class="post-category">
                                CDH
                            </a>
                        
                            <a href="/categories/CDH/Spark/" class="post-category">
                                Spark
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2020-09-14
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="Spark是什么？"><a href="#Spark是什么？" class="headerlink" title="Spark是什么？"></a>Spark是什么？</h3><p>Apache Spark 是专为<strong>大规模数据处理而设计的快速通用的计算引擎</strong>，Spark是用Scala编写的</p>
<h3 id="Spark和mr的区别？"><a href="#Spark和mr的区别？" class="headerlink" title="Spark和mr的区别？"></a>Spark和mr的区别？</h3><p>1、mr读取数据应该是多次,如果运算比较复杂应该是多个mr联合在一起使用的，处理的结果会先落地到磁盘上</p>
<p>2、mr中的算子太少map reduce spark:list–&gt;map –&gt;flatMap-&gt;filter–&gt;grroup…</p>
<p>3、mr没有容错。spark存在很优秀的容错概念</p>
<p>4、spark通用性比较强大 java scala python R</p>
<h3 id="Spark有几种部署方式"><a href="#Spark有几种部署方式" class="headerlink" title="Spark有几种部署方式?"></a>Spark有几种部署方式?</h3><h4 id="1、Local-模式"><a href="#1、Local-模式" class="headerlink" title="1、Local 模式"></a>1、Local 模式</h4><p>该模式被称为Local[N]单机模式，是用单机的多个线程来模拟Spark分布式计算，通常用来验证开发出来的应用程序逻辑上有没有问题。需要hadoop集群，本地还分为local单线程和local-cluster多线程</p>
<p><strong>指令：</strong>local单机模式：常用于本地开发测试，</p>
<pre><code>spark-shell --master local 

spark-shell --master local[4]
//代表4个核数来并发执行程序
</code></pre>
<h4 id="2、Standalone集群模式"><a href="#2、Standalone集群模式" class="headerlink" title="2、Standalone集群模式"></a>2、Standalone集群模式</h4><p>Standalone模式和单机运行的模式不同，这里必须在执行应用程序前，先<strong>启动Spark的Master和Worker</strong>守护进程。不过也能看出Master是有单点故障的；Spark支持ZooKeeper来实现 HA。启动Spark集群，<strong>不用启动Hadoop服务</strong>，除非你用到了HDFS的内容。使用独立的Spark集群模式提交任务</p>
<p>可以使用Spark的<strong>8080</strong> web ui来<strong>观察资源和应用程序的执行情况</strong>了。</p>
<h6 id="集群的结构"><a href="#集群的结构" class="headerlink" title="集群的结构"></a>集群的结构</h6><p>spark应用程序有一个<strong>Driver</strong>驱动，<strong>Driver可以运行在Client上也可以运行在master上</strong>。如果你使用spark-shell去提交job的话它会是运行在master上的，如果你使用spark-submit或者IDEA开发工具方式运行，那么它是运行在Client上的。这样我们知道了，<strong>Client的主体作用就是运行Driver</strong>。而<strong>master除了资源调度的作用还可以运行Driver</strong>。</p>
<p><strong>standalone是一个主从模式，master节点负责资源管理，worker节点负责任务的执行。</strong></p>
<p><strong>指令：</strong></p>
<pre><code>spark-shell --master spark://Linux02:7077

spark-shell --master spark://Linux02:7077 --deploy-mode client
//还可以配置属性
</code></pre>
<h6 id="产生的进程"><a href="#产生的进程" class="headerlink" title="产生的进程"></a>产生的进程</h6><p>1、<strong>Master</strong>进程做为cluster manager，用来对应用程序申请的资源进行管理</p>
<p>2、worker </p>
<p>3、<strong>SparkSubmit</strong> 做为Client端和运行driver程序，提交任务的客户端</p>
<p>4、<strong>CoarseGrainedExecutorBackend</strong> 	就是executor，用来并发执行应用程序</p>
<h4 id="3、Yarn"><a href="#3、Yarn" class="headerlink" title="3、Yarn"></a>3、Yarn</h4><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/IFksSe-VlegzzblE9WSByg">https://mp.weixin.qq.com/s/IFksSe-VlegzzblE9WSByg</a></p>
<p><strong>启动spark集群和Hadoop集群</strong></p>
<p>限于YARN自身的发展，目前仅支持粗粒度模式（Coarse-grained Mode）。这是由于YARN上的Container资源是不可以动态伸缩的，一旦Container启动之后，可使用的资源不能再发生变化，不过这个已经在YARN计划中了。</p>
<p><strong>yarn 负责资源管理</strong>，<strong>Spark 负责任务调度和计算</strong> </p>
<h4 id="Spark-on-Yarn的执行过程"><a href="#Spark-on-Yarn的执行过程" class="headerlink" title="Spark on Yarn的执行过程"></a>Spark on Yarn的执行过程</h4><p><img src="/cdh/spark/spark/image-20200904140401244.png" alt="image-20200904140401244"></p>
<ol>
<li>client向ResouceManager申请资源，返回一个applicationID</li>
<li>client上传spark jars下面的jar包，自己写的jar和配置</li>
<li>ResourceManager随机找一个资源充足的NodeManager</li>
<li>然后通过RPC让NodeManager从HDFS上下载jar包和配置，启动ApplicationMaster</li>
<li>ApplicationMaster向ResourceManager申请资源</li>
<li>ResourceManager中的ResourceScheduler找到符合条件的NodeManager，将NodeManager的信息返回给ApplicationMaster</li>
<li>ApplicationMaster和返回的NodeManager进行通信</li>
<li>NodeManager从HDFS下载依赖</li>
<li>NodeManager启动Executor</li>
<li>Executor启动之后反向向ApplicationMaster（Driver）注册</li>
</ol>
<h5 id="spark-on-yarn-的支持两种模式："><a href="#spark-on-yarn-的支持两种模式：" class="headerlink" title="spark on yarn 的支持两种模式："></a><strong>spark on yarn 的支持两种模式：</strong></h5><p>(1) <strong>yarn-cluster</strong>：适用于生产环境；</p>
<p>(2) <strong>yarn-client</strong>：适用于交互、调试，希望立即看到app的输出</p>
<p><strong>yarn-cluster和yarn-client的区别</strong>在于yarn appMaster，每个yarn app实例有一个appMaster进程，是为app启动的第一个container；负责从ResourceManager请求资源，获取到资源后，告诉NodeManager为其启动container。yarn-cluster和yarn-client模式内部实现还是有很大的区别。如果你需要用于生产环境，那么请选择yarn-cluster；而如果你仅仅是Debug程序，可以选择yarn-client。</p>
<h4 id="4、mesos"><a href="#4、mesos" class="headerlink" title="4、mesos"></a>4、mesos</h4><p>一个强大的分布式资源管理框架，它允许多种不同的框架部署在其上</p>
<h3 id="Spark的standalone集群的安装"><a href="#Spark的standalone集群的安装" class="headerlink" title="Spark的standalone集群的安装"></a>Spark的standalone集群的安装</h3><p>spark-2.2.0-bin-hadoop2.7获取jar包</p>
<p>1、解压完毕后，修改&#x2F;etc&#x2F;profile文件</p>
<pre><code>export JAVA_HOME=/root/Downloads/jdk1.8.0_161
export PATH=$PATH:$JAVA_HOME/bin
export HADOOP_HOME=/root/Downloads/hadoop-2.6.5
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export ZOOKEEPER_HOME=/root/Downloads/zookeeper-3.4.5
export PATH=$PATH:$ZOOKEEPER_HOME/bin
export HIVE_HOME=/root/Downloads/apache-hive-2.1.1-bin
export PATH=$PATH:$HIVE_HOME/bin
export FLUME_HOME=/root/Downloads/apache-flume-1.6.0-bin
export PATH=$PATH:$FLUME_HOME/bin 
export HBASE_HOME=/root/Downloads/hbase-1.2.6
export PATH=$PATH:$HBASE_HOME/bin 
export SQOOP_HOME=/root/Downloads/sqoop-1.4.6.bin__hadoop-2.0.4-alpha
export PATH=$PATH:$SQOOP_HOME/bin
export SCALA_HOME=/install/scala/scala-2.11.8
export PATH=$PATH:$SCALA_HOME/bin
export SPARK_HOME=/install/spark-2.2.0-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
</code></pre>
<p>source profile  刷新配置文件</p>
<p>2、修改conf&#x2F;spark-env.sh</p>
<pre><code>#主节点位置
SPARK_MASTER_HOST=Linux02
#端口，不配置默认7077
SPARK_MASTER_PORT=7077
#jdk配置
JAVA_HOME=/root/Downloads/jdk1.8.0_161
</code></pre>
<p>3、修改conf&#x2F;slaves</p>
<p>cp slaves.template slaves </p>
<pre><code>Linux01
Linux02
Linux03
</code></pre>
<p>4、发送到其他节点上</p>
<pre><code>scp -r spark-2.2.0-bin-hadoop2.7 root@Linux02:/install/
</code></pre>
<p>5、启动Spark</p>
<pre><code>start-master.sh  	//在主节点机器上启动master
start-slaves.sh		//启动其他机器的worker
</code></pre>
<h4 id="standalone集群搭建HA"><a href="#standalone集群搭建HA" class="headerlink" title="standalone集群搭建HA"></a>standalone集群搭建HA</h4><p>standalone模式中存在两个角色 master worker</p>
<p>zookeeper 进行masters节点的监控，并且切换主从</p>
<p>1、修改配置文件spark-env.sh  三台都改</p>
<pre><code class="scala">#SPARK_MASTER_HOST=univers02
#YARN_CONF_DIR=/home/ghostwowo/Downloads/hadoop-2.6.4/etc/hadoop
export 
SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER 
-Dspark.deploy.zookeeper.url=Linux01:2181,Linux02:2181,Linux03:2181
-Dspark.deploy.zookeeper.dir=/spark&quot;
SPARK_MASTER_PORT=7077
JAVA_HOME=/home/ghostwowo/Downloads/jdk1.8.0_73
</code></pre>
<p>2、启动zookeeper，三台启动</p>
<pre><code>ZKServer.sh start 
</code></pre>
<p>3、自动master和worker</p>
<pre><code>start-master.sh
start-slaves.sh
</code></pre>
<p>4、在另一台机器上再启动一个master</p>
<pre><code>start-master.sh
</code></pre>
<h3 id="Spark集群Yarn模式"><a href="#Spark集群Yarn模式" class="headerlink" title="Spark集群Yarn模式"></a>Spark集群Yarn模式</h3><p>1、修改hadoop下面的<strong>yarn-site.xml</strong></p>
<p>提交任务给yarn的时候，需要spark配置文件中指定yarn的配置地址，才能找到yarn的提交端口和地址</p>
<p>yarn会将磁盘中的一块区域当作内存使用，避免产生内存溢出</p>
<pre><code class="xml">&lt;property&gt;
&lt;name&gt;yarn.nodemanager.pmen-check-enabled&lt;/name&gt;
&lt;value&gt;false&lt;value&gt;
&lt;/property&gt;

&lt;!--关闭资源检测--&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.vmen-check-enabled&lt;/name&gt;
&lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;

vim capacity-scheduler.xml
  &lt;property&gt;
    &lt;name&gt;yarn.scheduler.capacity.resource-calculator&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.yarn.util.resource.DominantResourceCalculator&lt;/value&gt;
    &lt;description&gt;
      The ResourceCalculator implementation to be used to compare
      Resources in the scheduler.
      The default i.e. DefaultResourceCalculator only uses Memory while
      DominantResourceCalculator uses dominant-resource to compare
      multi-dimensional resources such as Memory, CPU etc.
    &lt;/description&gt;
  &lt;/property&gt;
</code></pre>
<h4 id="yarn集群两种部署模式"><a href="#yarn集群两种部署模式" class="headerlink" title="yarn集群两种部署模式"></a>yarn集群两种部署模式</h4><p>client&#x2F;cluster<br>client任务退出后，application断开，任务就死了<br>cluster的driver在container中，在终止任务后，任务继续在集群中运行，application没有断开</p>
<p>1、–deploy-mode部署模式，提交任务到集群中的时候，client模式下我们能够一直存在客户端，进行一直监听任务，driver在client端，可以以交互式的形式一直对任务进行管理</p>
<p>client模式中  appmaster默认占用的是512M</p>
<p>client模式下，driver在客户端，并且没有使用集群中的核数</p>
<pre><code class="scala">spark-submit --master yarn --deploy-mode client --class org.apache.spark.examples.SparkPi spark-examples_2.11-2.2.0.jar  10
</code></pre>
<p>2、–deploy-mode cluster :集群模式，driver在集群中，driver和applicationMaster,driver不在客户端，提交完毕的任务全部托管到集群</p>
<p>cluster模式，driver端在集群中，第1个containe是给application使用的，结果就在第一个container中</p>
<pre><code class="scala">spark-submit --master yarn --deploy-mode cluster --class org.apache.spark.examples.SparkPi spark-examples_2.11-2.2.0.jar  10
//下面的是错误的
//spark-shell跟driver进行交互，client模式，不能是cluster
spark-shell --master yarn --deploy-mode cluster --class org.apache.spark.examples.SparkPi spark-examples_2.11-2.2.0.jar  10
</code></pre>
<h4 id="hdfs和yarn的关系"><a href="#hdfs和yarn的关系" class="headerlink" title="hdfs和yarn的关系"></a>hdfs和yarn的关系</h4><p>yarn运行的时候需要在hdfs中创建一个临时空间，工作空间需要放入的数据是任务的代码包，描述文件，所以需要启动hdfs</p>
<h4 id="yarn模式进程"><a href="#yarn模式进程" class="headerlink" title="yarn模式进程"></a>yarn模式进程</h4><p>CoarseGrainedExecutorBackend     executor进程<br>SparkSubmit										提交任务的<br>ExecutorLanuncher	（applicationMaster）Yarn client 模式中独有，appmaster是cluster模式中的名称</p>
<h4 id="集群资源分配情况"><a href="#集群资源分配情况" class="headerlink" title="集群资源分配情况"></a>集群资源分配情况</h4><p><strong>cluster模式下的资源</strong></p>
<p>在第一台机器上启动第一个container就是appMaster 1cores 2G</p>
<p><img src="/cdh/spark/spark/image-20200401134900711.png" alt="image-20200401134900711"></p>
<p>其实executor是默认启动两个，executor默认使用1core 2G</p>
<p>因为<strong>在yarn中的分配资源最小单位1G</strong></p>
<p>在默认配置文件中的资源是1G+1cores,但是资源的使用情况是2G+1core</p>
<p><img src="/cdh/spark/spark/image-20200401135109494.png" alt="image-20200401135109494"></p>
<p>默认会增加driver中的一个缓冲区内容，防止oom错误，增加384M</p>
<p>1024+384 &#x3D; 1408M</p>
<p><strong>client模式下的资源</strong></p>
<p>appMaster 1G+1cores</p>
<p>每个executor启动的时候占用2G+1cores</p>
<p><img src="/cdh/spark/spark/image-20200401135338355.png" alt="image-20200401135338355"></p>
<p>在client模式中am占用的资源是512M+384&#x3D;896M –&gt;1G</p>
<h4 id="yarn运行流程"><a href="#yarn运行流程" class="headerlink" title="yarn运行流程"></a>yarn运行流程</h4><p>1、client提交一个任务给RM，RM把任务加入到队列queue(FIFO)，并返回一个APPId和一个hdfs路径（临时目录）。</p>
<p>2、client根据hdfs路径，把jar包配置文件、处理的文件描述信息发送到hdfs中（临时文件）。默认备份3个</p>
<p>3、rm节点开始启动第一个container，运行appMaster，将appMaster启动</p>
<p>4、appMaster反向向RM注册，</p>
<p>5、appMaster向RM申请资源</p>
<p>6、RM向appMaster返回一个List形式的container，这些container只是标识，不存在</p>
<p>7、appMaster拿到集合后，让NM启动container</p>
<p>8、driver开始解析任务<br>9、container中启动executor<br>10、executor方向和driver进行注册<br>………</p>
<h3 id="spark集群中的任务资源分配"><a href="#spark集群中的任务资源分配" class="headerlink" title="spark集群中的任务资源分配"></a>spark集群中的任务资源分配</h3><p>client通过submit或shell向executor发送任务，worker默认占有所有的资源(cores,)，内存默认当前大小减1G，如果只有一个G，则就使用一个G</p>
<p><strong>worker</strong>:集群启动的时候每个worker使用的默认一个8 cores  和 1G内存</p>
<p><strong>executor</strong>:默认占用1G，worker中就是1G，executor默认占用worker所有核数和内存</p>
<h3 id="Spark两种提交方式"><a href="#Spark两种提交方式" class="headerlink" title="Spark两种提交方式"></a>Spark两种提交方式</h3><h4 id="1、spark-submit提交"><a href="#1、spark-submit提交" class="headerlink" title="1、spark-submit提交"></a>1、spark-submit提交</h4><p>spark-submit模式，提交jar包任务运行，运行完毕直接停止，短应用</p>
<p><strong>指令：</strong></p>
<pre><code>spark-submit --master spark://Linux02:7077 --class com.star.spark.WordCount spark1902-1.0-SNAPSHOT.jar hdfs://Linux01:9000/aa.txt hdfs://Linux01:9000/wcres01
//spark-submit --master spark://Linux02:7077 --class  运行类的路径 jar包名  输入路径  输出路径
</code></pre>
<h5 id="常用的spark-submit参数"><a href="#常用的spark-submit参数" class="headerlink" title="常用的spark-submit参数"></a>常用的spark-submit参数</h5><p>–master：提交的集群主机</p>
<p>–class:运行jar包中哪个类</p>
<p>–name：任务在集群中的名称(单机必须指定)</p>
<p>–jars：一个任务中需要其他的jar，参数方式指定</p>
<p>–deploy-mode：部署模式(yarn模式使用)</p>
<p>–executor-memory 每个executor默认使用内存的大小</p>
<p>–executor-cores:每个executor默认使用的cores</p>
<p>–total-executor-cores:允许所有的executor公用的核数</p>
<pre><code>spark-submit --class org.apache.spark.examples.SparkP1 --executor-memory 512M --executor-cores 4 /spark-2.2.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.2.0.jar 10
//这里不用指定total,这个任务和上面任务共存，使用剩余核数
</code></pre>
<h4 id="2、spark-shell提交"><a href="#2、spark-shell提交" class="headerlink" title="2、spark-shell提交"></a>2、spark-shell提交</h4><p><strong>spark-shell交互式命令行</strong>，<strong>长应用</strong>，当时提交任务的时候<strong>底层也是用的spark-submit</strong></p>
<pre><code>//指定executor使用核数，需要指定total核数，否则自动用全部核数创建executor
spark-shell --master spark://Linux02:7077 --executor-memory 512M --executor-cores 4 --total-executor-cores 12
</code></pre>
<h4 id="spark-shell和spark-submit区别"><a href="#spark-shell和spark-submit区别" class="headerlink" title="spark-shell和spark-submit区别"></a>spark-shell和spark-submit区别</h4><p>①客户端的SparkSubmit进程会在应用程序提交给集群之后就退出</p>
<p>②Master会在集群中选择一个Worker进程生成一个子进程DriverWrapper来启动driver程序</p>
<p>③而该DriverWrapper 进程会占用Worker进程的一个core，所以同样的资源下配置下，会比第3种运行模式，少用1个core来参与计算</p>
<p>④应用程序的结果，会在执行driver程序的节点的stdout中输出，而不是打印在屏幕上</p>
<h3 id="Spark组件"><a href="#Spark组件" class="headerlink" title="Spark组件"></a>Spark组件</h3><p>client 提交任务</p>
<p>driver	driver进程运行在client端，对应用进行管理监控。Master节点指定某个Worker节点启动Driver进程，负责监控整个应用的执行。</p>
<p><strong>master+worker负责任务的资源调用的</strong></p>
<p>master	控制整个集群，监控worker。在YARN模式中为资源管理器</p>
<p>worker	从节点，负责控制计算节点，启动Executor或者Driver。	</p>
<p>executor	一个应用程序拥有一系列进程，叫做executors，他们在集群上运行，即使没有job在运行，这些executor仍然被这个应用程序占有。这种方式让数据存储在内存中，以支持快速访问，同时让task快速启动成为现实。</p>
<p><strong>driver+executors  负责任务执行，（一次性的，和每个app相关</strong>）<br>app：spark–submit spark-shel提交的任务就是一个ap</p>
<p>DAG图	在执行前生成执行规划图，优化的作用</p>
<h3 id="Spark的特点"><a href="#Spark的特点" class="headerlink" title="Spark的特点"></a>Spark的特点</h3><p>1、<strong>速度快</strong></p>
<p>Spark 使用DAG 调度器、查询优化器和物理执行引擎，能够在批处理和流数据获得很高的性能</p>
<p>2、<strong>使用简单</strong></p>
<p>Spark的易用性主要体现在两个方面。一方面，我们可以用较多的编程语言来写我们的应用程序，比如说Java,Scala,Python,R 和 SQL;另一方面，Spark 为我们提供了超过80个高阶操作，这使得我们十分容易地创建并行应用，除此之外，我们也可以使用Scala,Python,R和SQL shells,以实现对Spark的交互。</p>
<p>3、 <strong>通用性强</strong></p>
<p>以Spark为基础建立起来的模块(库)有**Spark SQL,Spark Streaming,MLlib(machine learning)和GraphX(graph)**。我们可以很容易地在同一个应用中将这些库结合起来使用，以满足我们的实际需求。</p>
<p>4、<strong>到处运行</strong></p>
<p>Spark应用程度可以运行十分多的框架之上。它可以运行在Hadoop,Mesos,Kubernetes,standalone,或者云服务器上。它有多种多种访问源数据的方式。可以用standalone cluster模式来运行Spark应用程序，并且其应用程序跑在Hadoop,EC2,YARN,Mesos,或者Kubernates。对于访问的数据源，我们可以通过使用Spark访问HDFS,Alluxio,Apache Cassandra,HBase,Hive等多种数据源。</p>
<h3 id="Spark端口号"><a href="#Spark端口号" class="headerlink" title="Spark端口号"></a>Spark端口号</h3><p>8080		tomcat端口，用于网页监控，底层tcp协议</p>
<p>8081		worker的端口号tcp协议的链接</p>
<p>7077		master主节点默认通信端口号（管理心跳等，心跳传输，命令接受）</p>
<p>6066		Spark外部服务端口</p>
<p>8081		可以监控workers的信息	</p>
<p>4040		监控任务执行情况（主节点上使用）</p>
<h3 id="Spark中的shuffle流程"><a href="#Spark中的shuffle流程" class="headerlink" title="Spark中的shuffle流程"></a>Spark中的shuffle流程</h3><p>将数据放入到上一个rdd分区所对应的那个机器上，进行本地化，第二个rdd对应的task任务进行拉取数据</p>
<h3 id="Spark的数据本地化策略"><a href="#Spark的数据本地化策略" class="headerlink" title="Spark的数据本地化策略"></a>Spark的数据本地化策略</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/jxhd1/p/6702224.html?utm_source=itdadao&amp;utm_medium=referral">https://www.cnblogs.com/jxhd1/p/6702224.html?utm_source=itdadao&amp;utm_medium=referral</a></p>
<p>1.Spark数据的本地化：移动计算，而不是移动数据</p>
<p>2.Spark中的数据本地化级别： </p>
<pre><code class="scala">//五个级别
PROCESS_LOCAL
NODE_LOCAL
NO_PREF
RACK_LOCAL
ANY
</code></pre>
<p><strong>1、PROCESS_LOCAL</strong>  进程本地化：task要计算的数据在同一个Executor中</p>
<p><strong>2、NODE_LOCAL</strong>   节点本地化：速度比 PROCESS_LOCAL 稍慢，<strong>因为数据需要在不同进程之间传递或从文件中读取</strong></p>
<p>情况一：task要计算的数据是在同一个Worker的不同Executor进程中</p>
<p> 情况二：task要计算的数据是在同一个Worker的磁盘上，或在 HDFS 上，恰好有 block 在同一个节点上。</p>
<p><img src="/cdh/spark/spark/1008304-20170310232213498-2042397906.jpg" alt="img"></p>
<p><strong>Spark计算数据来源于HDFS，那么最好的数据本地化级别就是NODE_LOCAL</strong></p>
<p><strong>3、NODE_PREF</strong>   没有最佳位置这一说，数据从哪里访问都一样快，不需要位置优先。比如说SparkSQL读取MySql中的数据</p>
<p><strong>4、RACK_LOCAL</strong> 机架本地化，数据在同一机架的不同节点上。需要通过网络传输数据及文件 IO，比 NODE_LOCAL 慢</p>
<p>情况一：task计算的数据在Worker2的Executor中</p>
<p>情况二：task计算的数据在Worker2的磁盘上</p>
<p><img src="/cdh/spark/spark/1008304-20170310232213967-209773010.png" alt="img"></p>
<p>5、<strong>ANY</strong>  跨机架，数据在非同一机架的网络上，速度最慢</p>
<h3 id="Spark的基本概念"><a href="#Spark的基本概念" class="headerlink" title="Spark的基本概念"></a>Spark的基本概念</h3><p><strong>application</strong>&#x3D;&#x3D;spark-submit spark-shell代码块组成的jar就是一个应用</p>
<p><strong>Job</strong>:遇见多少个action算子就有多少个job，包含很多的task进行并行计算。Spark 采用惰性机制，对 RDD 的创建和转换并不会立即执行，只有在遇到第一个 Action 时才会生成一个 Job，然后统一调度执行。一个 Job 包含 N 个Transformation 和 1 个 Action。</p>
<p><strong>stage阶段</strong>：job中存在几个阶段，是根据遇见shuffle当前任务就会切分阶段，<strong>stage&#x3D;shuffle+1</strong>。由于 Shuffle 的存在，不同的Stage 是不能并行计算的，如果是shuffle类型的，因为后面 Stage 的计算需要前面 ShuffleStage  的结果，所以shufflemapStage保存到本地</p>
<p>shuffleMapStage，结果数据保存到本地，供下一个rdd的shuffle拉取数据</p>
<p>resultStage，在其他的介质中存储或者打印</p>
<p><strong>RDD</strong></p>
<p>一个RDD就是你的数据的一个不可变的分布式元素集合，在集群中跨节点分布，可以通过若干提供了转换和处理的底层API进行并行处理。每个RDD都被分为多个分区，这些分区运行在集群不同的节点上。</p>
<p><strong>Task</strong>:具体执行任务。一个 Job 在每个 Stage 内都会<strong>按照Stage中的最后一个RDD 的 Partition 数量，一个分区对应一个task，创建对应数量的task</strong>。每个 Stage 内多个并发的 Task 执行逻辑完全相同，只是在不同的Partition，<strong>一个 Stage 的总 Task 的个数由 Stage 中最后的一个 RDD 的 Partition 的个数决定</strong></p>
<p>Spark 中有<strong>两种类型 task:</strong></p>
<p>ShuffleMapTask：输出是 shuffle 所需数据， stage 的划分也以此为依据， shuffle 之前的所有变换是一个 stage，shuffle 之后的操作是另一个 stage 。</p>
<p>ResultTask：输出是 result，比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有 shuffle，直接就输出了，那么它的 task 是 resultTask，stage 也只有一个；如果是 rdd.map(x &#x3D;&gt; (x, 1)).reduceByKey(_ + _).foreach(println)， 这个 job 因为有 reduce，所以有一个 shuffle 过程，那么 reduceByKey 之前的是一个 stage，执行 ShuffleMapTask，输出 shuffle 所需的数据，reduceByKey 到最后是一个 stage，直接就输出结果了。如果 job 中有多次 shuffle，那么每个 shuffle 之前都是一个stage。</p>
<p><strong>Master</strong></p>
<p>主节点，管理从节点，接受心跳</p>
<p><strong>Worker</strong></p>
<p>从节点，主要任务是接收master请求，启动executor，运行任务</p>
<p><strong>executor</strong>：本质是一个包装后的线程池，里面执行task任务，线程池只能执行多线程任务，但是task不是多线程的，task外层包装成了taskRunner，这个类是多线程的，运行它的Run方法。</p>
<p><strong>Partition</strong>：分区。一个 RDD 在物理上被切分为多个 Partition，即数据分区，这些 Partition 可以分布在不同的节点上。<strong>Partition 是 Spark 计算任务的基本处理单位</strong>，决定了并行计算的粒度，而 Partition 中的每一条 Record 为基本处理对象。例如对某个 RDD 进行 map 操作，在具体执行时是由多个并行的 Task 对各自分区的每一条记录进行 map 映射。</p>
<h4 id="Driver端的运行组件和运行流程"><a href="#Driver端的运行组件和运行流程" class="headerlink" title="Driver端的运行组件和运行流程"></a>Driver端的运行组件和运行流程</h4><p>提交任务到master时，driver通过spark-submit方法，一反射机制调用用户自己定义的main方法，driver端开始初始化组件（DAGScheduler，TaskScheduler，schedulerBackend）</p>
<p><strong>DAG</strong>(Directed Acyclic Graph)：有向无环图。在图论中，边没有方向的图称为无向图，如果边有方向称为有向图。在无向图的基础上，任何顶点都无法经过若干条边回到该点，则这个图就没有环路，称为有向无环图( DAG 图)。Spark 中使用 DAG 对 RDD 的关系进行建模，<strong>描述了 RDD 的依赖关系</strong>，这种关系也被称之为 lineage。</p>
<p>在看DAG图时：（粉色的大的代表阶段 蓝色的小的部分算子 点rdd 连线代表依赖关系）</p>
<p><strong>RDD object</strong>：生成DAG有向无环图</p>
<p>将这个DAG图交给DAGScheduler进行切分阶段</p>
<p><strong>DAGScheduler</strong>：根据Job构建基于Stage的DAG图（把生成的图进行切分），并提交Stage给TaskScheduler，其划分Stage的依据是RDD之间的依赖关系。（从最后一个阶段往前找，找到这个阶段的父阶段，继续判断父阶段是不是存在父阶段，如果依赖关系是宽依赖，则stage数量+1，是窄依赖，则这个stage中的rdd数量+1）最后一个rdd分区数量对应个数的task任务每个阶段都放入到taskSet中（每个taskset对应一个stage），如果组成task任务的个数不等于0，将生成的task放入到TaskSet中。在父阶段都全部查找完毕，开始提交任务给TaksScheduler</p>
<p><strong>Shuffle：</strong>产生宽依赖就会有一个shuffle</p>
<p><strong>TaskScheduler</strong>：将TaskSet提交给Worker（集群）运行，每个Executor运行什么Task就是在此处分配的，对任务的执行和监控的组件。首先初始化调度模式FIFO模式，通过定时器的形式不断检测有没有资源，如果资源足够通过schedulerBackend组件提交任务给executor执行，</p>
<p><strong>schedulerBackend</strong>：通信组件，负责driver和executor之间的通信，在提交任务到集群之前，先确定资源足够，将任务进行序列化，使用的是netty（类似akka）框架，每个任务发送给那个executor，发送完毕后的executor减去相应的资源</p>
<h4 id="Spark提交流程图"><a href="#Spark提交流程图" class="headerlink" title="Spark提交流程图"></a>Spark提交流程图</h4><p><img src="/cdh/spark/spark/20180627143538839" alt="img"></p>
<h4 id="Driver端工作组件"><a href="#Driver端工作组件" class="headerlink" title="Driver端工作组件"></a>Driver端工作组件</h4><p><img src="/cdh/spark/spark/307536-20190626225303718-2048524998.png" alt="img"></p>
<h4 id="Spark执行流程"><a href="#Spark执行流程" class="headerlink" title="Spark执行流程"></a>Spark执行流程</h4><p>1、driver进程启动，初始化后，发送请求给Master，进行注册</p>
<p>2、Master是一个消息队列（FIFIO），Master接收到Driver请求，把任务加入到master队列中，反馈任务已经加入到队列中并返回一个任务编号（id）</p>
<p>3、driver 调用DAGScheduler准备DAG图</p>
<pre><code class="scala">//driver组件工作流程
//1、RDD Object 
//解析代码，生成DAG图，构建一个简单的图解
//2、DAGScheduler  
//根据依赖关系进行任务的划分和构建，shuffle算子划分成不同的阶段，每个阶段stage的最后一个rdd的分区数量就是本阶段中task最终的数量，将一个stage中的所有task包装成taskSet进行提交，一个taskset中有多个task，task 会被分发到指定的 Executor 去执行，在任务执行的过程中，Executor 也会不断与 Driver 进行通信，报告任务运行情况
//3、taskScheduler
//task调度器，接收到数据taskSet，通过DagScheduler拿到任务对象，将任务通过集群管理器的形式进行提交给master
//master开始进行任务资源的划分，开始执行提交的任务，如果没有资源，进入消息队列中等待
//4、executor开始执行任务，执行任务的时候，按照阶段执行，stage分为两种，shuffleMapStage resultStage
//shuffleMapStage执行完毕后，结果保存到本地，
//resultStage将这个任务存储到其他介质，或者打印出来
（这期间driver和master工作是并行执行的，master通知worker，worker开始按照master分配的资源开始启动executor，executor启动后反向向driver注册，）
</code></pre>
<p>3、master发送请求给worker要求启动executor，worker接收到Master请求后为任务启动executor，负载均衡的调配</p>
<p>4、executor启动后，向Driver反注册，注册完毕driver给executor远程协议发送任务（task）</p>
<p>5、executor对RDD的partition进行并行计算，按照阶段执行，形成新的RDD</p>
<p><img src="/cdh/spark/spark/1775767-20200320164621730-1944918839.png" alt="img"></p>
<p>stage分为两种，shuffleMapStage resultStage<br>shuffleMapStage执行完毕后，结果保存到本地，<br>resultStage将这个任务存储到其他介质，或者打印出来</p>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><h4 id="什么是RDD？"><a href="#什么是RDD？" class="headerlink" title="什么是RDD？"></a>什么是RDD？</h4><p><strong>RDD</strong>：Spark提供的<strong>主要抽象是一个弹性分布式数据集(RDD)<strong>，它是一个</strong>跨集群节点分区的元素集合</strong>，可以<strong>并行操作</strong>。只读分区记录的集合，Spark 对所处理数据的基本抽象。<strong>RDD 是 Spark 分发数据和计算的基础抽象类</strong>。一个 RDD 是一个<strong>不可改变的分布式集合对象</strong>，因此在使用 scala 编写时，**前面加修饰符 val **。Spark 中 的计算可以简单抽象为对 RDD 的创建、转换和返回操作结果的过程。(RDD是一个装有很多数据的集合，这个集合分布在多个节点上面，允许集群进行并行化的操作，rdd是一个主抽象（rdd其实是不存在的，里面也没有数据），只读的，不可变的，默认被分区的数据集)</p>
<p><strong>RDD在driver端</strong>，负责代理数据的流动节点，告诉代码逻辑处理的时候将要受到什么样的处理</p>
<p>RDD代表处理数据的节点，真正的数据在原来的位置</p>
<p>RDD里面的操作方法叫做算子，算子才是处理数据的方法</p>
<h4 id="创建RDD的三种方式"><a href="#创建RDD的三种方式" class="headerlink" title="创建RDD的三种方式"></a>创建RDD的三种方式</h4><h5 id="1、读取hdfs这样的分布式文件系统的文件"><a href="#1、读取hdfs这样的分布式文件系统的文件" class="headerlink" title="1、读取hdfs这样的分布式文件系统的文件"></a>1、读取hdfs这样的分布式文件系统的文件</h5><p>&#x2F;&#x2F;block块数量就是rdd分区数量，如果是文件默认是2</p>
<p>这个方法产生rdd的时候，它的分区数量和block的数量一样，并且分区的数量最小是2</p>
<pre><code class="scala">scala&gt; sc.textFile(&quot;hdfs://Linux01:9000/aa.txt&quot;)
res0: org.apache.spark.rdd.RDD[String] = hdfs://Linux01:9000/aa.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:25
//这个是在集群中执行的
</code></pre>
<h5 id="2、集合并行化-在driver端生成一个本地集合，通过集合创建这个rdd"><a href="#2、集合并行化-在driver端生成一个本地集合，通过集合创建这个rdd" class="headerlink" title="2、集合并行化(在driver端生成一个本地集合，通过集合创建这个rdd)"></a>2、集合并行化(在driver端生成一个本地集合，通过集合创建这个rdd)</h5><p>如果是集合并行化的方式，那么<strong>产生的分区数量就是集群的核数</strong>，集群中的核数意味着能够并行的处理任务数量，rdd分区和core对应，才能让集群的处理能力最大化</p>
<p><img src="/cdh/spark/spark/image-20200325182509432.png" alt="image-20200325182509432"></p>
<p>makeRDD底层使用的是parallelize方法</p>
<pre><code class="scala">val arr = Array(1,2,3,4,5)
//这个是在driver本地执行的
sc.makeRDD(arr)
//将集合变成RDD
scala&gt; sc.parallelize(arr)
res3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:27
//也是将集合变成RDD
</code></pre>
<p><strong>如果是集合并行化的方式，那么产生的分区数量就是集群的核数，集群中的核数意味着能够并行的处理任务数量，rdd分区和core对应，才能让集群的处理能力最大化</strong></p>
<pre><code>scala&gt; var arr = Array(1,2,3,4,5,67)
arr: Array[Int] = Array(1, 2, 3, 4, 5, 67)

scala&gt; sc.makeRDD(arr)
res11: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at makeRDD at &lt;console&gt;:27

scala&gt; res11.partitions.size
res12: Int = 24
</code></pre>
<h5 id="3、转换类算子生成新的RDD"><a href="#3、转换类算子生成新的RDD" class="headerlink" title="3、转换类算子生成新的RDD"></a>3、转换类算子生成新的RDD</h5><p><strong>转换类算子的形势形成的新的RDD的分区数量和原来的rdd的分区数量一致（默认）</strong></p>
<p>转换类算子存在两种：1、可以修改分区的 2、不可以修改分区的，只要不是刻意的去修改分区，那么分区数量就不会发生任何改变</p>
<pre><code class="scala">scala&gt; res0.flatMap(_.split(&quot; &quot;))
res4: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at flatMap at &lt;console&gt;:27
</code></pre>
<h4 id="RDD特点"><a href="#RDD特点" class="headerlink" title="RDD特点"></a>RDD特点</h4><p>1、容错，数据能够自动恢复</p>
<p>2、分布式，自动分区，每个分区被一个executor处理</p>
<p>3、弹性的，可大可小，容错</p>
<p>4、数据集，在使用的时候可以当作存储数据的集合</p>
<h4 id="RDD的五大特性"><a href="#RDD的五大特性" class="headerlink" title="RDD的五大特性"></a>RDD的五大特性</h4><p><img src="/cdh/spark/spark/image-20200318205222799.png" alt="image-20200318205222799"></p>
<ol>
<li>每个rdd上面都存在着一系列的分区列表</li>
<li>存在一个函数，这个函数用于计算每一个分片</li>
<li>每个rdd都和其他的rdd存在一系列的依赖关系（算子）</li>
<li>在rdd上会存在一个可选择的分区器，必须放在k-v这样的rdd上面</li>
<li>优先位置进行计算(移动计算比移动数据本身更划算，本地化策略)</li>
</ol>
<h5 id="特性1，获取分区列表"><a href="#特性1，获取分区列表" class="headerlink" title="特性1，获取分区列表"></a>特性1，获取分区列表</h5><pre><code class="scala">protected def getPartitions: Array[Partition]
</code></pre>
<p>每个rdd上面都存在这样的一个方法，能够获取rdd上面的所有分区列表</p>
<p><img src="/cdh/spark/spark/image-20200318210518165.png" alt="image-20200318210518165"></p>
<p>对于代码来说，分区就是一个特质</p>
<p><img src="/cdh/spark/spark/image-20200318210606694.png" alt="image-20200318210606694"></p>
<p>分区的作用，每个分区以后再处理的数据每个分区对应一个线程，分区越多人物的并行化就越高，在没有运行任务的时候，首先解析代码，记录每个分区和blk的对应关系，以后这个分区中的数据将会从blk中读取数据（block在集群上备份3份）</p>
<p>blockRddPartition每个分区对应一个blk，计算的时候首先获取这个blk的位置</p>
<h5 id="特性2，computer方法"><a href="#特性2，computer方法" class="headerlink" title="特性2，computer方法"></a>特性2，computer方法</h5><p>spark中在处理rdd的时候，是一个写好的，完整的，成熟的处理框架，能够适配几乎所有的用户行为,面对于用户的不同业务需求和逻辑，spark中提供了一个<strong>compute的方法可以进行几乎任何逻辑的处理</strong>，compute方法可以将任何用户的函数进行总的计算</p>
<pre><code class="scala">override def compute(split: Partition, context: TaskContext): Iterator[U] =
    f(context, split.index, firstParent[T].iterator(split, context))
</code></pre>
<p>compute函数，这个函数通过用户自定义的函数逻辑，将<strong>rdd中每个分区的数据进行遍历迭代，然后获取每个分区中的每条数据，使用函数进行统一处理</strong></p>
<p>处理完毕以后将数据封装到一个iterator迭代器中返回</p>
<p><img src="/cdh/spark/spark/image-20200319102342436.png" alt="image-20200319102342436"></p>
<h5 id="特性3，依赖关系"><a href="#特性3，依赖关系" class="headerlink" title="特性3，依赖关系"></a>特性3，依赖关系</h5><p>RDD之间存在一系列的依赖关系：其实依赖关系就是rdd之间的算子</p>
<p>精髓：在一个任务中，一旦遇见宽依赖肯定切分阶段，宽依赖&#x3D;&#x3D;shuffle</p>
<pre><code class="scala">protected def getDependencies: Seq[Dependency[_]] = deps
//每个rdd上面都存在这样一个方法能够获取rdd上面的依赖关系
</code></pre>
<p>依赖关系是一个抽象类，存在两个大的实现类，<strong>shuffleDependency</strong>,<strong>narrowDependency</strong></p>
<p><img src="/cdh/spark/spark/image-20200319102712878.png" alt="image-20200319102712878"></p>
<p><strong>shuffleDependency</strong>：宽依赖，rdd之间存在shuffle流程</p>
<p><strong>narrowDependency</strong>：窄依赖，rdd和rdd之间的关系是一对一管道形式的</p>
<p><strong>narrowDependency窄依赖中又存在两种：</strong></p>
<p><strong>OneToOneDependency</strong></p>
<p>几乎所有的窄依赖都是OneToOneDependency的</p>
<p><strong>RangeDependency</strong></p>
<p><strong>union</strong>：rangeDependency（union比较特殊，是rangeDependency，可以合并分区，没有shuffle）</p>
<p><strong>宽依赖算子：reduceBykey groupByKey sortBy sortBykey distinct repartition</strong></p>
<p><strong>窄依赖算子：map flatMap filter mapPartitions coalesce …</strong></p>
<p><strong>map窄依赖案例</strong></p>
<img src="file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.jpg" alt="img">

<p><strong>reduceByKey宽依赖案例</strong></p>
<p><img src="/cdh/spark/spark/image-20200319104127633.png" alt="image-20200319104127633"></p>
<h5 id="特性4，分区器"><a href="#特性4，分区器" class="headerlink" title="特性4，分区器"></a>特性4，分区器</h5><p>在一个k-v 的rdd上面存在一个可选的分区器</p>
<p>一般的rdd上面不存在分区器，rdd数据从上一个到下一个rdd之间，数据相互移动，重新分配数据，分区器其实是算子，将上一个rdd的数据按照一定的规定给下一个rdd的不同分区进行分配数据，一般会在shuffle中产生，必须加在k-v类型的rdd上面</p>
<p>有两种Partitioner</p>
<h5 id="hashPartitionerrangePartitioner"><a href="#hashPartitionerrangePartitioner" class="headerlink" title="hashPartitioner	rangePartitioner"></a><strong>hashPartitioner	rangePartitioner</strong></h5><p>这两种是自带的分区器，一旦<strong>产生shuffle才会有分区器</strong></p>
<pre><code class="scala">//hash分区器的功能
int类型的hashcode就是自己本身
如果key是空就放入到0号分区中，如果不是空的就按照key的hashcode值进行按照分区编号取余数，放入对应分区

//range分区器的功能
rangePartition让rdd到下一个rdd之间尽量均匀的适配，但是分区的数量和元素数量一致或者小于元素数量（如果数据不够自定义分区的数量，分区数量自适应）
在rangePartitioner存在一个抽样方式，进行数据的适配，让数据在下一个rdd中尽量的分配均匀，
</code></pre>
<p><img src="/cdh/spark/spark/image-20200726162959005.png" alt="image-20200726162959005"></p>
<p>reduceByKey	groupByKey…</p>
<p><strong>几乎所有的shuffle算子使用的分区器都是hashPartitioner</strong></p>
<p><strong>只有sortByKey使用的是rangePartitioner</strong></p>
<p><strong>sortByKey不可以随便改分区，自适应</strong></p>
<pre><code class="scala">sortByKey(false,10)
partition.size=4
</code></pre>
<p><strong>源码：</strong></p>
<pre><code class="scala">@transient val partitioner: Option[Partitioner] = None
</code></pre>
<pre><code class="scala">abstract class Partitioner extends Serializable &#123;
  def numPartitions: Int
  def getPartition(key: Any): Int
&#125;
</code></pre>
<p><img src="/cdh/spark/spark/image-20200319104759983.png" alt="image-20200319104759983"></p>
<h5 id="特性5，移动计算"><a href="#特性5，移动计算" class="headerlink" title="特性5，移动计算"></a>特性5，移动计算</h5><p>优先位置进行计算，减少数据的移动，几乎不产生远程的io，让计算速度更快</p>
<h3 id="SparkSQL-–-Join-的三种方式"><a href="#SparkSQL-–-Join-的三种方式" class="headerlink" title="SparkSQL – Join 的三种方式"></a>SparkSQL – Join 的三种方式</h3><p>参考文章：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/0xcafedaddy/p/7614299.html">https://www.cnblogs.com/0xcafedaddy/p/7614299.html</a></p>
<p>参考文章：<a target="_blank" rel="noopener" href="https://juejin.cn/post/6844903998734991374">https://juejin.cn/post/6844903998734991374</a></p>
<h5 id="1、Shuffle-Hash-Join"><a href="#1、Shuffle-Hash-Join" class="headerlink" title="1、Shuffle Hash Join"></a>1、<strong>Shuffle Hash Join</strong></h5><p>一张小表，一张大表</p>
<p>过程：</p>
<ol>
<li>确定build table和probe table 两个概念。build table 会根据join key来进行hash table ，而probe table 使用join key进行匹配。通常情况，比较小的表当作build table ，大表当作probe table。</li>
<li>hash table 确定之后，读取build table 数据，每一条数据进行join key 进行hash 处理，hash到对应的bucket中。生成一张hashtable，hashtable缓存到内存中，内存放不下就放到对应的磁盘中。</li>
<li>扫描 probe table 数据，数据在hash table中进行匹配，匹配成功就join在一起。</li>
</ol>
<h5 id="2、Broadcast-Hash-Join"><a href="#2、Broadcast-Hash-Join" class="headerlink" title="2、Broadcast Hash Join"></a>2、Broadcast Hash Join</h5><p>broadcast hash join 就是 shuffle hash join 的小表广播版。</p>
<p>使用条件：</p>
<ol>
<li>被广播的表需要小于spark.sql.autoBroadcastJoinThreshold所配置的信息，默认是10M；</li>
<li>基表不能被广播，比如left outer join时，只能广播右表。</li>
</ol>
<p>一张小表（大于10M）和一张大表，小于10M的进行广播</p>
<p>将其中一张小表广播分发到另一张大表所在的所有主机</p>
<p>在每个executor上执行单机版hash join，小表映射，大表试探</p>
<p><strong>缺点</strong>：</p>
<p>这个方案只能广播较小的表，否则数据的冗余传输就是远大于shuffle的开销；</p>
<p>另外，广播时需要被广播的表collect到driver端，当频繁的广播出现时，对driver端的内存也是一个考验。</p>
<h5 id="3、Sort-Merge-join"><a href="#3、Sort-Merge-join" class="headerlink" title="3、Sort Merge join"></a>3、Sort Merge join</h5><p>shuffle阶段：将两	张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式并行处理</p>
<p>sort阶段：对单个分区节点的两表数据，分别进行排序</p>
<p>merge阶段：对排好序的两张分区表数据执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同join key就merge输出，否则取更小一边。全部结束后 union all 全部的结果</p>
<p>见下图示意：</p>
<p><img src="/cdh/spark/spark/0" alt="img"></p>
<h3 id="自定义分区器"><a href="#自定义分区器" class="headerlink" title="自定义分区器"></a>自定义分区器</h3><p>在数据倾斜的时候可以使用这种方式</p>
<p>通过继承Partitioner类，重写（numPartitions、getPartition）两个方法，实现自定义分区器</p>
<pre><code class="scala">coalesce//自定义分区器    hello分一个  其他的分一个
class MyPartitions extends Partitioner &#123;
  //几个分区
  override def numPartitions: Int = 2
  //分区的规则
  override def getPartition(key: Any): Int = &#123;
      //hello字符串的单独分一组
    if ((&quot;hello&quot;).equals(key.asInstanceOf[String])) &#123;
      0
      //其他的单独分一组
    &#125; else &#123;
      1
    &#125;
  &#125;
&#125;
</code></pre>
<pre><code class="scala">//调用自定义分区器  
def main(args: Array[String]): Unit = &#123;
    val conf = new SparkConf()
    //本机器所有核
    conf.setMaster(&quot;local[*]&quot;) //默认生成
    conf.setAppName(&quot;wc&quot;) //系统指定的
    val sc = new SparkContext(conf)
    //单机版读取本地文件
    val strs: RDD[String] = sc.textFile(&quot;aa.txt&quot;)
    val rdd1: RDD[String] = strs.flatMap(t =&gt; t.split(&quot; &quot;))
    val rdd2: RDD[(String, Int)] = rdd1.map((_, 1)).reduceByKey(_ + _)
    val rdd3: RDD[(String, Int)] = rdd2.partitionBy(new MyPartitions)
    //    val rdd3: RDD[(String, Iterable[(String, Int)])] = rdd2.groupBy(_._1)
    //    val rdd4: RDD[(String, Int)] = rdd3.mapValues(_.foldLeft(0)((a,b)=&gt;a+b._2))
    //    rdd4.foreach(println)
//    rdd3.saveAsTextFile(&quot;res&quot;)
  println(rdd3.toDebugString)
    rdd3.groupByKey()
  &#125;
</code></pre>
<h3 id="DAG有向无环图"><a href="#DAG有向无环图" class="headerlink" title="DAG有向无环图"></a>DAG有向无环图</h3><h4 id="DAG有向无环图查看方法"><a href="#DAG有向无环图查看方法" class="headerlink" title="DAG有向无环图查看方法"></a>DAG有向无环图查看方法</h4><p><img src="/cdh/spark/spark/image-20200726164017855.png" alt="image-20200726164017855"></p>
<p>DAG有向无环图 </p>
<p>粉色的大的代表阶段stage </p>
<p>蓝色的小代表一个算子 </p>
<p>黑色的点rdd的个数 </p>
<p>连线代表依赖关系</p>
<p>起点：整个任务中的第一个rdd</p>
<p>终点：action算子</p>
<h3 id="RDD依赖关系"><a href="#RDD依赖关系" class="headerlink" title="RDD依赖关系"></a>RDD依赖关系</h3><p>依赖关系叫做dependency,<strong>宽依赖shuffleDependency</strong> 和<strong>窄依赖 narrowDependency</strong></p>
<p><strong>RDD和RDD之间的依赖关系就是算子</strong></p>
<h4 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a><strong>窄依赖</strong></h4><p><strong>窄依赖不划分阶段</strong>，窄依赖是一个抽象类，实现类有两个（<strong>rangeDependency</strong>，<strong>OneToOneDependency</strong>）</p>
<p><strong>大部分窄依赖算子的依赖都是OneToOneDependency，不改变分区，不产生shuffle</strong></p>
<p><strong>rangDependency的算子只有union</strong></p>
<h4 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a><strong>宽依赖</strong></h4><p>宽依赖划分阶段</p>
<p><strong>带有shuffle流程的算子就是宽依赖</strong></p>
<h4 id="改变分区而不产生shuffle的特殊的算子"><a href="#改变分区而不产生shuffle的特殊的算子" class="headerlink" title="改变分区而不产生shuffle的特殊的算子"></a>改变分区而不产生shuffle的特殊的算子</h4><p><strong>join</strong></p>
<p>join算子既可以是窄依赖也可以是宽依赖，hash分区器，分区数量一样就是窄依赖</p>
<p>两个都是hashPartitioner，但是分区数量不一样，就是宽依赖</p>
<p>如果两个join的rdd之间分区器一样，那么就是窄依赖，不然就是宽依赖</p>
<p><strong>union</strong></p>
<p>union算子可以改变分区数量，但是不会产生阶段的划分，因为没有shuffle</p>
<p><strong>coalesce</strong></p>
<p>可以修改分区数量（缩小），不产生阶段的划分不存在shuffle，增大分区数会有shuffle过程</p>
<h4 id="依赖和容错："><a href="#依赖和容错：" class="headerlink" title="依赖和容错："></a>依赖和容错：</h4><p>1、数据的恢复，rdd中的数据在运行的时候丢失了，可以从上一个rdd对应的分区中查找数据，窄依赖数据恢复比较简单，宽依赖需要从上一个rdd对应的所有分区中查找</p>
<p>2、如果数据丢失可以从driver端进行查找数据的关系，进行数据恢复</p>
<p>3、  driver端除了进行数据的恢复记载，还有如果executor执行任务期间宕机了，driver端会进行任务的重试，重试3次</p>
<p>本地化策略 node_local process_local</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><ol>
<li><pre><code>计算过程中查看依赖关系，窄依赖是以管道方式进行运算的，都是在一个机器节点的上面处理数据，比如filter map flatMap,shuffle算子才会产生数据的节点间相互移动（mr中的shuffle一样）
</code></pre>
<ol start="2">
<li><pre><code>失败回复来看，窄依赖的数据恢复，只需要去上一个rdd的对应分区中去看，宽依赖需要找到上一个rdd中的所有分区，复杂程度比较高
</code></pre>
<ol start="3">
<li><pre><code>综上所述，引入一个新的概念stage,在一个job中通过shuffle算子切分的每个部分就是一个stage，一个stage其实一组一对一方式的rdd组成的，一个stage中的rdd对应关系都是管道的关系，执行任务的时候也会按照stage进行执行
</code></pre>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="RDD分区规则"><a href="#RDD分区规则" class="headerlink" title="RDD分区规则"></a>RDD分区规则</h3><p>读取HDFS文件创建RDD，有几个block块RDD几个分区（RDD分区数量和block数量一致），默认最小的分区数量最小是2，可以自己指定分区数量（不要轻易改变分区数量），保证一个线程处理一个分区的数据，一个exexurot可以多个线程</p>
<p>如果是集合并行化产生的RDD，那么分区数量就是集群的核数，因为集群中的核数意味能够并行处理的任务数量，RDD分区和core对应，才能让集群能力最大化</p>
<pre><code>scala&gt; val arr = Array(1,2,3,4,5,6)
arr: Array[Int] = Array(1, 2, 3, 4, 5, 6)

scala&gt; sc.makeRDD(arr)
res7: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at makeRDD at &lt;console&gt;:27

scala&gt; res7.partitions.size
res8: Int = 24
</code></pre>
<pre><code>sc.textFile(&quot;hdfs://Linux01:9000/aa.txt&quot;,1)
//指定分区数量是1
</code></pre>
<p>转化类算子形成新的RDD的分区数量和原来RDD分区的数量一致</p>
<p>转换类算子存在两种：</p>
<p>1、可以修改分区的</p>
<pre><code>groupBy(_._1)  会修改分区
</code></pre>
<p>2、不可以修改分区的</p>
<pre><code>map	就修改不了分区
</code></pre>
<h3 id="RDD的算子"><a href="#RDD的算子" class="headerlink" title="RDD的算子"></a>RDD的算子</h3><p><strong>算子是一种规范，只是规定了怎么进行rdd中数据的处理，而真正执行的逻辑是在算子的函数中，并且算子永远只在driver端，算子中的函数才是在executor端执行的</strong></p>
<p><strong>有几个action算子就有几个job，一个job对应多个task任务，task任务随机分配在各个executor中。</strong><br><strong>job任务之间分区没有关系，每个job重新进行分区</strong></p>
<p>RDD中所有的算子都分为两类</p>
<p>1、<strong>action算子（行动类算子）</strong></p>
<p>会触发 SparkContext 提交 Job 作业</p>
<p><strong>在调用时不会产生新的RDD</strong></p>
<p>2、<strong>transformation算子（转换类算子）</strong></p>
<p><strong>调用后产生新的RDD</strong></p>
<pre><code>sc.textFile(&quot;hdfs://Linux01:9000/aa.txt&quot;)
val rdd = rdd.map(_*10)
rdd1.collect 
//sc.textFile 运行在driver端
//hdfs://Linux01:9000/aa.txt 运行在executor端
</code></pre>
<h5 id="map："><a href="#map：" class="headerlink" title="map："></a><strong>map</strong>：</h5><p>算子遍历的次数是rdd中元素的个数 rdd.map(_*10)</p>
<ol>
<li>一一映射的关系</li>
<li>返回值的集合类型和原来的类型一样，元素个数不发生改变</li>
<li>没有修改分区的功能</li>
</ol>
<h5 id="mapValues："><a href="#mapValues：" class="headerlink" title="mapValues："></a><strong>mapValues：</strong></h5><ol>
<li>scala中只能作用在map集合的value上面</li>
<li>不能修改分区数量</li>
</ol>
<pre><code class="scala">scala&gt; res7.mapValues(t=&gt;t._1*t._2)
res9: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[10] at mapValues at &lt;console&gt;:35

scala&gt; res9.collect
res10: Array[(String, Int)] = Array((zhangsan,6000), (wangwu,10500), (lisi,8100))
</code></pre>
<h5 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions:"></a><strong>mapPartitions:</strong></h5><p>转换类算子</p>
<p>每次遍历一个分区中的内容，并且返回</p>
<p>如果存在一个业务逻辑，这个逻辑一旦在mapPartitions中进行运行，那么就会减少很多次数</p>
<pre><code class="scala">sc.makeRDD(arr)
res37.makeRDD(arr)
res38.mapPartitions(t=&gt;t.map(_*10))
res41.collect
</code></pre>
<h5 id="case-when"><a href="#case-when" class="headerlink" title="case when"></a>case when</h5><p>条件函数</p>
<pre><code class="scala">case when 条件1 then 结果1 else 结果2 end

列入：
case when aa==1 then 1  else 0 end
</code></pre>
<h5 id="zip"><a href="#zip" class="headerlink" title="zip:"></a><strong>zip:</strong></h5><p>转换类算子</p>
<p>拉链操作</p>
<pre><code class="scala">var arr1 = Array(1,2,3,4,5,6,7,8)
var arr = Array(1,2,3,4,5,6)
arr zip arr1

sc.makeRDD(arr,3)
sc.makeRDD(arr1,4)
res50 zip res51
res52.collect
//会报错，因为他们的分区数量不相同

sc.makeRDD(arr,3)
sc.makeRDD(arr1,3)
res50 zip res54
res52.collect
//会报错，虽然分区数量相同，但是每个分区的元素并不相同
</code></pre>
<h5 id="zipWithIndex"><a href="#zipWithIndex" class="headerlink" title="zipWithIndex:"></a><strong>zipWithIndex:</strong></h5><p>跟索引进行拉链，索引类型是long</p>
<pre><code>res50.zipWithIndex
res58.collect
//res59: Array[(Int, Long)] = Array((1,0), (2,1), (3,2), (4,3), (5,4), (6,5))
</code></pre>
<h5 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a><strong>mapPartitionsWithIndex</strong></h5><p>遍历一个分区并且带有分区下标（哪个分区的，里面数据）</p>
<p>遍历每一个分区，把分区下表和数据进行拉链</p>
<pre><code class="scala">scala&gt; var arr = Array(1,2,3,4,5,6,7,8,9)
arr: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)

scala&gt; sc.makeRDD(arr,3)
res60: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[46] at makeRDD at &lt;console&gt;:27

scala&gt; res60.mapPartitionsWithIndex((part:Int,data:Iterator[Int])=&gt;data.map((part,_)))
res61: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[47] at mapPartitionsWithIndex at &lt;console&gt;:29

scala&gt; res61.collect
res62: Array[(Int, Int)] = Array((0,1), (0,2), (0,3), (1,4), (1,5), (1,6), (2,7), (2,8), (2,9))
//（索引，数据）
</code></pre>
<h5 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a><strong>flatMap</strong></h5><p>map+flatten,<strong>在spark中没有flatten</strong></p>
<p>filter算子</p>
<p>元素的个数会发生改变，集合类型不变，数据类型不变</p>
<p>不能修改分区</p>
<pre><code class="scala">scala&gt; var arr = Array(1,2,3,4,5,6,7,8,9)
arr: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)

scala&gt; sc.makeRDD(arr,3)
res71: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[57] at makeRDD at &lt;console&gt;:27

scala&gt; res71.filter(_&gt;3)
res72: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[58] at filter at &lt;console&gt;:29

scala&gt; res72.mapPartitionsWithIndex((a,b)=&gt;b.map((a,_)))
res73: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[59] at mapPartitionsWithIndex at &lt;console&gt;:31

scala&gt; res73.collect
res74: Array[(Int, Int)] = Array((1,4), (1,5), (1,6), (2,7), (2,8), (2,9))
//filter算子如果将一个分区中的数据都过滤掉，那么这个分区依旧存在
</code></pre>
<h5 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a><strong>groupBy</strong></h5><p>groupBy的区别就是元素的<strong>value是一个iterable的类型</strong></p>
<p><strong>groupBy可以修改分区的数量</strong></p>
<pre><code class="scala">scala&gt; var arr = Array(1,2,3,4,5,6,7,8,9)
arr: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)

//这里分区是3
scala&gt; sc.makeRDD(arr,3)
res71: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[57] at makeRDD at &lt;console&gt;:27

scala&gt; res71.groupBy(_&gt;5)
res75: org.apache.spark.rdd.RDD[(Boolean, Iterable[Int])] = ShuffledRDD[61] at groupBy at &lt;console&gt;:29

scala&gt; res75.mapPartitionsWithIndex((a,b)=&gt;b.map((a,_)))
res76: org.apache.spark.rdd.RDD[(Int, (Boolean, Iterable[Int]))] = MapPartitionsRDD[62] at mapPartitionsWithIndex at &lt;console&gt;:31

//这里结果分区变成了1
scala&gt; res76.collect
res77: Array[(Int, (Boolean, Iterable[Int]))] = Array((1,(false,CompactBuffer(4, 5, 1, 2, 3))), (1,(true,CompactBuffer(6, 7, 8, 9))))
</code></pre>
<h5 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a><strong>groupByKey</strong></h5><p>转换类算子</p>
<p>groupByKey默认就按照key进行分组，所以<strong>操作的rdd必须是（k,v）类型的</strong></p>
<p><strong>可以修改分区数量</strong>，返回值类型 <strong>RDD[k,Iterable[v]]</strong></p>
<pre><code class="scala">scala&gt; var arr = Array((&quot;A&quot;,1),(&quot;A&quot;,1),(&quot;B&quot;,1),(&quot;C&quot;,1))
arr: Array[(String, Int)] = Array((A,1), (A,1), (B,1), (C,1))

scala&gt; sc.makeRDD(arr,3)
res78: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[63] at makeRDD at &lt;console&gt;:27

//这是groupBy结果，v是[(k,v)]
scala&gt; res78.groupBy(_._1)
res79: org.apache.spark.rdd.RDD[(String, Iterable[(String, Int)])] = ShuffledRDD[65] at groupBy at &lt;console&gt;:29

//这是groupByKey结果，v是[(v)]
scala&gt; res78.groupByKey()
res80: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[66] at groupByKey at &lt;console&gt;:29

//聚合一下v
scala&gt; res80.mapValues(_.sum)
res81: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[67] at mapValues at &lt;console&gt;:31

scala&gt; res81.collect
res82: Array[(String, Int)] = Array((B,1), (C,1), (A,2))
</code></pre>
<h5 id="reduceByKey："><a href="#reduceByKey：" class="headerlink" title="reduceByKey："></a>reduceByKey：</h5><p>转换类算子</p>
<p>带有局部的聚合combiner，带有聚合函数</p>
<p>取出key对应的value来聚合，可以修改分区</p>
<p>自动的分组和聚合</p>
<pre><code class="scala">scala&gt; var arr = Array((&quot;A&quot;,1),(&quot;A&quot;,1),(&quot;B&quot;,1),(&quot;C&quot;,1))
arr: Array[(String, Int)] = Array((A,1), (A,1), (B,1), (C,1))

scala&gt; sc.makeRDD(arr,3)
res78: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[63] at makeRDD at &lt;console&gt;:27

//加入聚合函数
scala&gt; res78.reduceByKey(_+_)
res87: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[69] at reduceByKey at &lt;console&gt;:29

scala&gt; res87.collect
res88: Array[(String, Int)] = Array((B,1), (C,1), (A,2))
</code></pre>
<h5 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy()"></a>sortBy()</h5><p>转换类算子，可以改变分区</p>
<p>对数据进行排序（指定排序字段）</p>
<pre><code>res0.sortBy(-_._2)//倒序
res0.sortBy(_._2)//正序
res0.sortBy(_._2,false,3)//倒序，分区数量是4
</code></pre>
<h5 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey()"></a>sortByKey()</h5><p>转换类算子</p>
<p>按照key进行排序，底层使用的分区器是rangepartition，</p>
<p>在rangePartitioner中存在一个抽样方式，进行数据的适配，让数据在下一个rdd中尽量的分配均匀，所以虽然sortByKey是转换类算子，但是<strong>依旧存在job的生成</strong></p>
<pre><code>var arr = Array((&quot;a&quot;,20),(&quot;a&quot;,34),(&quot;b&quot;,32))
sc.textFile(arr)
res0.reduceBykey(_+_)
res0.sortByKey(true,3)
//正序，分区数量是3，按照key排序

scala&gt; res12.sortByKey(false,2)
res15: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[14] at sortByKey at &lt;console&gt;:29

scala&gt; res15.partitions.size
res16: Int = 2

scala&gt; res12.sortByKey(false,10)
res17: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[17] at sortByKey at &lt;console&gt;:29

scala&gt; res17.partitions.size
res18: Int = 4

scala&gt; res12.reduceByKey(_+_,10)
res19: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[18] at reduceByKey at &lt;console&gt;:29

scala&gt; res19.partitions.size
res20: Int = 10
</code></pre>
<h5 id="union"><a href="#union" class="headerlink" title="union"></a>union</h5><p>并集，将两个RDD数据和分区进行组合</p>
<pre><code class="scala">scala&gt; var arr = Array(1,2,3,4,5,6)
arr: Array[Int] = Array(1, 2, 3, 4, 5, 6)

scala&gt; var arr1 = Array(4,5,6,7)
arr1: Array[Int] = Array(4, 5, 6, 7)

scala&gt; sc.makeRDD(arr,2)
res1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at &lt;console&gt;:27

scala&gt; sc.makeRDD(arr1,3)
res2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:27

scala&gt; res1 union res2
res3: org.apache.spark.rdd.RDD[Int] = UnionRDD[2] at union at &lt;console&gt;:33

scala&gt; res3.collect                                                               
res4: Array[Int] = Array(1, 2, 3, 4, 5, 6, 4, 5, 6, 7)

scala&gt; res3.partitions.size
res5: Int = 5

//通过mapPartitionsWithIndex看出元素没有发生变化
scala&gt; res3.mapPartitionsWithIndex((a,b)=&gt;b.map((a,_)))
res7: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[3] at mapPartitionsWithIndex at &lt;console&gt;:35

scala&gt; res7.collect
res8: Array[(Int, Int)] = Array((0,1), (0,2), (0,3), (1,4), (1,5), (1,6), (2,4), (3,5), (4,6), (4,7))
//五个分区
</code></pre>
<h5 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h5><p>转换类算子  <strong>交集</strong></p>
<p>对数据进行交集，分区数量根据分区数量大的那个来分区，改变分区数量</p>
<pre><code class="scala">scala&gt; var arr = Array(1,2,3,4,5,6)
arr: Array[Int] = Array(1, 2, 3, 4, 5, 6)

scala&gt; var arr1 = Array(4,5,6,7)
arr1: Array[Int] = Array(4, 5, 6, 7)

scala&gt; sc.makeRDD(arr,2)
res1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at &lt;console&gt;:27

scala&gt; sc.makeRDD(arr1,3)
res2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:27

scala&gt; res1 intersection res2
res9: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at intersection at &lt;console&gt;:33

scala&gt; res9.partitions.size
res10: Int = 3
</code></pre>
<h5 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h5><p>转换类算子</p>
<p>数据进行差集，分区是谁进行的，分区数就是谁的</p>
<pre><code class="scala">scala&gt; res1 subtract res2
res12: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at subtract at &lt;console&gt;:33

scala&gt; res12.partitions.size
res13: Int = 2

scala&gt; res2 subtract res1
res14: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[17] at subtract at &lt;console&gt;:33

scala&gt; res14.partitions.size
res15: Int = 3
</code></pre>
<h5 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h5><p>转换类算子</p>
<p>对数据进行去重</p>
<p><strong>distinct算法原理</strong></p>
<pre><code class="scala">//distinct算法原理
def distinct(numPartitions:Int)(implicit ord:Ordering[T] = null):RDD[T]= withScope&#123;
map(x=&gt;(x,null)).reduceByKey((x,y))=&gt;x.numPartitions).map(_._1)
&#125;
//使用的是reduceByKey，
//reduceByKey后变成(k,null),然后map(._1),获取key
</code></pre>
<p><img src="/cdh/spark/spark/image-20200316173534151.png" alt="image-20200316173534151"></p>
<h5 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h5><p><strong>aggregateByKey中的初始化值每个分区参与一次运算，整体聚合的时候不参与</strong></p>
<p><strong>aggreagate源码</strong></p>
<pre><code class="scala">def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U = withScope &#123;
    // Clone the zero value since we will also be serializing it as part of tasks
    var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance())
    val cleanSeqOp = sc.clean(seqOp)
    val cleanCombOp = sc.clean(combOp)
    val aggregatePartition = (it: Iterator[T]) =&gt; it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)
    val mergeResult = (index: Int, taskResult: U) =&gt; jobResult = combOp(jobResult, taskResult)
    sc.runJob(this, aggregatePartition, mergeResult)
    jobResult
  &#125;
</code></pre>
<p><strong>案例</strong></p>
<p><strong>每个分区参与一次运算，整体聚合的时候不参与</strong></p>
<pre><code class="scala">def main(args: Array[String]): Unit = &#123;
    val agg =  Array((&quot;A&quot;,1), (&quot;B&quot;,1), (&quot;A&quot;,1), (&quot;B&quot;,1), (&quot;C&quot;,1), (&quot;A&quot;,10))
    val conf = new SparkConf()
    conf.setMaster(&quot;local[*]&quot;)
    conf.setAppName(&quot;wc&quot;)
    val sc = new SparkContext(conf)
    val value: RDD[(String, Int)] = sc.makeRDD(agg,3)
    val value1: RDD[(String, Int)] = value.aggregateByKey(0)(_ + _, _ + _)
    value1.collect.foreach(println)
    val value2: RDD[(String, Int)] = value.aggregateByKey(10)(_ + _, _ + _)
    value2.collect.foreach(println)
  &#125;
//(B,22)(C,11)(A,42)
</code></pre>
<h5 id="join"><a href="#join" class="headerlink" title="join"></a>join</h5><p>join算子是关联算子，<strong>只能作用在k-v类型的数据</strong>上面，<strong>两个rdd之间做join的时候是按照k1&#x3D;&#x3D;k2进行的join</strong></p>
<pre><code class="scala">var arr = Array((&quot;zhangsan&quot;,200),(&quot;lisi&quot;,300),(&quot;wangwu&quot;,350),(&quot;zhaosi&quot;,320))
var arr1 = Array((&quot;zhangsan&quot;,30),(&quot;lisi&quot;,27),(&quot;wangwu&quot;,30),(&quot;liuneng&quot;,1))
sc.makeRDD(arr,3)
sc.makeRDD(arr1,3)
res5 join res6
res7.collect
//没有对应的k则丢弃
//Array[(String, (Int, Int))] = Array((zhangsan,(200,30)), (wangwu,(350,30)), (lisi,(300,27)))
res7.mapValues(t=&gt;t._1*t._2)
res9.collect
</code></pre>
<h5 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h5><p><strong>只能作用在k-v类型的数据</strong>，显示左侧的全部数据,没有对应数据的v是0，出来后v._2的数据类型是Option</p>
<p>Option两个子类： Some,None</p>
<pre><code class="scala">var arr = Array((&quot;zhangsan&quot;,200),(&quot;lisi&quot;,300),(&quot;wangwu&quot;,350),(&quot;zhaosi&quot;,320))
var arr1 = Array((&quot;zhangsan&quot;,30),(&quot;lisi&quot;,27),(&quot;wangwu&quot;,30),(&quot;liuneng&quot;,1))
sc.makeRDD(arr,3)
sc.makeRDD(arr1,3)
res5 leftOuterJoin res6
//org.apache.spark.rdd.RDD[(String, (Int, Option[Int]))] = MapPartitionsRDD[17] at leftOuterJoin at &lt;console&gt;:33
res16.mapValues(t=&gt;t._1*t._2.getOrElse(0))
res17.collect
//Array[(String, Int)] = Array((zhangsan,6000), (wangwu,10500), (lisi,8100), (zhaosi,0))
</code></pre>
<h5 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h5><p><strong>只能作用在k-v类型的数据</strong>，显示左侧的全部数据,没有对应数据的v是0，出来后v._1的数据类型是Option</p>
<pre><code class="scala">var arr = Array((&quot;zhangsan&quot;,200),(&quot;lisi&quot;,300),(&quot;wangwu&quot;,350),(&quot;zhaosi&quot;,320))
var arr1 = Array((&quot;zhangsan&quot;,30),(&quot;lisi&quot;,27),(&quot;wangwu&quot;,30),(&quot;liuneng&quot;,1))
sc.makeRDD(arr,3)
sc.makeRDD(arr1,3)
res5 rightOuterJoin  res6
//org.apache.spark.rdd.RDD[(String, (Option[Int], Int))] = MapPartitionsRDD[21] at rightOuterJoin at &lt;console&gt;:33
res16.mapValues(t=&gt;t._1*t._2.getOrElse(0))
res17.collect
//Array[(String, (Option[Int], Int))] = Array((zhangsan,(Some(200),30)), (wangwu,(Some(350),30)), (lisi,(Some(300),27)), (liuneng,(None,1)))
</code></pre>
<h5 id="cogroup-全连接"><a href="#cogroup-全连接" class="headerlink" title="cogroup  全连接"></a>cogroup  全连接</h5><p>全连接，v._1和v._2的类型都是Option</p>
<p>会将两个集合中k相同的值合并成一个</p>
<pre><code class="scala">var arr = Array((&quot;zhangsan&quot;,200),(&quot;lisi&quot;,300),(&quot;wangwu&quot;,350),(&quot;zhaosi&quot;,320))
var arr1 = Array((&quot;zhangsan&quot;,30),(&quot;lisi&quot;,27),(&quot;wangwu&quot;,30),(&quot;liuneng&quot;,1))
sc.makeRDD(arr,3)
sc.makeRDD(arr1,3)
res5 cogroup res6
//org.apache.spark.rdd.RDD[(String, (Iterable[Int], Iterable[Int]))] = MapPartitionsRDD[24] at cogroup at &lt;console&gt;:33
res24.collect
//Array[(String, (Iterable[Int], Iterable[Int]))] = 
//Array((zhangsan,(CompactBuffer(200),CompactBuffer(30))), 
//(wangwu,(CompactBuffer(350),CompactBuffer(30))), 
//(lisi,(CompactBuffer(300),CompactBuffer(27))), 
//(zhaosi,(CompactBuffer(320),CompactBuffer())), 
//(liuneng,(CompactBuffer(),CompactBuffer(1)))
</code></pre>
<h5 id="fulljoin（cogroup）"><a href="#fulljoin（cogroup）" class="headerlink" title="fulljoin（cogroup）"></a>fulljoin（cogroup）</h5><p>也是全连接</p>
<pre><code class="scala">var arr = Array((&quot;zhangsan&quot;,200),(&quot;lisi&quot;,300),(&quot;wangwu&quot;,350),(&quot;zhaosi&quot;,320))
var arr1 = Array((&quot;zhangsan&quot;,30),(&quot;lisi&quot;,27),(&quot;wangwu&quot;,30),(&quot;liuneng&quot;,1))
sc.makeRDD(arr,3)
sc.makeRDD(arr1,3)
res5 cogroup res6
//org.apache.spark.rdd.RDD[(String, (Iterable[Int], Iterable[Int]))] = MapPartitionsRDD[24] at cogroup at &lt;console&gt;:33
res24.collect
//Array[(String, (Iterable[Int], Iterable[Int]))] = 
//Array((zhangsan,(CompactBuffer(200),CompactBuffer(30))),
//(wangwu,(CompactBuffer(350),CompactBuffer(30))), 
//(lisi,(CompactBuffer(300),CompactBuffer(27))), 
//(zhaosi,(CompactBuffer(320),CompactBuffer())), 
//(liuneng,(CompactBuffer(),CompactBuffer(1))))
res24.mapValues(t=&gt;t._1.sum * t._2.sum)
scala&gt; res26.collect
//res27: Array[(String, Int)] = Array((zhangsan,6000), (wangwu,10500), (lisi,8100), (zhaosi,0), (liuneng,0))
</code></pre>
<h5 id="fullOuterJoin"><a href="#fullOuterJoin" class="headerlink" title="fullOuterJoin"></a>fullOuterJoin</h5><p>和fulljoin效果一样</p>
<h5 id="cartesian笛卡尔积"><a href="#cartesian笛卡尔积" class="headerlink" title="cartesian笛卡尔积"></a>cartesian笛卡尔积</h5><p>算子会将所有的分区数量都相乘，然后元素个数也会相乘</p>
<pre><code class="scala">var arr = Array(1,2,3,4,5,6,7,8,9)
sc.makeRDD(arr)
res29 cartesian res29
res30.collect
// Array[(Int, Int)] = Array((1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (1,7), (1,8), (1,9), (2,1), (2,2)
res30.count
//res32: Long = 81 
//9*9=81个job
</code></pre>
<h5 id="repartition-算子"><a href="#repartition-算子" class="headerlink" title="repartition 算子"></a>repartition 算子</h5><p>转换类算子</p>
<p>专门修改分区</p>
<p>repartition算子可以增加和减小分区数量</p>
<p><strong>repartition带有shuffle</strong></p>
<p> repartition<strong>底层使用的是coalesce算子</strong></p>
<pre><code class="scala">var arr = Array(1,2,3,4,5,6,7,8,9)
sc.makeRDD(arr,3)
res33.repartition(2)
res34.partitions.size
// Int = 2
//能增能减
</code></pre>
<h5 id="coalesce算子"><a href="#coalesce算子" class="headerlink" title="coalesce算子"></a>coalesce算子</h5><p>专门修改分区</p>
<p>coalesce算子<strong>只能减小分区的数量</strong>但是不能增加，<strong>coalesce算子不带有shuffle</strong></p>
<pre><code class="scala">var arr = Array(1,2,3,4,5,6,7,8,9)
sc.makeRDD(arr,3)
res33.coalesce(2)
res34.partitions.size
// Int = 2
res33.coalesce(6)
// Int = 3
//只能减不能增
coalesce(number,boolear)
//是否带有shuffle	
</code></pre>
<h5 id="aggregateByKey-1"><a href="#aggregateByKey-1" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h5><p>转换类算子</p>
<p>按照key进行聚合</p>
<p>aggregateByKey中的初始化值<strong>每个分区参与一次运算，整体聚合的时候不参与</strong></p>
<pre><code class="scala">var arr = Array((&quot;A&quot;,1),(&quot;B&quot;,1),(&quot;A&quot;,1),(&quot;B&quot;,1),(&quot;C&quot;,1),(&quot;A&quot;,10))
sc.makeRDD(arr,3)
res4.aggregateByKey(0)(_+_,_+_)
//org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[2] at aggregateByKey at &lt;console&gt;:29
res5.collect
// Array[(String, Int)] = Array((B,2), (C,1), (A,12))   
res4.aggregateByKey(10)(_+_,_+_)
//res7: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[3] at aggregateByKey at &lt;console&gt;:29
res7.collect
//res8: Array[(String, Int)] = Array((B,22), (C,11), (A,42))
</code></pre>
<h4 id="行动类算子"><a href="#行动类算子" class="headerlink" title="行动类算子"></a>行动类算子</h4><h5 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h5><p>action行动类算子</p>
<p>迭代每个元素</p>
<p>打印的时候<strong>每个分区对应一个executor进行执行，在executor中进行打印数据，driver端是看不见效果的</strong></p>
<p>foreach算子在打印多次数据的时候，出现每个executor中的数据不停变化，spark-shell是不是一个应用，一个应用中如果使用多次action算子，会产生多个job，这些job都会使用一套driver+executor，每个job在运行的时候分区和executor的对应关系是不断变化的</p>
<h5 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h5><p>遍历每个分区对象，是iterator类型的</p>
<p>每次遍历一个分区的数据进行打印，但是每次运行的结果不一定在一个executor上面</p>
<pre><code>res0.foreachPartition(t=&gt;t.foreach(println))
</code></pre>
<pre><code>每个分区单独使用一个connection对象
res.foreachPartition(t=&gt;&#123;
val connection = DriverManager.getconnetion
&#125;)
</code></pre>
<h5 id="take"><a href="#take" class="headerlink" title="take"></a>take</h5><p>行动类算子</p>
<p>是一个特殊的算子，截取rdd上面对应个数的元素，组成一个新的本地集合返回，如果<strong>元素相隔分区会产生多个的job，take算子的job任务个数和扫描分区的个数相关</strong></p>
<pre><code>var arr = Array(1,2,3,4,5,6,7,8,9)
sc.makeRDD(arr,3)
arr.take(3)
//一个job
res27.take(4)
//跨分区扫描，会有两个job
</code></pre>
<h5 id="first"><a href="#first" class="headerlink" title="first"></a>first</h5><p>行动类算子</p>
<p>获取第一个元素返回</p>
<pre><code class="scala">sc.makeRDD(Array(4,2,1,3,8,7),2)
res19.first
//Int = 4
</code></pre>
<h5 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h5><p>收集各个executor计算的结果形成一个集合，返回driver端</p>
<h5 id="collectAsMap"><a href="#collectAsMap" class="headerlink" title="collectAsMap"></a>collectAsMap</h5><p>将收集各个executor计算的结果形成一个Map，返回driver端</p>
<p>收集的时候数据类型必须得是对偶元组类型的，否则报错</p>
<h5 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h5><p>按照key求value的个数</p>
<pre><code>var arr = Array((&quot;a&quot;,20),(&quot;a&quot;,34))
sc.makeRDD(arr,2)
res0.countByKey()
//Map(a-&gt;2)
</code></pre>
<h5 id="aggreagate"><a href="#aggreagate" class="headerlink" title="aggreagate"></a>aggreagate</h5><p>行动类算子，初始化值参与四次运算，<strong>每个分区参与一次，整体聚合参与一次</strong></p>
<p><img src="/cdh/spark/spark/image-20200318203010014.png" alt="image-20200318203010014"></p>
<pre><code class="scala">aggregate(initial_value)(function1,function2)
</code></pre>
<pre><code class="scala">var arr = Array(1,2,3,4,5,6,7,8,9)
sc.makeRDD(arr,3)
res0.aggregate(0)(_+_,_+_)
//res1:Int = 45  
res0.aggregate(0)(_+_,_*_)
//res2: Int = 0  
res0.aggregate(10)(_+_,_+_)
//res3: Int = 85
//每个分区参与一次，整体聚合参与一次
//((1+2+3+10),(4+5+6+10),(7+8+9+10)+10)
</code></pre>
<h5 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered()"></a>takeOrdered()</h5><p>行动类算子</p>
<p>正序取索引个数</p>
<pre><code>var arr = Array(6,4,2,3,1)
res0.takeOrdered(2)
//1,2
</code></pre>
<h5 id="top"><a href="#top" class="headerlink" title="top()"></a>top()</h5><p>行动类算子</p>
<p>倒序之后的索引个数</p>
<pre><code>var arr = Array(6,4,2,3,1)
res0.top(2)
//6,4
</code></pre>
<h5 id="reduce"><a href="#reduce" class="headerlink" title="reduce()"></a>reduce()</h5><p>行动类算子</p>
<p>从左到又进行计算，然后根据传入的函数进行计算</p>
<pre><code class="scala">scala&gt; var arr = Array((&quot;zhangsan&quot;,2000),(&quot;hello&quot;,3000),(&quot;word&quot;,4000))
arr: Array[(String, Int)] = Array((zhangsan,2000), (hello,3000), (word,4000))
scala&gt; sc.makeRDD(arr,2)
res13: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:27
scala&gt; res13.reduce
def reduce(f: ((String, Int), (String, Int)) =&gt; (String, Int)): (String, Int)

scala&gt; res13.reduce((a,b)=&gt;(&quot;&quot;,a._2+b._2))
res14: (String, Int) = (&quot;&quot;,9000)
</code></pre>
<h5 id="count"><a href="#count" class="headerlink" title="count"></a>count</h5><p>行动类算子</p>
<p>把符合条件的元素进行统计计数,也能计算所有元素的个数</p>
<p>RDD带有分区的，rdd中不存在数据，所以<strong>必须将所有的元素都遍历一遍然后得出总的个数</strong></p>
<pre><code>res0.count(_._2&gt;2000)
</code></pre>
<p>把RDD的元素个数进行返回</p>
<pre><code>res0.count
//会遍历一遍所有分区的元素
</code></pre>
<h5 id="countByKey-1"><a href="#countByKey-1" class="headerlink" title="countByKey"></a>countByKey</h5><p>行动类算子</p>
<p>countByKey:按照key求出个数，</p>
<h5 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h5><p>行动类算子</p>
<h4 id="Shuffle算子"><a href="#Shuffle算子" class="headerlink" title="Shuffle算子"></a>Shuffle算子</h4><h5 id="distinct-1"><a href="#distinct-1" class="headerlink" title="distinct"></a>distinct</h5><h5 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h5><h5 id="groupByKey-1"><a href="#groupByKey-1" class="headerlink" title="groupByKey"></a>groupByKey</h5><h5 id="aggregateByKey-2"><a href="#aggregateByKey-2" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h5><h5 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h5><h5 id="SortByKey"><a href="#SortByKey" class="headerlink" title="SortByKey"></a>SortByKey</h5><h4 id="特殊算子"><a href="#特殊算子" class="headerlink" title="特殊算子"></a>特殊算子</h4><h5 id="textFile"><a href="#textFile" class="headerlink" title="textFile()"></a>textFile()</h5><p>不属于RDD的算子，是创建RDD的方法，是属于SparkContext，产生rdd的时候，<strong>它的分区数量和block的数量一样，并且分区的数量最小是2</strong></p>
<p>读取文件数据，会产生两个RDD</p>
<p>一个HadoopRDD一个mapPartitionRDD</p>
<pre><code>sc.textFile(&quot;aa.txt&quot;)
</code></pre>
<h5 id="makeRDD"><a href="#makeRDD" class="headerlink" title="makeRDD()"></a>makeRDD()</h5><p><img src="/cdh/spark/spark/image-20200726161427744.png" alt="image-20200726161427744"></p>
<p>不属于RDD的算子，是创建RDD的方法，通过集合并行化的方式（底层用的是parallelize方法）</p>
<pre><code>scala&gt; val arr = Array(1,2,3,4,5,6)
arr: Array[Int] = Array(1, 2, 3, 4, 5, 6)

scala&gt; sc.makeRDD(arr)
res2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at makeRDD at &lt;console&gt;:27
</code></pre>
<h4 id="mapPartition-foreachPartition不同点"><a href="#mapPartition-foreachPartition不同点" class="headerlink" title="mapPartition+foreachPartition不同点"></a>mapPartition+foreachPartition不同点</h4><p>他们的优点<br>map和foreach算子遍历次数是rdd中元素的个数<br>mapPartitoins每个分区遍历一次,在使用资源的情况下，每个分区使用一个资源，而map和foreach是每个元素一个资源</p>
<h4 id="groupByKey-reduceByKey-区别"><a href="#groupByKey-reduceByKey-区别" class="headerlink" title="groupByKey reduceByKey:区别"></a>groupByKey reduceByKey:区别</h4><p><strong>相同点：</strong></p>
<p>他们都应该作用在k-v的类型数据上面</p>
<p>都是根据key进行的数据聚合</p>
<p>都可以修改分区的数量，但是默认是不改变的，都会产生shuffle</p>
<p><strong>不同点：</strong></p>
<p>groupBykey就是简单的分类，不存在聚合函数</p>
<p>reduceBykey合并，带有聚合函数，并且带有combiner</p>
<h3 id="自定义排序"><a href="#自定义排序" class="headerlink" title="自定义排序"></a>自定义排序</h3><p><strong>两种方法</strong></p>
<p>1、通过继承<strong>Ordered</strong>类重写compare方法</p>
<p>2、也可以通过隐式转换来自动compare方法</p>
<p><strong>二次排序</strong></p>
<pre><code class="scala">object SecondarySort &#123;
  def main(args: Array[String]): Unit = &#123;
    val conf = new SparkConf()
    conf.setAppName(&quot;secondSort&quot;)
    conf.setMaster(&quot;local[1]&quot;)
    val sc = new SparkContext(conf)
//    var arr = Array(9,1,8,2,7,3,6,4,5)
    var arr = Array((&quot;zhangsan&quot;,90,89),(&quot;lisi&quot;,90,85),(&quot;yangkai&quot;,100,95),(&quot;changjian&quot;,90,95),(&quot;liting&quot;,97,98),(&quot;shuaishuai&quot;,97,99))
    val rdd = sc.makeRDD(arr,3)
//    val rdd1 = rdd.map(t=&gt;new Student(t._1,t._2,t._3))
    //shuffle
//    val rdd1 = rdd.sortBy(t=&gt;new Student(t._1,t._2,t._3))
//    val rdd1 = rdd.sortBy(t=&gt;(t._2,t._3))
    //证明元组可以排序，但是这个元组没有比较器的接口
    //将自己的类也变成样例类就带有了序列化的功能，写一个隐式的转换方法，可以将没有比较器的类转换为带有比较器的类

val rdd1 =  rdd.sortBy(t=&gt;Student(t._1,t._2,t._3))
    rdd1.foreach(println)
  &#125;
//隐式转换完成二次排序
  implicit def noOrdered2Ordered(s:Student):Ordered[Student]=&#123;
    new Ordered[Student] &#123;
      override def compare(that: Student): Int = &#123;
        if(that.math == s.math)&#123;
          that.chinese-s.chinese
        &#125;else&#123;
          that.math-s.math
        &#125;
      &#125;
    &#125;
  &#125;
&#125;
case class Student(val name:String,val math:Int,val chinese:Int)
</code></pre>
<h3 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h3><h4 id="为什么要持久化"><a href="#为什么要持久化" class="headerlink" title="为什么要持久化"></a>为什么要持久化</h4><p>在某种情况下，一个rdd中的数据要经过很多步骤处理才能得到的，以后大家都要使用这个数据，也就是这个rdd中的数据会被多次复用，所以我们<strong>将这个rdd中的数据缓存起来</strong>，以后使用直接从缓存中读取就可以了</p>
<h4 id="持久化的两个算子"><a href="#持久化的两个算子" class="headerlink" title="持久化的两个算子"></a>持久化的两个算子</h4><p><strong>缓存类的算子都是懒加载的，必须调用行动类算子才可以执行</strong></p>
<p><strong>cache 算子和 checkpoint 算子</strong></p>
<p><strong>他们底层都是persist算子，只是存储级别不同</strong></p>
<h4 id="cache算子说明"><a href="#cache算子说明" class="headerlink" title="cache算子说明"></a>cache算子说明</h4><p>rdd并不是存储在driver端，<strong>每个分区(task)单独存储自己的数据到executor的storage区域。内存比例查看spark的内存管理方案</strong></p>
<p>[spark内存管理方案]: Spark&#x2F;spark2.x内存管理方案pdf	“Spark&#x2F;spark2.x内存管理方案pdf”</p>
<p>，每个execuotr存在一个单独的blockManager存储自己的数据</p>
<p><strong>缓存类的算子都是懒加载的，必须调用行动类算子才可以执行</strong></p>
<h5 id="cache的使用"><a href="#cache的使用" class="headerlink" title="cache的使用"></a>cache的使用</h5><pre><code class="scala">rdd.cache()  
rdd.persist()
//持久化的两个算子
//cache底层用的persist方法
//默认的持久化级别是MEMORY_ONLY
</code></pre>
<p><img src="/cdh/spark/spark/image-20200323201828924.png" alt="image-20200323201828924"></p>
<p><strong>cache&#x3D;&#x3D;&gt;persist&#x3D;&#x3D;&gt;persist(存储级别)</strong></p>
<p>rdd如果缓存了数据，会变成绿色的，在以后使用这个rdd数据时，<strong>不会重新进行处理，依赖关系和流程都不会发生改变</strong></p>
<img src="/cdh/spark/spark/Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200323202349785.png" alt="image-20200323202349785" style="zoom:67%;">

<h5 id="去除缓存数据操作"><a href="#去除缓存数据操作" class="headerlink" title="去除缓存数据操作"></a>去除缓存数据操作</h5><p>rdd.unpersist()进行缓存数据的删除</p>
<pre><code class="scala">rdd.unpersist()
//缓存的数据是每个executor都存储自己处理的分区的数据，所以executor和缓存的数据是绑定的，application杀死，缓存的数据会随之消失
</code></pre>
<h4 id="checkpoint算子说明"><a href="#checkpoint算子说明" class="headerlink" title="checkpoint算子说明"></a>checkpoint算子说明</h4><p><strong>1、把RDD的数据以文本的形式存储到hdfs中，每个数据都是序列化以后的二进制数据，每个分区保存一份单独的数据</strong></p>
<p><strong>2、多个checkpoint可以使用同一个文件夹</strong></p>
<p><strong>3、使用checkpoint缓存，之前的依赖关系不再存在，依赖关系会发生改变</strong></p>
<p><strong>4、checkpint算子会单独启动一个线程进行处理</strong></p>
<p><strong>缓存类的算子都是懒加载的，必须调用行动类算子才可以执行</strong></p>
<h5 id="checkpoint的使用"><a href="#checkpoint的使用" class="headerlink" title="checkpoint的使用"></a>checkpoint的使用</h5><pre><code class="scala">sc.setCheckpointDir(&quot;hdfs://Linux01:9000/1902chpt&quot;)
res.checkpoint
//懒加载
//使用时必须先设置缓存路径！
</code></pre>
<h5 id="checkpoint将处理流程重新加载"><a href="#checkpoint将处理流程重新加载" class="headerlink" title="checkpoint将处理流程重新加载"></a>checkpoint将处理流程重新加载</h5><pre><code class="scala">//将checkpoint的rdd进行cache
//同一个application，checkpoint使用同一个文件夹的时候，根据应用不同生成的二级文件夹不一样
</code></pre>
<h4 id="persist算子"><a href="#persist算子" class="headerlink" title="persist算子"></a>persist算子</h4><p>cache和checkpoint都是使用的persist算子，它可以指定存储级别</p>
<h4 id="存储级别"><a href="#存储级别" class="headerlink" title="存储级别"></a>存储级别</h4><p>1.MEMORY_ONLY</p>
<p>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是<strong>默认的持久化策略</strong>，使用cache()方法时，实际就是使用的这种持久化策略。</p>
<p>2.MEMORY_AND_DISK</p>
<p>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</p>
<p>3.MEMORY_ONLY_SER</p>
<p>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</p>
<p>4.MEMORY_AND_DISK_SER</p>
<p>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</p>
<p>5.DISK_ONLY</p>
<p>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</p>
<p><img src="/cdh/spark/spark/image-20200323201714024.png" alt="image-20200323201714024"></p>
<pre><code>userDisk=存储rdd数据的时候是不是使用磁盘
userMemory:是不是使用内存
userOffHeap:是不是使用堆外内存
deserialized:是不是不序列化
replication:存储rdd的数据的副本数量是多少
</code></pre>
<h4 id="driver端的数据在executor使用的时候究竟传输多少次？"><a href="#driver端的数据在executor使用的时候究竟传输多少次？" class="headerlink" title="driver端的数据在executor使用的时候究竟传输多少次？"></a>driver端的数据在executor使用的时候究竟传输多少次？</h4><p>因为必须经过序列化和反序列化的转换</p>
<p><img src="/cdh/spark/spark/image-20200323205253359.png" alt="image-20200323205253359"></p>
<pre><code class="scala">//这样写，每个分区的每条数据都会产生一个数据
//有多少数据产生多少次数据
</code></pre>
<p><img src="/cdh/spark/spark/image-20200323204649598.png" alt="image-20200323204649598"></p>
<pre><code class="scala">//一个分区中不管有多少个数据，引用的都是同一份driver端的变量，有多少分区产生多少数据
</code></pre>
<p><img src="/cdh/spark/spark/image-20200323205616658.png" alt="image-20200323205616658"></p>
<pre><code class="scala">//使用广播变量，共享数据，一个executor中的所有分区(task)都使用这个数据
</code></pre>
<h2 id="全局累加器Accumulator"><a href="#全局累加器Accumulator" class="headerlink" title="全局累加器Accumulator"></a>全局累加器Accumulator</h2><h4 id="累加器概述"><a href="#累加器概述" class="headerlink" title="累加器概述"></a>累加器概述</h4><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/itboys/p/11056758.html">https://www.cnblogs.com/itboys/p/11056758.html</a></p>
<p>全局累加器的意思就是将所有的内容全部累加到一起<br>没有分类的概念<br><strong>主要的意义就是跨分区</strong>，所有的内容全部都会加到一起<br>主要操作的rdd的一个整体概念，每个分区执行往后的结果进行聚合，那就是最终结果</p>
<p>Spark内置了三种类型的Accumulator，分别是LongAccumulator用来累加整数型，DoubleAccumulator用来累加浮点型，CollectionAccumulator用来累加集合元素。</p>
<p>Accumulator在使用时有两个方法，已经定义好的:</p>
<pre><code class="scala">//方法已过期，需要手动定义默认值
val acc = sc.accumulator(0,&quot;count&quot;)
//方法没过期，不需要定义默认值，默认0
val acc1 = sc.longAccumulator(&quot;count_1&quot;)
</code></pre>
<h4 id="累加器的使用"><a href="#累加器的使用" class="headerlink" title="累加器的使用"></a>累加器的使用</h4><pre><code class="scala">object TestAcc &#123;
  def main(args: Array[String]): Unit = &#123;
    val arr = Array(1,2,3,4,5,6,7,8,9)
    val conf = new SparkConf()
    conf.setAppName(&quot;testAcc&quot;)
    conf.setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    //查看这个rdd中元素的个数
    val rdd = sc.makeRDD(arr,3)
//    println(rdd.count())
    //序列化问题
//    var count:Int = 0
    val acc = sc.accumulator(0,&quot;count&quot;)
    val acc1 = sc.longAccumulator(&quot;count_1&quot;)
    rdd.foreach(t=&gt;&#123;
      //这个count对象是一个executor中一个单独的对象和driver端的count不是一个但是是一个值
//      count+=1
//      println(count)
      acc.add(1)
      acc1.add(1)
    &#125;)
    println(acc.value)
    println(acc1.value)
//    println(count)
    //全局累加器，类似于全局都使用一个变量
  &#125;
&#125;
</code></pre>
<h4 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h4><p>需要继承AccumulatorV2类，重写六个方法</p>
<p>写完运行时，自定义累加器必须先注册 sc.register(acc)</p>
<pre><code>//六个方法
isZero   //判断是否为空
copy	//返回一个新的累加器
reset	//全局变空
add		//添加数据
marge	//全局合并，形成最后的结果
value	//返回哪个数据
使用累加器时需要注意只有Driver能够取到累加器的值，Task端进行的是累加操作
</code></pre>
<pre><code class="scala">class MyConcatAccumulator extends AccumulatorV2[String,Map[String,Int]]&#123;
  var map = Map[String,Int]()
  //判断初始化的值是不是为空
  override def isZero: Boolean = map.size==0
 //因为定义的是一个累加器，那么这个累加器就会被大家所是有，这个方法相当于一个
  //构造器可以放回一个累加器的对象
  override def copy(): AccumulatorV2[String, Map[String,Int]] =
    new MyConcatAccumulator

  override def reset(): Unit =&#123;
    map =  Map[String,Int]()
  &#125;

  //当前的分区中的累加器的内容，可以不断的进行累加，出现一个值就累加一次
  override def add(v: String): Unit = &#123;
    map.get(v) match&#123;
      case Some(number)=&gt; map+=(v-&gt;(number+1))
      case None=&gt; map+=(v-&gt;1)
    &#125;
  &#125;
  //合并的概念，每个分区中的累加结果进行一个整体的最终结果
  //Map[car1,12   car2,20] ++ Map[car1,21 car2,23  car4,19]
  override def merge(other: AccumulatorV2[String, Map[String,Int]]): Unit = &#123;
    val list:List[(String,Int)] =  this.map.toList ++  other.asInstanceOf[MyConcatAccumulator].map.toList
    val groupdata:Map[String,List[(String,Int)]] = list.groupBy(_._1)
    val res:Map[String,Int] = groupdata.mapValues(_.foldLeft(0)((a,b)=&gt;a+b._2))
    map = res
  &#125;
  //所有累加完毕的结果一次性返回
  override def value: Map[String,Int] = map
&#125;

//调用自定义累加器
object TestAccumulator &#123;
  def main(args: Array[String]): Unit = &#123;
      val conf = new SparkConf()
    conf.setAppName(&quot;acc&quot;)
    conf.setMaster(&quot;local[*]&quot;)
    val sc = new SparkContext(conf)
    val myacc = new MyConcatAccumulator
      //进行注册
    sc.register(myacc)
    sc.textFile(&quot;test.txt&quot;).map(_.split(&quot; &quot;)(1)).foreach(t=&gt;&#123;
      myacc.add(t)
    &#125;)
    val res = myacc.value.map(t=&gt;t._1+&quot;:&quot;+t._2).mkString(&quot;|&quot;)
    println(res)
  &#125;
&#125;
</code></pre>
<h2 id="JDBCRDD"><a href="#JDBCRDD" class="headerlink" title="JDBCRDD"></a>JDBCRDD</h2><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>用来读取mysql中的内容，跟普通RDD不同，JDBCRDD需要new</p>
<pre><code class="scala">new JdbcRDD(sc,
  ()=&gt;DriverManager.getConnection(&quot;jdbc:mysql://univers02:3306/1902_db?characterEncoding=utf-8&quot;,&quot;root&quot;,&quot;123456&quot;),
  &quot;select * from visit where id&gt;? and id&lt;?&quot;,
  1,
  5,
  2,
  t=&gt;(t.getString(&quot;address&quot;),t.getInt(&quot;count&quot;))
).foreach(println)
//1、连接地址
//2、name
//3、password
//4、sql语句
//5、
</code></pre>
<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><h3 id="什么是广播变量"><a href="#什么是广播变量" class="headerlink" title="什么是广播变量"></a>什么是广播变量</h3><p>使用SparkContext对象广播变量，是<strong>每一个executor使用一份广播数据</strong>，好处是1. 优化内容，join数据，尽量的避免join操作，使用广播变量的文件形式替换join</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><pre><code class="scala">sc.broadcast(***)
</code></pre>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>1、在driver端进行广播变量，不能再executor端</p>
<p>2、不能广播RDD</p>
<p>3、广播数据必须需要实现序列化<br>为了让每一个executor中的所有task使用同一份数据，<br>比特洪流技术，减少driver压力，不管要下载多少数据，driver导出一份，其他机器进行共享</p>
<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><pre><code class="scala"> val orderRDD = sc.makeRDD(Array(
   (1,&quot;京东&quot;,2000),
   (2,&quot;京东&quot;,120),
   (2,&quot;京东&quot;,300),
   (1,&quot;京东&quot;,1200),
   (1,&quot;京东&quot;,400),
   (3,&quot;淘宝&quot;,4100),
   (4,&quot;淘宝&quot;,200),
   (3,&quot;淘宝&quot;,900),
   (4,&quot;淘宝&quot;,100)
 )).map(t=&gt;(t._1,(t._2,t._3)))

 val userRDD = new JdbcRDD[(Int,String)](
   sc,
   ()=&gt;DriverManager.getConnection(&quot;jdbc:mysql://univers02:3306/1902_db?characterEncoding=utf-8&quot;,&quot;root&quot;,&quot;123456&quot;),
           &quot;select * from user where id&gt;=? and id&lt;=?&quot;,
   0,
   5,
           2,
           t=&gt;(t.getInt(&quot;id&quot;),t.getString(&quot;name&quot;))
 )
val map = userRDD.collect().toMap
 val bd = sc.broadcast(map)
 orderRDD.map(t=&gt;&#123;
  val name =  bd.value(t._1)
   (t._1,name,t._2)
 &#125;).foreach(println)
</code></pre>
<h2 id="Sparkstreaming消费kafka的两种方式，它们之间的区别是什么？"><a href="#Sparkstreaming消费kafka的两种方式，它们之间的区别是什么？" class="headerlink" title="Sparkstreaming消费kafka的两种方式，它们之间的区别是什么？"></a>Sparkstreaming消费kafka的两种方式，它们之间的区别是什么？</h2><h3 id="1、基于Receiver的方式"><a href="#1、基于Receiver的方式" class="headerlink" title="1、基于Receiver的方式"></a>1、基于Receiver的方式</h3><p>Receiver是使用Kafka的高层次Consumer API来实现的。receiver 从Kafka中获取的数据都是存储在Spark Executor的内存中的（如果突然数据暴增，大量batch堆积，很容易出现内存溢出的问题），然后Spark Streaming启动的job会去处理那些数据。 </p>
<p>然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的预写日志机制（Write Ahead Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。</p>
<h3 id="2、基于Direct的方式"><a href="#2、基于Direct的方式" class="headerlink" title="2、基于Direct的方式"></a>2、基于Direct的方式</h3><p>替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。</p>
<p><strong>优点如下：</strong> </p>
<p><strong>简化并行读取：</strong>如果要读取多个partition，不需要创建多个输入DStream然后对它们进行union操作。Spark会创建跟Kafka partition一样多的RDD partition，并且会并行从Kafka中读取数据。所以在Kafka partition和RDD partition之间，有一个一对一的映射关系。</p>
<p><strong>高性能：</strong>如果要保证零数据丢失，在基于receiver的方式中，需要开启WAL机制。这种方式其实效率低下，因为数据实际上被复制了两份，Kafka自己本身就有高可靠的机制，会对数据复制一份，而这里又会复制一份到WAL中。而基于direct的方式，不依赖Receiver，不需要开启WAL机制，只要Kafka中作了数据的复制，那么就可以通过Kafka的副本进行恢复。 </p>
<p><strong>一次且仅一次的事务机制</strong>。</p>
<h3 id="三、对比："><a href="#三、对比：" class="headerlink" title="三、对比："></a><strong>三、对比：</strong></h3><p>基于receiver的方式，是使用Kafka的高阶API来在<strong>ZooKeeper中保存消费过的offset的</strong>。这是消费Kafka数据的传统方式。这种方式配合着WAL机制可以保证数据零丢失的高可靠性，但是却无法保证数据被处理一次且仅一次，可能会处理两次。因为Spark和ZooKeeper之间可能是不同步的。</p>
<p>基于direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据是消费一次且仅消费一次。</p>
<p>在实际生产环境中大都用Direct方式</p>
<h4 id="Spark的bean类超过20个字段后需要继承Product接口和Serializable接口"><a href="#Spark的bean类超过20个字段后需要继承Product接口和Serializable接口" class="headerlink" title="Spark的bean类超过20个字段后需要继承Product接口和Serializable接口"></a>Spark的bean类超过20个字段后需要继承Product接口和Serializable接口</h4><p>需要重写<strong>productElement</strong>，<strong>productArity</strong>，<strong>canEqual</strong>，<strong>apply</strong>这四个方法</p>
<pre><code>productElement		角标和成员属性的映射关系
productArity		有多少个数据
canEqual			返回true就行了
</code></pre>
<h1 id="Spark调优"><a href="#Spark调优" class="headerlink" title="Spark调优"></a>Spark调优</h1><h3 id="1、资源调优"><a href="#1、资源调优" class="headerlink" title="1、资源调优"></a>1、资源调优</h3><p>1）搭建Spark集群的时候要给Spark集群足够的资源</p>
<pre><code>在spark安装包的conf下spark-env.sh

SPARK_WORKER_CORES

SPARK_WORKER_MEMORY

SPARK_WORKER_INSTANCE
</code></pre>
<p>2）在提交Application的时候给Application分配更多的资源。</p>
<pre><code>提交命令选项：（在提交Application的时候使用选项）
    --executor-cores
    --executor-memory
    --total-executor-cores
配置信息：（在Application的代码中设置
            在Spark-default.conf中设置）
    spark.executor.cores
    spark.executor.memory
    spark.max.cores
</code></pre>
<h3 id="2、并行度调优"><a href="#2、并行度调优" class="headerlink" title="2、并行度调优"></a>2、并行度调优</h3><p>原则：一个core一般分配2~3个task,每一个task一般处理1G数据</p>
<pre><code>提高并行度的方式：
1).如果读取的数据在HDFS上，降低block块的大小
2).sc.textFile(path,numPartitions) 
3)	sc.parallelize(list,numPartitions) 一般用于测试
4)	coalesce、repartition可以提高RDD的分区数。
5)	配置信息：
spark.default.parallelism  not set (默认executor core的总个数)
spark.sql.shuffle.partitions 200 
6)	自定义分区器
</code></pre>
<h3 id="3、代码调优"><a href="#3、代码调优" class="headerlink" title="3、代码调优"></a>3、代码调优</h3><h4 id="1）避免创建重复的RDD，复用同一个RDD"><a href="#1）避免创建重复的RDD，复用同一个RDD" class="headerlink" title="1）避免创建重复的RDD，复用同一个RDD"></a>1）避免创建重复的RDD，复用同一个RDD</h4><h4 id="2）对多次使用的RDD进行持久化"><a href="#2）对多次使用的RDD进行持久化" class="headerlink" title="2）对多次使用的RDD进行持久化"></a>2）对多次使用的RDD进行持久化</h4><pre><code>如何选择一种最合适的持久化策略？
默认情况下，性能最高的当然是MEMORY_ONLY，相当于调用cache算子。但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。
如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。
如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。
通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。
</code></pre>
<h6 id="持久化算子："><a href="#持久化算子：" class="headerlink" title="持久化算子："></a>持久化算子：</h6><p><strong>cache:</strong></p>
<p>  MEMORY_ONLY</p>
<p><strong>persist：</strong></p>
<p>  MEMORY_ONLY</p>
<p>  MEMORY_ONLY_SER</p>
<p>  MEMORY_AND_DISK_SER</p>
<p>  一般不要选择带有_2的持久化级别。</p>
<p><strong>checkpoint:</strong></p>
<p>①  如果一个RDD的计算时间比较长或者计算起来比较复杂，一般将这个RDD的计算结果保存到HDFS上，这样数据会更加安全。</p>
<p>②  如果一个RDD的依赖关系非常长，也会使用checkpoint,会切断依赖关系，提高容错的效率。</p>
<h4 id="3）避免使用shuffle算子"><a href="#3）避免使用shuffle算子" class="headerlink" title="3）避免使用shuffle算子"></a>3）避免使用shuffle算子</h4><p>使用广播变量来模拟使用join,使用情况：一个RDD比较大，一个RDD比较小。</p>
<p>join算子&#x3D;广播变量+filter、广播变量+map、广播变量+flatMap</p>
<h4 id="4）使用map-side预聚合的shuffle算子"><a href="#4）使用map-side预聚合的shuffle算子" class="headerlink" title="4）使用map-side预聚合的shuffle算子"></a>4）使用map-side预聚合的shuffle算子</h4><p>即尽量使用有combiner的shuffle类算子。<br>combiner概念：<br>在map端，每一个map task计算完毕后进行的局部聚合。<br>combiner好处：</p>
<ol>
<li><pre><code>降低shuffle write写磁盘的数据量。
</code></pre>
<ol start="2">
<li><pre><code>降低shuffle read拉取数据量的大小。
</code></pre>
<ol start="3">
<li><pre><code>降低reduce端聚合的次数。
</code></pre>
有combiner的shuffle类算子：<ol>
<li><pre><code>reduceByKey:这个算子在map端是有combiner的，在一些场景中可以使用reduceByKey代替groupByKey。
</code></pre>
<ol start="2">
<li><pre><code>aggregateByKey（fun1,func2）
</code></pre>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="5）尽量使用高性能的算子"><a href="#5）尽量使用高性能的算子" class="headerlink" title="5）尽量使用高性能的算子"></a>5）尽量使用高性能的算子</h4><p>使用reduceByKey替代groupByKey</p>
<p>使用mapPartition替代map</p>
<p>使用foreachPartition替代foreach</p>
<p>filter后使用coalesce减少分区数</p>
<p>使用使用repartitionAndSortWithinPartitions替代repartition与sort类操作</p>
<p>使用repartition和coalesce算子操作分区。</p>
<h4 id="6）使用广播变量"><a href="#6）使用广播变量" class="headerlink" title="6）使用广播变量"></a>6）使用广播变量</h4><p>使用广播变量可以大大降低集群中变量的副本数。不使用广播变量，变量的副本数和task数一致。使用广播变量变量的副本和Executor数一致。</p>
<p>使用广播变量可以大大的降低集群中变量的副本数。<br>不使用广播变量：变量的副本数和task数一致。<br>使用广播变量:变量的副本数与Executor数一致。<br>广播变量最大可以是多大?<br>ExecutorMemory*60%*90%*80%    &#x3D; executorMemory *0.42</p>
<h4 id="7）使用Kryo优化序列化性能"><a href="#7）使用Kryo优化序列化性能" class="headerlink" title="7）使用Kryo优化序列化性能"></a>7）使用Kryo优化序列化性能</h4><p>在Spark中，<strong>主要有三个地方涉及到了序列化：</strong><br>1)	在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输。(闭包引用)<br>2)	将自定义的类型作为RDD的泛型类型时（比如JavaRDD<SXT>，SXT是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。<br>               (New一个类)<br>3)	使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。(持久化)</SXT></p>
<h6 id="Kryo序列化器介绍："><a href="#Kryo序列化器介绍：" class="headerlink" title="Kryo序列化器介绍："></a>Kryo序列化器介绍：</h6><p>Spark支持使用Kryo序列化机制。Kryo序列化机制，比默认的Java序列化机制，速度要快，序列化后的数据要更小，大概是Java序列化机制的1&#x2F;10。所以Kryo序列化优化以后，可以让网络传输的数据变少；在集群中耗费的内存资源大大减少。<br>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream&#x2F;ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p>
<h6 id="Spark中使用Kryo："><a href="#Spark中使用Kryo：" class="headerlink" title="Spark中使用Kryo："></a>Spark中使用Kryo：</h6><pre><code class="scala">package cn.doitedu.yiee.serde

import java.util
import java.util.&#123;ArrayList, List&#125;

import org.apache.spark.SparkConf
import org.apache.spark.serializer.KryoSerializer
import org.apache.spark.sql.SparkSession

object SparkSerde &#123;

  def main(args: Array[String]): Unit = &#123;


    // spark中将对象序列化，默认调用都是jdk的objectoutputstream（serializable），效率低
    // 所以，我们在spark代码中，一般都要修改序列化器，可以用kryo序列化框架
    // kryo序列化框架的序列化结果要比jdk的序列化结果更精简（少了一些类的元信息）
    val spark1 = SparkSession.builder.config(&quot;spark.serializer&quot;,classOf[KryoSerializer].getName).appName(&quot;&quot;).master(&quot;local&quot;).getOrCreate
    import spark1.implicits._
    spark1.createDataset(Seq(new Person(&quot;zz&quot;, 1888.8, 28)));

    // 上面的做法，kryo在序列化时，还是会带上一些必要的类元信息，以便于下游task能正确反序列化

    // 下面的做法，可以提前将这些可能要被序列化的类型，注册到kryo的映射表中，这样，kryo在序列化时就不需要序列化类元信息了
    val conf = new SparkConf
    conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)
    conf.registerKryoClasses(Array(classOf[Person],classOf[Person2]))
    val spark2 = SparkSession.builder()
      .config(conf)
      .master(&quot;local&quot;)
      .appName(&quot;序列化案例&quot;)
      .getOrCreate()
  &#125;
&#125;
</code></pre>
<p><img src="/cdh/spark/spark/image-20210303165400048.png" alt="image-20210303165400048"></p>
<pre><code class="scala">Sparkconf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)
.registerKryoClasses(new Class[]&#123;SpeedSortKey.class&#125;)
</code></pre>
<h4 id="8）优化数据结构"><a href="#8）优化数据结构" class="headerlink" title="8）优化数据结构"></a>8）优化数据结构</h4><p>java中有<strong>三种类型比较消耗内存</strong>：<br>1)	<strong>对象</strong>，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。<br>2)	<strong>字符串</strong>，每个字符串内部都有一个字符数组以及长度等额外信息。<br>3)	<strong>集合类型</strong>，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。<br>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，<strong>尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</strong></p>
<h4 id="9）使用高性能的库fastutil"><a href="#9）使用高性能的库fastutil" class="headerlink" title="9）使用高性能的库fastutil"></a>9）使用高性能的库fastutil</h4><h6 id="fasteutil介绍："><a href="#fasteutil介绍：" class="headerlink" title="fasteutil介绍："></a>fasteutil介绍：</h6><p>fastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue；fastutil能够提供更小的内存占用，更快的存取速度；<strong>我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set，好处在于，fastutil集合类，可以减小内存的占用</strong>，并且在进行集合的遍历、根据索引（或者key）获取元素的值和设置元素的值的时候，提供更快的存取速度。<strong>fastutil的每一种集合类型，都实现了对应的Java中的标准接口（比如fastutil的map，实现了Java的Map接口），因此可以直接放入已有系统的任何代码中。</strong><br>fastutil最新版本要求Java 7以及以上版本。</p>
<h6 id="使用："><a href="#使用：" class="headerlink" title="使用："></a>使用：</h6><p>见RandomExtractCars.java类</p>
<h3 id="4、数据本地化"><a href="#4、数据本地化" class="headerlink" title="4、数据本地化"></a>4、数据本地化</h3><h5 id="数据本地化的级别："><a href="#数据本地化的级别：" class="headerlink" title="数据本地化的级别："></a>数据本地化的级别：</h5><h6 id="1）PROCESS-LOCAL"><a href="#1）PROCESS-LOCAL" class="headerlink" title="1）PROCESS_LOCAL"></a>1）PROCESS_LOCAL</h6><p>task要计算的数据在本进程（Executor）的内存中。</p>
<p><img src="/cdh/spark/spark/image-20200729163015599.png" alt="image-20200729163015599"></p>
<h6 id="2）NODE-LOCAL"><a href="#2）NODE-LOCAL" class="headerlink" title="2）NODE_LOCAL"></a>2）NODE_LOCAL</h6><p>task所计算的数据在本节点所在的磁盘上。</p>
<p>task所计算的数据在本节点其他Executor进程的内存中。</p>
<p><img src="/cdh/spark/spark/image-20200729163301225.png" alt="image-20200729163301225"></p>
<h6 id="3）NO-PREF"><a href="#3）NO-PREF" class="headerlink" title="3）NO_PREF"></a>3）NO_PREF</h6><p>在哪里速度都一样，公共的地方，比如mysql</p>
<p><img src="/cdh/spark/spark/image-20200729163413792.png" alt="image-20200729163413792"></p>
<h6 id="4）RACK-LOCAL"><a href="#4）RACK-LOCAL" class="headerlink" title="4）RACK_LOCAL"></a>4）RACK_LOCAL</h6><p>task所计算的数据在同机架的不同节点的磁盘或者Executor进程的内存中</p>
<p><img src="/cdh/spark/spark/image-20200729163530720.png" alt="image-20200729163530720"></p>
<h6 id="5）ANY"><a href="#5）ANY" class="headerlink" title="5）ANY"></a>5）ANY</h6><p>跨机架。</p>
<h3 id="5、SparkShuffle类型调优"><a href="#5、SparkShuffle类型调优" class="headerlink" title="5、SparkShuffle类型调优"></a>5、SparkShuffle类型调优</h3><p><img src="/cdh/spark/spark/image-20200729163942925.png" alt="image-20200729163942925"></p>
<p><strong>Spark中任务调度时，TaskScheduler在分发之前需要依据数据的位置来分发，最好将task分发到数据所在的节点上，如果TaskScheduler分发的task在默认3s依然无法执行的话，TaskScheduler会重新发送这个task到相同的Executor中去执行，会重试5次，如果依然无法执行，那么TaskScheduler会降低一级数据本地化的级别再次发送task。</strong><br><strong>如上图中，会先尝试1,PROCESS_LOCAL数据本地化级别，如果重试5次每次等待3s,会默认这个Executor计算资源满了，那么会降低一级数据本地化级别到2，NODE_LOCAL,如果还是重试5次每次等待3s还是失败，那么还是会降低一级数据本地化级别到3，RACK_LOCAL。这样数据就会有网络传输，降低了执行效率。</strong></p>
<h5 id="1-如何提高数据本地化的级别？"><a href="#1-如何提高数据本地化的级别？" class="headerlink" title="1)	如何提高数据本地化的级别？"></a>1)	如何提高数据本地化的级别？</h5><p><strong>可以增加每次发送task的等待时间（默认都是3s），将3s倍数调大，	结合WEBUI来调节：</strong><br>    • spark.locality.wait<br>    • spark.locality.wait.process<br>    • spark.locality.wait.node<br>    • spark.locality.wait.rack</p>
<p><strong>注意：等待时间不能调大很大，调整数据本地化的级别不要本末倒置，虽然每一个task的本地化级别是最高了，但整个Application的执行时间反而加长</strong>。</p>
<h5 id="2-如何查看数据本地化的级别？"><a href="#2-如何查看数据本地化的级别？" class="headerlink" title="2)    如何查看数据本地化的级别？"></a>2)    如何查看数据本地化的级别？</h5><p>通过日志或者WEBUI</p>
<h4 id="Spark的Shuffle管理类型"><a href="#Spark的Shuffle管理类型" class="headerlink" title="Spark的Shuffle管理类型"></a>Spark的Shuffle管理类型</h4><p><strong>SparkShuffle：</strong></p>
<p><strong>spark1.x 中有 两种类型的shuffle （hashShuffleManager  另外一个是sortShuffleManager）</strong></p>
<p><strong>到spark2.x以后  只有一种shuffle 机制  SortShuffle  管理器叫做SortShuffleManager</strong>  </p>
<h5 id="SparkShuffle概念"><a href="#SparkShuffle概念" class="headerlink" title="SparkShuffle概念"></a>SparkShuffle概念</h5><p>reduceByKey会将上一个RDD中的每一个key对应的所有value聚合成一个value，然后生成一个新的RDD，元素类型是&lt;key,value&gt;对的形式，这样每一个key对应一个聚合起来的value。<br>问题：聚合之前，每一个key对应的value不一定都是在一个partition中，也不太可能在同一个节点上，因为RDD是分布式的弹性的数据集，RDD的partition极有可能分布在各个节点上。</p>
<p><strong>如何聚合？</strong></p>
<p><strong>– Shuffle Write：</strong>上一个stage的每个map task就必须保证将自己处理的当前分区的数据相同的key写入一个分区文件中，可能会写入多个不同的分区文件中。</p>
<p> <strong>– Shuffle Read：</strong>reduce task就会从上一个stage的所有task所在的机器上寻找属于己的那些分区文件，这样就可以保证每一个key所对应的value都会汇聚到同一个节点上去处理和聚合。</p>
<p>Spark中有两种Shuffle管理类型，HashShufflManager和SortShuffleManager，Spark1.2之前是HashShuffleManager， Spark1.2引入SortShuffleManager,在Spark 2.0+版本中已经将HashShuffleManager丢弃。</p>
<h5 id="1、HashShuffleManager"><a href="#1、HashShuffleManager" class="headerlink" title="1、HashShuffleManager"></a>1、HashShuffleManager</h5><h6 id="1）普通机制"><a href="#1）普通机制" class="headerlink" title="1）普通机制"></a>1）普通机制</h6><p><img src="/cdh/spark/spark/image-20200729165321639.png" alt="image-20200729165321639"></p>
<p><strong>执行流程</strong><br>a)	每一个map task将不同结果写到不同的buffer中，每个buffer的大小为32K。buffer起到数据缓存的作用。<br>b)	每个buffer文件最后对应一个磁盘小文件。<br>c)	reduce task来拉取对应的磁盘小文件。</p>
<p><strong>总结</strong></p>
<p>①	.map task的计算结果会根据分区器（默认是hashPartitioner）来决定写入到哪一个磁盘小文件中去。ReduceTask会去Map端拉取相应的磁盘小文件。<br>②	.产生的磁盘小文件的个数：<br>M（map task的个数）*R（reduce task的个数）</p>
<p><strong>存在的问题</strong><br>产生的磁盘小文件过多，会导致以下问题：<br>a)	在Shuffle Write过程中会产生很多写磁盘小文件的对象。<br>b)	在Shuffle Read过程中会产生很多读取磁盘小文件的对象。<br>c)	在JVM堆内存中对象过多会造成频繁的gc,gc还无法解决运行所需要的内存 的话，就会OOM。<br>d)	在数据传输过程中会有频繁的网络通信，频繁的网络通信出现通信故障的可能性大大增加，一旦网络通信出现了故障会导致shuffle file cannot find 由于这个错误导致的task失败，TaskScheduler不负责重试，由DAGScheduler负责重试Stage。</p>
<h6 id="2）合并机制-considation机制"><a href="#2）合并机制-considation机制" class="headerlink" title="2）合并机制(considation机制)"></a>2）合并机制(considation机制)</h6><p><img src="/cdh/spark/spark/image-20200729165925527.png" alt="image-20200729165925527"></p>
<p><strong>总结</strong></p>
<p>产生磁盘小文件的个数：C(core的个数)*R（reduce的个数)</p>
<h5 id="2、-SortShuffleManager"><a href="#2、-SortShuffleManager" class="headerlink" title="2、 SortShuffleManager"></a>2、 SortShuffleManager</h5><h6 id="1）普通机制-1"><a href="#1）普通机制-1" class="headerlink" title="1）普通机制"></a>1）普通机制</h6><p><img src="/cdh/spark/spark/image-20200729170342023.png" alt="image-20200729170342023"></p>
<p><strong>执行流程</strong><br>a)	map task 的计算结果会写入到一个内存数据结构里面，内存数据结构默认是5M<br>b)	在shuffle的时候会有一个定时器，不定期的去估算这个内存结构的大小，当内存结构中的数据超过5M时，比如现在内存结构中的数据为5.01M，那么他会申请5.01*2-5&#x3D;5.02M内存给内存数据结构。<br>c)	如果申请成功不会进行溢写，如果申请不成功，这时候会发生溢写磁盘。<br>d)	在溢写之前内存结构中的数据会进行排序分区<br>e)	然后开始溢写磁盘，写磁盘是以batch的形式去写，一个batch是1万条数据，<br>f)	map task执行完成后，会将这些磁盘小文件合并成一个大的磁盘文件，同时生成一个索引文件。<br>g)	reduce task去map端拉取数据的时候，首先解析索引文件，根据索引文件再去拉取对应的数据。<br><strong>总结</strong><br>产生磁盘小文件的个数： 2*M（map task的个数）</p>
<h6 id="2）bypass机制"><a href="#2）bypass机制" class="headerlink" title="2）bypass机制"></a>2）bypass机制</h6><p><img src="/cdh/spark/spark/image-20200729170502993.png" alt="image-20200729170502993"></p>
<p><strong>总结</strong><br>①	.bypass运行机制的触发条件如下：<br>shuffle reduce task的数量小于spark.shuffle.sort.bypassMergeThreshold的参数值。这个值默认是200。<br>②	.产生的磁盘小文件为：2*M（map task的个数）</p>
<h5 id="Shuffle文件寻址"><a href="#Shuffle文件寻址" class="headerlink" title="Shuffle文件寻址"></a>Shuffle文件寻址</h5><h6 id="1-MapOutputTracker"><a href="#1-MapOutputTracker" class="headerlink" title="1)   MapOutputTracker"></a>1)   MapOutputTracker</h6><p>MapOutputTracker是Spark架构中的一个模块，是一个主从架构。管理磁盘小文件的地址。</p>
<p>MapOutputTrackerMaster是主对象，存在于Driver中。</p>
<p>MapOutputTrackerWorker是从对象，存在于Excutor中。</p>
<h6 id="2-BlockManager"><a href="#2-BlockManager" class="headerlink" title="2)	BlockManager"></a>2)	BlockManager</h6><p>BlockManager块管理者，是Spark架构中的一个模块，也是一个主从架构。<br>BlockManagerMaster,主对象，存在于Driver中。<br>BlockManagerMaster会在集群中有用到广播变量和缓存数据或者删除缓存数据的时候，通知BlockManagerSlave传输或者删除数据。<br>BlockManagerSlave，从对象，存在于Excutor中。<br>BlockManagerSlave会与BlockManagerSlave之间通信。<br>无论在Driver端的BlockManager还是在Excutor端的BlockManager都含有三个对象：<br>①	<strong>DiskStore</strong>:负责磁盘的管理。<br>②	<strong>MemoryStore</strong>：负责内存的管理。<br>③	<strong>BlockTransferService</strong>:负责数据的传输。</p>
<h3 id="6、Spark内存调优"><a href="#6、Spark内存调优" class="headerlink" title="6、Spark内存调优"></a>6、Spark内存调优</h3><h5 id="两种内存管理方案"><a href="#两种内存管理方案" class="headerlink" title="两种内存管理方案"></a>两种内存管理方案</h5><p>静态内存管理</p>
<p>动态内存管理</p>
<p>C:\Users\Administrator\Desktop\大数据总结\面试文件夹\spark2.x内存管理方案.pdf</p>
<h6 id="reduce中OOM如何处理？"><a href="#reduce中OOM如何处理？" class="headerlink" title="reduce中OOM如何处理？"></a><strong>reduce中OOM如何处理？</strong></h6><ol>
<li><pre><code>减少每次拉取的数据量
</code></pre>
<ol start="2">
<li><pre><code>提高shuffle聚合的内存比例
</code></pre>
<ol start="3">
<li><pre><code>提高Excutor的总内存
</code></pre>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h6 id="调节Executor的堆外内存"><a href="#调节Executor的堆外内存" class="headerlink" title="调节Executor的堆外内存"></a>调节Executor的堆外内存</h6><p>Spark底层shuffle的传输方式是使用netty传输，netty在进行网络传输的过程会申请堆外内存（netty是零拷贝），所以使用了堆外内存。默认情况下，这个堆外内存上限默认是每一个executor的内存大小的10%；真正处理大数据的时候，这里都会出现问题，导致spark作业反复崩溃，无法运行；此时就会去调节这个参数，到至少1G（1024M），甚至说2G、4G。</p>
<p>executor在进行shuffle write，优先从自己本地关联的mapOutPutWorker中获取某份数据，如果本地block manager没有的话，那么会通过TransferService，去远程连接其他节点上executor的block manager去获取，尝试建立远程的网络连接，并且去拉取数据。频繁创建对象让JVM堆内存满溢，进行垃圾回收。正好碰到那个exeuctor的JVM在垃圾回收。处于垃圾回过程中，所有的工作线程全部停止；相当于只要一旦进行垃圾回收，spark &#x2F; executor停止工作，无法提供响应，spark默认的网络连接的超时时长是60s；如果卡住60s都无法建立连接的话，那么这个task就失败了。<strong>task失败了就会出现shuffle file cannot find的错误。</strong></p>
<h3 id="7、数据倾斜"><a href="#7、数据倾斜" class="headerlink" title="7、数据倾斜"></a>7、数据倾斜</h3><h6 id="1）提高shuffle的read-task的并行度"><a href="#1）提高shuffle的read-task的并行度" class="headerlink" title="1）提高shuffle的read task的并行度"></a>1）提高shuffle的read task的并行度</h6><p>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p>
<p><strong>方案实现原理：</strong></p>
<p>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个不同的key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。</p>
<h6 id="2-双重聚合-双MR的道理"><a href="#2-双重聚合-双MR的道理" class="headerlink" title="2)双重聚合(双MR的道理)"></a>2)双重聚合(双MR的道理)</h6><p><strong>方案适用场景</strong>：<br>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。<br><strong>方案实现思路：</strong><br>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。<br><strong>方案实现原理：</strong><br>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。</p>
<h6 id="3-mapjoin"><a href="#3-mapjoin" class="headerlink" title="3)mapjoin"></a>3)mapjoin</h6><p><strong>方案适用场景</strong>：</p>
<p>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p>
<p><strong>方案实现思路：</strong></p>
<p>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p>
<p><strong>方案实现原理：</strong></p>
<p>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。</p>
<h6 id="4-采样倾斜key并分拆join操作"><a href="#4-采样倾斜key并分拆join操作" class="headerlink" title="4)采样倾斜key并分拆join操作"></a>4)采样倾斜key并分拆join操作</h6><p><strong>方案适用场景：</strong></p>
<p>两个RDD&#x2F;Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD&#x2F;Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD&#x2F;Hive表中的少数几个key的数据量过大，而另一个RDD&#x2F;Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p>
<p><strong>方案实现思路：</strong></p>
<p>对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。而另外两个普通的RDD就照常join即可。最后将两次join的结果使用union算子合并起来即可，就是最终的join结果</p>
<p><img src="/cdh/spark/spark/image-20200801163326016.png" alt="image-20200801163326016"></p>
<h6 id="5-一个随机前缀一个扩容RDD进行join"><a href="#5-一个随机前缀一个扩容RDD进行join" class="headerlink" title="5)一个随机前缀一个扩容RDD进行join"></a>5)一个随机前缀一个扩容RDD进行join</h6><p><strong>方案适用场景：</strong><br>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。<br><strong>方案实现思路：</strong><br>首先查看RDD&#x2F;Hive表中的数据分布情况，找到那个造成数据倾斜的RDD&#x2F;Hive表，比如有多个key都对应了超过1万条数据。然后将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。</p>
<p><img src="/cdh/spark/spark/image-20200801163730390.png" alt="image-20200801163730390"></p>
<h3 id="8、算子调优"><a href="#8、算子调优" class="headerlink" title="8、算子调优"></a>8、算子调优</h3><h5 id="1-mapPartitions算子"><a href="#1-mapPartitions算子" class="headerlink" title="1)mapPartitions算子"></a>1)mapPartitions算子</h5><p>mapPartitions算子中的数据是iterator<br>尽量使用mapPartitions而不是map算子  比如按照数据中的某个字段查询mysql中的信息，尽量使用这个算子减少connection的创建次数</p>
<p>比如批次处理数据数据量会很大，那么这个数据很可能产生内存溢出，map算子随便遍历次数比较多，但是能够一个一个处理</p>
<h5 id="2-foreachPartition算子"><a href="#2-foreachPartition算子" class="headerlink" title="2)foreachPartition算子"></a>2)foreachPartition算子</h5><p>使用场景一般都是在链接数据库并且写出数据的时候</p>
<h5 id="3-filter算子在使用的时候最好加上coalesce算子"><a href="#3-filter算子在使用的时候最好加上coalesce算子" class="headerlink" title="3)filter算子在使用的时候最好加上coalesce算子"></a>3)filter算子在使用的时候最好加上coalesce算子</h5><p>因为数据减少很多，分区中的数据不需要一个单独的线程进行处理会浪费资源，coalsec算子进行分区的合并，数据合并的时候可能出现数据倾斜<br>1.可以先找出数据的一个分部情况<br>2.如果倾斜的比例比较多 repartition使用shuffle流程操作<br>分区合并的情况<br>多数分区合并为少数分区，合并的比例比较大 coalsce算子<br>数据的量的差别比较大 shuffle–&gt;repartition<br>某一个单独的分区数据量特别大，其他的都比较小<br>扩容分区repartition</p>
<h5 id="4-sparksql的并行度设置和其他普通阶段的并行度不关联"><a href="#4-sparksql的并行度设置和其他普通阶段的并行度不关联" class="headerlink" title="4)sparksql的并行度设置和其他普通阶段的并行度不关联"></a>4)sparksql的并行度设置和其他普通阶段的并行度不关联</h5><p>SparkSQL并行度是SparkSQL的第一个调优点，默认的并行度是200，需要根据实际情况进行设置，它有有两种设置方法</p>
<pre><code>val spark = SparkSession.builder()
      .config(&quot;spark.sql.shuffle.partitions&quot;,100)//设置并行度100
      .getOrCreate()

2.提交任务时指定
--conf spark.sql.shuffle.partitions=100 \
</code></pre>
<h5 id="5-reduceByKey算子，相当于存在一个map端的reduce也就是combiner"><a href="#5-reduceByKey算子，相当于存在一个map端的reduce也就是combiner" class="headerlink" title="5)reduceByKey算子，相当于存在一个map端的reduce也就是combiner"></a>5)reduceByKey算子，相当于存在一个map端的reduce也就是combiner</h5><p>1.本地聚合后在map端的数据量减少很多<br>2.shuffle read阶段复制的数据要减少<br>3.shuffleRead端的内存使用减少<br>4.map端存在聚合所有reduce端聚合数据减少<br>groupByKey和reduceBykey的区别</p>
<h3 id="9、故障处理"><a href="#9、故障处理" class="headerlink" title="9、故障处理"></a>9、故障处理</h3><h5 id="一、shuffle-file-cannot-find：磁盘小文件找不到"><a href="#一、shuffle-file-cannot-find：磁盘小文件找不到" class="headerlink" title="一、shuffle file cannot find：磁盘小文件找不到"></a>一、shuffle file cannot find：磁盘小文件找不到</h5><h6 id="1-connection-timeout-—-shuffle-file-cannot-find"><a href="#1-connection-timeout-—-shuffle-file-cannot-find" class="headerlink" title="1)  connection timeout —-shuffle file cannot find"></a>1)  connection timeout —-shuffle file cannot find</h6><p>提高建立连接的超时时间，或者降低gc，降低gc了那么spark不能堆外提供服务的时间就少了，那么超时的可能就会降低。</p>
<h6 id="2-fetch-data-fail-—-shuffle-file-cannot-find"><a href="#2-fetch-data-fail-—-shuffle-file-cannot-find" class="headerlink" title="2)  fetch data fail —- shuffle file cannot find"></a>2)  fetch data fail —- shuffle file cannot find</h6><p>提高拉取数据的重试次数以及间隔时间。</p>
<h6 id="3-OOM-x2F-executor-lost-—-shuffle-file-cannot-find"><a href="#3-OOM-x2F-executor-lost-—-shuffle-file-cannot-find" class="headerlink" title="3)  OOM&#x2F;executor lost —- shuffle file cannot find"></a>3)  OOM&#x2F;executor lost —- shuffle file cannot find</h6><p>提高堆外内存大小，提高堆内内存大小。</p>
<h5 id="二、reduce-OOM"><a href="#二、reduce-OOM" class="headerlink" title="二、reduce OOM"></a>二、reduce OOM</h5><p>BlockManager拉取的数据量大，reduce task处理的数据量小</p>
<p>解决方法：</p>
<ol>
<li><p>降低每次拉取的数据量</p>
</li>
<li><p>提高shuffle聚合的内存比例</p>
</li>
<li><p>提高Executor的内存比例</p>
</li>
</ol>
<h5 id="三、序列化问题"><a href="#三、序列化问题" class="headerlink" title="三、序列化问题"></a>三、序列化问题</h5><h5 id="四、Null值问题"><a href="#四、Null值问题" class="headerlink" title="四、Null值问题"></a>四、Null值问题</h5><pre><code class="scala">val rdd = rdd.map&#123;x=&gt;&#123;
    x+”~”;
&#125;&#125;
rdd.foreach&#123;x=&gt;&#123;
    System.out.println(x.getName())
&#125;&#125;
</code></pre>
<h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><h3 id="1、wordcount中产生几个RDD？"><a href="#1、wordcount中产生几个RDD？" class="headerlink" title="1、wordcount中产生几个RDD？"></a>1、wordcount中产生几个RDD？</h3><p>产生6个RDD</p>
<pre><code>val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;aaa&quot;)
val sc = new SparkContext(conf)
val rdd = sc.textfile(&quot;tt.log&quot;).map(_._2,1).flatMap((a,b)=&gt;(a._1,a._2+b._2)).reducebykey()
rdd.saveTextFile()
</code></pre>
<p><img src="/cdh/spark/spark/image-20200726164017855.png" alt="image-20200726164017855"></p>
<p>textFile产生两个rdd：</p>
<p><img src="/cdh/spark/spark/image-20200726165402164.png" alt="image-20200726165402164"></p>
<p><img src="/cdh/spark/spark/image-20200726165419690.png" alt="image-20200726165419690"></p>
<p>在hadoopRDD上面去除掉key,RDD.map(_._2)</p>
<p><img src="/cdh/spark/spark/image-20200726165529217.png" alt="image-20200726165529217"></p>
<p>flatMap算子中产生一个RDD</p>
<p><img src="/cdh/spark/spark/image-20200726165545560.png" alt="image-20200726165545560"></p>
<p>map方法产生一个rdd</p>
<p><img src="/cdh/spark/spark/image-20200726165559770.png" alt="image-20200726165559770"></p>
<p>reducebykey产生shuffledRDD</p>
<p><img src="/cdh/spark/spark/image-20200726165616080.png" alt="image-20200726165616080"></p>
<p>saveAsTextFile保存数据的算子产生一个新的RDD</p>
<p><img src="/cdh/spark/spark/image-20200726165626808.png" alt="image-20200726165626808"></p>
<h3 id="2、dag在哪里构建的？"><a href="#2、dag在哪里构建的？" class="headerlink" title="2、dag在哪里构建的？"></a>2、dag在哪里构建的？</h3><p>driver</p>
<h3 id="3、rdd在哪段生成的？"><a href="#3、rdd在哪段生成的？" class="headerlink" title="3、rdd在哪段生成的？"></a>3、rdd在哪段生成的？</h3><p>drive</p>
<h3 id="4、调用rdd的算子在哪里"><a href="#4、调用rdd的算子在哪里" class="headerlink" title="4、调用rdd的算子在哪里?"></a>4、调用rdd的算子在哪里?</h3><p>driver</p>
<h3 id="5、rdd中调用的算子中的函数在哪里"><a href="#5、rdd中调用的算子中的函数在哪里" class="headerlink" title="5、rdd中调用的算子中的函数在哪里"></a>5、rdd中调用的算子中的函数在哪里</h3><p>executor</p>
<h3 id="6、dag的切分"><a href="#6、dag的切分" class="headerlink" title="6、dag的切分"></a>6、dag的切分</h3><p>dagschecutor</p>
<h3 id="7、自定义的分区器，在哪里实例化的"><a href="#7、自定义的分区器，在哪里实例化的" class="headerlink" title="7、自定义的分区器，在哪里实例化的"></a>7、自定义的分区器，在哪里实例化的</h3><p>drive</p>
<h3 id="8、简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系-（笔试重点）"><a href="#8、简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系-（笔试重点）" class="headerlink" title="8、简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系? （笔试重点）"></a>8、简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系? （笔试重点）</h3><p>1）RDD</p>
<p>优点:</p>
<p>编译时类型安全 </p>
<p>编译时就能检查出类型错误</p>
<p>面向对象的编程风格 </p>
<p>直接通过类名点的方式来操作数据</p>
<p>缺点:</p>
<p>序列化和反序列化的性能开销 </p>
<p>无论是集群间的通信, 还是IO操作都需要对对象的结构和数据进行序列化和反序列化。</p>
<p>GC的性能开销，频繁的创建和销毁对象, 势必会增加GC</p>
<p>2）DataFrame</p>
<p>DataFrame引入了schema和off-heap</p>
<p>schema : RDD每一行的数据, 结构都是一样的，这个结构就存储在schema中。 Spark通过schema就能够读懂数据, 因此在通信和IO时就只需要序列化和反序列化数据, 而结构的部分就可以省略了。</p>
<p>3）DataSet</p>
<p>DataSet结合了RDD和DataFrame的优点，并带来的一个新的概念Encoder。</p>
<p>当序列化数据时，Encoder产生字节码与off-heap进行交互，能够达到按需访问数据的效果，而不用反序列化整个对象。Spark还没有提供自定义Encoder的API，但是未来会加入。</p>
<p>DataSet &lt; Row&gt;就是DataFrame</p>
<h3 id="9、Spark消费kafka数据如何保证exactly-once语义？"><a href="#9、Spark消费kafka数据如何保证exactly-once语义？" class="headerlink" title="9、Spark消费kafka数据如何保证exactly-once语义？"></a>9、Spark消费kafka数据如何保证exactly-once语义？</h3><p>可以在处理完毕数据后在进行提交offset</p>
<p>可以保存checkpoint，再失败的时候，读取这个文件中的offset，再次进行消费</p>
<h3 id="10、spark和mr的区别"><a href="#10、spark和mr的区别" class="headerlink" title="10、spark和mr的区别"></a>10、spark和mr的区别</h3><p>spark最核心的概念是RDD（弹性分布式数据集），它的所有rdd在并行运算过程程中，可以做到数据共享，也就是可以重复使用mr在计算过程中</p>
<p>1、spark把运算的中间数据存放在内存，迭代计算效率更高，Spark中除了基于内存计算外，还有DAG有向无环图来切分任务的执行先后顺序；mapreduce的中间结果需要落地，需要保存到磁盘，这样必然会有磁盘io操做，影响性能。</p>
<p>2、spark容错性高，它通过弹性分布式数据集RDD来实现高效容错，RDD是一组分布式的存储在节点内存中的只读性质的数据集，这些集合是弹性的，某一部分丢失或者出错，可以通过整个数据集的计算流程的血缘关系来实现重建；mapreduce的话容错可能只能重新计算了，成本较高。</p>
<p> 3、spark更加通用，spark提供了transformation和action这两大类的多个功能api，另外还有流式处理sparkstreaming模块、图计算GraphX等等；mapreduce只提供了map和reduce两种操作，流计算以及其他模块的支持比较缺乏。</p>
<p> 4、spark框架和生态更为复杂，首先有RDD、血缘lineage、执行时的有向无环图DAG、stage划分等等，很多时候spark作业都需要根据不同业务场景的需要进行调优已达到性能要求；mapreduce框架及其生态相对较为简单，对性能的要求也相对较弱，但是运行较为稳定，适合长期后台运行。</p>
<h3 id="11、SparkStreaming-on-Kafka-Direct与Receiver-的对比："><a href="#11、SparkStreaming-on-Kafka-Direct与Receiver-的对比：" class="headerlink" title="11、SparkStreaming on Kafka Direct与Receiver 的对比："></a>11、SparkStreaming on Kafka Direct与Receiver 的对比：</h3><p>Receiver 是通过Kafka中高层次的消费者API连续不断地从Kafka中读取数据</p>
<p>接收到的数据被存储在Spark workers&#x2F;executors中的内存，同时也被写入到WAL中，在处理完毕后，才会发送偏移量到zookeeper中更新offset</p>
<p>接收到的数据和WAL存储位置信息被可靠地存储，如果期间出现故障，这些信息被用来从错误中恢复，并继续处理数据。 </p>
<p>这个方法可以保证从Kafka接收的数据不被丢失。但是在失败的情况下，有些数据很有<strong>可能会被处理不止一次</strong>！这种情况在一些接收到的数据被可靠地保存到WAL中，但是还没有来得及更新Zookeeper中Kafka偏移量，系统出现故障的情况下发生。这导致数据出现不一致性：Spark Streaming知道数据被接收，但是Kafka那边认为数据还没有被接收，这样在系统恢复正常时，Kafka会再一次发送这些数据。</p>
<p>Direct</p>
<p>0.10之后只有Direct没有Receiver</p>
<p>简单地给出每个batch区间需要读取的偏移量位置，最后，每个batch的Job被运行，那些对应偏移量的数据在Kafka中已经准备好了。这些偏移量信息也被可靠地存储（checkpoint），在从失败中恢复可以直接读取这些偏移量信息。</p>
<h3 id="12、spark的部署模式"><a href="#12、spark的部署模式" class="headerlink" title="12、spark的部署模式"></a>12、spark的部署模式</h3><p>1）Local:运行在一台机器上，通常是练手或者测试环境。</p>
<p>2）Standalone:构建一个基于Mster+Slaves的资源调度集群，Spark任务提交给Master运行。是Spark自身的一个调度系统。</p>
<p>3）Yarn: Spark客户端直接连接Yarn，不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。</p>
<p>4）Mesos：国内大环境比较少用。</p>
<h3 id="13、Spark的架构与作业提交流程"><a href="#13、Spark的架构与作业提交流程" class="headerlink" title="13、Spark的架构与作业提交流程"></a>13、Spark的架构与作业提交流程</h3><h3 id="14、宽窄依赖的算子每个说5个"><a href="#14、宽窄依赖的算子每个说5个" class="headerlink" title="14、宽窄依赖的算子每个说5个"></a>14、宽窄依赖的算子每个说5个</h3><p>宽依赖：groupby，groupbykey，reducebykey，take，first等等</p>
<p>窄依赖：map，mappartitoin，filter，flatmap，union</p>
<h3 id="15、Spark-Shuffle原理？"><a href="#15、Spark-Shuffle原理？" class="headerlink" title="15、Spark Shuffle原理？"></a>15、Spark Shuffle原理？</h3><p>不同RDD触发数据重新分发，map task 将数据中相同的key写入一个分区文件中(会写入多个分区文件中)，reduce task 从上一个stage阶段的task 机器上找到对应的分区文件，然后汇聚在同一个节点执行。</p>
<p>不同的管理机制看上面的<code>Spark的Shuffle管理类型</code></p>
<h3 id="16、spark-shuffle-几种？有什么区别-排序不排序有什么区别？"><a href="#16、spark-shuffle-几种？有什么区别-排序不排序有什么区别？" class="headerlink" title="16、spark shuffle 几种？有什么区别?排序不排序有什么区别？"></a>16、spark shuffle 几种？有什么区别?排序不排序有什么区别？</h3><p>三种：hash join、boastcast hashjoin、sortmerge join</p>
<h3 id="17、Shuffle产生文件个数有什么关系？倍数什么关系？"><a href="#17、Shuffle产生文件个数有什么关系？倍数什么关系？" class="headerlink" title="17、Shuffle产生文件个数有什么关系？倍数什么关系？"></a>17、Shuffle产生文件个数有什么关系？倍数什么关系？</h3><p>hashshufflemanager：Map*Reduce  个文件数</p>
<p>优化后的：                     Core*Reduce   个文件数</p>
<p>这里的2是：索引文件+磁盘文件</p>
<p>SortShuffleManager：    2*Reduce  个文件数</p>
<p>bypass：                            2*Reduce  个文件数    （他只是不需要排序了）</p>
<h3 id="18、Spark-Sql数据导入产生大量小文件？"><a href="#18、Spark-Sql数据导入产生大量小文件？" class="headerlink" title="18、Spark Sql数据导入产生大量小文件？"></a>18、Spark Sql数据导入产生大量小文件？</h3><p>对原始数据按照分区进行shuffle，但是可能数据倾斜。</p>
<p>可以distribute by 指定分发字段或者 rand() 均匀分发数据。高版本spark3.0以上可以自动合并小文件。也可以使用repartition 和其他改变分区的算子。</p>
<h3 id="19、Spark数据倾斜"><a href="#19、Spark数据倾斜" class="headerlink" title="19、Spark数据倾斜"></a>19、Spark数据倾斜</h3><p>增大分区个数，相当于增加reduce个数。</p>
<p>可以通过repartition、coalesce 等算子增加分区数。</p>
<p>自定义分区规则</p>
<p>对数据添加随机值，实现均匀分配。</p>
<p>通过distribute by 定义分发的规则。distribute by rand()</p>
<p>单个key数据过多，通过样例采集，获取倾斜的key进行单独处理。然后union 结果。</p>
<h3 id="sparksql-的执行流程和-spark的执行流程有什么不同"><a href="#sparksql-的执行流程和-spark的执行流程有什么不同" class="headerlink" title="sparksql 的执行流程和 spark的执行流程有什么不同"></a>sparksql 的执行流程和 spark的执行流程有什么不同</h3>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">三山</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://star-light-star-bright.github.io/cdh/spark/spark/">https://star-light-star-bright.github.io/cdh/spark/spark/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">三山</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/CDH/">
                                    <span class="chip bg-color">CDH</span>
                                </a>
                            
                                <a href="/tags/Spark/">
                                    <span class="chip bg-color">Spark</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/cdh/spark/sparksql/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/11.jpg" class="responsive-img" alt="SparkSql笔记">
                        
                        <span class="card-title">SparkSql笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-09-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/CDH/" class="post-category">
                                    CDH
                                </a>
                            
                            <a href="/categories/CDH/Spark/" class="post-category">
                                    Spark
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/CDH/">
                        <span class="chip bg-color">CDH</span>
                    </a>
                    
                    <a href="/tags/Spark/">
                        <span class="chip bg-color">Spark</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/scala/scala-suan-zi/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/12.jpg" class="responsive-img" alt="Scala学习笔记">
                        
                        <span class="card-title">Scala学习笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-07-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Scala/" class="post-category">
                                    Scala
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Scala/">
                        <span class="chip bg-color">Scala</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2025</span>
            
            <a href="/about" target="_blank">三山</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/blinkfox" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:1181062873@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1181062873" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1181062873" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/sakura.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
