<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ji-zhu-wen-ti-hui-zong/"/>
      <url>/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ji-zhu-wen-ti-hui-zong/</url>
      
        <content type="html"><![CDATA[<h2 id="●-缓慢变化维"><a href="#●-缓慢变化维" class="headerlink" title="● 缓慢变化维"></a>● 缓慢变化维</h2><h3 id="●-关于缓慢变化维"><a href="#●-关于缓慢变化维" class="headerlink" title="● 关于缓慢变化维"></a>● 关于缓慢变化维</h3><pre><code>什么是缓慢变化维？有哪些常见的数据案例？数据仓库与缓慢变维的关系？</code></pre><p>在从 OLTP 业务数据库向 DW 数据仓库抽取数据的过程中，特别是<strong>第一次导入之后的每一次增量抽取往往会遇到这样的问题</strong>、<strong>在新业务上会遇到占比很高的问题</strong>：业务数据库中的一些数据发生了更改，到底要不要将这些变化也反映到数据仓库中？在数据仓库中，<strong>哪些数据应该随之变化</strong>？<strong>哪些可以不用变化？</strong>考虑到这些变化，在数据仓库中的维度表又应该如何设计以满足这些需要。</p><p>在业务数据库中数据的变化是非常自然和正常的，例如：顾客的<strong>联系方式</strong>、<strong>手机号码</strong>等信息可能随着顾客的所在地的更改发生变化；<strong>商品价格</strong>在不同时期有上涨和下降的变化。</p><p>那么在业务数据库中，很自然的就会修改并马上反映到实际业务当中去。但是在<strong>数据仓库</strong>中，其<strong>数据主要的特征</strong>：<strong>①是静态历史数据</strong>，<strong>②是少改变不删除</strong>，<strong>③是定期增长</strong>，其作用主要用来<strong>数据分析</strong>。</p><p>因此分析的过程中对历史数据就提出了要求，有一些数据是需要能够反映出在周期内的变化历史，有一些数据缺不需要，那么这些数据应该如何来控制。</p><h3 id="●-案例解析"><a href="#●-案例解析" class="headerlink" title="● 案例解析"></a>● 案例解析</h3><pre><code>通过案例了解缓慢变化维</code></pre><p>假设在第一次从业务数据库中加载了一批数据到数据仓库中，当时业务数据库有这样的一条顾客的信息。</p><p><img src="https://skyline-markdown.oss-cn-shanghai.aliyuncs.com/markdownimage/image-20200910184044539.png" alt="image-20200910184044539"></p><p>顾客BIWORK，居住在北京，目前是一名BI的开发工程师。假设BIWORK因为北京空气质量 PM2.5 等原因从北京搬到了三亚。那么这条信息在业务数据库中应该被更新了。</p><p><img src="https://skyline-markdown.oss-cn-shanghai.aliyuncs.com/markdownimage/image-20200910184107053.png" alt="image-20200910184107053"></p><p>那么当下次从业务数据库中抽取这类信息的时候，数据仓库又应该如何处理呢？我们假设在数据仓库中实现了与业务数据库之间的同步，数据仓库中也直接将词条数据修改更新。后来我们创建报表做一些简单的数据统计分析，这时在数据仓库中所有对顾客 BIWORK 的销售都指向了 BIWORK 新的所在地 - 城市三亚，但是实际上 BIWORK 在之前所有的购买都发生在 BIWORK 居住在北京的时候。这是一个非常简单的例子，它描述了因一些基本信息的更改可能会引起数据归纳和分析出现的问题。但是有时，这种场景的的确确可能是存在的。</p><p>为了解决类似于这样的问题需要了解数据仓库中的一个非常重要的概念 - 缓慢渐变维度。</p><h3 id="●-解决方案"><a href="#●-解决方案" class="headerlink" title="● 解决方案"></a>● 解决方案</h3><h4 id="●-不记录历史数据"><a href="#●-不记录历史数据" class="headerlink" title="● 不记录历史数据"></a>● 不记录历史数据</h4><pre><code>解决方案说明？该解决方案适用于哪些应用场景？</code></pre><p><strong>不记录历史数据，新数据覆盖旧数据。</strong>在数据仓库中，我们可以保持业务数据和数据仓库中的数据始终处于一致。可以在 Customer 维度中使用来自业务数据库中的 Business Key - CustomerID 来追踪业务数据的变化，一旦发生变化那么就将旧的业务数据覆盖重写。DW中的记录根据业务数据库中的CustomerID 获取了最新的 City 信息，直接更新到 DW 中。针对无业务意义、无分析意义的数据可以直接进行覆盖，例如地区编码、国家名称，具体由实际业务情况决定。</p><p><img src="https://skyline-markdown.oss-cn-shanghai.aliyuncs.com/markdownimage/image-20200910185336595.png" alt="image-20200910185336595"></p><h4 id="●-保存每次变更"><a href="#●-保存每次变更" class="headerlink" title="● 保存每次变更"></a>● 保存每次变更</h4><pre><code>解决方案说明？该解决方案适用于哪些应用场景？</code></pre><p><strong>保存多条记录,直接新添一条记录，同时保留原有记录，并用单独的专用的字段保存区别。</strong>在数据仓库中更多是对相对静态的历史数据进行数据的汇总和分析，因此会<strong>尽可能的维护来自业务系统中的历史数据</strong>，能够真正捕获到这种历史数据的变化。</p><p><strong>（案例）</strong>假设 <code>BIWORK</code> 在 2012年的时候购买额度整体平稳，但是从2013年开始购买额度减少了，出现的原因可能与所在的城市有关系，在北京的门店可能比在三亚的门店相对要多一些。像这种情况，就不能很简单在数据仓库中将 BIWORK 当前所在城市直接更新，而应该新增加一条数据来说明现在 <code>BIWORK</code> 所在地是在 <code>Sanya</code>。</p><p><img src="https://skyline-markdown.oss-cn-shanghai.aliyuncs.com/markdownimage/11161730-a90e8ac2676840d8a1822761bc1e7d78.png" alt="img"></p><h5 id="●-新增代理键-DW唯一键"><a href="#●-新增代理键-DW唯一键" class="headerlink" title="● 新增代理键\DW唯一键"></a>● 新增代理键\DW唯一键</h5><p>但是如果仅仅在 DW 中新增一条新的数据仍然会出现新的问题，因为在 DW 中标识这个顾客是通过<code>CustomerID</code> 来实现的，这条 <code>CustomerID</code> 来源于业务数据库，它是唯一的。然而在 DW 中新增一条数据来保存业务数据库中历史信息，就无法保证这条数据在 DW 中的唯一性了，其它的 DW 数据表关联到这张表就无法知道应该如何引用这个 <code>Customer</code> 的信息。实际上，如果 <code>CustomerID</code> 在 DW 中也作为主键来唯一标识 <code>Customer</code> 的话，在插入新数据的时候就会发生失败。</p><p>因此我们需要继续保持 <code>Business Key</code> 业务键，因为它是关联到业务数据库的唯一纽带。做出改变的部分就是新增加一个 Key，一个数据仓库的键。在数据仓库的术语里面，这个唯一标识数据仓库表记录的键我们称之为 <code>Surrogate Key</code> 代理键，通常设置为DW表的主键。</p><p><img src="https://skyline-markdown.oss-cn-shanghai.aliyuncs.com/markdownimage/image-20200910192338378.png" alt="image-20200910192338378"></p><p>在上面这张表中，其中 <code>CustomerID - Business Key</code> 业务键，用来连接业务数据库和数据仓库的键，注意无论在业务数据库还是数据仓库无论任何时候都不应该发生改变。<code>DWID - Surrogate Key</code> 代理键，一般设置为 DW 维度表的主键，用来在数据仓库内部中的维度表和事实表建立关联。</p><h5 id="●-新增有效性标识字段"><a href="#●-新增有效性标识字段" class="headerlink" title="● 新增有效性标识字段"></a>● 新增有效性标识字段</h5><p>接着上面的表结构讲，光这样设置了新的 Surrogate Key - DWID 是不够的，因为还需要告诉数据仓库哪一条信息是现在正在使用的。当然可以根据 DWID 的顺序来查出最新的记录，但是每次都要比较 CustomerID 然后找出最大的 DWID 这样的查询比较麻烦。因此可以额外一个标志表示这条数据是最新更改的。</p><p><img src="https://skyline-markdown.oss-cn-shanghai.aliyuncs.com/markdownimage/image-20200910192434706.png" alt="image-20200910192434706"></p><p>另外的一种方式就是通过起始时间来标识，<strong>Valid To 为 NULL</strong> 的标识当前数据。</p><p><img src="https://skyline-markdown.oss-cn-shanghai.aliyuncs.com/markdownimage/image-20200911084924957.png" alt="image-20200911084924957"></p><p>当然，也有将两者都综合的。</p><p><img src="https://skyline-markdown.oss-cn-shanghai.aliyuncs.com/markdownimage/image-20200911084945336.png" alt="image-20200911084945336"></p><p>还有一种情况就是混合使用 Type 1 和 Type 2 的，比如说 Occupation 这个字段在业务数据库中发生了变化，但是可以不用维护这个历史信息，因此可能的做法是直接将最新的 Occupation 在数据仓库中覆盖掉。</p><p><img src="https://skyline-markdown.oss-cn-shanghai.aliyuncs.com/markdownimage/11162011-2bff5e451a39412e93fe461c9e0797d2.png" alt="img"></p><p>根据实际情况，还有一种做法就是全部覆盖掉。</p><p><img src="https://skyline-markdown.oss-cn-shanghai.aliyuncs.com/markdownimage/11162017-b197cc4bfd2943438fdb1845cafd0856.png" alt="img"></p><h4 id="●-保存本次和上一次数据"><a href="#●-保存本次和上一次数据" class="headerlink" title="● 保存本次和上一次数据"></a>● 保存本次和上一次数据</h4><p><strong>添加历史列，用不同的字段保存变化痕迹.它只能保存两次变化记录.适用于变化不超过两次的维度。</strong>实际上 Type 1 and 2 可以满足大多数需求了，但是仍然有其它的解决方案，比如说 Type 3 SCD。 Type 3 SCD 希望只维护更少的历史记录。比如说把要维护的历史字段新增一列，然后每次只更新 Current Column 和 Previous Column。这样，只保存了最近两次的历史记录。但是如果要维护的字段比较多，就比较麻烦，因为要更多的 Current 和 Previous 字段。所以 Type 3 SCD 用的还是没有 Type 1 和 Type 2 那么普遍。</p><p><img src="https://skyline-markdown.oss-cn-shanghai.aliyuncs.com/markdownimage/image-20200910192102887.png" alt="image-20200910192102887"></p><h3 id="●-关于代理键的说明"><a href="#●-关于代理键的说明" class="headerlink" title="● 关于代理键的说明"></a>● 关于代理键的说明</h3><p>为什么使用代理键，有什么好处？</p><ul><li>假设业务数据库来自于不同的系统，对这些数据进行整合时有可能出现相同的 <code>Business Key</code>，这时通过<code>Surrogate Key</code>就可以解决这个问题。</li><li>一般来自业务数据库中的 <code>Business Key</code> 可能字段较长，比如 GUID，长字符串标识等，使用 <code>Surrogate Key</code> 可以直接设置成整形的。事实表本身体积就很大，关联 <code>Surrogate Key</code> 与关联 <code>Business Key</code> 相比，<code>Surrogate Key</code> 效率更高，并且节省事实表体积。</li><li>最重要的一点就是上面举到的这个例子，使用 <code>Surrogate Key</code> 可以更好的解决这种缓慢渐变维度，维护历史信息记录。</li></ul><p>什么时候可以不用代理键？我觉得可以结合我们的实际业务，比如像有些业务表本身的 <code>Business Key</code> 就已经是整形的了，并且表中的属性基本上不随着时间或地理发生改变。比如像某些国家名称，地区编码等等基本上不会怎么发生改变，即使改变了也不需要维护历史记录这样的情况下可以直接使用业务数据库中的 <code>Business Key</code> 而不需要设置新的 Surrogate Key。</p><p><img src="https://skyline-markdown.oss-cn-shanghai.aliyuncs.com/markdownimage/11161918-c7e4c8e4c2514526ba2ec55ef97088e2.png" alt="img"></p><h3 id="●-问题总结"><a href="#●-问题总结" class="headerlink" title="● 问题总结"></a>● 问题总结</h3><p>问题的本身并不是在技术实现上有什么难度。最困难的地方在于，应该根据<strong>什么情况</strong>、<strong>业务场景</strong>、<strong>技术成本</strong>，来选择1还是2还是3。</p><ul><li>若遇到涉及关键数据指标的，则应尽可能保留所有记录；</li><li>遇到没有太大业务价值的，可只保留最后一次记录即可；</li><li>如果是就目前看来还没有使用到，但未来会使用到或者还不知道如何使用时，也可以先保留所有记录；</li><li>如果是频繁发生变化、可能存在分析价值、但目前尚未发现其规律性，也可以先单独找个地方存储，以后再拿出来看如何处理；</li></ul><p>缓慢变化维一般常见于数据基础层，若存在上述3类方案的混用，则针对数据使用人员可能会产生困扰。此时应完善数据字典，针对独立的业务分析场景也可以在数据主题层和数据集市层解决这项问题。 </p><pre><code class="mermaid">graph LRA[是否涉及关键数据指标\影响数据分析\是否有业务价值] --&gt; B[选取技术方案]B --&gt; C[A.不记录历史数据]B --&gt; D[B.保存本次和上一次数据]B --&gt; E[C.保存每次变更]      E --&gt; F[&quot;创建代理键(DW主键)&quot;]      E --&gt; G[创建有效性标识]            G --&gt; H[创建有效唯一标识]            G --&gt; I[创建起始日期时间戳]</code></pre><h2 id="●-数据漂移"><a href="#●-数据漂移" class="headerlink" title="● 数据漂移"></a>● 数据漂移</h2><h3 id="●-关于数据漂移"><a href="#●-关于数据漂移" class="headerlink" title="● 关于数据漂移"></a>● 关于数据漂移</h3><p>通常我们把从源系统同步进人数据仓库的第一层数据称为ODS或者staging层数据，阿里巴巴统称为ODS 。数据漂移是ODS 数据的一个顽疾，通常是指 ODS 表的同一个业务日期数据中包含前一天或后凌晨附近的数据或者丢失当天的变更数据。</p><p>由于ODS需要承接面向历史的细节数据查询需求，这就需要物理落地到数据仓库的ODS表按时间段来切分进行分区存储，通常的做法是按某些时间戳字段来切分，而实际上往往由于时间戳字段的准确性问题导致发生数据漂移。</p><h3 id="●-常见的时间戳字段"><a href="#●-常见的时间戳字段" class="headerlink" title="● 常见的时间戳字段"></a>● 常见的时间戳字段</h3><p>常见的4类时间戳字段</p><ul><li><strong>数据库表</strong>中用来标识**”数据记录更新时间”**的时间戳字段（假设这类字段叫 <code>modified time</code> ）。</li><li><strong>数据库日志</strong>中用来标识**”数据记录更新时间”**的时间戳字段·（假设这类宇段叫 <code>log_time</code> ）。</li><li><strong>数据库表</strong>中用来记录具体**”业务过程发生时间”**的时间戳字段 （假设这类字段叫 <code>proc_time</code> ）。</li><li>标识**”数据记录被抽取到时间”**的时间戳字段（假设这类字段 <code>extract time</code> ）。</li></ul><p>理论上这几个时间应该是一致的，但是在实际生产中，这几个时间往往会出现差异，可能的原因有以下几点：</p><ul><li>由于数据抽取是需要时间的 <code>extract_time</code> 往往会晚于前三个时间。</li><li>前台业务系统手工订正数据时未更新 <code>modified_time</code></li><li>由于网络或者系统压力问题，<code>log_time</code> 或者 <code>modified_time</code> 会晚 <code>proc_time</code></li></ul><h3 id="●-常见的数据漂移情况"><a href="#●-常见的数据漂移情况" class="headerlink" title="● 常见的数据漂移情况"></a>● 常见的数据漂移情况</h3><p>通常的做法是根据其中的某个字段来切分ODS表，这就导致产生数据漂移。常见的数据漂移几种场景如下：</p><ul><li>根据 <code>extract_time</code> 来获取数据。这种情况数据漂移的问题最明显</li><li>根据 <code>modified_time</code> 限制。在实际生产中这种情况最常见，但是往往会发生不更新 <code>modified time</code> 而导致的数据遗漏，或者凌晨时间产生的数据记录漂移到后天。</li><li>根据 <code>log_time</code> 限制。由于网络或者系统压力问题， <code>log_time</code> 会晚 <code>proc_time</code>，从而导致凌晨时间产生的数据记录漂移到后一天。例如，在淘宝“双 11 ”大促期间凌晨时间产生的数据量非常大，用户支付需要调用多个接口，从而导致 <code>log time</code> 晚于实际的支付时间。</li><li>根据 <code>proc_time</code> 限制。仅仅根据 <code>proc_time</code> 限制，我们所获取的ODS 表只是包含一个业务过程所产生的记 ，会遗漏很多其他过程的变化记录，这违背了 ODS 和业务系统保持一致的设计原则。</li></ul><h3 id="●-两种处理方式"><a href="#●-两种处理方式" class="headerlink" title="● 两种处理方式"></a>● 两种处理方式</h3><p>处理方法主要有以下两种：</p><ul><li><p><strong>多获取后一天的数据：</strong>既然很难解决数据漂移的问题，那么就在ODS 每个时间分区中向前、向后多冗余一些数据，保障数据只会多不会少，而具体的数据切分让下游根据自身不同的业务场景用不同的业务时间proc_time 来限制。但是这种方式会有一些数据误差，例如一个订单是当天支付的，但是第二天凌晨申请退款关闭了该订单，那么这条记录的订单状态会被更新，下游在统计支付订单状态时会出现错误。</p></li><li><p><strong>通过多个时间戳字段限制时间来获取相对准确的数据</strong></p></li><li><p>首先根据log_time 分别冗余前一天最后15 分钟的数据和后一天凌晨开始15 分钟的数据，并用modified time过滤非当天数据，确保数据不会因为系统问题而遗漏。</p></li><li><p>然后根据log_ time 获取后一天15 分钟的数据z 针对此数据，按照主键根据log_time 做升序排列去重。因为我们需要获取的是最接近当天记录变化的数据（数据库日志将保留所有变化的数据，但是落地到O DS 表的是根据主键去重获取最后状态变化的数据）。</p></li><li><p>最后将前两步的结果数据做全外连</p></li></ul><h3 id="●-淘宝交易订单案例"><a href="#●-淘宝交易订单案例" class="headerlink" title="● 淘宝交易订单案例"></a>● 淘宝交易订单案例</h3><p>下面来看处理淘宝交易订单的数据漂移的实际案例。</p><p>我们在处理“双11”交易订单时发现，有一大批在11月11日23:59:59 左右支付的交易订单漂移到了12日。主要原因是用户下单支付后系统需要调用支付宝的接口而有所延迟，从而导致这些订单最终生成的时间跨天了。即<code>modified_time</code> 和 <code>log_time</code> 都晚于 <code>proc_time</code> 。</p><p>如果订单只有一个支付业务过程，则可以用支付时间来限制就能获取到正确的数据。但是往往实际订单有多个业务过程： 下单、支付、成功，每个业务过程都有相应的时间戳字段，并不只有支付数据会漂移。</p><p>如果直接通过多获取后一天的数据，然后限制这些时间，则可以获取到相关数据，但是后一天的数据可能已经更新多次，我们直接获取到的那条记录已经是更新多次后的状态，数据的准确性存在一定的问题。因此，我们可以根据实际情况获取后一天15 分钟的数据，并限制多个业务过程的时间戳字段（下单、支付、成功）都是“双l l ”当天<br>的，然后对这些数据按照订单的 <code>mo dified_time</code> 做升序排列，获取每个订单首次数据变更的那条记录。</p><p>此外，我们可以根据 <code>log_time</code> 分别冗余前一天最后15 分钟的数据和后一天凌晨开始15 分钟的数据，并用<code>modified_time</code> 过滤非当天数据，针对每个订单按照 <code>log_time</code> 进行降序排列，取每个订单当天最后一次数据变更的那条记录。最后将两份数据根据订单做全外连接，将漂移数据回补到当天数据中。</p><h2 id="●-参考文献资料"><a href="#●-参考文献资料" class="headerlink" title="● 参考文献资料"></a>● 参考文献资料</h2><table><thead><tr><th>资料名称</th><th>涉及内容</th><th>资料来源</th></tr></thead><tbody><tr><td><font size="2">缓慢变化维常见的三种类型及原型设计</font></td><td><font size="2">缓慢变化维；</font></td><td><font size="2"><a href="https://www.cnblogs.com/xqzt/p/4472005.html">CSDN</a></font></td></tr><tr><td><font size="2">数据漂移</font></td><td><font size="2">数据漂移；</font></td><td><font size="2">大数据之路</font></td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/"/>
      <url>/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink课程第一天"><a href="#Flink课程第一天" class="headerlink" title="Flink课程第一天"></a>Flink课程第一天</h1><h2 id="1️⃣-一-、课前准备"><a href="#1️⃣-一-、课前准备" class="headerlink" title="1️⃣ 一 、课前准备"></a>1️⃣ 一 、课前准备</h2><ol><li>搭建好Hadoop集群</li></ol><h2 id="two-二-、课堂主题"><a href="#two-二-、课堂主题" class="headerlink" title=":two: 二 、课堂主题"></a>:two: 二 、课堂主题</h2><ol><li>了解Flink基本原理</li><li>掌握Flink任务如何提交到集群</li></ol><h2 id="3️⃣-三-、课程目标"><a href="#3️⃣-三-、课程目标" class="headerlink" title="3️⃣ 三 、课程目标"></a>3️⃣ 三 、课程目标</h2><ul><li><p>了解Flink的基本原理</p></li><li><p>搭建Flink on YARN集群</p></li><li><p>掌握Flink应用程序的编写</p></li></ul><h2 id="4️⃣-四-、知识要点"><a href="#4️⃣-四-、知识要点" class="headerlink" title="4️⃣ 四 、知识要点"></a>4️⃣ 四 、知识要点</h2><h3 id="📖-1-Flink简介"><a href="#📖-1-Flink简介" class="headerlink" title="📖  1.  Flink简介"></a>📖  1.  Flink简介</h3><ul><li><strong>Apache Flink® — Stateful Computations over Data Streams</strong></li><li>Apache Flink 是一个分布式<font color="red">大数据处理引擎</font>，可对<font color="red">有界数据流</font>和<font color="red">无界数据流</font>进行<font color="red">有状态</font>的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算<ul><li>官网地址：<a href="http://flink.apache.org/">http://flink.apache.org</a></li></ul></li></ul><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/flink.png" alt="flink"></p><h4 id="1-1-处理无界和有界数据"><a href="#1-1-处理无界和有界数据" class="headerlink" title="1.1 处理无界和有界数据"></a>1.1 处理无界和有界数据</h4><ul><li><p>任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。</p></li><li><p>数据可以被作为&#x3D;&#x3D;无界&#x3D;&#x3D;或者&#x3D;&#x3D;有界&#x3D;&#x3D;流来处理。</p></li></ul><pre><code>(1) 无界流     有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。    (2) 有界流     有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理</code></pre><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/bounded-unbounded.png" alt="bounded-unbounded"></p><ul><li><strong>Apache Flink 擅长处理无界和有界数据集</strong> 精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。</li></ul><h4 id="1-2-部署应用到任意地方"><a href="#1-2-部署应用到任意地方" class="headerlink" title="1.2 部署应用到任意地方"></a>1.2 部署应用到任意地方</h4><ul><li><p>Apache Flink 是一个分布式系统，它需要计算资源来执行应用程序。Flink 集成了所有常见的集群资源管理器，例如 <a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html">Hadoop YARN</a>、 <a href="https://mesos.apache.org/">Apache Mesos</a> 和 <a href="https://kubernetes.io/">Kubernetes</a>，但同时也可以作为独立集群运行。</p><pre><code>    Flink 被设计为能够很好地工作在上述每个资源管理器中，这是通过资源管理器特定(resource-manager-specific)的部署模式实现的。Flink 可以采用与当前资源管理器相适应的方式进行交互。    部署 Flink 应用程序时，Flink 会根据应用程序配置的并行性自动标识所需的资源，并从资源管理器请求这些资源。在发生故障的情况下，Flink 通过请求新资源来替换发生故障的容器。提交或控制应用程序的所有通信都是通过 REST 调用进行的，这可以简化 Flink 与各种环境中的集成</code></pre></li></ul><h4 id="1-3-运行任意规模应用"><a href="#1-3-运行任意规模应用" class="headerlink" title="1.3 运行任意规模应用"></a>1.3 运行任意规模应用</h4><ul><li><p>Flink 旨在任意规模上运行有状态流式应用。因此，应用程序被并行化为可能数千个任务，这些任务分布在集群中并发执行。所以应用程序能够充分利用无尽的 CPU、内存、磁盘和网络 IO。而且 Flink 很容易维护非常大的应用程序状态。其异步和增量的检查点算法对处理延迟产生最小的影响，同时保证精确一次状态的一致性。</p><pre><code>Flink 用户报告了其生产环境中一些令人印象深刻的扩展性数字：    每天处理数万亿的事件    可以维护几TB大小的状态    可以部署上千个节点的集群</code></pre></li></ul><h4 id="1-4-利用内存性能"><a href="#1-4-利用内存性能" class="headerlink" title="1.4 利用内存性能"></a>1.4 利用内存性能</h4><ul><li>有状态的 Flink 程序针对本地状态访问进行了优化。任务的&#x3D;&#x3D;状态始终保留在内存中&#x3D;&#x3D;，如果状态大小超过可用内存，则会保存在能高效访问的磁盘数据结构中。任务通过访问本地（通常在内存中）状态来进行所有的计算，从而产生非常低的处理延迟。Flink 通过定期和异步地对本地状态进行持久化存储来保证故障场景下精确一次的状态一致性。</li></ul><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/local-state.png" alt="local-state"></p><h3 id="📖-2-Flink-的应用场景"><a href="#📖-2-Flink-的应用场景" class="headerlink" title="📖  2. Flink 的应用场景"></a>📖  2. Flink 的应用场景</h3><pre><code>    在实际生产的过程中，大量数据在不断地产生，例如金融交易数据、互联网订单数据、 GPS 定位数据、传感器信号、移动终端产生的数据、通信信号数据等，以及我们熟悉的网络 流量监控、服务器产生的日志数据，这些数据最大的共同点就是实时从不同的数据源中产生， 然后再传输到下游的分析系统。针对这些数据类型主要包括实时智能推荐、复杂事件处理、 实时欺诈检测、实时数仓与 ETL 类型、流数据分析类型、实时报表类型等实时业务场景，而 Flink 对于这些类型的场景都有着非常好的支持。</code></pre><ul><li><p>1、实时智能推荐</p><pre><code>    智能推荐会根据用户历史的购买行为，通过推荐算法训练模型，预测用户未来可能会购 买的物品。对个人来说，推荐系统起着信息过滤的作用，对 Web/App 服务端来说，推荐系统 起着满足用户个性化需求，提升用户满意度的作用。推荐系统本身也在飞速发展，除了算法 越来越完善，对时延的要求也越来越苛刻和实时化。利用 Flink 流计算帮助用户构建更加实 时的智能推荐系统，对用户行为指标进行实时计算，对模型进行实时更新，对用户指标进行 实时预测，并将预测的信息推送给 Wep/App 端，帮助用户获取想要的商品信息，另一方面也 帮助企业提升销售额，创造更大的商业价值。</code></pre></li><li><p>2、复杂事件处理</p><pre><code>    对于复杂事件处理，比较常见的案例主要集中于工业领域，例如对车载传感器、机械设 备等实时故障检测，这些业务类型通常数据量都非常大，且对数据处理的时效性要求非常高。 通过利用 Flink 提供的 CEP（复杂事件处理）进行事件模式的抽取，同时应用 Flink 的 Sql 进行事件数据的转换，在流式系统中构建实时规则引擎，一旦事件触发报警规则，便立即将 告警结果传输至下游通知系统，从而实现对设备故障快速预警监测，车辆状态监控等目的。</code></pre></li><li><p>3、实时欺诈检测</p><pre><code>    在金融领域的业务中，常常出现各种类型的欺诈行为，例如信用卡欺诈、信贷申请欺诈等，而如何保证用户和公司的资金安全，是来近年来许多金融公司及银行共同面对的挑战。 随着不法分子欺诈手段的不断升级，传统的反欺诈手段已经不足以解决目前所面临的问题。 以往可能需要几个小时才能通过交易数据计算出用户的行为指标，然后通过规则判别出具有 欺诈行为嫌疑的用户，再进行案件调查处理，在这种情况下资金可能早已被不法分子转移， 从而给企业和用户造成大量的经济损失。而运用 Flink 流式计算技术能够在毫秒内就完成对 欺诈判断行为指标的计算，然后实时对交易流水进行规则判断或者模型预测，这样一旦检测 出交易中存在欺诈嫌疑，则直接对交易进行实时拦截，避免因为处理不及时而导致的经济损失。</code></pre></li><li><p>4、实时数仓与 ETL</p><pre><code>    结合离线数仓，通过利用流计算诸多优势和 SQL 灵活的加工能力，对流式数据进行实时 清洗、归并、结构化处理，为离线数仓进行补充和优化。另一方面结合实时数据 ETL 处理能力，利用有状态流式计算技术，可以尽可能降低企业由于在离线数据计算过程中调度逻辑的 复杂度，高效快速地处理企业需要的统计结果，帮助企业更好地应用实时数据所分析出来的结果。</code></pre></li><li><p>5、流数据分析</p><pre><code>    实时计算各类数据指标，并利用实时结果及时调整在线系统相关策略，在各类内容投放、 无线智能推送领域有大量的应用。流式计算技术将数据分析场景实时化，帮助企业做到实时化分析 Web 应用或者 App 应用的各项指标，包括 App 版本分布情况、Crash 检测和分布等， 同时提供多维度用户行为分析，支持日志自主分析，助力开发者实现基于大数据技术的精细 化运营、提升产品质量和体验、增强用户黏性。</code></pre></li><li><p>6、实时报表分析</p><pre><code>    实时报表分析是近年来很多公司采用的报表统计方案之一，其中最主要的应用便是实时 大屏展示。利用流式计算实时得出的结果直接被推送到前端应用，实时显示出重要指标的变 换情况。最典型的案例便是淘宝的双十一活动，每年双十一购物节，除疯狂购物外，最引人 注目的就是天猫双十一大屏不停跳跃的成交总额。在整个计算链路中包括从天猫交易下单购 买到数据采集、数据计算、数据校验，最终落到双十一大屏上展现的全链路时间压缩在 5 秒以内，顶峰计算性能高达数三十万笔订单/秒，通过多条链路流计算备份确保万无一失。 而在其他行业，企业也在构建自己的实时报表系统，让企业能够依托于自身的业务数据，快速提取出更多的数据价值，从而更好地服务于企业运行过程中。</code></pre></li></ul><h3 id="📖-3-Flink基本技术栈"><a href="#📖-3-Flink基本技术栈" class="headerlink" title="📖  3.  Flink基本技术栈"></a>📖  3.  Flink基本技术栈</h3><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/1563615522100.png" alt="1563615522100"></p><blockquote><p>在flink整个软件架构体系中。同样遵循着分层的架构设计理念，在降低系统耦合度的同时，也为上层用户构建flink应用提供了丰富且友好的接口。</p></blockquote><ul><li><input disabled type="checkbox"> &#x3D;&#x3D;API &amp; libraries层&#x3D;&#x3D;</li></ul><pre><code>    作为分布式数据处理框架，fink同时提供了支撑流计算和批计算的接口，同时在此基础之上抽象出不同的应用类型的组件库。如：基于流处理的CEP（复杂事件处理库）、SQL&amp;Table库、FlinkML(机器学习库)、Gelly(图处理库)有流式处理API，批处理API。流式处理的支持事件处理，表操作。批处理的，支持机器学习，图计算，也支持表操作。</code></pre><ul><li><input disabled type="checkbox"> &#x3D;&#x3D;Runtime核心层&#x3D;&#x3D;</li></ul><pre><code>    该层主要负责对上层的接口提供基础服务，也就是flink分布式计算的核心实现。flink底层的执行引擎。</code></pre><ul><li><input disabled type="checkbox"> &#x3D;&#x3D;物理部署层&#x3D;&#x3D;</li></ul><pre><code>该层主要涉及到flink的部署模式，目前flink支持多种部署模式：本地 local集群 standalone/yarn云 GCE/EC2  谷歌云、亚马逊云kubenetes</code></pre><h3 id="📖-4-Flink基本架构"><a href="#📖-4-Flink基本架构" class="headerlink" title="📖  4. Flink基本架构"></a>📖  4. Flink基本架构</h3><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/flink%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86.png" alt="flink运行原理"></p><p>​Flink 整个系统主要由两个组件组成，分别为 JobManager 和 TaskManager，Flink 架构也遵循 Master-Slave 架构设计原则，JobManager 为 Master 节点，TaskManager 为 Worker（Slave）节点。所有组件之间的通信都是借助于 Akka Framework，包括任务的状态以及 Checkpoint 触发等信息</p><ul><li><p><strong>Client</strong> </p><pre><code>    客户端负责将任务提交到集群，与 JobManager 构建 Akka 连接，然后将任务提交JobManager，通过和 JobManager 之间进行交互获取任务执行状态。客户端提交任务可以采用CLI 方式或者通过使用 Flink WebUI 提交，也可以在应用程序中指定 JobManager 的 RPC 网络端口构建 ExecutionEnvironment 提交 Flink 应用。</code></pre></li><li><p><strong>JobManager</strong> </p><pre><code>    JobManager 负责整个 Flink 集群任务的调度以及资源的管理，从客户端中获取提交的应用，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的应用分配相应的 TaskSlot 资源并命令 TaskManager 启动从客户端中获取的应用。        JobManager 相当于整个集群的 Master 节点，且整个集群有且只有一个活跃的 JobManager ，负责整个集群的任务管理和资源管理。        JobManager 和 TaskManager 之间通过 Actor System 进行通信，获取任务执行的情况并通过 Actor System 将应用的任务执行情况发送给客户端。同时在任务执行的过程中，Flink JobManager 会触发 Checkpoint 操作，每个 TaskManager 节点 收到 Checkpoint 触发指令后，完成 Checkpoint 操作，所有的 Checkpoint 协调过程都是在 Fink JobManager 中完成。        当任务完成后，Flink 会将任务执行的信息反馈给客户端，并且释放掉 TaskManager 中的资源以供下一次提交任务使用。</code></pre></li><li><p><strong>TaskManager</strong> </p><pre><code>    TaskManager 相当于整个集群的 Slave 节点，负责具体的任务执行和对应任务在每个节点上的资源申请和管理。客户端通过将编写好的 Flink 应用编译打包，提交到 JobManager，然后 JobManager 会根据已注册在 JobManager 中 TaskManager 的资源情况，将任务分配给有资源的 TaskManager节点，然后启动并运行任务。    TaskManager 从 JobManager 接收需要部署的任务，然后使用 Slot 资源启动 Task，建立数据接入的网络连接，接收数据并开始数据处理。同时 TaskManager 之间的数据交互都是通过数据流的方式进行的。    可以看出，Flink 的任务运行其实是采用多线程的方式，这和 MapReduce 多 JVM 进行的方式有很大的区别，Flink 能够极大提高 CPU 使用效率，在多个任务和 Task 之间通过 TaskSlot 方式共享系统资源，每个 TaskManager 中通过管理多个 TaskSlot 资源池进行对资源进行有效管理。</code></pre></li></ul><h3 id="📖-5-Flink的源码编译（了解）"><a href="#📖-5-Flink的源码编译（了解）" class="headerlink" title="📖  5.  Flink的源码编译（了解）"></a>📖  5.  Flink的源码编译（了解）</h3><ul><li><p>我们可以对flink的源码进行编译，方便对我们各种hadoop的版本进行适配</p></li><li><p>参见：<a href="https://blog.csdn.net/h335146502/article/details/96483310">https://blog.csdn.net/h335146502/article/details/96483310</a></p></li></ul><pre><code class="shell">cd /kkb/soft编译flink-shaded包wget  https://github.com/apache/flink-shaded/archive/release-7.0.tar.gztar -zxvf flink-shaded-release-7.0.tar.gz -C /kkb/install/cd /kkb/install/flink-shaded-release-7.0/mvn clean install -DskipTests -Dhadoop.version=2.6.0-cdh5.14.2编译flink源码wget http://archive.apache.org/dist/flink/flink-1.9.2/flink-1.9.2-src.tgztar -zxf flink-1.9.2-src.tgz -C /kkb/install/cd /kkb/install/flink-1.9.2/mvn -T2C clean install -DskipTests -Dfast -Pinclude-hadoop -Pvendor-repos -Dhadoop.version=2.6.0-cdh5.14.2</code></pre><h3 id="📖-6-Local模式安装"><a href="#📖-6-Local模式安装" class="headerlink" title="📖  6.  Local模式安装"></a>📖  6.  Local模式安装</h3><ul><li><p>1、安装jdk，配置JAVA_HOME，建议使用jdk1.8以上</p></li><li><p>2、安装包下载地址：</p><p><a href="https://mirrors.tuna.tsinghua.edu.cn/apache/flink/flink-1.9.2/flink-1.9.2-bin-scala_2.11.tgz">https://mirrors.tuna.tsinghua.edu.cn/apache/flink/flink-1.9.2/flink-1.9.2-bin-scala_2.11.tgz</a></p></li><li><p>3、直接上传安装包到服务器</p></li><li><p>4、解压安装包并配置环境变量</p><pre><code class="shell">tar -zxf flink-1.9.2-bin-scala_2.11.tgz -C /kkb/install/配置环境变量sudo vim /etc/profileexport FLINK_HOME=/kkb/install/flink-1.9.2export PATH=:$FLINK_HOME/bin:$PATH</code></pre></li><li><p>5、启动服务</p><ul><li><p>local模式，什么配置项都不需要配，直接启动服务器即可</p><pre><code class="shell">cd /kkb/install/flink-1.9.2#启动flinkbin/start-cluster.sh #停止flinkbin/stop-cluster.sh </code></pre></li></ul></li><li><p>6、Web页面浏览</p><ul><li><a href="http://node01:8081/#/overview">http://node01:8081/#/overview</a></li></ul></li></ul><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/1587549790710.png" alt="1587549790710"></p><h3 id="📖-7-Standalone模式安装"><a href="#📖-7-Standalone模式安装" class="headerlink" title="📖  7. Standalone模式安装"></a>📖  7. Standalone模式安装</h3><ul><li>（1）集群规划</li></ul><table><thead><tr><th align="left">主机名</th><th align="right">JobManager</th><th align="center">TaskManager</th></tr></thead><tbody><tr><td align="left">node01</td><td align="right">是</td><td align="center">是</td></tr><tr><td align="left">node02</td><td align="right">是</td><td align="center">是</td></tr><tr><td align="left">node03</td><td align="right"></td><td align="center">是</td></tr></tbody></table><ul><li><p>（2）依赖 </p><ul><li>jdk1.8以上，配置JAVA_HOME</li><li>主机之间免密码</li></ul></li><li><p>（3）安装步骤</p></li></ul><pre><code class="shell">node01修改以下配置文件(a) 修改conf/flink-conf.yaml#jobmanager地址jobmanager.rpc.address: node01#使用zookeeper搭建高可用high-availability: zookeeper##存储JobManager的元数据到HDFShigh-availability.storageDir: hdfs://node01:8020/flinkhigh-availability.zookeeper.quorum: node01:2181,node02:2181,node03:2181(b) 修改conf/slavesnode01node02node03(c) 修改conf/mastersnode01:8081node02:8081(d)上传flink-shaded-hadoop-2-uber-2.7.5-10.0.jar到flink的lib目录下将flink-shaded-hadoop-2-uber-2.7.5-10.0.jar 这个jar包上传到flink的安装目录的lib下(e) 拷贝到其他节点scp -r /kkb/install/flink-1.9.2 node02:/kkb/installscp -r /kkb/install/flink-1.9.2 node03:/kkb/install(f)：node01(JobMananger)节点启动注意：启动之前先启动hadoop和zookeeper集群cd /kkb/install/flink-1.9.2bin/start-cluster.sh(g)：访问http://node01:8081http://node02:8081(h)：关闭flink集群, 在主节点上执行cd /kkb/install/flink-1.9.2bin/stop-cluster.sh</code></pre><ul><li>(4) StandAlone模式需要考虑的参数</li></ul><pre><code>jobmanager.heap.mb：     jobmanager节点可用的内存大小taskmanager.heap.mb：    taskmanager节点可用的内存大小taskmanager.numberOfTaskSlots：   每台taskmanager节点可用的cpu数量parallelism.default：             默认情况下任务的并行度taskmanager.tmp.dirs：            taskmanager的临时数据存储目录</code></pre><h3 id="📖-8-Flink-on-Yarn模式安装"><a href="#📖-8-Flink-on-Yarn模式安装" class="headerlink" title="📖  8. Flink on Yarn模式安装"></a>📖  8. Flink on Yarn模式安装</h3><ol><li>首先安装好Hadoop（yarn）</li><li>上传一个flink的包，配置好hadoop的环境变量就可以了</li></ol><ul><li>flink on yarn有两种方式</li></ul><h4 id="8-1-第一种方式"><a href="#8-1-第一种方式" class="headerlink" title="8.1 第一种方式"></a>8.1 第一种方式</h4><ul><li><p>内存集中管理模式（&#x3D;&#x3D;Yarn Session&#x3D;&#x3D;）</p><ul><li>在Yarn中初始化一个Flink集群，开辟指定的资源，之后我们提交的Flink Jon都在这个Flink yarn-session中，也就是说不管提交多少个job，这些job都会共用开始时在yarn中申请的资源。这个Flink集群会常驻在Yarn集群中，除非手动停止。</li></ul></li></ul><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/yarn-session.png" alt="1587522616288"></p><h4 id="8-2-第二种方式"><a href="#8-2-第二种方式" class="headerlink" title="8.2 第二种方式"></a>8.2 第二种方式</h4><ul><li>内存Job管理模式&#x3D;&#x3D;【yarn-cluster 推荐使用】&#x3D;&#x3D;<ul><li>在Yarn中，每次提交job都会创建一个新的Flink集群，任务之间相互独立，互不影响并且方便管理。任务执行完成之后创建的集群也会消失。</li></ul></li></ul><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/yarn-cluster.png" alt="1587522665015"></p><h4 id="8-3-不同模式的任务提交"><a href="#8-3-不同模式的任务提交" class="headerlink" title="8.3 不同模式的任务提交"></a>8.3 不同模式的任务提交</h4><ul><li><p>第一种模式</p><ul><li><p>【yarn-session.sh(开辟资源) + flink run(提交任务)】</p><ul><li><p>1、在flink目录启动yarn-session</p><pre><code class="shell">bin/yarn-session.sh -n 2 -tm 1024 -s 1 -d# -n 表示申请2个容器，# -s 表示每个容器启动多少个slot# -tm 表示每个TaskManager申请1024M内存# -d 表示以后台程序方式运行</code></pre></li><li><p>2、使用 flink 脚本提交任务</p><pre><code class="shell">bin/flink run examples/batch/WordCount.jar \-input  hdfs://node01:8020/words.txt \-output hdfs://node01:8020/output/result.txt##说明：如果启动了很多的yarn-session, 在提交任务的时候可以通过参数 -yid 指定作业提交到哪一个yarn-session中运行##例如：bin/flink run \-yid application_1597295374041_0008 \examples/batch/WordCount.jar \-input  hdfs://node01:8020/words.txt \-output hdfs://node01:8020/output/result.txt</code></pre><ul><li>3、停止任务</li></ul></li></ul><pre><code class="shell">yarn application -kill application_1587024622720_0001</code></pre></li></ul></li><li><p>第二种模式</p><ul><li><p>【flink run -m yarn-cluster (开辟资源+提交任务)】</p><ul><li><p>1、启动集群，执行任务</p><pre><code class="shell">bin/flink run -m yarn-cluster -yn 2 -yjm 1024 -ytm 1024 \examples/batch/WordCount.jar \-input hdfs://node01:8020/words.txt \-output hdfs://node01:8020/output1注意：client端必须要设置YARN_CONF_DIR或者HADOOP_CONF_DIR或者HADOOP_HOME环境变量，通过这个环境变量来读取YARN和HDFS的配置信息，否则启动会失败。</code></pre></li></ul></li></ul></li><li><p>help信息</p><ul><li><p>yarn-session.sh 脚本参数</p><pre><code class="shell">用法:   必选     -n,--container &lt;arg&gt;   分配多少个yarn容器 (=taskmanager的数量)   可选     -D &lt;arg&gt;                        动态属性     -d,--detached                   独立运行     -jm,--jobManagerMemory &lt;arg&gt;    JobManager的内存 [in MB]     -nm,--name                     在YARN上为一个自定义的应用设置一个名字     -q,--query                      显示yarn中可用的资源 (内存, cpu核数)     -qu,--queue &lt;arg&gt;               指定YARN队列.     -s,--slots &lt;arg&gt;                每个TaskManager使用的slots数量     -tm,--taskManagerMemory &lt;arg&gt;   每个TaskManager的内存 [in MB]     -z,--zookeeperNamespace &lt;arg&gt;   针对HA模式在zookeeper上创建NameSpace    -id,--applicationId &lt;yarnAppId&gt; YARN集群上的任务id，附着到一个后台运行的yarn    session中</code></pre></li><li><p>flink run  脚本参数</p><pre><code class="shell">run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt;   &quot;run&quot; 操作参数:  -c,--class &lt;classname&gt;  如果没有在jar包中指定入口类，则需要在这里通过这个参数指定  -m,--jobmanager &lt;host:port&gt;  指定需要连接的jobmanager(主节点)地址，使用这个参数可以指定一个不同于配置文件中的jobmanager  -p,--parallelism &lt;parallelism&gt;   指定程序的并行度。可以覆盖配置文件中的默认值。默认查找当前yarn集群中已有的yarn-session信息中的jobmanager【/tmp/.yarn-properties-root】：./bin/flink run ./examples/batch/WordCount.jar -input hdfs://hostname:port/hello.txt -output hdfs://hostname:port/result1连接指定host和port的jobmanager：./bin/flink run -m node01:6123 ./examples/batch/WordCount.jar -input hdfs://hostname:port/hello.txt -output hdfs://hostname:port/result1启动一个新的yarn-session：./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar -input hdfs://hostname:port/hello.txt -output hdfs://hostname:port/result1注意：yarn session命令行的选项也可以使用./bin/flink 工具获得。它们都有一个y或者yarn的前缀例如：flink run -m yarn-cluster -yn 2 examples/batch/WordCount.jar </code></pre></li></ul></li></ul><h4 id="8-4-Flink-on-YARN集群部署"><a href="#8-4-Flink-on-YARN集群部署" class="headerlink" title="8.4 Flink on YARN集群部署"></a>8.4 Flink on YARN集群部署</h4><ul><li>(1) flink on yarn运行原理</li></ul><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/flink%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86.png" alt="flink运行原理"></p><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/FlinkOnYarn.png" alt="FlinkOnYarn"></p><ul><li>其实Flink on YARN部署很简单，就是只要部署好hadoop集群即可，我们只需要部署一个Flink客户端，然后从flink客户端提交Flink任务即可。类似于spark on yarn模式。</li></ul><h3 id="📖-9-入门案例演示"><a href="#📖-9-入门案例演示" class="headerlink" title="📖  9.  入门案例演示"></a>📖  9.  入门案例演示</h3><h4 id="9-1-实时需求分析"><a href="#9-1-实时需求分析" class="headerlink" title="9.1 实时需求分析"></a>9.1 实时需求分析</h4><pre><code>实时统计每隔1秒统计最近2秒单词出现的次数</code></pre><ul><li>创建maven工程，添加pom依赖</li></ul><pre><code class="xml">   &lt;properties&gt;          &lt;flink.version&gt;1.9.2&lt;/flink.version&gt;          &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;      &lt;/properties&gt;        &lt;dependencies&gt;          &lt;dependency&gt;              &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;              &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;              &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;          &lt;/dependency&gt;          &lt;dependency&gt;              &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;              &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt;              &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;          &lt;/dependency&gt;          &lt;dependency&gt;              &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;              &lt;artifactId&gt;scala-library&lt;/artifactId&gt;              &lt;version&gt;2.11.8&lt;/version&gt;          &lt;/dependency&gt;      &lt;/dependencies&gt;      &lt;build&gt;          &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;          &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;          &lt;plugins&gt;              &lt;plugin&gt;                  &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;                  &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;                  &lt;version&gt;3.2.2&lt;/version&gt;                  &lt;executions&gt;                      &lt;execution&gt;                          &lt;goals&gt;                              &lt;goal&gt;compile&lt;/goal&gt;                              &lt;goal&gt;testCompile&lt;/goal&gt;                          &lt;/goals&gt;                          &lt;configuration&gt;                              &lt;args&gt;                                  &lt;arg&gt;-dependencyfile&lt;/arg&gt;                                  &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt;                              &lt;/args&gt;                          &lt;/configuration&gt;                      &lt;/execution&gt;                  &lt;/executions&gt;              &lt;/plugin&gt;              &lt;plugin&gt;                  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                  &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;                  &lt;version&gt;2.4.3&lt;/version&gt;                  &lt;executions&gt;                      &lt;execution&gt;                          &lt;phase&gt;package&lt;/phase&gt;                          &lt;goals&gt;                              &lt;goal&gt;shade&lt;/goal&gt;                          &lt;/goals&gt;                          &lt;configuration&gt;                              &lt;filters&gt;                                  &lt;filter&gt;                                      &lt;artifact&gt;*:*&lt;/artifact&gt;                                      &lt;excludes&gt;                                          &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;                                          &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;                                          &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;                                      &lt;/excludes&gt;                                  &lt;/filter&gt;                              &lt;/filters&gt;                              &lt;transformers&gt;                                  &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;                                      &lt;mainClass&gt;&lt;/mainClass&gt;                                  &lt;/transformer&gt;                              &lt;/transformers&gt;                          &lt;/configuration&gt;                      &lt;/execution&gt;                  &lt;/executions&gt;              &lt;/plugin&gt;          &lt;/plugins&gt;      &lt;/build&gt;          </code></pre><h5 id="9-1-1-实时代码开发（scala版本）"><a href="#9-1-1-实时代码开发（scala版本）" class="headerlink" title="9.1.1 实时代码开发（scala版本）"></a>9.1.1 实时代码开发（scala版本）</h5><ul><li><p>代码开发</p><pre><code class="scala">package com.kaikeba.demo1import org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment&#125;import org.apache.flink.streaming.api.windowing.time.Time/**  * 使用滑动窗口  * 每隔1秒钟统计最近2秒钟的每个单词出现的次数  */object FlinkStream &#123;  def main(args: Array[String]): Unit = &#123;      //构建流处理的环境        val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment     //从socket获取数据       val sourceStream: DataStream[String] = env.socketTextStream(&quot;node01&quot;,9999)     //导入隐式转换的包      import org.apache.flink.api.scala._     //对数据进行处理     val result: DataStream[(String, Int)] = sourceStream          .flatMap(x =&gt; x.split(&quot; &quot;)) //按照空格切分          .map(x =&gt; (x, 1))           //每个单词计为1          .keyBy(0)                   //按照下标为0的单词进行分组                .timeWindow(Time.seconds(2),Time.seconds(1)) //每隔1s处理2s的数据          .sum(1)            //按照下标为1累加相同单词出现的次数        //对数据进行打印        result.print()        //开启任务         env.execute(&quot;FlinkStream&quot;)      &#125;&#125;</code></pre></li><li><p>发送 socket 数据</p><pre><code class="shell">##在node01上安装nc服务sudo yum -y install ncnc -lk 9999</code></pre></li><li><p>打成jar包提交到yarn中运行</p><pre><code class="shell">flink run -m yarn-cluster -yn 2 -yjm 1024 -ytm 1024 -c com.kaikeba.demo1.FlinkStream original-flink_study-1.0-SNAPSHOT.jar </code></pre></li></ul><h5 id="9-1-2-实时代码开发（java版本）"><a href="#9-1-2-实时代码开发（java版本）" class="headerlink" title="9.1.2 实时代码开发（java版本）"></a>9.1.2 实时代码开发（java版本）</h5><ul><li><p>代码开发</p><pre><code class="java">package com.kaikeba.demo1;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.streaming.api.datastream.*;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;/** * java代码开发实时统计每隔1秒统计最近2秒单词出现的次数 */public class WindowWordCountJava &#123;    public static void main(String[] args) throws Exception &#123;        //步骤一：获取流式处理环境        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        //步骤二：获取socket数据        DataStreamSource&lt;String&gt; sourceDstream = env.socketTextStream(&quot;node01&quot;, 9999);        //步骤三：对数据进行处理        DataStream&lt;WordCount&gt; wordAndOneStream = sourceDstream.flatMap(new FlatMapFunction&lt;String, WordCount&gt;() &#123;            public void flatMap(String line, Collector&lt;WordCount&gt; collector) throws Exception &#123;                String[] words = line.split(&quot; &quot;);                for (String word : words) &#123;                    collector.collect(new WordCount(word, 1L));                &#125;            &#125;        &#125;);DataStream&lt;WordCount&gt; resultStream = wordAndOneStream      .keyBy(&quot;word&quot;)  //按照单词分组      .timeWindow(Time.seconds(2), Time.seconds(1)) //每隔1s统计2s的数据      .sum(&quot;count&quot;);   //按照count字段累加结果  //步骤四：结果打印  resultStream.print();   //步骤五：任务启动  env.execute(&quot;WindowWordCountJava&quot;);    &#125;    public static class WordCount&#123;        public String word;        public long count;        //记得要有这个空构建        public WordCount()&#123;        &#125;        public WordCount(String word,long count)&#123;            this.word = word;            this.count = count;        &#125;        @Override        public String toString() &#123;            return &quot;WordCount&#123;&quot; +                    &quot;word=&#39;&quot; + word + &#39;\&#39;&#39; +                    &quot;, count=&quot; + count +                    &#39;&#125;&#39;;        &#125;        &#125;&#125;</code></pre></li><li><p>发送socket数据</p><pre><code class="shell">#在node01上执行命令，发送数据nc -lk 9999</code></pre></li></ul><h4 id="9-2-离线需求分析"><a href="#9-2-离线需求分析" class="headerlink" title="9.2 离线需求分析"></a>9.2 离线需求分析</h4><pre><code>对文件进行单词计数，统计文件当中每个单词出现的次数。</code></pre><h5 id="9-2-1-离线代码开发（scala）"><a href="#9-2-1-离线代码开发（scala）" class="headerlink" title="9.2.1 离线代码开发（scala）"></a>9.2.1 离线代码开发（scala）</h5><pre><code class="scala">package com.kaikeba.demo1import org.apache.flink.api.scala.&#123; DataSet, ExecutionEnvironment&#125;/**  * scala开发flink的批处理程序  */object FlinkFileCount &#123;  def main(args: Array[String]): Unit = &#123;     //todo:1、构建Flink的批处理环境    val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment    //todo:2、读取数据文件    val fileDataSet: DataSet[String] = env.readTextFile(&quot;d:\\words.txt&quot;)     import org.apache.flink.api.scala._    //todo: 3、对数据进行处理      val resultDataSet: AggregateDataSet[(String, Int)] = fileDataSet                                                .flatMap(x=&gt; x.split(&quot; &quot;))                                                .map(x=&gt;(x,1))                                                .groupBy(0)                                                .sum(1)    //todo: 4、打印结果        resultDataSet.print()    //todo: 5、保存结果到文件       resultDataSet.writeAsText(&quot;d:\\result&quot;)       env.execute(&quot;FlinkFileCount&quot;)  &#125;&#125;</code></pre><h3 id="📖-10-Flink并行度-amp-Slot-amp-Task"><a href="#📖-10-Flink并行度-amp-Slot-amp-Task" class="headerlink" title="📖  10. Flink并行度&amp;Slot&amp;Task"></a>📖  10. Flink并行度&amp;Slot&amp;Task</h3><pre><code>    Flink的每个TaskManager为集群提供solt。每个task slot代表了TaskManager的一个固定大小的资源子集。 solt的数量通常与每个TaskManager节点的可用CPU内核数成比例。一般情况下你的slot数是你每个节点的cpu的核数。</code></pre><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/1587211958715.png" alt="1587211958715"></p><h4 id="10-1-并行度"><a href="#10-1-并行度" class="headerlink" title="10.1 并行度"></a>10.1 并行度</h4><p>一个Flink程序由多个任务组成(source、transformation和 sink)。 一个任务由多个并行的实例(线程)来执行， 一个任务的并行实例 (线程) 数目就被称为该任务的并行度。</p><h4 id="10-2-并行度的设置"><a href="#10-2-并行度的设置" class="headerlink" title="10.2  并行度的设置"></a>10.2  并行度的设置</h4><ul><li>一个任务的并行度设置可以从多个级别指定<ul><li>Operator Level（算子级别）</li><li>Execution Environment Level（执行环境级别）</li><li>Client Level（客户端级别）</li><li>System Level（系统级别）</li></ul></li><li>这些并行度的优先级为 <ul><li>Operator Level &gt; Execution Environment Level &gt; Client Level &gt; System Level</li></ul></li></ul><h5 id="10-2-1-算子级别"><a href="#10-2-1-算子级别" class="headerlink" title="10.2.1 算子级别"></a>10.2.1 算子级别</h5><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/1574472860477.png" alt="1574472860477"></p><h5 id="10-2-2-执行环境级别"><a href="#10-2-2-执行环境级别" class="headerlink" title="10.2.2 执行环境级别"></a>10.2.2 执行环境级别</h5><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/1574472880358.png" alt="1574472880358"></p><p>​</p><h5 id="10-2-3-客户端级别"><a href="#10-2-3-客户端级别" class="headerlink" title="10.2.3 客户端级别"></a>10.2.3 客户端级别</h5><ul><li><p>并行度可以在客户端将job提交到Flink时设定，对于CLI客户端，可以通过-p参数指定并行度</p><pre><code class="shell">bin/flink run -p 10 examples/batch/WordCount.jar</code></pre></li></ul><h5 id="10-2-4-系统级别"><a href="#10-2-4-系统级别" class="headerlink" title="10.2.4 系统级别"></a>10.2.4 系统级别</h5><ul><li><p>在系统级可以通过设置&#x3D;&#x3D;flink-conf.yaml&#x3D;&#x3D;文件中的&#x3D;&#x3D;parallelism.default&#x3D;&#x3D;属性来指定所有执行环境的默认并行度</p><pre><code class="yaml">parallelism.default: 1</code></pre></li></ul><h4 id="10-3-并行度操作演示"><a href="#10-3-并行度操作演示" class="headerlink" title="10.3 并行度操作演示"></a>10.3 并行度操作演示</h4><ul><li><p>为了方便在本地测试观察任务并行度信息，可以在本地工程添加以下依赖</p><pre><code class="xml">&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-runtime-web_2.11&lt;/artifactId&gt;    &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt;</code></pre></li><li><p>案例</p><ul><li>注意获取程序的执行环境发生变化了</li><li>&#x3D;&#x3D;val environment&#x3D;StreamExecutionEnvironment.createLocalEnvironmentWithWebUI()&#x3D;&#x3D;</li></ul><pre><code class="scala">package com.kaikeba.demo1import org.apache.flink.core.fs.FileSystem.WriteModeimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment&#125;import org.apache.flink.streaming.api.windowing.time.Time/**  * 本地调试并行度  */object TestParallelism &#123;  def main(args: Array[String]): Unit = &#123;     //使用createLocalEnvironmentWithWebUI方法，构建本地流式处理环境    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI()     //执行环境级别    //environment.setParallelism(4)    import org.apache.flink.api.scala._    //接受socket数据    val sourceStream: DataStream[String] = environment.socketTextStream(&quot;node01&quot;,9999)    val countStream: DataStream[(String, Int)] = sourceStream                        .flatMap(x =&gt; x.split(&quot; &quot;)).setParallelism(5) //算子级别                        .map(x =&gt; (x, 1))                        .keyBy(0)                        .timeWindow(Time.seconds(2), Time.seconds(1))                        .sum(1)    countStream.print()    environment.execute()  &#125;&#125;</code></pre><ul><li>设置并行度，观察localhost:8081界面</li></ul><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/1587363866623.png" alt="1587363866623"></p><p><img src="/si-fang-cai-zi-liao-zheng-li/flink/shen-ru-qian-chu-flink-di-yi-tian/1587363947147.png" alt="1587363947147"></p></li></ul><h2 id="five-五、总结"><a href="#five-五、总结" class="headerlink" title=":five: 五、总结"></a>:five: 五、总结</h2><ul><li><p>掌握Flink的编程规范</p></li><li><p>了解Flink的集群模式</p></li></ul><h2 id="six-六-、作业"><a href="#six-六-、作业" class="headerlink" title=":six: 六 、作业"></a>:six: 六 、作业</h2><ol><li>实现上课中的例子</li><li>搭建Flink on YARN集群</li><li>往集群上面提交任务</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/shu-ju-cang-ku/yong-hu-hua-xiang/"/>
      <url>/shu-ju-cang-ku/yong-hu-hua-xiang/</url>
      
        <content type="html"><![CDATA[<h1 id="用户画像"><a href="#用户画像" class="headerlink" title="用户画像"></a>用户画像</h1><p>数据仓库：报表分析侠义，</p><p>olap平台</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Mysql锁</title>
      <link href="/mysql/mysql-suo/"/>
      <url>/mysql/mysql-suo/</url>
      
        <content type="html"><![CDATA[<h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><h2 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>锁是计算机协调多个进程或线程并发访问某个资源的机制。在数据库中，除传统的计算资源（CPU、RAM、I&#x2F;O）争用以外，数据也是一种供许多用户共享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题。锁冲突也是影响数据库并发访问性能的一个重要因素。从这个角度来说，所对数据库很重要，也更加复杂。</p><h3 id="全局锁"><a href="#全局锁" class="headerlink" title="全局锁"></a>全局锁</h3><h4 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h4><p>全局锁是对整个数据库实例加锁，加锁后整个实例处于只读状态，后续的DML的语句、DDL语句、已经更新操作的事务提交语句都会被阻塞，DQL可执行。</p><p>典型的使用场景就是全库的逻辑备份，对所有的表进行锁定，获取一致性视图，保证数据完整性。</p><h4 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h4><p>添加全局锁</p><pre><code class="sql">flush tables with read lock</code></pre><p>解锁</p><pre><code class="sql">unlock tables; </code></pre><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><p>数据库中全局锁，存在以下问题：</p><p>1.做了主从复制，读写分离。在主库上备份，那么备份期间都不能执行更新，业务完全停止。</p><p>2.在从库上备份，备份期间从库不能同步主库的二进制文件(binlog)，会导致主从延迟。</p><h3 id="表级锁"><a href="#表级锁" class="headerlink" title="表级锁"></a>表级锁</h3><h4 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h4><p>表级锁，每次操作锁住整张表。</p><p>力度大，发生锁冲突的概率最高，并发度最低。</p><p>在MyISAM、InnoDB、BDB等存储引擎都支持。</p><p><strong>为分三类：</strong></p><h4 id="1、表锁"><a href="#1、表锁" class="headerlink" title="1、表锁"></a>1、表锁</h4><h5 id="表共享读锁（read-lock）"><a href="#表共享读锁（read-lock）" class="headerlink" title="表共享读锁（read lock）"></a>表共享读锁（read lock）</h5><h6 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h6><pre><code class="sql">#加锁local tables 表名 read/write。unlock tables | 客户端断开连接。</code></pre><h6 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h6><p>读锁会阻塞<strong>所有客户端</strong>（包含自己）的写入操作，不会阻塞读操作。</p><p><img src="/mysql/mysql-suo/image-20220214223848443.png" alt="image-20220214223848443"></p><p><img src="/mysql/mysql-suo/image-20220214223439681.png" alt="image-20220214223439681"></p><h5 id="表独占写锁（write-lock）"><a href="#表独占写锁（write-lock）" class="headerlink" title="表独占写锁（write lock）"></a>表独占写锁（write lock）</h5><h6 id="语法-2"><a href="#语法-2" class="headerlink" title="语法"></a>语法</h6><pre><code class="sql">#加锁local tables 表名 write/read。unlock tables | 客户端断开连接。</code></pre><h6 id="特点-2"><a href="#特点-2" class="headerlink" title="特点"></a>特点</h6><p>写锁会阻塞其他会话的查询和写入操作，只能当前会话进行查询和写入</p><p><img src="/mysql/mysql-suo/image-20220214223913772.png" alt="image-20220214223913772"></p><p><img src="/mysql/mysql-suo/image-20220214224213299.png" alt="image-20220214224213299"></p><h4 id="2、元数据锁（MDL）"><a href="#2、元数据锁（MDL）" class="headerlink" title="2、元数据锁（MDL）"></a>2、元数据锁（MDL）</h4><h5 id="介绍-3"><a href="#介绍-3" class="headerlink" title="介绍"></a>介绍</h5><p>meta data lock ,MDL。MDL加锁过程是系统自动的，不需要显示控制，在访问一张表的时候会自动加上。MDL锁主要作用是维护表的元数据一致性，在表上有活动事物的时候，不可以对元数据进行写入操作。为了避免DML和DDL冲突，保证读写正确性。</p><p>对表CRUD时，加上读锁（共享）。但表结构变更操作时，加MDL写锁（排他）。</p><table><thead><tr><th>SQL</th><th>锁类型</th><th>说明</th></tr></thead><tbody><tr><td>lock tables xxx read|write</td><td>SHARED_READ_ONLY&#x2F;SHARED_NO_READ_WRITE</td><td></td></tr><tr><td>select、select… lock in share mode</td><td>SHARED_READ</td><td>与SHARED_READ、SHARED_WRITE兼容，与EXCLUSIVE互斥</td></tr><tr><td>insert、update、delete、select…for update</td><td>SHARED_WRITE</td><td>与SHARED_READ、SHARED_WRITE兼容，与EXCLUSIVE互斥</td></tr><tr><td>alter atble</td><td>EXCLUSIVE</td><td>与其他的MDL都互斥</td></tr></tbody></table><p><img src="/mysql/mysql-suo/image-20220214224911347.png" alt="image-20220214224911347"></p><p><img src="/mysql/mysql-suo/image-20220214225824363.png" alt="image-20220214225824363"></p><h5 id="查看元数据锁"><a href="#查看元数据锁" class="headerlink" title="查看元数据锁"></a>查看元数据锁</h5><pre><code class="sql">select object_type,object_schema,object_name,lock_type,lock_duration from performance_schema.metadata_locks;</code></pre><p><img src="/mysql/mysql-suo/image-20220214230149906.png" alt="image-20220214230149906"></p><h4 id="3、意向锁"><a href="#3、意向锁" class="headerlink" title="3、意向锁"></a>3、意向锁</h4><h5 id="介绍-4"><a href="#介绍-4" class="headerlink" title="介绍"></a>介绍</h5><p>为了避免DML在执行中，加的行锁与表锁冲突，在InnoDB中引入了意向锁，表锁不必检查每一行的数据是否加锁，使用意向锁减少了表锁的检查。</p><p><img src="/mysql/mysql-suo/image-20220214232416428.png" alt="image-20220214232416428"></p><p>线程A添加行锁后，同时对表添加意向锁。线程B检索时只需要检索表的意向锁即可，有意向锁，就等待。</p><p>等待线程A释放行锁后，意向锁一同释放，然后线程B添加表锁</p><p><img src="/mysql/mysql-suo/image-20220214232710108.png" alt="image-20220214232710108"></p><h5 id="意向共享锁（IS）"><a href="#意向共享锁（IS）" class="headerlink" title="意向共享锁（IS）"></a>意向共享锁（IS）</h5><p>与表锁共享锁(read)兼容，与表锁排他锁(write)互斥。</p><h5 id="意向排他锁（IX）"><a href="#意向排他锁（IX）" class="headerlink" title="意向排他锁（IX）"></a>意向排他锁（IX）</h5><p>与表锁共享锁(read)及排他锁(write)都互斥。意向锁之间不会互斥。</p><h5 id="查看元数据锁情况"><a href="#查看元数据锁情况" class="headerlink" title="查看元数据锁情况"></a>查看元数据锁情况</h5><pre><code class="sql">select object_schema,object_name,index_name,lock_type,lock_mode,lock_data from performance_schema.data_locks;</code></pre><h3 id="行级锁"><a href="#行级锁" class="headerlink" title="行级锁"></a>行级锁</h3><h4 id="介绍-5"><a href="#介绍-5" class="headerlink" title="介绍"></a>介绍</h4><p>行级锁，每次操作锁住对应的行数据。锁定力度小，发生锁冲突的概率很低，并发度最高。应用在InnoDB存储引擎中。</p><p>InnoDB数据是基于索引组织的，行锁是通过<strong>对索引项加锁</strong>实现的，而不是对记录枷锁。</p><p>行级锁分类：</p><h4 id="1、行锁-Record-Lock"><a href="#1、行锁-Record-Lock" class="headerlink" title="1、行锁(Record Lock)"></a>1、行锁(Record Lock)</h4><p>锁定单个行记录的锁，防止其他事务对此行进行update和delete。在RC（read commit），RR（Replace read）隔离级别都支持。</p><p><img src="/mysql/mysql-suo/image-20220215103548589.png" alt="image-20220215103548589"></p><p>默认情况下，InnoDB在REPEATABLE READ事务隔离级别运行，InnoDB使用next-key 锁进行搜索和索引扫描，以防止幻读。</p><ol><li>针对唯一索引进行检索时，对已存在的记录进行等值匹配等，将会自动优化为行锁。</li><li>InnoDB的行锁是针对于索引加的锁，不通过索引条件检索数据，那么InnoDB将对表中的所有记录加索，此时会<strong>升级为表锁</strong>。</li></ol><h5 id="共享锁（S）"><a href="#共享锁（S）" class="headerlink" title="共享锁（S）"></a>共享锁（S）</h5><p>允许一个事务去读一行，阻止其他事物获得相同数据集的排他锁。</p><h5 id="排他锁（X）"><a href="#排他锁（X）" class="headerlink" title="排他锁（X）"></a>排他锁（X）</h5><p>允许获取排他锁的事务更新数据，组织其他事务获得相同数据集的共享锁和排他锁。</p><p><img src="/mysql/mysql-suo/image-20220215103935033.png" alt="image-20220215103935033"></p><h5 id="SQL对应的锁"><a href="#SQL对应的锁" class="headerlink" title="SQL对应的锁"></a>SQL对应的锁</h5><p><img src="/mysql/mysql-suo/image-20220215104719979.png" alt="image-20220215104719979"></p><h5 id="查看意向锁及行锁的加锁情况"><a href="#查看意向锁及行锁的加锁情况" class="headerlink" title="查看意向锁及行锁的加锁情况"></a>查看意向锁及行锁的加锁情况</h5><pre><code class="sql">select object_schema,object_name,index_name,lock_type,lock_mode,lock_data from performance_schema.data_locks;</code></pre><h4 id="2、间隙锁（Gap-Lock）"><a href="#2、间隙锁（Gap-Lock）" class="headerlink" title="2、间隙锁（Gap Lock）"></a>2、间隙锁（Gap Lock）</h4><p>锁定索引记录间隙（不含该记录），确保索引记录间隙不变，防止其他事务在这个间隙进行insert，产生幻读。在RR隔离级别下都支持。</p><p>默认情况下，InnoDB在 REPEATABLE READ事务隔离级别运行，InnoDB使用next-key锁进行搜索和索引扫描，以防止幻读。</p><p><img src="/mysql/mysql-suo/image-20220215103247197.png" alt="image-20220215103247197"></p><p>注意：间隙锁唯一目的是防止其他事务插入间隙。间隙锁可以共存，一个事务采用的间隙锁不会阻止另一个事务在同一间隙上采用间隙锁。</p><p>1.索引上的等值查询(唯一索引)，给<strong>不存在的纪录</strong>加锁时，优化为间隙锁。</p><h5 id="实例1"><a href="#实例1" class="headerlink" title="实例1"></a>实例1</h5><p>下面的实例就锁住了3-8之间的间隙。不包含3和8.</p><p><img src="/mysql/mysql-suo/image-20220215111147983.png" alt="image-20220215111147983"></p><p><img src="/mysql/mysql-suo/image-20220215111249260.png" alt="image-20220215111249260"></p><p>这里锁住了3-8范围内的行，会话1没有提交事务时，其他会话就无法在这个区间插入数据</p><p><img src="/mysql/mysql-suo/image-20220215111442240.png" alt="image-20220215111442240"></p><p>2.索引上的等值查询（普通查询），向右遍历时最后一个值不满足查询需求时，next-key lock 退化为间隙锁。</p><h5 id="实例2"><a href="#实例2" class="headerlink" title="实例2"></a>实例2</h5><p>创建了非唯一索引，检索数据时会一直获取数据，直到下一个值不否和条件，给当前key(3)添加共享锁的同时，给3-7添加间隙锁，3之前的间隙也锁住。</p><p><img src="/mysql/mysql-suo/image-20220215113009458.png" alt="image-20220215113009458"></p><p><img src="/mysql/mysql-suo/image-20220215113022919.png" alt="image-20220215113022919"></p><p>3.索引上的范围查询（唯一索引），会访问到不满足条件的第一个值为止</p><h5 id="实例3"><a href="#实例3" class="headerlink" title="实例3"></a>实例3</h5><p>范围查找锁住19和25，并把19-25锁了间隙锁，还锁了正无穷大到25的间隙锁。</p><p><img src="/mysql/mysql-suo/image-20220215114440518.png" alt="image-20220215114440518"></p><p><img src="/mysql/mysql-suo/image-20220215114307672.png" alt="image-20220215114307672"></p><p><img src="/mysql/mysql-suo/image-20220215114722617.png" alt="image-20220215114722617"></p><h4 id="3、临键锁（Next-Key-Lock）"><a href="#3、临键锁（Next-Key-Lock）" class="headerlink" title="3、临键锁（Next-Key Lock）"></a>3、临键锁（Next-Key Lock）</h4><p>行锁和间隙锁组合，同时锁住数据，并锁住数据前面的间隙Gap。在RR隔离级别下支持。</p><p><img src="/mysql/mysql-suo/image-20220215103459913.png" alt="image-20220215103459913"></p>]]></content>
      
      
      <categories>
          
          <category> Mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/linux/shell-jiao-ben/jian-kong-flume-zhuang-tai-yi-chang-zhi-hou-zi-dong-chong-qi/"/>
      <url>/linux/shell-jiao-ben/jian-kong-flume-zhuang-tai-yi-chang-zhi-hou-zi-dong-chong-qi/</url>
      
        <content type="html"><![CDATA[<p>#!&#x2F;bin&#x2F;bash</p><p>export FLUME_HOME&#x3D;&#x2F;opt&#x2F;apps&#x2F;flume-1.9.0<br>while true<br>do<br>pc&#x3D;<code>ps -ef | grep flume | grep -v &quot;grep&quot; | wc -l</code></p><p>if [[ $pc -lt 1 ]]<br>then<br>echo “detected no flume process…. preparing to launch flume agent…… “<br>${FLUME_HOME}&#x2F;bin&#x2F;flume-ng agent -n a1 -c ${FLUME_HOME}&#x2F;conf&#x2F; -f ${FLUME_HOME}&#x2F;agentconf&#x2F;failover.properties 1&gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 &amp;<br>else<br>echo “detected flume process number is : $pc “<br>fi</p><p>sleep 1</p><p>done</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Shell脚本编写</title>
      <link href="/linux/shell-jiao-ben/centos-bian-xie-shell-jiao-ben/"/>
      <url>/linux/shell-jiao-ben/centos-bian-xie-shell-jiao-ben/</url>
      
        <content type="html"><![CDATA[<h1 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h1><p>shell是用户和内核进行交互操作的一种接口</p><p>shell  适合处理文本 不适合做数字运算</p><p>是与linux系统交互命令集合</p><p><strong>原文链接</strong></p><p>部分转载自：<a href="https://www.cnblogs.com/jacktian-it/p/11556826.html">https://www.cnblogs.com/jacktian-it/p/11556826.html</a></p><h2 id="命令的语法"><a href="#命令的语法" class="headerlink" title="$命令的语法"></a>$命令的语法</h2><p>$0 shell命令本身，当前脚本名称</p><p>$1-$9表示shell的第几个参数</p><p>$？显示最后命令的执行情况</p><p>$#传递到脚本的参数个数</p><p>$$脚本运行的当前进程id号</p><p>$*和$@ 单字符串显示所有向脚本传递的参数列表</p><p>$!后台运行的最后一个进程的 ID 号   </p><p>$-显示 Shell 使用的当前选项 </p><p>$?      使用<code>$?</code>获取上一条命令的返回值</p><p>&#x2F;dev&#x2F;nulllinux特殊的目录，黑洞，不存任何东西</p><h2 id="引用变量"><a href="#引用变量" class="headerlink" title="引用变量"></a>引用变量</h2><p>引用变量时，使用<code>$</code>符号直接来进行引用，以及包括循环变量；</p><pre><code class="shell">[root@localhost ~]# x=1024[root@localhost ~]# echo &quot;x = $x&quot;x = 1024</code></pre><p>使用 ${ } 作为单词边界。</p><pre><code class="shell">[root@localhost ~]# x=1024[root@localhost ~]# echo &quot;x = $&#123;x&#125;xy&quot;x = 1024xy</code></pre><p>使用 &#96;$</p>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
            <tag> Centos </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ELK的安装</title>
      <link href="/elk/elasticsearch7.10-an-zhuang/"/>
      <url>/elk/elasticsearch7.10-an-zhuang/</url>
      
        <content type="html"><![CDATA[<h2 id="下载es的包"><a href="#下载es的包" class="headerlink" title="下载es的包"></a>下载es的包</h2><p>官网地址：<a href="https://www.elastic.co/cn/downloads/elasticsearch">https://www.elastic.co/cn/downloads/elasticsearch</a></p><h2 id="解压es包"><a href="#解压es包" class="headerlink" title="解压es包"></a>解压es包</h2><pre><code class="shell">tar -zxvf elasticsearch-7.10.0-linux-x86_64.tar.gz</code></pre><h2 id="配置文件修改"><a href="#配置文件修改" class="headerlink" title="配置文件修改"></a>配置文件修改</h2><p>进入解压完毕后的包，查看目录，进入到config目录下，修改<code>elasticsearch.yml</code>文件<br><img src="https://img-blog.csdnimg.cn/20201130142309621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/20201130142429381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="启动es"><a href="#启动es" class="headerlink" title="启动es"></a>启动es</h2><p>按照上图修改完毕后，<strong>切换到非root用户</strong>，然后启动es</p><pre><code>bin/elasticsearch命令行启动bin/elasticsearch -d后台启动</code></pre><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>可以访问ip:9200<br>查看返回值，返回下方返回值就正确</p><pre><code>&#123;    &quot;name&quot;: &quot;node-1&quot;,    &quot;cluster_name&quot;: &quot;my-application&quot;,    &quot;cluster_uuid&quot;: &quot;9R_huW9yRcmGpH-fIcioAQ&quot;,    &quot;version&quot;: &#123;        &quot;number&quot;: &quot;7.10.0&quot;,        &quot;build_flavor&quot;: &quot;default&quot;,        &quot;build_type&quot;: &quot;tar&quot;,        &quot;build_hash&quot;: &quot;51e9d6f22758d0374a0f3f5c6e8f3a7997850f96&quot;,        &quot;build_date&quot;: &quot;2020-11-09T21:30:33.964949Z&quot;,        &quot;build_snapshot&quot;: false,        &quot;lucene_version&quot;: &quot;8.7.0&quot;,        &quot;minimum_wire_compatibility_version&quot;: &quot;6.8.0&quot;,        &quot;minimum_index_compatibility_version&quot;: &quot;6.0.0-beta1&quot;    &#125;,    &quot;tagline&quot;: &quot;You Know, for Search&quot;&#125;</code></pre><h2 id="关闭es"><a href="#关闭es" class="headerlink" title="关闭es"></a>关闭es</h2><pre><code>命令行启动的  退出时就关闭了后台启动的可以查询进程  来关闭进程ps -ef | grep elasticsearchkill -9 </code></pre>]]></content>
      
      
      <categories>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ELK </tag>
            
            <tag> ES </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/cdh/spark/sparksteam/"/>
      <url>/cdh/spark/sparksteam/</url>
      
        <content type="html"><![CDATA[<h1 id="Spark-Steaming"><a href="#Spark-Steaming" class="headerlink" title="Spark Steaming"></a>Spark Steaming</h1><p>Spark Streaming 是一个基于 Spark Core 之上的实时计算框架，可以从很多数 据源消费数据并对数据进行实时的处理，具有高吞吐量和容错能力强等特点</p><p><img src="/cdh/spark/sparksteam/image-20220207190223160.png" alt="image-20220207190223160"></p><p>Spark Streaming 的特点：</p><ol><li>易用 可以像编写离线批处理一样去编写流式程序，支持 java&#x2F;scala&#x2F;python 语言。</li><li>容错 SparkStreaming 在没有额外代码和配置的情况下可以恢复丢失的工作。</li><li>易整合到 Spark 体系 流式处理与批处理和交互式查询相结合。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CDH5.16的搭建</title>
      <link href="/cdh/centos7-chi-xian-an-zhuang-cdh5.16-ji-qun/"/>
      <url>/cdh/centos7-chi-xian-an-zhuang-cdh5.16-ji-qun/</url>
      
        <content type="html"><![CDATA[<p>手里有三台服务器，每台16内存，磁盘大小100G，搭建CDH集群<br>版本如下：</p><blockquote><p>Centos7.6.1810<br>Mysql5.6.50<br>Cloudera-manager5.16.2</p></blockquote><p>本文参考了网络上的CDH安装的文章<br><a href="https://blog.csdn.net/czz1141979570/article/details/86658416">https://blog.csdn.net/czz1141979570/article/details/86658416)</a><br><a href="https://blog.csdn.net/weixin_40079336/article/details/86648553">https://blog.csdn.net/weixin_40079336&#x2F;article&#x2F;details&#x2F;86648553</a><br><a href="https://blog.csdn.net/u012448904/article/details/103702433">https://blog.csdn.net/u012448904/article/details/103702433</a><br><a href="https://yq.aliyun.com/articles/341408">https://yq.aliyun.com/articles/341408</a><br>以及自己搭建时遇到的一些问题，进行说明</p><h1 id="搭建CDH过程："><a href="#搭建CDH过程：" class="headerlink" title="搭建CDH过程："></a>搭建CDH过程：</h1><blockquote><p>这里前提是默认已经配置好centos的ip，关闭防火墙，ssh免疫，ntp同步，hosts映射和连接外网等一些配置了<br>因为服务器上的是最小化安装，什么都没带，自己还得安装很多东西，个人安装centos时不推荐最小化安装</p></blockquote><p>安装使用的root用户来安装</p><h2 id="1、-安装JDK"><a href="#1、-安装JDK" class="headerlink" title="1、 安装JDK"></a>1、 安装JDK</h2><p>安装jdk1.8以上版本，安装的目录<strong>必须</strong>要在&#x2F;usr&#x2F;java下面，因为CDH默认会从这里找<br>第一次安装时我安装在&#x2F;opt下面，CDH会报错 检测不到jdk</p><p>查看机器是否自带jdk，进行卸载</p><pre><code>1.创建这个文件夹，存放jdkmkdir -p /usr/java/jdk1.8tar -zxvf jdk-8u161-linux-x64.tar.gz -C  /usr/java/必须存放在这个目录，否则后期机器默认找不到jdk，还得手动指定，挺麻烦的</code></pre><pre><code>1.配置环境变量    vim /etc/profile    2./etc/profile中添加下面配置项    export JAVA_HOME=/usr/java/jdk1.8    export CLASSPATH=.:$&#123;JAVA_HOME&#125;/jre/lib/rt.jar:$&#123;JAVA_HOME&#125;/lib/dt.jar:$&#123;JAVA_HOME&#125;/lib/tools.jar    export PATH=$PATH:$&#123;JAVA_HOME&#125;/bin3.刷新环境变量使配置立即生效    source /etc/profile    4.检查JDK是否配置成功    java -version</code></pre><h2 id="2、安装Mysql"><a href="#2、安装Mysql" class="headerlink" title="2、安装Mysql"></a>2、安装Mysql</h2><p>先安装Mysql，CHD集群的初始化数据和后续的组件(hive，oozie，cm的监控器和hue)的数据都会存放到Mysql中</p><pre><code>1、先查看linux是否带有mariadbrpm -qa|grep -i mariadb2、 卸载mariadbrpm -e --nodeps mariadb-libs3、如果之前安装过mysql，需要卸载干净rpm -qa |grep -i mysqlyum remove  **********find / -name mysqlrm -rf rm -rf /etc/my.cnfrm -rf /var/log/mysqld.log4、安装mysqlyum install mysqlyum install mysql-serveryum install mysql-devel这里安装mysql-server时可能会报错，这里我安装的是Mysql5.6.5，可以通过select version();查看可以参考下面https://www.cnblogs.com/yowamushi/p/8043054.html安装完成后启动mysql服务，首次没有密码service mysql startmysql##注意点，网上查找的mysql赋值语句可能因为Mysql版本的问题导致不支持。5、//首先更改root的密码update user set Password = password(&#39;123456&#39;) where user=&#39;root&#39;;   //查看用户密码和权限select host, user, authentication_string, plugin from user;    //刷新权限，下次登陆就使用&#39;123456&#39;flush privileges;6、开启远程登录grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;123456&#39; with grant option;FLUSH PRIVILEGES;//grant 赋值的权限(这里是全部特权) on 授权的数据库 to 在任何主机上登录 密码为 &#39;123456&#39; 查看当前数据库的用户配置：7、授权root用户在主节点拥有所有数据库的访问权限grant all privileges on *.* to &#39;root&#39;@&#39;hadoop1&#39; identified by &#39;123456&#39; with grant option;//这里是赋值了root本地登录权限8、创建数据库#hivecreate database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci; #activity monitorcreate database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci; #huecreate database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci; #ooziecreate database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</code></pre><p>如果Mysql因为权限配置问题导致无法登录问题，处理方法：</p><pre><code>//关闭Mysql服务systemctl stop mysqld//修改my.cnf文件vim /etc/my.cnf//新增内容skip-grant-tables #添加这句话，这时候登入mysql就不需要密码symbolic-links=0//然后再开启Mysql服务，这时候登陆就不需要密码了修改完数据库之后，再重新启动Mysql服务service mysqld start # 启动mysql服务</code></pre><h2 id="3、安装依赖-所有机器"><a href="#3、安装依赖-所有机器" class="headerlink" title="3、安装依赖(所有机器)"></a>3、安装依赖(所有机器)</h2><p>CDH安装需要python的依赖，我没有安装，之后报错了，需要提前安装</p><pre><code class="yum">yum -y install chkconfig python bind-utils psmisc libxslt zlib sqlite cyrus-sasl-plain cyrus-sasl-gssapi fuse fuse-libs redhat-lsb httpd mod_ssl</code></pre><h2 id="4、CM下载"><a href="#4、CM下载" class="headerlink" title="4、CM下载"></a>4、CM下载</h2><p>  CM下载地址：<a href="http://archive.cloudera.com/cm5/cm/5/">http://archive.cloudera.com/cm5/cm/5/</a><br>  <img src="/cdh/centos7-chi-xian-an-zhuang-cdh5.16-ji-qun/20201103105948599.png" alt="CM下载界面"><br>  下载完毕后上传，解压到&#x2F;opt下面，解压后会生成两个文件夹cloudera和cm-5.16.2</p><p> 作者：码游菌 <a href="https://www.bilibili.com/read/cv12633102">https://www.bilibili.com/read/cv12633102</a> 出处：bilibili</p><pre><code>tar -zxvf /opt/software/cloudera-manager-centos7-cm5.16.2_x86_64.tar.gz  -C /opt</code></pre><h2 id="5、添加系统用户-所有机器"><a href="#5、添加系统用户-所有机器" class="headerlink" title="5、添加系统用户(所有机器)"></a>5、添加系统用户(所有机器)</h2><p><code>cloudera-scm</code>用户是CM的配置文件中的系统默认用户，新建这个名称的用户就不用修改CM的配置文件了，是最简单的方式</p><pre><code>useradd --system --home=/opt/module/cloudera-manager/cm-5.12.1/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm//Cloudera Manager默认用户为cloudera-scm，安装完成后，将自动使用此用户。</code></pre><h2 id="6、配置-x2F-opt-x2F-cloudera-manager-x2F-cm-5-16-2-x2F-etc-x2F-cloudera-scm-agent-x2F-config-ini"><a href="#6、配置-x2F-opt-x2F-cloudera-manager-x2F-cm-5-16-2-x2F-etc-x2F-cloudera-scm-agent-x2F-config-ini" class="headerlink" title="6、配置&#x2F;opt&#x2F;cloudera-manager&#x2F;cm-5.16.2&#x2F;etc&#x2F;cloudera-scm-agent&#x2F; config.ini"></a>6、配置&#x2F;opt&#x2F;cloudera-manager&#x2F;cm-5.16.2&#x2F;etc&#x2F;cloudera-scm-agent&#x2F; config.ini</h2><p>server_host改成主节点名称，server_port不用动</p><pre><code>vim /opt/cloudera-manager/cm-5.16.2/etc/cloudera-scm-agent/ config.ini[General]# Hostname of the CM server.server_host=hadoop1# Port that the CM server is listening on.server_port=7182</code></pre><h2 id="7、获取mysql的连接包"><a href="#7、获取mysql的连接包" class="headerlink" title="7、获取mysql的连接包"></a>7、获取mysql的连接包</h2><p>我这里用的是mysql5.1.28的连接包<br>下载地址：<a href="https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-5.1.28.tar.gz">https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-5.1.28.tar.gz</a></p><p>这里我并没有修改jar包的名称，网上说需要修改名称变为 <code>mysql-connector-java.jar</code></p><pre><code>cp /opt/software/mysql-connector-java-5.1.28/mysql-connector-java-5.1.28-bin.jar /opt/cm-5.16.2cm/share/cmf/lib/</code></pre><p>将mysql连接的包放到这个&#x2F;cm&#x2F;share&#x2F;cmf&#x2F;lib目录下<br>网上两种方案存放这个包，一种是放在&#x2F;usr&#x2F;java&#x2F;，一种是存放在&#x2F;cm-5.16.2&#x2F;share&#x2F;cmf&#x2F;lib&#x2F;</p><p>这里经过这几天，证明只用放在&#x2F;cm-5.16.2&#x2F;share&#x2F;cmf&#x2F;lib&#x2F;即可</p><h2 id="8、分发解压后的文件到其他节点上"><a href="#8、分发解压后的文件到其他节点上" class="headerlink" title="8、分发解压后的文件到其他节点上"></a>8、分发解压后的文件到其他节点上</h2><pre><code>scp -r cm5.16.2 hadoop2:/opt/scp -r cm5.16.2 hadoop3:/opt/</code></pre><p>传过去文件确认一下你的权限</p><pre><code>//给传过去的文件赋值给cloudera-scm用户chown -R cloudera-scm:cloudera-scm /opt/cm5.16.2</code></pre><h2 id="9、初始化CM数据库"><a href="#9、初始化CM数据库" class="headerlink" title="9、初始化CM数据库"></a>9、初始化CM数据库</h2><pre><code>/opt/cm-5.16.2/share/cmf/schema/scm_prepare_database.sh mysql cm -h hadoop1 -u root -p 123456 --scm-host localhost scm scm scm//这里的scm用户，数据库中如果没有它会自动创建此用户，所以前面没有新建立用户</code></pre><p>这是数据库初始化语句，scm用户会在mysql冲初始化一个cm的数据库</p><p>如果初始化失败，还发现本地mysql登陆不成功等问题，可以在修改&#x2F;etc&#x2F;my.conf文件</p><p>进入mysql后修改mysql数据库的user表</p><blockquote><p>参考文章：</p><p><a href="https://blog.csdn.net/weixin_40079336/article/details/86648553">https://blog.csdn.net/weixin_40079336/article/details/86648553</a><br><img src="/cdh/centos7-chi-xian-an-zhuang-cdh5.16-ji-qun/20190125162936470.png" alt="初始化cm数据库的语句含义"></p></blockquote><pre><code>//这里并没有使用root用来初始化，使用的scm用户，root用户的配置只是为了未来方便root的远程操作如果已经增加权限还是不能登录可使用下面语句，执行完是结果是user表会增加记录，mysql的user表条数有12条//这里的grant 语句可以用来新增用户或者调整用户的权限和密码grant all privileges on *.* to &#39;root&#39;@&#39;localhost&#39; identified by &#39;123456&#39; with grant option;grant all privileges on *.* to &#39;root&#39;@&#39;hadoop1&#39; identified by &#39;123456&#39; with grant option;grant all privileges on *.* to &#39;root&#39;@&#39;127*.*0*.*0*.*1&#39; identified by &#39;123456&#39; with grant option;grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;123456&#39; with grant option;grant all privileges on *.* to &#39;scm&#39;@&#39;localhost&#39; identified by &#39;scm&#39; with grant option;grant all privileges on *.* to &#39;scm&#39;@&#39;hadoop1&#39; identified by &#39;scm&#39; with grant option;grant all privileges on *.* to &#39;scm&#39;@&#39;127*.*0*.*0*.*1&#39; identified by &#39;scm&#39; with grant option;grant all privileges on *.* to &#39;scm&#39;@&#39;%&#39; identified by &#39;scm&#39; with grant option;</code></pre><h2 id="10、CDH离线包下载"><a href="#10、CDH离线包下载" class="headerlink" title="10、CDH离线包下载"></a>10、CDH离线包下载</h2><p>这里的版本跟你的CM版本对应上，后面的el，指的是Centos的版本，这里是7，所以选择el7</p><p><a href="http://archive.cloudera.com/cdh5/parcels/latest/">http://archive.cloudera.com/cdh5/parcels/latest/</a></p><p><img src="/cdh/centos7-chi-xian-an-zhuang-cdh5.16-ji-qun/20201103103318108.png" alt="CDH下载内容"><br>下载三个文件</p><h2 id="11、创建Parcel-repo-目录（主节点操作）"><a href="#11、创建Parcel-repo-目录（主节点操作）" class="headerlink" title="11、创建Parcel-repo 目录（主节点操作）"></a>11、创建Parcel-repo 目录（主节点操作）</h2><pre><code>  mkdir -p /opt/cloudera/parcel-repo  chown -R cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo/</code></pre><p><code>将下载好的3个CDH文件放到parcel-repo这个目录中</code></p><p>SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012-el7.parcel.sha1</p><p>需要改名</p><p>SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012-el7.parcel.sha</p><p>把1去掉就行了</p><h2 id="12、所有节点创建parcels目录-所有节点"><a href="#12、所有节点创建parcels目录-所有节点" class="headerlink" title="12、所有节点创建parcels目录(所有节点)"></a>12、所有节点创建parcels目录(所有节点)</h2><pre><code>mkdir -p /opt/cloudera/parcelschown -R cloudera-scm:cloudera-scm /opt/cloudera/parcels</code></pre><h2 id="13、启动CM服务"><a href="#13、启动CM服务" class="headerlink" title="13、启动CM服务"></a>13、启动CM服务</h2><pre><code>hadoop1:/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-server start/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-agent starthadoop2:/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-agent starthadoop3:/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-agent start</code></pre><h2 id="14、访问WebUI"><a href="#14、访问WebUI" class="headerlink" title="14、访问WebUI"></a>14、访问WebUI</h2><pre><code>hadoop1:7180</code></pre><p>启动命令后等一会，会有个启动过程，可能比较慢，到这里应该就能正常访问了，默认的用户名和密码都是<code>admin</code><br>进去添加服务组件，配置他们的节点位置和他们的存储目录</p><p>也可以点击集群-&gt;操作-&gt;添加服务<br><img src="/cdh/centos7-chi-xian-an-zhuang-cdh5.16-ji-qun/20201106092122174.png" alt="在这里插入图片描述"></p><p>到此，CDH安装完成，文章有不足之处还请告知<br>CDH安装遇到问题可以参考以下连接<br><a href="https://blog.csdn.net/BalaBalaYi/article/details/76904274">https://blog.csdn.net/BalaBalaYi/article/details/76904274</a><br><a href="https://blog.csdn.net/qq_39680564/article/details/100013275">https://blog.csdn.net/qq_39680564&#x2F;article&#x2F;details&#x2F;100013275</a></p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础学习文档</title>
      <link href="/java/java-xue-xi-wen-dang/"/>
      <url>/java/java-xue-xi-wen-dang/</url>
      
        <content type="html"><![CDATA[<h1 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h1><h2 id="Java技术体系平台"><a href="#Java技术体系平台" class="headerlink" title="Java技术体系平台"></a>Java技术体系平台</h2><pre><code>java SE 标准版支持面向桌面级应用的java平台，提供了完整的java核心APIJava EE 企业版为企业卡环境下的应用程序提供了一套解决方案。该技术体系中包含的技术如：Servlet、Jsp，主要针对Web应用程序开发。Java Me 小型版支持Java程序运行在移动终端上的平台，对Java API有所精简，并加入了针对移动终端的支持</code></pre><h2 id="JVM"><a href="#JVM" class="headerlink" title="JVM"></a>JVM</h2><ol><li>JVM 是一个虚拟的计算机，具有指令集并使用不同的存储区域。负责执行指令，管理数据、内存、寄存器，包含在 JDK 中. </li><li>对于不同的平台，有不同的虚拟机。</li><li>Java 虚拟机机制屏蔽了底层运行平台的差别，实现了”一次编译，到处运行”</li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220601193448967.png" alt="image-20220601193448967"></p><p>在内存里面只要分配了空间一定会对应一个内存地址。</p><h3 id="类和对象的内存分配机制"><a href="#类和对象的内存分配机制" class="headerlink" title="类和对象的内存分配机制"></a>类和对象的内存分配机制</h3><p>Java 内存的结构分析</p><ol><li>栈： 一般存放基本数据类型(局部变量) </li><li>堆： 存放对象(Cat cat , 数组等) </li><li>方法区：常量池(常量，比如字符串)， 类加载信息</li></ol><pre><code class="java">Person p = new Person();p.name = &quot;jack&quot;;p.age = 10//先加载Person类信息，属性和方法，只会加载一次，在方法区中//堆 开辟空间，并初始化默认值//常量池中添加&quot;jack&quot;，堆 引用这个位置//堆内存的基本类型变量直接修改值//调用构造器，初始化</code></pre><h2 id="JDK和JRE"><a href="#JDK和JRE" class="headerlink" title="JDK和JRE"></a>JDK和JRE</h2><h3 id="JDK"><a href="#JDK" class="headerlink" title="JDK"></a>JDK</h3><ol><li>JDK 的全称(Java Development Kit Java 开发工具包) JDK &#x3D; JRE + java 的开发工具 [java, javac,javadoc,javap 等] </li><li>JDK 是提供给 Java 开发人员使用的，其中包含了 java 的开发工具，也包括了 JRE。所以安装了 JDK，就不用在单独 安装 JRE 了</li></ol><h3 id="JRE"><a href="#JRE" class="headerlink" title="JRE"></a>JRE</h3><ol><li>JRE(Java Runtime Environment Java 运行环境) JRE &#x3D; JVM + Java 的核心类库[类] </li><li>包括 Java 虚拟机(JVM Java Virtual Machine)和 Java 程序所需的核心类库等，如果想要运行一个开发好的 Java 程序， 计算机中只需要安装 JRE 即可</li></ol><h3 id="JDK、JRE-和-JVM-的包含关系"><a href="#JDK、JRE-和-JVM-的包含关系" class="headerlink" title="JDK、JRE 和 JVM 的包含关系"></a>JDK、JRE 和 JVM 的包含关系</h3><ol><li>JDK &#x3D; JRE + 开发工具集（例如 Javac,java 编译工具等)</li><li>JRE &#x3D; JVM + Java SE 标准类库（java 核心类库） </li><li>如果只想运行开发好的 .class 文件 只需要 JRE</li></ol><h3 id="下载安装JDK"><a href="#下载安装JDK" class="headerlink" title="下载安装JDK"></a>下载安装JDK</h3><h2 id="Java执行流程"><a href="#Java执行流程" class="headerlink" title="Java执行流程"></a>Java执行流程</h2><p><img src="/java/java-xue-xi-wen-dang/image-20220601193800191.png" alt="image-20220601193800191"></p><h2 id="转义字符"><a href="#转义字符" class="headerlink" title="转义字符"></a>转义字符</h2><pre><code>\t ：一个制表位，实现对齐的功能\n ：换行符\\ ：一个\\&quot; :一个&quot;\&#39; ：一个&#39; \r :一个回车</code></pre><h2 id="注解"><a href="#注解" class="headerlink" title="注解"></a>注解</h2><p>被注释的文字，不会被jvm解释执行</p><p>单行注解</p><pre><code>单行注释 //</code></pre><p>多行注解</p><pre><code>多行注释 /* */</code></pre><p>文档注释</p><pre><code>文档注释 /** *</code></pre><p>文档注解内容可以被JDK提供的工具javaDoc所解析，生成一套以网页文件形式体现的该程序的说明文档。</p><pre><code>javadoc -d 路径  -author -version 类名.java</code></pre><h2 id="代码规范"><a href="#代码规范" class="headerlink" title="代码规范"></a>代码规范</h2><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p>变量相当于内存中一个数据存储空间的表示，你可以把变量看做是一个房间的门牌号，通过门牌号我们可以找到房 间，而通过变量名可以访问到变量(值)</p><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><ol><li>java 数据类型分为两大类 基本数据类型， 引用类型 </li><li>基本数据类型有 8 中 数值型 [byte , short , int , long , float ,double] char , boolean </li><li>引用类型 [类，接口， 数组]</li></ol><table><thead><tr><th>类型</th><th>占用存储空间</th><th>范围</th></tr></thead><tbody><tr><td>byte[字节]</td><td>1字节</td><td>-128-127</td></tr><tr><td>short[短整型]</td><td>2字节</td><td>-32768-32767</td></tr><tr><td>int[整性]</td><td>4字节</td><td>-2147483648-2147483647</td></tr><tr><td>long[长整型]</td><td>8字节</td><td></td></tr><tr><td>float[单精度]</td><td>4字节</td><td>-3.4.3E38~3.403E38</td></tr><tr><td>Double[双精度]</td><td>8字节</td><td>-1.798E308~1.798E308</td></tr><tr><td>char[字节]</td><td>1字节</td><td>本质是整数，直接赋值整数会输出一个unicode字符，Ascll编码表</td></tr><tr><td>Boolean</td><td>1字节</td><td>只有ture和false两种，没有null</td></tr></tbody></table><p>bit最小存储单位，1 byte &#x3D; 8 bit</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>浮点数运算时存在精度的丢失，不要对运算过后的小数进行相等判断。默认使用Double，float会舍弃掉尾数部分。Math.abs()绝对值</p><h3 id="基本数据类型转化"><a href="#基本数据类型转化" class="headerlink" title="基本数据类型转化"></a>基本数据类型转化</h3><p>精度小的类型可以自动转换为精度大的数据类型，这个就是自动转化<br>$$<br>char–&gt;int–&gt;long–&gt;float–&gt;double<br>$$</p><p>$$<br>byte–&gt;short–&gt;int–&gt;long–&gt;float–&gt;double<br>$$</p><h4 id="类型转换注意事项"><a href="#类型转换注意事项" class="headerlink" title="类型转换注意事项"></a>类型转换注意事项</h4><p>精度大的数据类型赋值给精度小的数据类型时，就会报错，防止就会进行自动类型转换。</p><p>byte，short和char之间不会相互自动转换。</p><p>byte，short，char可以计算，在计算时首先转换为int类型。</p><h3 id="强制转换"><a href="#强制转换" class="headerlink" title="强制转换"></a>强制转换</h3><p>自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转换符 ( )，但可能造成 精度降低或溢出,格外要注意。</p><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><p>中文在线文档：<a href="https://www.matools.com/">https://www.matools.com/</a></p><h2 id="字符编码"><a href="#字符编码" class="headerlink" title="字符编码"></a>字符编码</h2><p>字符型存储到计算机中，将字符对应的码值（整数）找出来，</p><p>比如</p><p>存储<code>&#39;a&#39;==&gt;码值97==&gt;二进制（110 0001）==&gt;</code>存储</p><p>读取<code>二进制（110 0001）==&gt;97===&gt;&#39;a&#39;=&gt;显示</code></p><p>ASCLL编码表</p><p>一个字节表示，一个128个字符，只用了128个。缺点是不能表示所有字符</p><p>Unicode</p><p>两个字节表示，字母和汉字都是统一占用两个字节。Unicode码兼容ASCLL码。</p><p>缺点：浪费存储空间</p><p>utf-8编码表</p><p>大小可变的编码，字母使用1个字节，汉字使用3个字节。是Unicode的实现方式（改进）</p><p>gbk编码表</p><p>可以表示汉字，字母使用1个字节，汉字两个字节</p><p>gb2312 编码表</p><p>可以表示汉字，<code>gb2312&lt;gbk</code></p><p>big5编码表</p><p>繁体中文，台湾香港用的多</p><h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><p>运算符是一种特殊的符号，用以表示数据的运算、赋值和比较等。</p><ol><li>算术运算符</li><li>赋值运算符 </li><li>关系运算符 [比较运算符] </li><li>逻辑运算符 </li><li>位运算符 [需要二进制基础] </li><li>三元运算符</li></ol><h3 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a><strong>算术运算符</strong></h3><p><img src="/java/java-xue-xi-wen-dang/image-20220607191149589.png" alt="image-20220607191149589"></p><h3 id="关系运算符-比较运算符"><a href="#关系运算符-比较运算符" class="headerlink" title="关系运算符 [比较运算符]"></a><strong>关系运算符 [比较运算符]</strong></h3><p><img src="/java/java-xue-xi-wen-dang/image-20220607191959206.png" alt="image-20220607191959206"></p><h3 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h3><ol><li><p>短路与 &amp;&amp; ， 短路或 ||，取反 !</p></li><li><p>逻辑与 &amp;，逻辑或 |，^ 逻辑异或</p></li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220607192458024.png" alt="image-20220607192458024"></p><pre><code>1) a&amp;b : &amp; 叫逻辑与：规则：当 a 和 b 同时为 true ,则结果为 true, 否则为 false 2) a&amp;&amp;b : &amp;&amp; 叫短路与：规则：当 a 和 b 同时为 true ,则结果为 true,否则为 false 3) a|b : | 叫逻辑或，规则：当 a 和 b ，有一个为 true ,则结果为 true,否则为 false 4) a||b : || 叫短路或，规则：当 a 和 b ，有一个为 true ,则结果为 true,否则为 false 5) !a : 叫取反，或者非运算。当 a 为 true, 则结果为 false, 当 a 为 false 是，结果为 true 6) a^b: 叫逻辑异或，当 a 和 b 不同时，则结果为 true, 否则为 false</code></pre><h3 id="赋值运算符"><a href="#赋值运算符" class="headerlink" title="赋值运算符"></a>赋值运算符</h3><p>基本赋值运算符 <code>=</code></p><p>复合运算符 <code>+= </code>，<code>-= </code>，<code>*=</code> ，<code> /=</code> ，<code>%=</code> 等 ,</p><h3 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h3><p>条件表达式 ? 表达式 1: 表达式</p><ol><li>如果条件表达式为 true，运算后的结果是表达式 1； </li><li>如果条件表达式为 false，运算后的结果是表达式 2；</li></ol><h2 id="命名规范"><a href="#命名规范" class="headerlink" title="命名规范"></a>命名规范</h2><ol><li>包名：多单词组成时所有字母都小写：aaa.bbb.ccc &#x2F;&#x2F;比如 com.hsp.crm </li><li>类名、接口名：多单词组成时，所有单词的首字母大写：XxxYyyZzz [大驼峰] 比如： TankShotGame </li><li>变量名、方法名：多单词组成时，第一个单词首字母小写，第二个单词开始每个单词首字母大写：xxxYyyZzz [小 驼峰， 简称 驼峰法] 比如： tankShotGame </li><li>常量名：所有字母都大写。多单词时每个单词用下划线连接：XXX_YYY_ZZZ 比如 ：定义一个所得税率 TAX_RA</li></ol><h2 id="程序控制结构"><a href="#程序控制结构" class="headerlink" title="程序控制结构"></a>程序控制结构</h2><p>顺序控制（前向引用）</p><p>分支控制（if，else，switch）</p><p>循环控制（for，while，dowhile，多重循环）</p><p>break</p><p>continue</p><p>return</p><h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><p>数组可以存放多个同一类型的数据。数组也是一种数据类型，是引用类型。</p><h3 id="数组存储机制"><a href="#数组存储机制" class="headerlink" title="数组存储机制"></a>数组存储机制</h3><ol><li><p>基本数据类型赋值，这个值就是具体的数据，而且相互不影响。 </p><p>int n1 &#x3D; 2; int n2 &#x3D; n1;</p></li><li><p>数组在默认情况下是引用传递，赋的值是地址。 看一个案例，并分析数组赋值的内存图(重点,</p></li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220608162448143.png" alt="image-20220608162448143"></p><h3 id="数组反转"><a href="#数组反转" class="headerlink" title="数组反转"></a>数组反转</h3><h3 id="数组添加-x2F-扩容"><a href="#数组添加-x2F-扩容" class="headerlink" title="数组添加&#x2F;扩容"></a>数组添加&#x2F;扩容</h3><h3 id="数组排序"><a href="#数组排序" class="headerlink" title="数组排序"></a>数组排序</h3><h4 id="内部排序"><a href="#内部排序" class="headerlink" title="内部排序"></a>内部排序</h4><h4 id="外部排序"><a href="#外部排序" class="headerlink" title="外部排序"></a>外部排序</h4><h4 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h4><p>冒泡排序（Bubble Sorting）的基本思想是：通过对待排序序列从后向前（从下标较大的元素开始），依次比较相邻元素 的值，若发现逆序则交换，使值较大的元素逐渐从前移向后部，就象水底下的气泡一样逐渐向上冒</p><pre><code class="java">for( int i = 0; i &lt; arr.length - 1; i++) &#123;//外层循环是 4 次for( int j = 0; j &lt; arr.length - 1 - i; j++) &#123;//4 次比较-3 次-2 次-1 次//如果前面的数&gt;后面的数，就交换if(arr[j] &gt; arr[j + 1]) &#123;temp = arr[j];arr[j] = arr[j+1];arr[j+1] = temp;&#125;&#125;</code></pre><h3 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h3><h4 id="顺序查找"><a href="#顺序查找" class="headerlink" title="顺序查找"></a>顺序查找</h4><h4 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h4><h3 id="二维数组"><a href="#二维数组" class="headerlink" title="二维数组"></a>二维数组</h3><h1 id="面向对象编程-基础部分"><a href="#面向对象编程-基础部分" class="headerlink" title="面向对象编程(基础部分)"></a>面向对象编程(基础部分)</h1><h2 id="类和对象"><a href="#类和对象" class="headerlink" title="类和对象"></a>类和对象</h2><ol><li>类是抽象的，概念的，代表一类事物,比如人类,猫类.., 即它是数据类型.</li></ol><ol start="2"><li>对象是具体的，实际的，代表一个具体事物, 即 是实例. </li><li>类是对象的模板，对象是类的一个个体，对应一个实例</li></ol><h2 id="对象在内存中的存在形式"><a href="#对象在内存中的存在形式" class="headerlink" title="对象在内存中的存在形式"></a>对象在内存中的存在形式</h2><p>String 是 <code>引用类型</code>，放在方法区中</p><p>创建对象时在方法区中初始化类的属性和方法</p><p><img src="/java/java-xue-xi-wen-dang/image-20220608182943814.png" alt="image-20220608182943814"></p><p><strong>基本数据类型是值拷贝的行式</strong></p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="方法的调用机制内存原理"><a href="#方法的调用机制内存原理" class="headerlink" title="方法的调用机制内存原理"></a>方法的调用机制内存原理</h3><ol><li><p><strong>main栈</strong>初始化Person对象</p></li><li><p>执行getSum方法</p></li><li><p>单独开辟一个独立的<strong>方法栈</strong>空间，执行getSum方法</p></li><li><p>将结果返回给main栈后，getSum方法的栈空间被释放。</p></li><li><p>继续执行后面的代码，全部执行完毕后，main栈继续执行，全部执行完毕后程序整个退出。（main栈代表整个程序）</p></li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220610081802457.png" alt="image-20220610081802457"></p><h3 id="成员方法的好处"><a href="#成员方法的好处" class="headerlink" title="成员方法的好处"></a>成员方法的好处</h3><p>1）提高代码的复用性</p><ol start="2"><li>可以将实现的细节封装起来，然后供其他用户来调用即可</li></ol><h3 id="方法名"><a href="#方法名" class="headerlink" title="方法名"></a>方法名</h3><p>遵循驼峰命名法，最好见名知义，表达出该功能的意思即可, 比如 得到两个数的和 getSum, 开发中按照规范</p><h2 id="成员方法传参机制"><a href="#成员方法传参机制" class="headerlink" title="成员方法传参机制"></a>成员方法传参机制</h2><h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><p>基本数据类型，传递是值拷贝的方式。形参的任何改变不影响实参。</p><p><img src="/java/java-xue-xi-wen-dang/image-20220610084335547.png" alt="image-20220610084335547"></p><h3 id="引用数据类型"><a href="#引用数据类型" class="headerlink" title="引用数据类型"></a>引用数据类型</h3><p>引用类型传递的是地址，可以通过形参影响实参。</p><p>各个栈中的单独对象引用的是同一个对象地址，可以不同，互不影响。</p><p><img src="/java/java-xue-xi-wen-dang/image-20220610085026058.png" alt="image-20220610085026058"></p><h2 id="方法递归调用"><a href="#方法递归调用" class="headerlink" title="方法递归调用"></a>方法递归调用</h2><p>递归就是方法自己调用自己,每次调用时传入不同的变量.</p><p><img src="/java/java-xue-xi-wen-dang/image-20220610090640488.png" alt="image-20220610090640488"></p><h3 id="递归重要规则"><a href="#递归重要规则" class="headerlink" title="递归重要规则"></a>递归重要规则</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220610090704330.png" alt="image-20220610090704330"></p><h2 id="方法重载-OverLoad"><a href="#方法重载-OverLoad" class="headerlink" title="方法重载(OverLoad)"></a>方法重载(OverLoad)</h2><p>java 中允许同一个类中，多个同名方法的存在，但要求 形参列表不一致！</p><h3 id="注意事项-1"><a href="#注意事项-1" class="headerlink" title="注意事项"></a>注意事项</h3><p>方法名称相同</p><p>形参列表：必须不同（类型，数量，顺序，至少有一项不一致）</p><p>返回类型无要求</p><h2 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a>可变参数</h2><p>java 允许将同一个类中多个同名同功能但参数个数不同的方法，封装成一个方法。</p><pre><code class="java">访问修饰符 返回类型 方法名(数据类型... 形参名) &#123;&#125;</code></pre><pre><code class="java">//接受任意数量的int值。public int sum(int... nums) &#123;//System.out.println(&quot;接收的参数个数=&quot; + nums.length);int res = 0;for(int i = 0; i &lt; nums.length; i++) &#123;res += nums[i];&#125;return res;&#125;&#125;</code></pre><h3 id="注意事项-2"><a href="#注意事项-2" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li>可变参数可以为0哥或者任意多个</li><li>可变参数的实参可以为数组</li><li>可变参数的实质就是数组</li><li>可变参数可以和普通类型参数一起放在形参列表，但必须保证可变参数放在最后</li><li>一个形参列表中只能出现一个可变参数。</li></ol><h2 id="作用域"><a href="#作用域" class="headerlink" title="作用域"></a>作用域</h2><p><img src="/java/java-xue-xi-wen-dang/image-20220610092940004.png" alt="image-20220610092940004"></p><h3 id="注意事项-3"><a href="#注意事项-3" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li><p>全局属性和局部变量可以重名，访问时遵循就近原则。</p></li><li><p>同一个作用域中，两个变量不能重名</p></li><li><p>属性声明周期较长，伴随着对象的创建而创建，伴随着对象的销毁而销毁。局部变量，生命周期较短，伴随着代码块的执行而创建，伴随着代码块的结束而销毁。</p></li><li><p>作用域范围不同</p><p>全局变量&#x2F;属性：可以被本类或其他类使用</p><p>局部变量：只能在本类中对应的方法调用</p></li><li><p>修饰符不同</p><p>全局变量&#x2F;属性：可以添加修饰符</p><p>局部变量不可以加修饰符</p></li></ol><h2 id="构造方法-x2F-构造器"><a href="#构造方法-x2F-构造器" class="headerlink" title="构造方法&#x2F;构造器"></a>构造方法&#x2F;构造器</h2><p>类的一种特殊方法，主要作用是完成对<strong>新对象的初始化</strong>，不是创建对象</p><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><pre><code class="java">[修饰符] 方法名(形参列表)&#123;方法体;&#125;</code></pre><h3 id="注意事项-4"><a href="#注意事项-4" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li>构造器的修饰符可以默认， 也可以是 public protected private</li><li>构造器没有返回值</li><li>构造器名称和类名字必须一致</li><li>构造器是完成对象的初始化，并不是创建对象</li><li>如果没有定义构造器，系统会自动给类生成一个默认的无参构造器</li><li>一旦定义了自己的构造器，默认的构造器就覆盖了，不能使用默认的无参构造器了，需要显示定义</li></ol><h2 id="对象创建的流程分析"><a href="#对象创建的流程分析" class="headerlink" title="对象创建的流程分析"></a>对象创建的流程分析</h2><ol><li>方法区加载类信息（属性、方法）</li><li>分配堆空间创建对象</li><li>完成对象默认初始化</li><li>基本数据类型初始化</li><li>常量在方法区常量池，构造器初始化指向</li><li>返回对象地址给’p‘</li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220610095235078.png" alt="image-20220610095235078"></p><h2 id="this-关键字"><a href="#this-关键字" class="headerlink" title="this 关键字"></a>this 关键字</h2><p>java虚拟机会给每个对象分配this，代表当前对象。</p><p>那个对象调用，this就代表哪个对象。</p><h3 id="注意事项-5"><a href="#注意事项-5" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li><p>this 关键字可以用来访问<strong>本类</strong>的属性、方法、构造器 </p></li><li><p>this 用于区分当前<strong>类</strong>的属性和局部变量 </p></li><li><p>访问成员方法的语法：this.方法名(参数列表);</p></li><li><p>访问构造器语法：this(参数列表); 注意只能在构造器中使用(即只能在构造器中访问另外一个构造器, 必须放在第一 条语句）</p></li><li><p>this 不能在类定义的外部使用，只能在类定义的方法中使用。只能和<strong>类</strong>关联。</p></li></ol><h1 id="面向对象编程-中级部分"><a href="#面向对象编程-中级部分" class="headerlink" title="面向对象编程(中级部分)"></a>面向对象编程(中级部分)</h1><h2 id="IDEA"><a href="#IDEA" class="headerlink" title="IDEA"></a>IDEA</h2><h2 id="IDEA-常用快捷键"><a href="#IDEA-常用快捷键" class="headerlink" title="IDEA 常用快捷键"></a>IDEA 常用快捷键</h2><ol><li>删除当前行, 默认是 ctrl + Y 自己配置 ctrl + d </li><li>复制当前行, 自己配置 ctrl + alt + 向下光标 </li><li>补全代码 alt + &#x2F;</li><li>添加注释和取消注释 ctrl + &#x2F; 【第一次是添加注释，第二次是取消注释】</li><li>导入该行需要的类 先配置 auto import , 然后使用 alt+enter 即可 </li><li>快速格式化代码 ctrl + alt + L </li><li>生成构造器等 alt + insert [提高开发效率] </li><li>查看一个类的层级关系 ctrl + H [学习继承后，非常有用] </li><li>将光标放在一个方法上，输入 ctrl + B , 可以定位到方法 [学继承后，非常有用] </li><li>自动的分配变量名 , 通过在后面加.var</li><li>ctrl+J，提示所有的快捷模板。</li></ol><h2 id="包"><a href="#包" class="headerlink" title="包"></a>包</h2><h3 id="包的作用"><a href="#包的作用" class="headerlink" title="包的作用"></a>包的作用</h3><ol><li><p>区分相同名字的类</p></li><li><p>很好的进行管理</p></li><li><p>控制访问范围</p></li></ol><h2 id="包的命名"><a href="#包的命名" class="headerlink" title="包的命名"></a>包的命名</h2><p><img src="/java/java-xue-xi-wen-dang/image-20220610120611399.png" alt="image-20220610120611399"></p><h2 id="访问修饰符"><a href="#访问修饰符" class="headerlink" title="访问修饰符"></a>访问修饰符</h2><p>java 提供四种访问控制修饰符号，用于控制方法和属性(成员变量)的访问权限（范围）: </p><ol><li>公开级别:用 public 修饰,对外公开 </li><li>受保护级别:用 protected 修饰,对子类和同一个包中的类公开 </li><li>默认级别:没有修饰符号,向同一个包的类公开. </li><li>私有级别:用 private 修饰,只有类本身可以访问,不对外公开.</li></ol><h3 id="4中访问修饰符访问范围"><a href="#4中访问修饰符访问范围" class="headerlink" title="4中访问修饰符访问范围"></a>4中访问修饰符访问范围</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220610120901607.png" alt="image-20220610120901607"></p><h2 id="面向对象编程三大特征"><a href="#面向对象编程三大特征" class="headerlink" title="面向对象编程三大特征"></a>面向对象编程三大特征</h2><p>面向对象编程有三大特征：封装、继承和多态。</p><h3 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h3><p>封装就是把抽象出的数据和对数据的操作封装在一起，数据被保护在内部，程序的其他部分只有通过被授权的操作，才能对数据进行操作。</p><h4 id="封装的理解和好处"><a href="#封装的理解和好处" class="headerlink" title="封装的理解和好处"></a>封装的理解和好处</h4><ol><li>隐藏实现细节</li><li>可以对数据进行验证，保证安全合理</li></ol><h4 id="封装实现步骤"><a href="#封装实现步骤" class="headerlink" title="封装实现步骤"></a>封装实现步骤</h4><ol><li>将属性私有化private</li><li>提供一个公共的set方法，用于对属性进行判断和赋值</li><li>提供一个公共的get方法，用于获取属性的值</li></ol><h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p>继承可以解决代码复用,让我们的编程更加靠近人类思维.当多个类存在相同的属性(变量)和方法时,可以从这些类中 抽象出父类,在父类中定义这些相同的属性和方法，所有的子类不需要重新定义这些属性和方法，只需要通过 extends 来 声明继承父类即可</p><p><img src="/java/java-xue-xi-wen-dang/image-20220610142503441.png" alt="image-20220610142503441"></p><h4 id="继承的语法"><a href="#继承的语法" class="headerlink" title="继承的语法"></a>继承的语法</h4><pre><code class="java">class 子类 extends 父类&#123;&#125;//1.子类会自动拥有父类定义的属性和方法//2.父类又叫超类，基类//3.子类又叫派生类</code></pre><h4 id="继承的好处"><a href="#继承的好处" class="headerlink" title="继承的好处"></a>继承的好处</h4><ol><li>代码的复用性提高了</li><li>代码的扩展性和维护性提高了</li></ol><h4 id="继承的深入讨论-x2F-细节问题"><a href="#继承的深入讨论-x2F-细节问题" class="headerlink" title="继承的深入讨论&#x2F;细节问题"></a>继承的深入讨论&#x2F;细节问题</h4><ol><li>子类继承了所有的属性和方法，非私有的属性和方法可以在子类直接访问, 但是私有属性和方法不能在子类直接访 问，要通过父类提供公共的方法去访问  </li><li>子类必须调用父类的构造器， 完成父类的初始化 </li><li>当创建子类对象时，不管使用子类的哪个构造器，默认情况下总会去调用父类的无参构造器，如果父类没有提供无参构造器，则必须在子类的构造器中用 super 去指定使用父类的哪个构造器完成对父类的初始化工作，否则，编译不会通过</li><li>如果希望指定去调用父类的某个构造器，则显式的调用一下 : super（参数列表）</li><li>super 在使用时，必须放在构造器第一行(super 只能在构造器中使用)</li><li>super() 和 this() 都只能放在构造器第一行，因此这两个方法不能共存在一个构造器</li><li>java 所有类都是 Object 类的子类, Object 是所有类的基类</li><li>父类构造器的调用不限于直接父类！将一直往上追溯直到 Object 类(顶级父类)</li><li>子类最多只能继承一个父类(指直接继承)，即 java 中是单继承机制。</li><li>不能滥用继承，子类和父类之间必须满足 is-a 的逻辑关系</li></ol><h4 id="继承的本质分析（内存布局）"><a href="#继承的本质分析（内存布局）" class="headerlink" title="继承的本质分析（内存布局）"></a>继承的本质分析（内存布局）</h4><pre><code class="java">package com.hspedu.extend_;/*** 讲解继承的本质*/public class ExtendsTheory &#123;public static void main(String[] args) &#123;Son son = new Son();//内存的布局//?-&gt; 这时请大家注意，要按照查找关系来返回信息//(1) 首先看子类是否有该属性//(2) 如果子类有这个属性，并且可以访问，则返回信息//(3) 如果子类没有这个属性，就看父类有没有这个属性(如果父类有该属性，并且可以访问，就返回信息..)//(4) 如果父类没有就按照(3)的规则，继续找上级父类，直到 Object... System.out.println(son.name);//返回就是大头儿子//System.out.println(son.age);//返回的就是 39//System.out.println(son.getAge());//返回的就是 39System.out.println(son.hobby);//返回的就是旅游&#125;&#125;class GrandPa &#123; //爷类String name = &quot;大头爷爷&quot;;String hobby = &quot;旅游&quot;;&#125;class Father extends GrandPa &#123;//父类String name = &quot;大头爸爸&quot;;private int age = 39;public int getAge() &#123;return age;&#125;&#125;class Son extends Father &#123; //子类String name = &quot;大头儿子&quot;;&#125;</code></pre><ol><li>先加载父类的信息（Object）</li><li>在加载二级父类（GrandPa）</li><li>在加载三级父类（Father）</li><li>最后加载自己的类（Son）</li><li>在堆内存开辟空间默认初始化，然后赋值</li><li>最后将地址返回给main栈中的对象。</li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220610144741543.png" alt="image-20220610144741543"></p><h3 id="super-关键字"><a href="#super-关键字" class="headerlink" title="super 关键字"></a>super 关键字</h3><p>super 代表父类的引用，用于访问父类的属性、方法、构造器</p><h4 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h4><ol><li><p>访问父类的属性，但不能访问父类的private属性（super.属性名）</p></li><li><p>访问父类的方法，不能访问父类的private方法（super。方法名（参数列表））</p></li><li><p>访问父类的构造器（super(参数列表))，只能放在构造器的第一句，只能出现一次</p></li></ol><h4 id="super的好处"><a href="#super的好处" class="headerlink" title="super的好处"></a>super的好处</h4><ol><li>调用父类构造器的好处（分工明确，父类属性由父类初始化，子类的属性由子类初始化）</li><li>当子类中有和父类中的成员（属性和方法）重名时，为了访问父类的成员，必须通过super。如果没有重名，使用super、this、直接访问是一样的效果！</li><li>super的访问不限于直接父类，如果爷爷类和本类中有相同的成员，也可以使用super区访问爷爷类的成员；如果多个基类（上级类）中都有同名的成员，使用super访问遵循就近原则。A-&gt;B-&gt;C，也需要遵守访问权限相关规定</li></ol><h4 id="super-和-this-的比较"><a href="#super-和-this-的比较" class="headerlink" title="super 和 this 的比较"></a>super 和 this 的比较</h4><p><img src="/java/java-xue-xi-wen-dang/image-20220610150344383.png" alt="image-20220610150344383"></p><h3 id="方法重写-x2F-覆盖-override"><a href="#方法重写-x2F-覆盖-override" class="headerlink" title="方法重写&#x2F;覆盖(override)"></a>方法重写&#x2F;覆盖(override)</h3><p>方法覆盖（重写）就是子类有一个方法，和父类的某个方法名称、返回类型、参数一样，那么子类的这个方法就覆盖了父类的方法。</p><h4 id="注意事项-6"><a href="#注意事项-6" class="headerlink" title="注意事项"></a>注意事项</h4><ol><li>子类的方法形参列表，方法名称，要和父类方法的形参列表，方法名称完全一样</li><li>子类方法的返回类型和父类方法返回类型一样，或是父类返回类型的子类</li><li>子类方法不能缩小父类方法的访问权限</li></ol><h4 id="重写和重载的区别"><a href="#重写和重载的区别" class="headerlink" title="重写和重载的区别"></a>重写和重载的区别</h4><p><img src="/java/java-xue-xi-wen-dang/image-20220610151004410.png" alt="image-20220610151004410"></p><h3 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h3><p>方法或对象具有多种形态。是面向对象的第三大特征，多态是建立在封装和继承基础之上的！</p><ol><li>一个对象的编译类型和运行类型可以不一致</li><li>编译类型在定义对象时，就确定了，不能改变</li><li>运行类型是可以变化的</li><li>编译类型看定义时&#x3D;号的左边，运行类型看&#x3D;号的右边</li></ol><h4 id="多台具体体现在在哪些？"><a href="#多台具体体现在在哪些？" class="headerlink" title="多台具体体现在在哪些？"></a>多台具体体现在在哪些？</h4><ol><li><p>方法的多态</p><p>重载体现多态，重载体现多态</p></li><li><p>对象多态</p><p>对象的编译类型和运行类型可以不一致，编译类型在定义时，就确定。运行类型是可以动态变化的，可以通过getClass()来查看运行类型</p><p>编译类型看&#x3D;号左边，运行类型看&#x3D;号右边。</p></li></ol><h4 id="多态注意事项和细节讨论"><a href="#多态注意事项和细节讨论" class="headerlink" title="多态注意事项和细节讨论"></a>多态注意事项和细节讨论</h4><ul><li><p>多态的前提是：两个对象(类)存在继承关系</p></li><li><p>多态的向上转型</p></li></ul><p><img src="/java/java-xue-xi-wen-dang/image-20220610152047642.png" alt="image-20220610152047642"></p><ul><li>多态的向下转型</li></ul><p><img src="/java/java-xue-xi-wen-dang/image-20220610152057979.png" alt="image-20220610152057979"></p><ul><li>属性没有重写之说！属性的值看编译类型</li></ul><pre><code class="java">package com.hspedu.poly_.detail_;public class PolyDetail02 &#123;public static void main(String[] args) &#123;//属性没有重写之说！属性的值看编译类型Base base = new Sub();//向上转型System.out.println(base.count);// ？ 看编译类型 10Sub sub = new Sub();System.out.println(sub.count);//? 20&#125;&#125;class Base &#123; //父类int count = 10;//属性&#125;class Sub extends Base &#123;//子类int count = 20;//属性&#125;</code></pre><h4 id="instanceOf比较运算符"><a href="#instanceOf比较运算符" class="headerlink" title="instanceOf比较运算符"></a>instanceOf比较运算符</h4><p>用于判断对象的<strong>运行类型</strong>是否为 XX 类型或 XX 类型的子类型</p><pre><code class="java">public class PolyDetail03 &#123;public static void main(String[] args) &#123;BB bb = new BB();System.out.println(bb instanceof BB);// trueSystem.out.println(bb instanceof AA);// true//aa 编译类型 AA, 运行类型是 BB//BB 是 AA 子类AA aa = new BB();System.out.println(aa instanceof AA);//看的是运行类型（BB类型）System.out.println(aa instanceof BB)//true;Object obj = new Object();System.out.println(obj instanceof AA);//falseString str = &quot;hello&quot;;//System.out.println(str instanceof AA);System.out.println(str instanceof Object);//true&#125;&#125;class AA &#123;&#125; //父类</code></pre><h4 id="java-的动态绑定机制（重要）"><a href="#java-的动态绑定机制（重要）" class="headerlink" title="java 的动态绑定机制（重要）"></a>java 的动态绑定机制（重要）</h4><ol><li>调用对象方法时，该方法与该对象的内存地址&#x2F;运行类型绑定</li><li>当调用对象属性时，没有动态绑定机制，哪里声明，哪里使用。</li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220610155346181.png" alt="image-20220610155346181"></p><pre><code class="java">package com.hspedu.poly_.dynamic_;public class DynamicBinding &#123;public static void main(String[] args) &#123;//a 的编译类型 A, 运行类型 BA a = new B();//向上转型System.out.println(a.sum());//?40 -&gt; 方法注释之后：30System.out.println(a.sum1());//?30-&gt; 方法注释之后：20&#125;&#125;class A &#123;//父类public int i = 10;//动态绑定机制:public int sum() &#123;//父类 sum()return getI() &#125;//属性没有动态绑定机制public int sum1() &#123;//父类 sum1()return i + 10;//10 + 10&#125;public int getI() &#123;//父类 getIreturn i;&#125;&#125;class B extends A &#123;//子类public int i = 20;// public int sum() &#123;// return i + 20;// &#125;public int getI() &#123;//子类 getI()return i;&#125;// public int sum1() &#123;// return i + 10;// &#125;&#125;</code></pre><h4 id="多态的应用"><a href="#多态的应用" class="headerlink" title="多态的应用"></a>多态的应用</h4><h4 id="多态数组"><a href="#多态数组" class="headerlink" title="多态数组"></a>多态数组</h4><p>数组的定义类型为父类类型，里面保存的实际元素类型为子类类型</p><pre><code class="java">public class PloyArray &#123;public static void main(String[] args) &#123;//应用实例:现有一个继承结构如下：要求创建 1 个 Person 对象、// 2 个 Student 对象和 2 个 Teacher 对象, 统一放在数组中，并调用每个对象 say 方法Person[] persons = new Person[5];persons[0] = new Person(&quot;jack&quot;, 20);persons[1] = new Student(&quot;mary&quot;, 18, 100);persons[2] = new Student(&quot;smith&quot;, 19, 30.1);persons[3] = new Teacher(&quot;scott&quot;, 30, 20000);persons[4] = new Teacher(&quot;king&quot;, 50, 25000);//循环遍历多态数组，调用 sayfor (int i = 0; i &lt; persons.length; i++) &#123;//老师提示: person[i] 编译类型是 Person ,运行类型是是根据实际情况有 JVM 来判断System.out.println(persons[i].say());//动态绑定机制//这里大家聪明. 使用 类型判断 + 向下转型. if(persons[i] instanceof Student) &#123;//判断 person[i] 的运行类型是不是 StudentStudent student = (Student)persons[i];//向下转型student.study();//小伙伴也可以使用一条语句 ((Student)persons[i]).study();&#125; else if(persons[i] instanceof Teacher) &#123;Teacher teacher = (Teacher)persons[i];teacher.teach();&#125; else if(persons[i] instanceof Person)&#123;//System.out.println(&quot;你的类型有误, 请自己检查...&quot;);&#125; else &#123;System.out.println(&quot;你的类型有误, 请自己检查...&quot;);&#125;&#125;&#125;&#125;</code></pre><h4 id="多态参数"><a href="#多态参数" class="headerlink" title="多态参数"></a>多态参数</h4><p>方法定义的形参类型为父类类型，实参类型允许为子类类型</p><p><img src="/java/java-xue-xi-wen-dang/image-20220610162515574.png" alt="image-20220610162515574"></p><h2 id="Object类详解"><a href="#Object类详解" class="headerlink" title="Object类详解"></a>Object类详解</h2><h3 id="equals-方法"><a href="#equals-方法" class="headerlink" title="equals 方法"></a>equals 方法</h3><h4 id="x3D-x3D-和-equals-的对比"><a href="#x3D-x3D-和-equals-的对比" class="headerlink" title="&#x3D;&#x3D; 和 equals 的对比"></a>&#x3D;&#x3D; 和 equals 的对比</h4><ol><li><p>&#x3D;&#x3D;：既可以判断基本类型，有可以判断引用类型</p></li><li><p>&#x3D;&#x3D;：如果判断基本类型，判断的是值是否相等</p></li><li><p>&#x3D;&#x3D;：如果判断引用类型，判断的是，即判断是不是同一个对象</p></li><li><p>equals：是Object类中的方法，只能判断引用类型</p></li><li><p>默认判断是地址是否相等，子类往往重写该方法，用于判断内容是否相等。</p></li></ol><h4 id="如何重写-equals"><a href="#如何重写-equals" class="headerlink" title="如何重写 equals"></a>如何重写 equals</h4><pre><code class="java">//重写 Object 的 equals 方法public boolean equals(Object obj) &#123;//判断如果比较的两个对象是同一个对象，则直接返回 trueif(this == obj) &#123;return true;&#125;//类型判断if(obj instanceof Person) &#123;//是 Person，我们才比//进行 向下转型, 因为我需要得到 obj 的 各个属性Person p = (Person)obj;return this.name.equals(p.name) &amp;&amp; this.age == p.age &amp;&amp; this.gender == p.gender;&#125;//如果不是 Person ，则直接返回 falsereturn false;&#125;</code></pre><h3 id="hashCode-方法"><a href="#hashCode-方法" class="headerlink" title="hashCode 方法"></a>hashCode 方法</h3><ol><li>提高具有哈希结构的容器的效率！</li><li>两个引用，如果指向的是同一个对象，则哈希值肯定是一样的！</li><li>两个引用，如果指向的是不同对象，则哈希值是不一样的</li><li>哈希值主要根据地址号来的！， 不能完全将哈希值等价于地址</li></ol><h3 id="toString-方法"><a href="#toString-方法" class="headerlink" title="toString 方法"></a>toString 方法</h3><p>默认返回：全类名+@+哈希值的十六进制</p><ol><li><p>子类往往重写 toString 方法，用于返回对象的属性信息</p></li><li><p>重写 toString 方法，打印对象或拼接对象时，都会自动调用该对象的 toString 形式.</p></li><li><p>当直接输出一个对象时，toString 方法会被默认的调用, 比如 System.out.println(monster)； 就会默认调用 monster.toString()</p></li></ol><h3 id="finalize-方法"><a href="#finalize-方法" class="headerlink" title="finalize 方法"></a>finalize 方法</h3><ol><li>当对象被回收时，系统自动调用该对象的 finalize 方法。子类可以重写该方法，做一些释放资源的操作</li><li>什么时候被回收：当某个对象没有任何引用时，则 jvm 就认为这个对象是一个垃圾对象，就会使用垃圾回收机制来 销毁该对象，在销毁该对象前，会先调用 finalize 方法</li><li>垃圾回收机制的调用，是由系统来决定(即有自己的 GC 算法), 也可以通过 System.gc() 主动触发垃圾回收机制</li></ol><p>老韩提示： 我们在实际开发中，几乎不会运用 finalize , 所以更多就是为了应付面试</p><pre><code class="java">//演示 Finalize 的用法public class Finalize_ &#123;public static void main(String[] args) &#123;Car bmw = new Car(&quot;宝马&quot;);//这时 car 对象就是一个垃圾,垃圾回收器就会回收(销毁)对象, 在销毁对象前，会调用该对象的 finalize 方法//,程序员就可以在 finalize 中，写自己的业务逻辑代码(比如释放资源：数据库连接,或者打开文件..)//,如果程序员不重写 finalize,那么就会调用 Object 类的 finalize, 即默认处理//,如果程序员重写了 finalize, 就可以实现自己的逻辑bmw = null;System.gc();//主动调用垃圾回收器System.out.println(&quot;程序退出了....&quot;);&#125;&#125;class Car &#123;private String name;//属性, 资源。。public Car(String name) &#123;this.name = name;&#125;//重写 finalize@Overrideprotected void finalize() throws Throwable &#123;System.out.println(&quot;我们销毁 汽车&quot; + name );System.out.println(&quot;释放了某些资源...&quot;);&#125;&#125;</code></pre><h2 id="断点调试-debug"><a href="#断点调试-debug" class="headerlink" title="断点调试(debug)"></a>断点调试(debug)</h2><p>在断点调试过程中，是运行状态，是以对象的<strong>运行类型</strong>来执行的。</p><h3 id="断点调试介绍"><a href="#断点调试介绍" class="headerlink" title="断点调试介绍"></a>断点调试介绍</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220610165043839.png" alt="image-20220610165043839"></p><h3 id="断点调试的快捷键"><a href="#断点调试的快捷键" class="headerlink" title="断点调试的快捷键"></a>断点调试的快捷键</h3><p>F9(resume,执行到下一个断点) </p><p>F7：跳入方法内 </p><p>F8: 逐行执行代码. </p><p>shift+F8: 跳出方法</p><p>可以动态的下断点，在执行过程中也可以下。</p><h1 id="面向对象编程-高级部分"><a href="#面向对象编程-高级部分" class="headerlink" title="面向对象编程(高级部分)"></a>面向对象编程(高级部分)</h1><h2 id="Static关键字"><a href="#Static关键字" class="headerlink" title="Static关键字"></a>Static关键字</h2><p>Static静态变量被同一个类的所有对象共享。</p><p>Static变量在类加载的时候就生成了。没有创建对象也可以访问（遵循访问权限）。</p><h3 id="内变量内存布局"><a href="#内变量内存布局" class="headerlink" title="内变量内存布局"></a>内变量内存布局</h3><p>JDK8之前，静态域是在方法区的。</p><p>JDK8之后，静态域是放在堆中的。在class加载时，通过反射机制，在堆中生成一个class对象，static变量保存在class实例的尾部。</p><h3 id="类变量使用细节"><a href="#类变量使用细节" class="headerlink" title="类变量使用细节"></a>类变量使用细节</h3><ol><li><p>什么时候需要用类变量按我们需要让某个类的所有对象都共享一个变量时，就可以考虑使用类变量。</p></li><li><p>类变量与实例变量的区别</p><p>类变量是该类的所有对象共享的，而实例变量是每个对象独享的。</p></li><li><p>加上Static成为静态变量或者类变量，否则称为实例变量&#x2F;普通变量&#x2F;非静态变量</p></li><li><p>类变量可以通过类名。类变量或者对象名.类变量名来访问，但java设计者推荐我们使用类名.类变量名方式访问。【满足访问修饰符的访问权限和范围】</p></li><li><p>实例变量不能通过类名.类变量名 方式访问</p></li><li><p>类变量是在类加载时就初始化了，也就是说，即使你没有创建对象，只要类加载了，就可以使用类变量了。</p></li><li><p>周期是随类的加载开始，随着类的消亡而销毁</p></li></ol><h3 id="类方法"><a href="#类方法" class="headerlink" title="类方法"></a>类方法</h3><p>类方法也叫静态方法，<code>类名.类方法名</code> 调用</p><pre><code class="java">访问修饰符 static 数据返回类型 方法名()&#123;&#125;</code></pre><h4 id="类方法注意事项"><a href="#类方法注意事项" class="headerlink" title="类方法注意事项"></a>类方法注意事项</h4><ol><li><p>类方法和普通方法都是随着类的加载而加载，将结构信息存储在方法区</p><p>类方法中无this的参数，static和类挂钩，和对象没关系了。</p><p>普通方法中隐含着this参数</p></li><li><p>类方法可以通过类名调用，也可以通过对象名调用</p></li><li><p>普通方法和对象有关，需要通过对象名调用，不能通过类名调用</p></li><li><p>类方法中不允许使用和对象有关的关键字，比如this和super，普通方法可以。</p></li><li><p>静态方法中，只能访问静态变量或静态方法</p></li><li><p>普通成员方法，及可以访问普通变量(方法)，也可以访问静态变量（方法）</p></li></ol><h2 id="深入理解-main-方法"><a href="#深入理解-main-方法" class="headerlink" title="深入理解 main 方法"></a>深入理解 main 方法</h2><p>main方法的形式：</p><pre><code class="java">public static void main(Static[] args)&#123;&#125;</code></pre><p><img src="/java/java-xue-xi-wen-dang/image-20220612095524793.png" alt="image-20220612095524793"></p><h2 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h2><p>代码块又称为&#x3D;&#x3D;初始化块&#x3D;&#x3D;，属于类中的成员，类似于方法，讲逻辑语句封装在方法体中，通过{}包围起来</p><p>但和方法不同，没有方法名，没有返回，没有参数，只有方法体，不通过对象或类显式调用，而是加载类时，或创建对象时&#x3D;&#x3D;隐式调用&#x3D;&#x3D;</p><h3 id="基本语法-1"><a href="#基本语法-1" class="headerlink" title="基本语法"></a>基本语法</h3><pre><code class="java">[修饰符]&#123;代码&#125;</code></pre><ol><li><p>修饰符可选，要写也只能选static</p></li><li><p>代码块分为两类，使用static修饰的叫静态代码块，没有static修饰的，叫普通代码块。</p></li><li><p>逻辑语句可以为任何逻辑语句</p></li></ol><h3 id="代码块的好处"><a href="#代码块的好处" class="headerlink" title="代码块的好处"></a>代码块的好处</h3><ol><li><p>相当于另一种形式构造器（对构造器的补充机制），可以做初始化操作。</p><p>&#x3D;&#x3D;普通代码块在构造器执行之前被调用，被创建一次，就会调用一次&#x3D;&#x3D;</p><p>&#x3D;&#x3D;静态代码块只会执行一次，在类加载时执行&#x3D;&#x3D;</p></li></ol><h3 id="代码块使用注意事项"><a href="#代码块使用注意事项" class="headerlink" title="代码块使用注意事项"></a>代码块使用注意事项</h3><ol><li><p>static代码块也叫静态代码块，作用是对类进行初始化，而且它随着类的加载而执行，并且只会执行一次。如果是普通代码块，每创建一个对象，就执行。</p></li><li><p>类什么时候被加载</p><ol><li>创建对象实例时（new）</li><li>创建子类对象实例，父类也被加载</li><li>使用类的静态成员时（静态属性，静态方法）</li></ol></li><li><p>普通的代码块，在创建对象实例时，会被隐式调用</p><p>被创建一次，就会调用一次</p><p>如果只是使用类的静态成员时，普通代码块并不会被执行</p></li><li><p>创建一个对象时，在一个类的调用顺序是：</p><ul><li>调用静态代码块和静态属性初始化（静态代码块和静态属性初始化调用的优先级一样，如果有多个静态代码块和多个静态变量初始化，则按他们定义的顺序调用）</li><li>调用普通代码块和普通属性的初始化（普通代码块和普通初始化调用优先级一样，如果多个普通代码块和多个普通属性初始化，则按定义顺序调用）</li><li>最后调用构造方法</li></ul></li><li><p>构造器最前面其实隐含了super（）和调用普通代码块</p></li></ol><pre><code class="java">class A&#123;public  A&#123;1. super()//加载父类2. 调用普通代码块System.out.print（&quot;ok&quot;）    &#125;&#125;</code></pre><ol start="6"><li>创建子类对象时，<strong>调用顺序</strong>:<ol><li>父类的静态代码块和静态属性（按定义顺序调用）</li><li>子类的静态代码块和静态属性（按定义顺序调用）</li><li>父类的普通代码块和普通属性（按定义顺序调用）</li><li>父类的构造器</li><li>子类的普通代码块和普通属性（按定义顺序调用）</li><li>子类的构造器</li></ol></li><li>静态代码块只能直接调用静态成员（静态属性和静态方法），普通代码块可以调用任意成员</li></ol><h2 id="单例设计模式"><a href="#单例设计模式" class="headerlink" title="单例设计模式"></a>单例设计模式</h2><h3 id="什么是设计模式"><a href="#什么是设计模式" class="headerlink" title="什么是设计模式"></a>什么是设计模式</h3><ol><li>静态方法和属性的经典使用</li><li>设计模式是大量实践中总结和理论化之后优选的代码结构，编程风格，以及解决问题的思考方式</li></ol><h3 id="什么是单例模式"><a href="#什么是单例模式" class="headerlink" title="什么是单例模式"></a>什么是单例模式</h3><ol><li>所谓类的单例设计模式，就是采取一定的方法保证在整个软件系统中对某个类只能存在一个对象实例，并且该类只提供一个取得其对象实例的方法</li><li>单例模式有两种方式：1）饿汉式 2）懒汉式</li></ol><h3 id="单例模式实例"><a href="#单例模式实例" class="headerlink" title="单例模式实例"></a>单例模式实例</h3><ol><li>构造器私有化，防止new</li><li>类的内部创建对象</li><li>向外暴露一个静态的公共方法</li></ol><h4 id="饿汉式"><a href="#饿汉式" class="headerlink" title="饿汉式"></a>饿汉式</h4><pre><code class="java">//有一个类， GirlFriend（饿汉式）//只能有一个女朋友class GirlFriend &#123;private String name;//为了能够在静态方法中，返回 gf 对象，需要将其修饰为 static//對象，通常是重量級的對象, 餓漢式可能造成創建了對象，但是沒有使用. private static GirlFriend gf = new GirlFriend(&quot;小红红&quot;);//如何保障我们只能创建一个 GirlFriend 对象//步骤[单例模式-饿汉式]//1. 将构造器私有化//2. 在类的内部直接创建对象(该对象是 static)//3. 提供一个公共的 static 方法，返回 gf 对象private GirlFriend(String name) &#123;System.out.println(&quot;構造器被調用.&quot;);this.name = name;&#125;public static GirlFriend getInstance() &#123;return gf;&#125;@Overridepublic String toString() &#123;return &quot;GirlFriend&#123;&quot; +&quot;name=&#39;&quot; + name + &#39;\&#39;&#39; +&#39;&#125;&#39;;&#125;&#125;</code></pre><h4 id="懒汉式"><a href="#懒汉式" class="headerlink" title="懒汉式"></a>懒汉式</h4><pre><code class="java">//使用單例模式class Cat &#123;private String name;public static int n1 = 999;private static Cat cat ; //默認是 null//步驟//1.仍然構造器私有化//2.定義一個 static 靜態屬性對象//3.提供一個 public 的 static 方法，可以返回一個 Cat 對象//4.懶漢式，只有當用戶使用 getInstance 時，才返回 cat 對象, 後面再次調用時，會返回上次創建的 cat 對象// 從而保證了單例private Cat(String name) &#123;System.out.println(&quot;構造器調用...&quot;);this.name = name;&#125;    public static Cat getInstance() &#123;if(cat == null) &#123;//如果還沒有創建 cat 對象cat = new Cat(&quot;小可愛&quot;);&#125;return cat;&#125;    @Overridepublic String toString() &#123;return &quot;Cat&#123;&quot; +&quot;name=&#39;&quot; + name + &#39;\&#39;&#39; +&#39;&#125;&#39;;&#125;&#125;</code></pre><h3 id="饿汉式-VS-懒汉式"><a href="#饿汉式-VS-懒汉式" class="headerlink" title="饿汉式 VS 懒汉式"></a>饿汉式 VS 懒汉式</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220612105855835.png" alt="image-20220612105855835"></p><h2 id="final关键字"><a href="#final关键字" class="headerlink" title="final关键字"></a>final关键字</h2><p>final中文意思：最终的，最后的</p><p>final可以修饰类、属性、方法、局部变量</p><p>某些情况下有以下需求，就会使用到final</p><ol><li>不希望类被继承时，使用final</li><li>不希望父类的某个方法被子类重写（override），使用final</li><li>不希望类的某个属性值被修改，使用final</li><li>不希望某个局部变量被修改，使用final</li></ol><h3 id="final注意事项"><a href="#final注意事项" class="headerlink" title="final注意事项"></a>final注意事项</h3><ol><li>final修饰的属性又叫常量，一般用XX_XX_XX命名</li><li>final修饰的属性在定义是，必须赋初值，并以后不能修改，赋值位置可以在以下位置：<ol><li>定义时</li><li>构造器</li><li>代码块</li></ol></li><li>如果final修饰的属性是静态的，则初始化的位置只能是<ol><li>定义时</li><li>静态代码块，不能再构造器中赋值</li></ol></li><li>final类不能被继承，但可以实例化对象</li><li>如果类不是final类，但含有final方法，该方法不能被重写，但可以被继承。</li><li>一般来说，一个类已经是final类了，就没有必要再将方法修饰成final方法</li><li>final不能修饰构造方法（即构造器）</li><li>final和static往往搭配使用，效率更高，不会导致类加载，底层编译器做了优化。</li><li>包装类都是final，String也是final</li></ol><h2 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h2><p>当父类的某些方法，需要声明，但不确定如何实现，可以将其声明为抽象方法，那么这个类就是抽象类。</p><pre><code class="java">abstract class Animal &#123;private String name;public Animal(String name) &#123;this.name = name;&#125;//思考：这里 eat 这里你实现了，其实没有什么意义//即： 父类方法不确定性的问题//===&gt; 考虑将该方法设计为抽象(abstract)方法//===&gt; 所谓抽象方法就是没有实现的方法//===&gt; 所谓没有实现就是指，没有方法体//===&gt; 当一个类中存在抽象方法时，需要将该类声明为 abstract 类//===&gt; 一般来说，抽象类会被继承，有其子类来实现抽象方法. // public void eat() &#123;// System.out.println(&quot;这是一个动物，但是不知道吃什么..&quot;);// &#125;public abstract void eat() ;&#125;</code></pre><h3 id="抽象类语法"><a href="#抽象类语法" class="headerlink" title="抽象类语法"></a>抽象类语法</h3><pre><code class="java">abstract class Animal&#123;String name;int age;    abstract public void cry();&#125;</code></pre><ol><li>用abstract 关键字修饰一个类，这个类就叫抽象类</li><li>用abstract关键字修饰一个方法，这个方法就叫抽象方法</li><li>抽象类的价值更多在于设计，设计者设计好后，让子类继承并实现</li><li>抽象类，在框架和设计模式使用较多。</li></ol><h3 id="抽象类注意事项"><a href="#抽象类注意事项" class="headerlink" title="抽象类注意事项"></a>抽象类注意事项</h3><ol><li><p>抽象类<strong>不能被实例化</strong></p></li><li><p>抽象类不一定要包含abstract方法，也就是说抽象类可以没有抽象方法，还可以有实现方法</p></li><li><p>一旦类包含了abstract方法，则这个类必须声明为abstract</p></li><li><p>abstract只能修饰类和方法，不能修饰属性和其他</p></li><li><p>抽象类可以有任意成员，非抽象方法，构造器，静态属性等等</p></li><li><p>抽象方法不能有主体（方法体）</p></li><li><p>如果一个类继承了抽象类，则它必须实现抽象类的所有抽象方法，除非他自己也声明为abstract类</p></li><li><p>抽象方法不能使用 private、final 和 static 来修饰，因为这些关键字都是和重写相违背的</p></li></ol><h2 id="模板设计模式-抽象类最佳实践"><a href="#模板设计模式-抽象类最佳实践" class="headerlink" title="模板设计模式-抽象类最佳实践"></a>模板设计模式-抽象类最佳实践</h2><p>抽象类体现的就是一种模板模式的设计，抽象类作为多个子类的通用模板，子类在抽象类的基础上进行扩展，改造，当子类总体上汇保留抽象类的行为方式</p><h3 id="模板设计模式解决的问题"><a href="#模板设计模式解决的问题" class="headerlink" title="模板设计模式解决的问题"></a>模板设计模式解决的问题</h3><ol><li>当一部分实现是确定的，一部分实现不确定时。可以把不确定的部分暴露出去，让子类实现。</li><li>抽象父类，父类提供多个子类通用方法，并把一个或多个方法留给子类实现，就是一种模板模式。</li></ol><h3 id="模板模式实例"><a href="#模板模式实例" class="headerlink" title="模板模式实例"></a>模板模式实例</h3><pre><code class="java">abstract public class Template &#123; //抽象类-模板设计模式public abstract void job();//抽象方法public void calculateTime() &#123;//实现方法，调用 job 方法//得到开始的时间long start = System.currentTimeMillis();job(); //动态绑定机制//得的结束的时间long end = System.currentTimeMillis();System.out.println(&quot;任务执行时间 &quot; + (end - start));&#125;&#125;public class AA extends Template &#123; @Overridepublic void job() &#123; //实现 Template 的抽象方法 joblong num = 0;for (long i = 1; i &lt;= 800000; i++) &#123;num += i;&#125;   &#125;        public class BB extends Template&#123;public void job() &#123;//这里也去，重写了 Template 的 job 方法long num = 0;for (long i = 1; i &lt;= 80000; i++) &#123;num *= i;&#125;&#125;&#125;        public class TestTemplate &#123;public static void main(String[] args) &#123;AA aa = new AA();aa.calculateTime(); //这里还是需要有良好的 OOP 基础，对多态BB bb = new BB();bb.calculateTime();&#125;&#125;</code></pre><h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2><p>接口是更加抽象的抽象类，抽象类里的方法可以有方法体，接口里的所有方法都没有方法体。接口体现了程序设计的多肽和高内聚低耦合的设计思想。</p><p>特别说明：JDK8.0之后的接口类可以有<strong>静态方法</strong>，<strong>默认方法</strong>（default修饰方法），也就是说接口可以有方法的具体实现。JDK7.0之前接口的所有方法都没有方法体。</p><h3 id="接口注意事项"><a href="#接口注意事项" class="headerlink" title="接口注意事项"></a>接口注意事项</h3><ol><li><p>接口不能被实例化</p></li><li><p>接口中所有方法都是public方法，就口中抽象方法，不用abstract修饰</p></li><li><p>一个普通类实现接口，就必须将接口的所有方法都实现</p></li><li><p>抽象类实现接口，可以不用实现接口的方法</p></li><li><p>一个类可以实现多个接口</p></li><li><p>接口的属性，只能是final的，而且是public static final 修饰符。</p><p>例如：int n1 &#x3D; 10; &#x2F;&#x2F;等价 public static final int n1 &#x3D; 10</p></li><li><p>接口中属性的访问形式：接口名.属性名</p></li><li><p>接口不能继承其他类，但是可以继承多个别的接口</p></li><li><p>接口修饰符只能是public和默认，这点和类的修饰符是一样的</p></li></ol><h2 id="接口-VS-抽象类（继承类）"><a href="#接口-VS-抽象类（继承类）" class="headerlink" title="接口 VS 抽象类（继承类）"></a>接口 VS 抽象类（继承类）</h2><ul><li>解决的问题不同</li></ul><p>继承的价值在于：解决代码的复用性和可维护性</p><p>接口的价值在于：设计，设计好各种规范（方法），让其它类去实现这个方法。及更加的灵活。</p><ul><li>接口比继承更加灵活</li></ul><p>接口比继承更加灵活，继承是is-a的关系，而接口只需要满足like -a的关系</p><ul><li>接口一定程度上实现代码解耦【接口规范性+动态绑定机制】</li></ul><ol><li>多态参数</li><li>多态数组</li><li>接口存在多态传递现象。</li></ol><pre><code class="java">/*** 演示多态传递现象*/public class InterfacePolyPass &#123;public static void main(String[] args) &#123;//接口类型的变量可以指向，实现了该接口的类的对象实例IG ig = new Teacher();//如果 IG 继承了 IH 接口，而 Teacher 类实现了 IG 接口//那么，实际上就相当于 Teacher 类也实现了 IH 接口. //这就是所谓的 接口多态传递现象. IH ih = new Teacher();&#125;&#125;interface IH &#123;void hi();&#125;interface IG extends IH&#123; &#125;class Teacher implements IG &#123;@Overridepublic void hi() &#123;&#125;&#125;</code></pre><h2 id="内部类"><a href="#内部类" class="headerlink" title="内部类"></a>内部类</h2><p>一个类的内部又完整的嵌套了另一个类结构。被嵌套的类被称为内部类（inner class），嵌套其他类的类称为外部类（outer class）。是我们类的第五大成员【属性，方法，构造器，代码块，内部类】内部类的最大特点就是可以直接访问私有属性，并且可以体现类与类之间的包含关系。</p><p>注意，内部类是学习的难点，同时也是重点，底层源码有大量的内部类</p><h3 id="基本语法-2"><a href="#基本语法-2" class="headerlink" title="基本语法"></a>基本语法</h3><pre><code class="java">class Outer&#123;//外部类    class intter&#123;//内部类        &#125;&#125;</code></pre><h3 id="内部类的分类"><a href="#内部类的分类" class="headerlink" title="内部类的分类"></a>内部类的分类</h3><p>如果定义类在局部位置(方法中&#x2F;代码块) :</p><p>(1) 局部内部类（有类名） </p><p>(2) 匿名内部类（没有类名，重点） </p><p>定义在成员位置 </p><p>(1) 成员内部类 （没用static修饰）</p><p>(2) 静态内部类（使用static修饰）</p><h3 id="局部内部类的使用"><a href="#局部内部类的使用" class="headerlink" title="局部内部类的使用"></a>局部内部类的使用</h3><ol><li><p>可以直接访问外部类的所有成员，包含私有</p></li><li><p>不能添加访问修饰符，因为它的地位就是一个局部变量。局部变量不能使用访问修饰符。但是可以使用final修饰，因为局部变量也可以使用final</p></li><li><p>作用域：仅仅在定义它的方法或代码块中。</p></li><li><p>局部内部类–》访问–》外部类的成员【访问方式：直接访问】</p></li><li><p>外部类–》访问–》局部内部类的成员</p><p>访问方式：创建对象，在访问（必须在作用域内）</p><p>记住：</p><ol><li><p>局部内部类定义在方法中&#x2F;代码块</p></li><li><p>作用于在方法体或者代码块中 </p></li><li><p>本质仍然是一个类</p></li></ol></li><li><p>外部其他类–&gt;不能访问–&gt;局部内部类（因为局部内部类地位是一个局部变量）</p></li><li><p>如果外部类和局部内部类的成员重名，遵循就近原则，如果想要访问外部类的成员，可以使用（外部类名.this.成员）去访问</p></li></ol><h3 id="匿名内部类的使用"><a href="#匿名内部类的使用" class="headerlink" title="匿名内部类的使用"></a>匿名内部类的使用</h3><ol><li><p>本质还是类、内部类，该类没有名字，同时还是一个对象</p><p>说明：匿名内部类是定在外部类的局部位置，比如方法中，并且没有类名</p></li><li><p>匿名内部类语法</p></li></ol><pre><code class="java">new 类或者接口（参数列表）&#123;&#125;</code></pre><ol start="2"><li>匿名内部类的既是一个类的定义，同时它本身也是一个对象，因此从语法上看，它既有定义类的特征，也有创建对象的特征，对前面代码分析可以看出这个特点，因此可以调用匿名内部类方法。</li><li>可以直接访问外部类的所有成员，包含私有的</li><li>不能添加访问修饰符，因为它的地位就是一个局部变量。局部变量不能使用访问修饰符。</li><li>作用域：仅仅在定义它的方法或代码块中。</li><li>匿名内部类–》访问–》外部类的成员【访问方式：直接访问】</li><li>外部其他类–》不能访问–》匿名内部类（匿名内部类地位是一个局部变量）</li><li>外部类和匿名内部类成员重名时，匿名内部类访问遵循就近原则。如果想要访问外部类的成员，可以使用（外部类名.this.成员）去访问</li></ol><h4 id="匿名内部类实例"><a href="#匿名内部类实例" class="headerlink" title="匿名内部类实例"></a>匿名内部类实例</h4><p>匿名类的名称是调用匿名类的外部类名称+$1，如果有多个匿名类则累加值</p><h5 id="基于接口的匿名内部类"><a href="#基于接口的匿名内部类" class="headerlink" title="基于接口的匿名内部类"></a>基于接口的匿名内部类</h5><pre><code class="java">public class AnonymousInnerClass &#123;    public static void main(String[] args) &#123;        IA anony = new IA() &#123;            @Override            public void cry() &#123;                System.out.println(&quot;匿名内部类&quot;);            &#125;        &#125;;        anony.cry();        //编译类型是：interface hsppedu.Homework.anonymousInnerhomework.IA        System.out.println(&quot;编译类型是：&quot;+IA.class);        //运行类型是:class hsppedu.Homework.anonymousInnerhomework.AnonymousInnerClass$1        System.out.println(&quot;运行类型是:&quot;+anony.getClass());    &#125;&#125;interface IA &#123;//接口    public void cry();&#125;</code></pre><h5 id="基于类的匿名内部类"><a href="#基于类的匿名内部类" class="headerlink" title="基于类的匿名内部类"></a>基于类的匿名内部类</h5><pre><code class="java">public class AnonymousInnerClass &#123;    public static void main(String[] args) &#123;     Father anony = new Father(&quot;ff&quot;)&#123;            @Override            public void test() &#123;                System.out.println(&quot;重写了test方法&quot;);            &#125;        &#125;;        anony.test();        //编译类型是：class hsppedu.Homework.anonymousInnerhomework.Father        System.out.println(&quot;编译类型是：&quot;+Father.class);        //运行类型是:class hsppedu.Homework.anonymousInnerhomework.AnonymousInnerClass$1        System.out.println(&quot;运行类型是:&quot;+anony.getClass());    &#125;&#125;class Father&#123;    public Father(String name) &#123;//构造器        System.out.println(&quot;接收到 name=&quot; + name);    &#125;    public void test() &#123;//方法    &#125;&#125;//重写了test方法//编译类型是：class hsppedu.Homework.anonymousInnerhomework.Father//运行类型是:class hsppedu.Homework.anonymousInnerhomework.AnonymousInnerClass$1</code></pre><h4 id="匿名内部类最佳实践"><a href="#匿名内部类最佳实践" class="headerlink" title="匿名内部类最佳实践"></a>匿名内部类最佳实践</h4><pre><code class="java">public class InnerClassExercise01 &#123;public static void main(String[] args) &#123;//当做实参直接传递，简洁高效f1(new IL() &#123;@Overridepublic void show() &#123;System.out.println(&quot;这是一副名画~~...&quot;);&#125;&#125;);    //传统方法f1(new Picture());&#125;    //静态方法,形参是接口类型public static void f1(IL il) &#123;il.show();&#125;&#125;//接口interface IL &#123;void show();&#125;//类-&gt;实现 IL =&gt; 编程领域 (硬编码)class Picture implements IL &#123;@Overridepublic void show() &#123;System.out.println(&quot;这是一副名画 XX...&quot;);&#125;&#125;</code></pre><pre><code class="java">public class InnerClassExercise01 &#123;public static void main(String[] args) &#123;//当做实参直接传递，简洁高效f1(new IL() &#123;@Overridepublic void show() &#123;System.out.println(&quot;这是一副名画~~...&quot;);&#125;&#125;);    //传统方法f1(new Picture());&#125;    //静态方法,形参是接口类型public static void f1(IL il) &#123;il.show();&#125;&#125;//接口interface IL &#123;void show();&#125;//类-&gt;实现 IL =&gt; 编程领域 (硬编码)class Picture implements IL &#123;@Overridepublic void show() &#123;System.out.println(&quot;这是一副名画 XX...&quot;);&#125;&#125;</code></pre><h3 id="成员内部类"><a href="#成员内部类" class="headerlink" title="成员内部类"></a>成员内部类</h3><p>成员内部类是<strong>定义在外部类的成员位置</strong>，并且没有static修饰</p><ol><li>可以直接访问外部类的所有成员，包含私有</li><li>可以添加任意访问修饰符（public,protected,默认,private）,因为他的地位就是一个成员。</li><li>作用域 MemberInnerClass01和外部类的其他成员一样，为整个类</li><li>成员内部类–&gt;访问–&gt;外部类成员【访问方式：直接访问】</li><li>外部类–&gt;访问–&gt;成员内部类【访问方式：创建对象，在访问】</li><li>外部其他类–&gt;访问–&gt;成员内部类</li><li>如果外部类和内部类的成员重名时，内部类访问，默认遵循就近原则，如果想访问外部类的成员，则可以使用（外部类名.this.成员）去访问</li></ol><pre><code class="java">class  outer01&#123;private int n1 = 10;    class Innter01&#123;        public void say()&#123;        System.out.println(&quot;Outer01 n1&quot;+n1);        &#125;    &#125;&#125;</code></pre><pre><code class="java">//外部其他类使用内部类outer01 outer01 = new outer01();Innter01 innter01 = outer01.new Innter01();</code></pre><h3 id="静态内部类"><a href="#静态内部类" class="headerlink" title="静态内部类"></a>静态内部类</h3><p>静态内部类是定义在外部类的成员位置上，并且<strong>有static修饰</strong></p><ol><li>可以直接访问外部类的所有静态成员，包含私有的，但不能直接访问非静态成员</li><li>可以添加任意访问修饰符（public、protected、默认、private）,因为他的地位就是一个成员</li><li>作用域：同其他的成员，为整个类体</li><li>静态内部类–&gt;访问–&gt;外部类【可以直接访问所有静态成员】</li><li>外部类–&gt;访问–&gt;静态内部类【创建对象，在访问】</li><li>外部其他类–访问–&gt;静态内部类</li><li>如果外部类和静态内部类的成员重名时，静态内部类访问的时，默认遵守就近原则，如果先访问外部类的成员，则可以使用（外部类.成员）访问</li></ol><pre><code class="java">class Outer10 &#123; //外部类private int n1 = 10;private static String name = &quot;张三&quot;;private static void cry() &#123;&#125;/Inner10 就是静态内部类//1. 放在外部类的成员位置//2. 使用 static 修饰//3. 可以直接访问外部类的所有静态成员，包含私有的，但不能直接访问非静态成员//4. 可以添加任意访问修饰符(public、protected 、默认、private),因为它的地位就是一个成员//5. 作用域 ：同其他的成员，为整个类体static class Inner10 &#123;private static String name = &quot;韩顺平教育&quot;;public void say() &#123;//如果外部类和静态内部类的成员重名时，静态内部类访问的时，//默认遵循就近原则，如果想访问外部类的成员，则可以使用 （外部类名.成员）System.out.println(name + &quot; 外部类 name= &quot; + Outer10.name);cry();&#125;&#125;public void m1() &#123; //外部类---访问------&gt;静态内部类 访问方式：创建对象，再访问Inner10 inner10 = new Inner10();inner10.say();&#125;public Inner10 getInner10() &#123;return new Inner10();&#125;public static Inner10 getInner10_() &#123;return new Inner10();&#125;&#125;</code></pre><h1 id="枚举和注解"><a href="#枚举和注解" class="headerlink" title="枚举和注解"></a>枚举和注解</h1><h2 id="枚举"><a href="#枚举" class="headerlink" title="枚举"></a>枚举</h2><ol><li>枚举对应英文(enumeration, 简写 enum)</li><li>枚举是一组常量的集合。</li><li>可以这里理解：枚举属于一种特殊的类，里面只包含一组有限的特定的对象。</li></ol><h2 id="枚举的两种实现方式"><a href="#枚举的两种实现方式" class="headerlink" title="枚举的两种实现方式"></a>枚举的两种实现方式</h2><ol><li>自定义类实现枚举</li><li>使用enum 关键字实现枚举</li></ol><h3 id="自定义实现枚举"><a href="#自定义实现枚举" class="headerlink" title="自定义实现枚举"></a>自定义实现枚举</h3><ol><li>构造器私有化，防止直接new  </li><li>不需要提供set方法，因为枚举对象是只读的，防止修改</li><li>对枚举对象&#x2F;属性使用final+static共同修饰，实现底层优化</li><li>枚举对象名通常使用全部大写，常量的命名规范</li><li>枚举对象根据需要，也可以有多个属性</li></ol><pre><code class="java">//演示字定义枚举实现class Season &#123;//类private String name;private String desc;//描述//定义了四个对象, 固定. public static final Season SPRING = new Season(&quot;春天&quot;, &quot;温暖&quot;);public static final Season WINTER = new Season(&quot;冬天&quot;, &quot;寒冷&quot;);public static final Season AUTUMN = new Season(&quot;秋天&quot;, &quot;凉爽&quot;);public static final Season SUMMER = new Season(&quot;夏天&quot;, &quot;炎热&quot;);//1. 将构造器私有化,目的防止 直接 new//2. 去掉 setXxx 方法, 防止属性被修改//3. 在 Season 内部，直接创建固定的对象//4. 优化，可以加入 final 修饰private Season(String name, String desc) &#123;this.name = name;this.desc = desc;&#125;public String getName() &#123;return name;&#125;public String getDesc() &#123;return desc;&#125;@Overridepublic String toString() &#123;return &quot;Season&#123;&quot; +&quot;name=&#39;&quot; + name + &#39;\&#39;&#39; +&quot;, desc=&#39;&quot; + desc + &#39;\&#39;&#39; +&#39;&#125;&#39;;&#125;&#125;</code></pre><h3 id="enum关键字实现枚举"><a href="#enum关键字实现枚举" class="headerlink" title="enum关键字实现枚举"></a>enum关键字实现枚举</h3><pre><code class="java">//演示使用 enum 关键字来实现枚举类enum Season2 &#123;//类//定义了四个对象, 固定. // public static final Season SPRING = new Season(&quot;春天&quot;, &quot;温暖&quot;);// public static final Season WINTER = new Season(&quot;冬天&quot;, &quot;寒冷&quot;);// public static final Season AUTUMN = new Season(&quot;秋天&quot;, &quot;凉爽&quot;);// public static final Season SUMMER = new Season(&quot;夏天&quot;, &quot;炎热&quot;);//如果使用了 enum 来实现枚举类//1. 使用关键字 enum 替代 class//2. public static final Season SPRING = new Season(&quot;春天&quot;, &quot;温暖&quot;) 直接使用// SPRING(&quot;春天&quot;, &quot;温暖&quot;) 解读 常量名(实参列表)//3. 如果有多个常量(对象)， 使用 ,号间隔即可//4. 如果使用 enum 来实现枚举，要求将定义常量对象，写在前面//5. 如果我们使用的是无参构造器，创建常量对象，则可以省略 ()SPRING(&quot;春天&quot;, &quot;温暖&quot;), WINTER(&quot;冬天&quot;, &quot;寒冷&quot;), AUTUMN(&quot;秋天&quot;, &quot;凉爽&quot;), SUMMER(&quot;夏天&quot;, &quot;炎热&quot;)/*, What()*/;private String name;private String desc;//描述private Season2() &#123;//无参构造器&#125;private Season2(String name, String desc) &#123;this.name = name;this.desc = desc;&#125;public String getName() &#123;return name;&#125;public String getDesc() &#123;return desc;&#125;@Overridepublic String toString() &#123;return &quot;Season&#123;&quot; +&quot;name=&#39;&quot; + name + &#39;\&#39;&#39; +&quot;, desc=&#39;&quot; + desc + &#39;\&#39;&#39; +&#39;&#125;&#39;;&#125;&#125;</code></pre><h3 id="enum-关键字实现枚举注意事项"><a href="#enum-关键字实现枚举注意事项" class="headerlink" title="enum 关键字实现枚举注意事项"></a>enum 关键字实现枚举注意事项</h3><ol><li>当我们使用 enum 关键字开发一个枚举类时，默认会继承 Enum 类, 而且是一个 final 类</li><li>传统的 public static final Season2 SPRING &#x3D; new Season2(“春天”, “温暖”); 简化成 SPRING(“春天”, “温暖”)， 这里必须知道，它调用的是哪个构造器.</li><li>如果使用无参构造器 创建 枚举对象，则实参列表和小括号都可以省略</li><li>当有多个枚举对象时，使用’,’间隔，最后有一个分号结尾</li><li>枚举对象必须放在枚举类的行首</li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220612184831178.png" alt="image-20220612184831178"></p><h3 id="enum常用方法说明"><a href="#enum常用方法说明" class="headerlink" title="enum常用方法说明"></a>enum常用方法说明</h3><p>使用关键字&#x3D;&#x3D;enum&#x3D;&#x3D;时，会隐式继承Enum类，这样我们就可以使用Enum类相关方法</p><p><img src="/java/java-xue-xi-wen-dang/image-20220612190548580.png" alt="image-20220612190548580"></p><table><thead><tr><th>方法名</th><th>详细描述</th></tr></thead><tbody><tr><td>name</td><td>获取枚举对象的名称</td></tr><tr><td>ordinal</td><td>获取枚举对象的次序，从0开始</td></tr><tr><td>values</td><td>隐藏方法，返回一个数组，class【】，包含所有的枚举对象</td></tr><tr><td>valueof</td><td>传入字符串，在枚举类中查找，如果找到了，就返回一个枚举对象，没有则会报错。java.lang.IllegalArgumentException</td></tr><tr><td>compareTo</td><td>比较两个枚举对象的大小编号，就是ordinal的次序号码，返回数值</td></tr></tbody></table><h2 id="enum-实现接口"><a href="#enum-实现接口" class="headerlink" title="enum 实现接口"></a>enum 实现接口</h2><ol><li>使用enum关键字后，就不能在继承其他类了，因为enum会隐式继承Enum，而java是单继承机制。</li><li>枚举类和普通类一样，可以实现接口</li></ol><pre><code class="java">enum 类名 implements 接口1,接口2&#123;&#125;</code></pre><h2 id="注解的理解"><a href="#注解的理解" class="headerlink" title="注解的理解"></a>注解的理解</h2><ol><li>注解(Annotation)也被称为元数据(Metadata)，用于修饰解释 包、类、方法、属性、构造器、局部变量等数据信息。</li><li>和注释一样，注解不影响程序逻辑，但注解可以被编译或运行，相当于嵌入在代码中的补充信息。 </li><li>在 JavaSE 中，注解的使用目的比较简单，例如标记过时的功能，忽略警告等。在 JavaEE 中注解占据了更重要的角 色，例如用来配置应用程序的任何切面，代替 java EE 旧版中所遗留的繁冗代码和 XML 配置</li></ol><h2 id="JDK内置的基本注解类型"><a href="#JDK内置的基本注解类型" class="headerlink" title="JDK内置的基本注解类型"></a>JDK内置的基本注解类型</h2><p>使用 Annotation 时要在其前面增加 @ 符号, 并把该 Annotation 当成一个修饰符使用。用于修饰它支持的程序元 素 三个基本的 Annotation: </p><ol><li><p>@Override: 限定某个方法，是重写父类方法, 该注解只能用于方法</p><ul><li>@Override表示重写父类方法，如果父类方法没有被修饰的方法，则会报错</li><li>如果不写@Override注解，而父类仍有相同方法，仍构成重写</li><li>查看@Override注解源码为@Target(ElementType.METHOD),说明只能修饰方法</li><li>@Target是修饰注解的注解，称为元注解。</li></ul></li><li><p>@Deprecated: 用于表示某个程序元素(类, 方法等)已过时</p><ul><li>即不再推荐使用，仍然可以使用</li><li>可以修饰方法，类，字段，包，参数，等等</li><li>做版本升级过渡使用</li></ul></li><li><p>@SuppressWarnings: 抑制编译器警告</p><ul><li>但不希望看到警告时，可以使用@SuppressWarnings抑制警告</li><li>在{“”}中，可以写入你希望抑制的警告信息</li><li>作用范围和你放置的位置相关，通常我们放在具体的语句，方法，类上</li></ul><pre><code>@SuppressWarning 中的属性介绍以及属性说明all，抑制所有警告boxing，抑制与封装/拆装作业相关的警告cast，抑制与强制转型作业相关的警告dep-ann，抑制与淘汰注释相关的警告deprecation，抑制与淘汰的相关警告fallthrough，抑制与switch陈述式中遗漏break相关的警告finally，抑制与未传回finally区块相关的警告hiding，抑制与隐藏变数的区域变数相关的警告incomplete-switch，抑制与switch陈述式(enum case)中遗漏项目相关的警告javadoc，抑制与javadoc相关的警告nls，抑制与非nls字串文字相关的警告null，抑制与空值分析相关的警告rawtypes，抑制与使用raw类型相关的警告resource，抑制与使用Closeable类型的资源相关的警告restriction，抑制与使用不建议或禁止参照相关的警告serial，抑制与可序列化的类别遗漏serialVersionUID栏位相关的警告static-access，抑制与静态存取不正确相关的警告static-method，抑制与可能宣告为static的方法相关的警告super，抑制与置换方法相关但不含super呼叫的警告synthetic-access，抑制与内部类别的存取未最佳化相关的警告sync-override，抑制因为置换同步方法而遗漏同步化的警告unchecked，抑制与未检查的作业相关的警告unqualified-field-access，抑制与栏位存取不合格相关的警告unused，抑制与未用的程式码及停用的程式码相关的警告</code></pre></li></ol><h2 id="元注解：对注解进行注解"><a href="#元注解：对注解进行注解" class="headerlink" title="元注解：对注解进行注解"></a>元注解：对注解进行注解</h2><p>JDK的元Annotation用于修饰其他Annotation<br>了解即可</p><h3 id="元注解种类"><a href="#元注解种类" class="headerlink" title="元注解种类"></a>元注解种类</h3><ol><li>@Retention指定注解作用范围，三种 SOURCE,CLASS,RUNTIME</li><li>@Target指定注解可以在那些地方使用</li><li>@Documented指定该注解是否在javadoc体现</li><li>@Inherited子类会继承成父类注解</li></ol><h3 id="Retention注解"><a href="#Retention注解" class="headerlink" title="@Retention注解"></a>@Retention注解</h3><p>只能用于修饰一个 Annotation 定义, 用于指定该 Annotation 可以保留多长时间, @Rentention 包含一个 RetentionPolicy 类型的成员变量, 使用 @Rentention 时必须为该 value 成员变量指定值: @Retention 的三种值</p><ol><li>RetentionPolicy.SOURCE: 编译器使用后，直接丢弃这种策略的注释</li><li>RetentionPolicy.CLASS: 编译器将把注解记录在 class 文件中. 当运行 Java 程序时, JVM 不会保留注解。 这是默认值</li><li>RetentionPolicy.RUNTIME:编译器将把注解记录在 class 文件中. 当运行 Java 程序时, JVM 会保留注解. 程序可以 通过反射获取该注解</li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220612202436410.png" alt="image-20220612202436410"></p><h3 id="Target"><a href="#Target" class="headerlink" title="@Target"></a>@Target</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220612202507148.png" alt="image-20220612202507148"></p><h3 id="Documented"><a href="#Documented" class="headerlink" title="@Documented"></a>@Documented</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220612202538634.png" alt="image-20220612202538634"></p><h3 id="Inherited-注解"><a href="#Inherited-注解" class="headerlink" title="@Inherited 注解"></a>@Inherited 注解</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220612202601365.png" alt="image-20220612202601365"></p><h1 id="Exception"><a href="#Exception" class="headerlink" title="Exception"></a>Exception</h1><h2 id="异常概念"><a href="#异常概念" class="headerlink" title="异常概念"></a>异常概念</h2><p>Java语言中，将程序执行中发生的不正常情况称为异常。（开发过程中的语法错误和逻辑错误不是异常）</p><p>执行过程中的异常事件分为两大类</p><ol><li>Error（错误），Java虚拟机无法解决的严重问题。如：Java系统内部错误、资源耗尽等严重情况。如：StackOverflowError【栈溢出】和OOM【out of memory】，Error是严重错误，程序会崩溃。</li><li>Exception：其他因编程错误或偶然的外在因素导致的一般性问题，可以使用针对性的代码进行处理。例如空指针访问，试图读取不存在的文件，网络连接中断等等，Exception分为两大类：运行时异常【程序运行时，发生的异常】和编译时异常【编译器检查出的异常】。</li></ol><h2 id="异常体系图"><a href="#异常体系图" class="headerlink" title="异常体系图"></a>异常体系图</h2><p><img src="/java/java-xue-xi-wen-dang/image-20220613105233801.png" alt="image-20220613105233801"></p><h3 id="异常体系图小结"><a href="#异常体系图小结" class="headerlink" title="异常体系图小结"></a>异常体系图小结</h3><p>1．异常分为两大类，运行时异常和编译时异常．</p><p>2．运行时异常，编译器检查不出来。一般是指编程时的逻辑错误，是程序员应该避免其出现的异常。java.lang.RuntimeException类及它的子类都是运行时 异常</p><p>3．对于运行时异常，可以不作处理，因为这类异常很普遍，若全处理可能会对程序的可读性和运行效率产生影响</p><p>4．编译时异常，是编译器要求必须处置的异常。</p><h2 id="常见异常"><a href="#常见异常" class="headerlink" title="常见异常"></a>常见异常</h2><h3 id="常见的运行时异常"><a href="#常见的运行时异常" class="headerlink" title="常见的运行时异常"></a>常见的运行时异常</h3><ol><li>NullPointerException 空指针异常</li><li>ArithmeticException 数学运算异常 </li><li>ArrayIndexOutOfBoundsException 数组下标越界异常 </li><li>ClassCastException 类型转换异常 </li><li>NumberFormatException 数字格式不正确异常[]</li></ol><h3 id="编译异常"><a href="#编译异常" class="headerlink" title="编译异常"></a>编译异常</h3><p>编译异常是指在编译期间，就必须处理的异常，否则代码不能通过编译。</p><h3 id="常见的编译异常"><a href="#常见的编译异常" class="headerlink" title="常见的编译异常"></a>常见的编译异常</h3><ol><li>SQLException／／操作数据库时，查询表可能发生异常</li><li>IOException／／操作文件时，发生的异常</li><li>FileNotFoundException／／当操作一个不存在的文件时，发生异常</li><li>ClassNotFoundException／／加载类，而该类不存在时，异常 </li><li>EOFException／／操作文件，到文件末尾，发生异常</li><li>IllegalArguementException／／参数异常</li></ol><h2 id="异常的处理"><a href="#异常的处理" class="headerlink" title="异常的处理"></a>异常的处理</h2><h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><p>异常处理就是当异常发生时，对异常处理的方式</p><h3 id="异常处理的方式"><a href="#异常处理的方式" class="headerlink" title="异常处理的方式"></a>异常处理的方式</h3><ol><li>try-catch-finally</li></ol><p>程序员在代码中捕获发生的异常，自行处理</p><ol start="2"><li>throws</li></ol><p>将发生的异常抛出，交给调用者（方法）来处理，最顶级的处理者就是JVM</p><p><img src="/java/java-xue-xi-wen-dang/image-20220613110120165.png" alt="try-catch-finally机制图"></p><p><img src="/java/java-xue-xi-wen-dang/image-20220613110208766.png" alt="throws处理机制图"></p><h3 id="异常处理分类"><a href="#异常处理分类" class="headerlink" title="异常处理分类"></a>异常处理分类</h3><h4 id="try-catch-异常处理"><a href="#try-catch-异常处理" class="headerlink" title="try-catch 异常处理"></a>try-catch 异常处理</h4><p><img src="/java/java-xue-xi-wen-dang/image-20220613110534485.png" alt="image-20220613110534485"></p><h4 id="try-catch方式处理异常-注意事项"><a href="#try-catch方式处理异常-注意事项" class="headerlink" title="try-catch方式处理异常-注意事项"></a>try-catch方式处理异常-注意事项</h4><ol><li><p>如果异常发生了，则异常发生后面的代码不会执行，直接进入到catch块.</p></li><li><p>如果异常没有发生，则顺序执行try的代码块，不会进入到catch.</p></li><li><p>如果希望不管是否发生异常，都执行某段代码（比如关闭连接，释放资源等）则使用如下代码— finally{}</p></li></ol><pre><code class="java">try&#123;int a = Integer.parselnt(str);System.out.println（&quot;数字： &quot;+a）;&#125;catch (Exception e)&#123;e.printStackTrace(); &#125; finally&#123;System.out.println(&quot;不管是否发生异常，始终执行的代码～～&quot;);&#125;</code></pre><ol start="4"><li>可以有多个catch语句，捕获不同的异常（进行不同的业务处理），要求父类异常在后，自雷异常在前，比如（Exception在后，NullPointerException在前），如果发生异常，只会匹配一个catch。</li></ol><pre><code class="java">public static void main(String[] args) &#123;//老韩解读//1.如果 try 代码块有可能有多个异常//2.可以使用多个 catch 分别捕获不同的异常，相应处理//3.要求子类异常写在前面，父类异常写在后面try &#123;Person person = new Person();//person = null;System.out.println(person.getName());//NullPointerExceptionint n1 = 10;int n2 = 0;int res = n1 / n2;//ArithmeticException&#125; catch (NullPointerException e) &#123;System.out.println(&quot;空指针异常=&quot; + e.getMessage());&#125; catch (ArithmeticException e) &#123;System.out.println(&quot;算术异常=&quot; + e.getMessage());&#125; catch (Exception e) &#123;System.out.println(e.getMessage());&#125; finally &#123;&#125;&#125;&#125;class Person &#123;private String name = &quot;jack&quot;;public String getName() &#123;return name;&#125;&#125;</code></pre><ol start="5"><li>可以进行try-finally配合使用，这种用法相当于没有捕获异常，因此程序会直接崩掉&#x2F;退出。应用场景，就是执行一段代码，不管是否发生异常，都必须执行某个业务逻辑。</li></ol><pre><code class="java">public static void main(String[] args) &#123;/*可以进行 try-finally 配合使用, 这种用法相当于没有捕获异常，因此程序会直接崩掉/退出。应用场景，就是执行一段代码，不管是否发生异常，都必须执行某个业务逻辑*/try&#123;int n1 = 10;int n2 = 0;System.out.println(n1 / n2);&#125;finally &#123;System.out.println(&quot;执行了 finally..&quot;);&#125;System.out.println(&quot;程序继续执行..&quot;);&#125;</code></pre><h4 id="throws-异常处理"><a href="#throws-异常处理" class="headerlink" title="throws 异常处理"></a>throws 异常处理</h4><ol><li>如果一个方法中的语句执行时，可能生成某种异常，但是并不能确定如何处理这种异常，则此方法应显示的声明抛出异常，表明该方法将不对这些异常进行处理，而由该方法的调用者负责处理。</li><li>在方法声明中用throws语句可以声明抛出异常的列表，throws后面的异常类型可以是方法中产生的异常类型，也可以是它的父类。</li></ol><h4 id="throws-异常处理-注意事项"><a href="#throws-异常处理-注意事项" class="headerlink" title="throws 异常处理-注意事项"></a>throws 异常处理-注意事项</h4><ol><li>对于编译异常，程序中必须处理，比如 try-catch或者 throws</li><li>对于运行时异常，程序中如果没有处理，默认就是throws的方式判断</li><li>子类重写父类方法是，对抛出异常的规定：子类重写的方法，所抛出的异常类型要么和父类抛出的异常一致，要么韦弗蕾抛出的异常的类型的子类型</li><li>在throws过程中，如果有方法try-catch，就相当于处理异常，就可以不必throws。</li></ol><h2 id="自定义异常"><a href="#自定义异常" class="headerlink" title="自定义异常"></a>自定义异常</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>当程序中出现了某些“错误“，但该错误信息井没有在Throwable子类中描述处理，这个时候可以自己设计异常类，用于描述该错误信息。</p><h3 id="自定义异常的步骤"><a href="#自定义异常的步骤" class="headerlink" title="自定义异常的步骤"></a>自定义异常的步骤</h3><p>1）定义类：自定义异常类名（稷序员自己写》继承Exception或RuntimeException<br>2）如果继承Exception,属于编译异常<br>3）如果继属于运行异常（一般来说，继承RuntimeException)</p><h3 id="自定义异常的应用实例-CustomException-java"><a href="#自定义异常的应用实例-CustomException-java" class="headerlink" title="自定义异常的应用实例 CustomException.java"></a>自定义异常的应用实例 CustomException.java</h3><p>当我们接收Person对象年龄时，要求范围在18_120之间，否则抛出一个自定义异常<br>继承RuntimeExcetion,并给出提示信息</p><pre><code class="java">/*** @author 韩顺平* @version 1.0*/public class CustomException &#123;public static void main(String[] args) /*throws AgeException*/ &#123;int age = 180;//要求范围在 18 – 120 之间，否则抛出一个自定义异常if(!(age &gt;= 18 &amp;&amp; age &lt;= 120)) &#123;//这里我们可以通过构造器，设置信息throw new AgeException(&quot;年龄需要在 18~120 之间&quot;);&#125;System.out.println(&quot;你的年龄范围正确.&quot;);&#125;&#125;//自定义一个异常//老韩解读//1. 一般情况下，我们自定义异常是继承 RuntimeException//2. 即把自定义异常做成 运行时异常，好处时，我们可以使用默认的处理机制//3. 即比较方便class AgeException extends RuntimeException &#123;public AgeException(String message) &#123;//构造器super(message);&#125;&#125;</code></pre><h2 id="throw和throws对比"><a href="#throw和throws对比" class="headerlink" title="throw和throws对比"></a>throw和throws对比</h2><p><img src="/java/java-xue-xi-wen-dang/image-20220613113145687.png" alt="image-20220613113145687"></p><h1 id="常用类"><a href="#常用类" class="headerlink" title="常用类"></a>常用类</h1><h2 id="包装类"><a href="#包装类" class="headerlink" title="包装类"></a>包装类</h2><ol><li>针对八种基本数据类型相应的引用类型—包装类</li></ol><ol start="2"><li>有了类的特点，就可以调用类中的方法。</li><li>如图</li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220613113534832.png" alt="image-20220613113534832"></p><p><img src="/java/java-xue-xi-wen-dang/image-20220613113747935.png" alt="image-20220613113747935"></p><p><img src="/java/java-xue-xi-wen-dang/image-20220613113805756.png" alt="image-20220613113805756"></p><p><img src="/java/java-xue-xi-wen-dang/image-20220613113819170.png" alt="image-20220613113819170"></p><h3 id="包装类和基本数据的转换"><a href="#包装类和基本数据的转换" class="headerlink" title="包装类和基本数据的转换"></a>包装类和基本数据的转换</h3><ol><li>jdk5前的手动装箱和拆箱方式，装箱：基本类型·&gt;包装类型，反之，拆箱</li><li>jdk5以后（含jdk5》的自动装箱和拆箱方式</li><li>自动装箱底层调用的是valueOf方法，比如Integer.valueOf()</li><li>其它包装类的用法类似，不一一举例</li></ol><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><pre><code class="java">public static void main(String[] args) &#123;//演示 int &lt;--&gt; Integer 的装箱和拆箱//jdk5 前是手动装箱和拆箱//手动装箱 int-&gt;Integerint n1 = 100;Integer integer = new Integer(n1);Integer integer1 = Integer.valueOf(n1);//手动拆箱//Integer -&gt; intint i = integer.intValue();//jdk5 后，就可以自动装箱和自动拆箱int n2 = 200;//自动装箱 int-&gt;IntegerInteger integer2 = n2; //底层使用的是 Integer.valueOf(n2)//自动拆箱 Integer-&gt;intint n3 = integer2; //底层仍然使用的是 intValue()方法&#125;</code></pre><h3 id="包装类型和-String-类型的相互转换-WrapperVSString-java"><a href="#包装类型和-String-类型的相互转换-WrapperVSString-java" class="headerlink" title="包装类型和 String 类型的相互转换 WrapperVSString.java"></a>包装类型和 String 类型的相互转换 WrapperVSString.java</h3><pre><code class="java">/*** @author 韩顺平* @version 1.0*/public class WrapperVSString &#123;public static void main(String[] args) &#123;//包装类(Integer)-&gt;StringInteger i = 100;//自动装箱//方式 1String str1 = i + &quot;&quot;;//方式 2String str2 = i.toString();//方式 3String str3 = String.valueOf(i);//String -&gt; 包装类(Integer)String str4 = &quot;12345&quot;;Integer i2 = Integer.parseInt(str4);//使用到自动装箱Integer i3 = new Integer(str4);//构造器System.out.println(&quot;ok~~&quot;);&#125;&#125;</code></pre><h3 id="Integer-类和-Character类常用方法"><a href="#Integer-类和-Character类常用方法" class="headerlink" title="Integer 类和 Character类常用方法"></a>Integer 类和 Character类常用方法</h3><pre><code class="java">public class WrapperMethod &#123;public static void main(String[] args) &#123;System.out.println(Integer.MIN_VALUE); //返回最小值System.out.println(Integer.MAX_VALUE);//返回最大值System.out.println(Character.isDigit(&#39;a&#39;));//判断是不是数字System.out.println(Character.isLetter(&#39;a&#39;));//判断是不是字母System.out.println(Character.isUpperCase(&#39;a&#39;));//判断是不是大写System.out.println(Character.isLowerCase(&#39;a&#39;));//判断是不是小写System.out.println(Character.isWhitespace(&#39;a&#39;));//判断是不是空格System.out.println(Character.toUpperCase(&#39;a&#39;));//转成大写System.out.println(Character.toLowerCase(&#39;A&#39;));//转成小写&#125;&#125;</code></pre><h3 id="Integer-类面试题-1"><a href="#Integer-类面试题-1" class="headerlink" title="Integer 类面试题 1"></a>Integer 类面试题 1</h3><p>WrapperExercise02.java</p><pre><code class="java">public static void main(String[] args) &#123;Integer i = new Integer(1);Integer j = new Integer(1);System.out.println(i == j); //False//所以，这里主要是看范围 -128 ~ 127 就是直接返回/*老韩解读//1. 如果 i 在 IntegerCache.low(-128)~IntegerCache.high(127),就直接从数组返回//2. 如果不在 -128~127,就直接 new Integer(i)public static Integer valueOf(int i) &#123;if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)return IntegerCache.cache[i + (-IntegerCache.low)];return new Integer(i);&#125;*/Integer m = 1; //底层 Integer.valueOf(1); -&gt; 阅读源码Integer n = 1;//底层 Integer.valueOf(1);System.out.println(m == n); //T//所以，这里主要是看范围 -128 ~ 127 就是直接返回//，否则，就 new Integer(xx);Integer x = 128;//底层 Integer.valueOf(1);Integer y = 128;//底层 Integer.valueOf(1);System.out.println(x == y);//False&#125;&#125;</code></pre><h3 id="Intege-类面试题总结"><a href="#Intege-类面试题总结" class="headerlink" title="Intege 类面试题总结"></a>Intege 类面试题总结</h3><pre><code class="java">public class WrapperExercise03 &#123;public static void main(String[] args) &#123;    //示例一，不是调用valueof，而是创建新对象Integer i1 = new Integer(127);Integer i2 = new Integer(127);System.out.println(i1 == i2);//F    //示例二Integer i3 = new Integer(128);Integer i4 = new Integer(128);System.out.println(i3 == i4);//F    //示例三Integer i5 = 127;//底层 Integer.valueOf(127)Integer i6 = 127;//-128~127System.out.println(i5 == i6); //T    //示例四Integer i7 = 128;Integer i8 = 128;System.out.println(i7 == i8);//F    //示例五Integer i9 = 127; //Integer.valueOf(127)Integer i10 = new Integer(127);System.out.println(i9 == i10);//F    //示例六Integer i11=127;int i12=127;//只有有基本数据类型，判断的是//值是否相同，会自动拆箱操作称为int比较System.out.println(i11==i12); //T    //示例七Integer i13=128;int i14=128;System.out.println(i13==i14);//T&#125;&#125;</code></pre><h2 id="String"><a href="#String" class="headerlink" title="String"></a>String</h2><h3 id="String-类的理解和创建对象"><a href="#String-类的理解和创建对象" class="headerlink" title="String 类的理解和创建对象"></a>String 类的理解和创建对象</h3><ol><li>String 对象用于保存字符串，也就是一组字符序列</li><li>字符串常量对象用双引号括起的字符序列。</li><li>字符串的字符使用Unicode字符编码，一个字符（不区分字母还是汉字），占两个字符</li><li>String类较常用构造器</li></ol><pre><code class="java">String s1 = new String();String s2 = new String(String original);String s3 = new String(char[] a);String s4 = new String(char[] a,int startIndex,int count);</code></pre><p><img src="/java/java-xue-xi-wen-dang/image-20220613122622418.png" alt="image-20220613122622418"></p><ol start="5"><li><p>String 类实现了接口 Serializable【String 可以串行化:可以在网络传输】 </p><p>实现了接口 Comparable [String 对象可以比较大小]</p></li><li><p>String 是 final 类，不能被其他的类继承</p></li><li><p>String 有属性 private final char value[]; 用于存放字符串内容</p></li><li><p>一定要注意：value 是一个 final 类型，不可以修改，即value不能指向新的地址，但是单个字符串内容可以变化。</p></li></ol><h3 id="创建-String-对象的两种方式"><a href="#创建-String-对象的两种方式" class="headerlink" title="创建 String 对象的两种方式"></a>创建 String 对象的两种方式</h3><ol><li>直接赋值String s &#x3D;””;</li><li>直接构造器 String s2 &#x3D; new String(“”);</li></ol><h3 id="两种创建-String-对象的区别"><a href="#两种创建-String-对象的区别" class="headerlink" title="两种创建 String 对象的区别"></a>两种创建 String 对象的区别</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220613144636121.png" alt="image-20220613144636121"></p><h3 id="String练习题"><a href="#String练习题" class="headerlink" title="String练习题"></a>String练习题</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220613144730748.png" alt="image-20220613144730748"></p><h3 id="字符串的特性"><a href="#字符串的特性" class="headerlink" title="字符串的特性"></a>字符串的特性</h3><ol><li>String是一个final类，代表不可变的字符串序列</li><li>字符串是不可变的。一个字符串对象一旦被分配，其内容是不可变的</li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220613144945734.png" alt="image-20220613144945734"></p><h3 id="面试题-第二题是难题"><a href="#面试题-第二题是难题" class="headerlink" title="面试题(第二题是难题)"></a>面试题(第二题是难题)</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220613145040765.png" alt="image-20220613145040765"></p><p><img src="/java/java-xue-xi-wen-dang/image-20220613150045820.png" alt="image-20220613150045820"></p><p>c&#x3D;a+b;</p><ol><li>先创建StringBuilder，先append（hello），在append（abc），然后调用tostring方法返回。</li><li>c对象指向堆内存的String类型，String指向内存的helloabc!!难点</li><li>intern方法返回的是方法区的引用</li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220613150814848.png" alt="image-20220613150814848"></p><p><img src="/java/java-xue-xi-wen-dang/image-20220613150143809.png" alt="image-20220613150143809"></p><h3 id="String-类的常见方法"><a href="#String-类的常见方法" class="headerlink" title="String 类的常见方法"></a>String 类的常见方法</h3><p>String类是保存字符串常量的。每次更新都需要开辟新的空间，效率很低，因此java设计者还提供了StringBuilder和StringBuffer来增强String功能，并提高效率。</p><pre><code class="java">String s = new String(&quot;&quot;);for(int i = 0;i&lt;80000;i++)&#123;    s += &quot;hello&quot;;&#125;</code></pre><p><img src="/java/java-xue-xi-wen-dang/image-20220613151247544.png" alt="image-20220613151247544"></p><p><img src="/java/java-xue-xi-wen-dang/image-20220613151628898.png" alt="image-20220613151628898"></p><pre><code class="java">public class StringMethod01 &#123;public static void main(String[] args) &#123;//1. equals 前面已经讲过了. 比较内容是否相同，区分大小写String str1 = &quot;hello&quot;;String str2 = &quot;Hello&quot;;System.out.println(str1.equals(str2));//// 2.equalsIgnoreCase 忽略大小写的判断内容是否相等String username = &quot;johN&quot;;if (&quot;john&quot;.equalsIgnoreCase(username)) &#123;System.out.println(&quot;Success!&quot;);&#125; else &#123;System.out.println(&quot;Failure!&quot;);&#125;// 3.length 获取字符的个数，字符串的长度System.out.println(&quot;韩顺平&quot;.length());// 4.indexOf 获取字符在字符串对象中第一次出现的索引，索引从 0 开始，如果找不到，返回-1String s1 = &quot;wer@terwe@g&quot;;int index = s1.indexOf(&#39;@&#39;);System.out.println(index);// 3System.out.println(&quot;weIndex=&quot; + s1.indexOf(&quot;we&quot;));//0// 5.lastIndexOf 获取字符在字符串中最后一次出现的索引，索引从 0 开始，如果找不到，返回-1s1 = &quot;wer@terwe@g@&quot;;index = s1.lastIndexOf(&#39;@&#39;);System.out.println(index);//11System.out.println(&quot;ter 的位置=&quot; + s1.lastIndexOf(&quot;ter&quot;));//4// 6.substring 截取指定范围的子串String name = &quot;hello,张三&quot;;//下面 name.substring(6) 从索引 6 开始截取后面所有的内容System.out.println(name.substring(6));//截取后面的字符//name.substring(0,5)表示从索引 0 开始截取，截取到索引 5-1=4 位置System.out.println(name.substring(2,5));//llo&#125;&#125;</code></pre><pre><code class="java">public class StringMethod02 &#123;public static void main(String[] args) &#123;// 1.toUpperCase 转换成大写String s = &quot;heLLo&quot;;System.out.println(s.toUpperCase());//HELLO// 2.toLowerCaseSystem.out.println(s.toLowerCase());//hello// 3.concat 拼接字符串String s1 = &quot;宝玉&quot;;s1 = s1.concat(&quot;林黛玉&quot;).concat(&quot;薛宝钗&quot;).concat(&quot;together&quot;);System.out.println(s1);//宝玉林黛玉薛宝钗 together// 4.replace 替换字符串中的字符s1 = &quot;宝玉 and 林黛玉 林黛玉 林黛玉&quot;;//在 s1 中，将 所有的 林黛玉 替换成薛宝钗// 老韩解读: s1.replace() 方法执行后，返回的结果才是替换过的. // 注意对 s1 没有任何影响String s11 = s1.replace(&quot;宝玉&quot;, &quot;jack&quot;);System.out.println(s1);//宝玉 and 林黛玉 林黛玉 林黛玉System.out.println(s11);//jack and 林黛玉 林黛玉 林黛玉// 5.split 分割字符串, 对于某些分割字符，我们需要 转义比如 | \\等String poem = &quot;锄禾日当午,汗滴禾下土,谁知盘中餐,粒粒皆辛苦&quot;;//老韩解读：// 1. 以 , 为标准对 poem 进行分割 , 返回一个数组// 2. 在对字符串进行分割时，如果有特殊字符，需要加入 转义符 \String[] split = poem.split(&quot;,&quot;);poem = &quot;E:\\aaa\\bbb&quot;;split = poem.split(&quot;\\\\&quot;);System.out.println(&quot;==分割后内容===&quot;);for (int i = 0; i &lt; split.length; i++) &#123;System.out.println(split[i]);&#125;// 6.toCharArray 转换成字符数组s = &quot;happy&quot;;char[] chs = s.toCharArray();for (int i = 0; i &lt; chs.length; i++) &#123;System.out.println(chs[i]);&#125;// 7.compareTo 比较两个字符串的大小，如果前者大，// 则返回正数，后者大，则返回负数，如果相等，返回 0// 老韩解读// (1) 如果长度相同，并且每个字符也相同，就返回 0// (2) 如果长度相同或者不相同，但是在进行比较时，可以区分大小// 就返回 if (c1 != c2) &#123;// return c1 - c2;// &#125;// (3) 如果前面的部分都相同，就返回 str1.len - str2.lenString a = &quot;jcck&quot;;// len = 3String b = &quot;jack&quot;;// len = 4System.out.println(a.compareTo(b)); // 返回值是 &#39;c&#39; - &#39;a&#39; = 2 的值// 8.format 格式字符串/* 占位符有:* %s 字符串 %c 字符 %d 整型 %.2f 浮点型**/String name = &quot;john&quot;;int age = 10;double score = 56.857;char gender = &#39;男&#39;;//将所有的信息都拼接在一个字符串. String info =&quot;我的姓名是&quot; + name + &quot;年龄是&quot; + age + &quot;,成绩是&quot; + score + &quot;性别是&quot; + gender + &quot;。希望大家喜欢我！&quot;;System.out.println(info);//老韩解读//1. %s , %d , %.2f %c 称为占位符//2. 这些占位符由后面变量来替换//3. %s 表示后面由 字符串来替换//4. %d 是整数来替换//5. %.2f 表示使用小数来替换，替换后，只会保留小数点两位, 并且进行四舍五入的处理//6. %c 使用 char 类型来替换String formatStr = &quot;我的姓名是%s 年龄是%d，成绩是%.2f 性别是%c.希望大家喜欢我！&quot;;String info2 = String.format(formatStr, name, age, score, gender);System.out.println(&quot;info2=&quot; + info2);&#125;&#125;</code></pre><h2 id="StringBuffer"><a href="#StringBuffer" class="headerlink" title="StringBuffer"></a>StringBuffer</h2><p>java.lang.StringBuffer代表可变的字符序列，可以对字符串内容进行增删。</p><p>很多方法与String相同，带式StringBuffer是可变长度的。</p><p>StringBuffer是一个容器。</p><pre><code class="java">//老韩解读//1. StringBuffer 的直接父类 是 AbstractStringBuilder//2. StringBuffer 实现了 Serializable, 即 StringBuffer 的对象可以串行化//3. 在父类中 AbstractStringBuilder 有属性 char[] value,不是 final// 该 value 数组存放 字符串内容，引出存放在堆中的//4. StringBuffer 是一个 final 类，不能被继承//5. 因为 StringBuffer 字符内容是存在 char[] value, 所有在变化(增加/删除)// 不用每次都更换地址(即不是每次创建新对象)， 所以效率高于 String</code></pre><p><img src="/java/java-xue-xi-wen-dang/image-20220613152337110.png" alt="image-20220613152337110"></p><h3 id="String-VS-StringBuffer"><a href="#String-VS-StringBuffer" class="headerlink" title="String VS StringBuffer"></a>String VS StringBuffer</h3><ol><li>String保存的是字符串常量，里面的值不能更改，每次String类的更新实际上就是更改地址，效率较低&#x2F;&#x2F;private final char value[]</li><li>StringBuffer保存的是字符串变量，里面的值可以更改，每次StringBuffer的更新实际上可以更新内容，不更新地址，效率较高。</li></ol><h3 id="String-和-StringBuffer相互转换"><a href="#String-和-StringBuffer相互转换" class="headerlink" title="String 和 StringBuffer相互转换"></a>String 和 StringBuffer相互转换</h3><pre><code class="java">public class StringAndStringBuffer &#123;public static void main(String[] args) &#123;    //看 String——&gt;StringBufferString str = &quot;hello tom&quot;;//方式 1 使用构造器//注意： 返回的才是 StringBuffer 对象，对 str 本身没有影响StringBuffer stringBuffer = new StringBuffer(str);//方式 2 使用的是 append 方法StringBuffer stringBuffer1 = new StringBuffer();stringBuffer1 = stringBuffer1.append(str);    //看看 StringBuffer -&gt;StringStringBuffer stringBuffer3 = new StringBuffer(&quot;韩顺平教育&quot;);//方式 1 使用 StringBuffer 提供的 toString 方法String s = stringBuffer3.toString();//方式 2: 使用构造器来搞定String s1 = new String(stringBuffer3);&#125;&#125;</code></pre><h3 id="StringBuffer类常见方法"><a href="#StringBuffer类常见方法" class="headerlink" title="StringBuffer类常见方法"></a>StringBuffer类常见方法</h3><pre><code class="java">/*** @author 韩顺平* @version 1.0*/public class StringBufferMethod &#123;public static void main(String[] args) &#123;StringBuffer s = new StringBuffer(&quot;hello&quot;);//增s.append(&#39;,&#39;);// &quot;hello,&quot;s.append(&quot;张三丰&quot;);//&quot;hello,张三丰&quot;s.append(&quot;赵敏&quot;).append(100).append(true).append(10.5);//&quot;hello,张三丰赵敏 100true10.5&quot; System.out.println(s);//&quot;hello,张三丰赵敏 100true10.5&quot;//删/** 删除索引为&gt;=start &amp;&amp; &lt;end 处的字符* 解读: 删除 11~14 的字符 [11, 14)*/s.delete(11, 14);System.out.println(s);//&quot;hello,张三丰赵敏 true10.5&quot;//改//老韩解读，使用 周芷若 替换 索引 9-11 的字符 [9,11)s.replace(9, 11, &quot;周芷若&quot;);System.out.println(s);//&quot;hello,张三丰周芷若 true10.5&quot;//查找指定的子串在字符串第一次出现的索引，如果找不到返回-1int indexOf = s.indexOf(&quot;张三丰&quot;);System.out.println(indexOf);//6//插//老韩解读，在索引为 9 的位置插入 &quot;赵敏&quot;,原来索引为 9 的内容自动后移s.insert(9, &quot;赵敏&quot;);System.out.println(s);//&quot;hello,张三丰赵敏周芷若 true10.5&quot;//长度System.out.println(s.length());//22System.out.println(s);&#125;&#125;</code></pre><h3 id="StringBuffer练习题"><a href="#StringBuffer练习题" class="headerlink" title="StringBuffer练习题"></a>StringBuffer练习题</h3><pre><code class="java">public class StringBufferExercise01 &#123;public static void main(String[] args) &#123;String str = null;// okStringBuffer sb = new StringBuffer(); //oksb.append(str);//需要看源码 , 底层调用的是 AbstractStringBuilder 的 appendNullSystem.out.println(sb.length());//4System.out.println(sb);//null//下面的构造器，会抛出 NullpointerExceptionStringBuffer sb1 = new StringBuffer(str);//看底层源码 super(str.length() + 16);str.length因为是null，所以报nullpointerSystem.out.println(sb1);&#125;&#125;</code></pre><p><img src="/java/java-xue-xi-wen-dang/image-20220613154705966.png" alt="image-20220613154705966"></p><p><img src="/java/java-xue-xi-wen-dang/image-20220613154725124.png" alt="image-20220613154725124"></p><h2 id="StringBuilder"><a href="#StringBuilder" class="headerlink" title="StringBuilder"></a>StringBuilder</h2><ol><li>一个可变的字符序列。此类提供一个于StringBuffer兼容的API，但不保证同步（StringBuilder不是线程安全）。该类被设计用作String Buffer的一个简易替换，用在字符串缓冲区被单个线程使用的时候。如果可能，优先采用该类，因为大多数实现中，它比StringBuffer要快。</li><li>在StringBuilder上的主要操作是append和instert方法，可重载这些方法，以接收任意类型的数据。</li></ol><pre><code class="java">//老韩解读//1. StringBuffer 的直接父类 是 AbstractStringBuilder//2. 实现了 Serializable ,说明 StringBuilder 对象是可以串行化(对象可以网络传输,可以保存到文件)//3. StringBuilder 是 final 类, 不能被继承//4. StringBuilder 对象字符序列仍然是存放在其父类                  AbstractStringBuilder 的 char[] value;//   不是final修饰的// 该 value 数组存放 字符串内容，引出存放在堆中//4. StringBuffer 是一个 final 类，不能被继承//5. 因为 StringBuffer 字符内容是存在 char[] value, 所有在变化(增加/删除)//6. . StringBuilder 的方法，没有做互斥的处理,即没有 synchronized 关键字,因此在单线程的情况下使用// 不用每次都更换地址(即不是每次创建新对象)， 所以效率高于 Strin</code></pre><h3 id="StringBuilder常用方法"><a href="#StringBuilder常用方法" class="headerlink" title="StringBuilder常用方法"></a>StringBuilder常用方法</h3><p>StringBuilder和StringBuffer都代表可变的字符序列，方法是一样的。</p><p><img src="/java/java-xue-xi-wen-dang/image-20220613155521045.png" alt="image-20220613155521045"></p><h3 id="String、StringBuffer和StringBuilder的比较"><a href="#String、StringBuffer和StringBuilder的比较" class="headerlink" title="String、StringBuffer和StringBuilder的比较"></a>String、StringBuffer和StringBuilder的比较</h3><ol><li><p>StringBuilder和StringBuffer非常类似，均代表可变字符序列，而且方法也一样。</p></li><li><p>String：不可变字符序列，效率低，但是<strong>复用率高</strong></p></li><li><p>StringBuffer：可变字符序列、效率较高（增删）、线程安全，看源码</p></li><li><p>StringBuilder：可变字符序列、效率最高，线程不安全</p></li><li><p>String实用注意：</p><p>String s&#x3D;”a”;&#x2F;&#x2F;创建了一个字符串</p><p>s +&#x3D; “b”;&#x2F;&#x2F;实际上原来的”a”字符串已经丢弃了，现在又产生了一个字符串s+”b”,也就是”ab”。如果多次执行这些改变串内容的操作，会导致大量副本字符串对象六存在内存中，降低效率。如果这样的操作放到循环中，会极大地影响性能。结论：如果我们对String做大量频繁修改，不要使用String；</p></li></ol><h3 id="String、StringBuffer-和-StringBuilder-的效率测试"><a href="#String、StringBuffer-和-StringBuilder-的效率测试" class="headerlink" title="String、StringBuffer 和 StringBuilder 的效率测试"></a>String、StringBuffer 和 StringBuilder 的效率测试</h3><pre><code class="java">public class StringVsStringBufferVsStringBuilder &#123;    public static void main(String[] args) &#123;        long startTime = 0L;        long endTime = 0L;        StringBuffer buffer = new StringBuffer(&quot;&quot;);        startTime = System.currentTimeMillis();        for (int i = 0; i &lt; 80000; i++) &#123;//StringBuffer 拼接 20000 次            buffer.append(String.valueOf(i));        &#125;        endTime = System.currentTimeMillis();        System.out.println(&quot;StringBuffer 的执行时间：&quot; + (endTime - startTime));        StringBuilder builder = new StringBuilder(&quot;&quot;);        startTime = System.currentTimeMillis();        for (int i = 0; i &lt; 80000; i++) &#123;//StringBuilder 拼接 20000 次            builder.append(String.valueOf(i));        &#125;        endTime = System.currentTimeMillis();        System.out.println(&quot;StringBuilder 的执行时间：&quot; + (endTime - startTime));        String text = &quot;&quot;;        startTime = System.currentTimeMillis();        for (int i = 0; i &lt; 80000; i++) &#123;//String 拼接 20000            text = text + i;        &#125;        endTime = System.currentTimeMillis();        System.out.println(&quot;String 的执行时间：&quot; + (endTime - startTime));    &#125;&#125;/*StringBuffer 的执行时间：28StringBuilder 的执行时间：19String 的执行时间：7698*/</code></pre><h3 id="String、StringBuffer-和-StringBuilder的选择"><a href="#String、StringBuffer-和-StringBuilder的选择" class="headerlink" title="String、StringBuffer 和 StringBuilder的选择"></a>String、StringBuffer 和 StringBuilder的选择</h3><ol><li>如果字符串存在大量的修改操作，一般使用StringBuffer或StringBuilder</li><li>如果字符串存在大量的修改操作，并在单线程的情况，使用StringBuilder</li><li>如果字符串存在大量的修改操作，并在多线程的情况，使用StringBuffer</li><li>如果我们字符串很少修改，被多个对象引用，使用String，比如配置文件等</li></ol><h2 id="Math类"><a href="#Math类" class="headerlink" title="Math类"></a>Math类</h2><p>Math 类包含用于执行基本数学运算的方法，如初等指数、对数、平方根和三角函数。</p><h3 id="Math常见方法"><a href="#Math常见方法" class="headerlink" title="Math常见方法"></a>Math常见方法</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220613174057742.png" alt="image-20220613174057742"></p><pre><code class="java">public class MathMethod &#123;public static void main(String[] args) &#123;//看看 Math 常用的方法(静态方法)//1.abs 绝对值int abs = Math.abs(-9);System.out.println(abs);//9//2.pow 求幂double pow = Math.pow(2, 4);//2 的 4 次方System.out.println(pow);//16//3.ceil 向上取整,返回&gt;=该参数的最小整数(转成 double);double ceil = Math.ceil(3.9);System.out.println(ceil);//4.0//4.floor 向下取整，返回&lt;=该参数的最大整数(转成 double)double floor = Math.floor(4.001);System.out.println(floor);//4.0//5.round 四舍五入 Math.floor(该参数+0.5)long round = Math.round(5.51);System.out.println(round);//6//6.sqrt 求开方double sqrt = Math.sqrt(9.0);System.out.println(sqrt);//3.0//7.random 求随机数// random 返回的是 0 &lt;= x &lt; 1 之间的一个随机小数// 思考：请写出获取 a-b 之间的一个随机整数,a,b 均为整数 ，比如 a = 2, b=7// 即返回一个数 x 2 &lt;= x &lt;= 7// 老韩解读 Math.random() * (b-a) 返回的就是 0 &lt;= 数 &lt;= b-a// (1) (int)(a) &lt;= x &lt;= (int)(a + Math.random() * (b-a +1) )// (2) 使用具体的数给小伙伴介绍 a = 2 b = 7// (int)(a + Math.random() * (b-a +1) ) = (int)( 2 + Math.random()*6)// Math.random()*6 返回的是 0 &lt;= x &lt; 6 小数// 2 + Math.random()*6 返回的就是 2&lt;= x &lt; 8 小数// (int)(2 + Math.random()*6) = 2 &lt;= x &lt;= 7// (3) 公式就是 (int)(a + Math.random() * (b-a +1) )for(int i = 0; i &lt; 100; i++) &#123;System.out.println((int)(2 + Math.random() * (7 - 2 + 1)));&#125;//max , min 返回最大值和最小值int min = Math.min(1, 9);int max = Math.max(45, 90);System.out.println(&quot;min=&quot; + min);System.out.println(&quot;max=&quot; + max);&#125;&#125;</code></pre><h2 id="Arrays"><a href="#Arrays" class="headerlink" title="Arrays"></a>Arrays</h2><p>该类包含用于操作数组的各种静态方法（如排序和搜索）。 </p><p>该类还包含一个静态工厂，可以将数组视为列表。 </p><p>用于管理或操作数组（比如索引和搜索）</p><h3 id="Arrays常用方法"><a href="#Arrays常用方法" class="headerlink" title="Arrays常用方法"></a>Arrays常用方法</h3><pre><code class="java">1. toString 返回数组的字符串形式Arrays.toString(arr);2. sort 排序（自然排序和定制排序）    3. binarySearch通过二分搜索法进行查找，要求必须排好序   没有排好序的数组进行查找就返回应在 -（索引下标+1），看源码int index = Arrays.binarySearch(arr,3);4. copyOf 数组元素的复制   如果拷贝的长度大于arr.length，在后面添加NULL默认值Integer[] newArr = Arrays.copyOf(arr,arr.length);5. fill 数组元素的填充   把数组中的元素都替换成99Integer[] num = new Integer[]&#123;9,3,2&#125;;Arrays.fill(num,99);6. equals 比较两个数组元素内容是否完全一致boolean equals = Arrays.equals(arr,arr2);7. asList 将一组值，转换成listList&lt;Integer&gt; asList = Arrays.asList(2,3,4,5,6,1);8. </code></pre><h3 id="冒泡-定制排序"><a href="#冒泡-定制排序" class="headerlink" title="冒泡+定制排序"></a>冒泡+定制排序</h3><pre><code class="java">//结合冒泡 + 定制public static void bubble02(int[] arr, Comparator c) &#123;int temp = 0;for (int i = 0; i &lt; arr.length - 1; i++) &#123;for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123;//数组排序由 c.compare(arr[j], arr[j + 1])返回的值决定if (c.compare(arr[j], arr[j + 1]) &gt; 0) &#123;temp = arr[j];arr[j] = arr[j + 1];arr[j + 1] = temp;&#125;&#125;&#125;&#125;//简化版本        int temp ;        for(int i =0;i&lt;integers.length-1;i++)&#123;            for(int j = 0;j&lt;integers.length-1-i;j++)&#123;                //integers[j+1]-integers[j])  从大到小                //integers[j]-integers[j+1])  从小到大                if((integers[j+1]-integers[j])&lt;0)&#123;                    temp = integers[j+1];                    integers[j+1] = integers[j];                    integers[j] = temp;                &#125;            &#125;        &#125;</code></pre><h2 id="System"><a href="#System" class="headerlink" title="System"></a>System</h2><h3 id="System-类常见方法和案例"><a href="#System-类常见方法和案例" class="headerlink" title="System 类常见方法和案例"></a>System 类常见方法和案例</h3><ol><li>exit退出当前程序</li><li>arraycopy：复制数组元素，比较适合底层调用，一般使用Arrays.copyOf完成复制数组</li><li>currentTimeMillens()，返回当前时间距离1970-1-1的毫秒数</li><li>gc：运行垃圾回收机制 System.gc();</li></ol><pre><code class="java">exit退出当前程序//exit(0) 正常退出//exit(1) 异常退出System.exit(0);</code></pre><h2 id="BigInteger-BigDecimal"><a href="#BigInteger-BigDecimal" class="headerlink" title="BigInteger BigDecimal"></a>BigInteger BigDecimal</h2><p>BigInteger适合保存比较大的整数</p><p>BigDecimal适合保存精度更高的浮点型（小数）</p><h3 id="BigInteger-和-BigDecimal-常见方法"><a href="#BigInteger-和-BigDecimal-常见方法" class="headerlink" title="BigInteger 和 BigDecimal 常见方法"></a>BigInteger 和 BigDecimal 常见方法</h3><ol><li>add</li><li>subtract（减去）</li><li>multiply（乘）</li><li>divide（除）</li></ol><pre><code class="java">BigInteger bigInteger = new BigInteger(&quot;23788888899999999999999999999&quot;);BigInteger bigInteger2 = new BigInteger(&quot;10099999999999999999999999999999999999999999999999999999999999999999999999999999999&quot;);//老韩解读//1. 在对 BigInteger 进行加减乘除的时候，需要使用对应的方法，不能直接进行 + - * ///2. 可以创建一个 要操作的 BigInteger 然后进行相应操作BigInteger add = bigInteger.add(bigInteger2);System.out.println(add);//BigInteger subtract = bigInteger.subtract(bigInteger2);System.out.println(subtract);//减BigInteger multiply = bigInteger.multiply(bigInteger2);System.out.println(multiply);//乘BigInteger divide = bigInteger.divide(bigInteger2);System.out.println(divide);//除</code></pre><pre><code class="java">public static void main(String[] args) &#123;//当我们需要保存一个精度很高的数时，double 不够用//可以是 BigDecimal// double d = 1999.11111111111999999999999977788d;// System.out.println(d);BigDecimal bigDecimal = new BigDecimal(&quot;1999.11&quot;);BigDecimal bigDecimal2 = new BigDecimal(&quot;3&quot;);System.out.println(bigDecimal);//老韩解读//1. 如果对 BigDecimal 进行运算，比如加减乘除，需要使用对应的方法//2. 创建一个需要操作的 BigDecimal 然后调用相应的方法即可System.out.println(bigDecimal.add(bigDecimal2));System.out.println(bigDecimal.subtract(bigDecimal2));System.out.println(bigDecimal.multiply(bigDecimal2));//System.out.println(bigDecimal.divide(bigDecimal2));//可能抛出异常 ArithmeticException//在调用 divide 方法时，指定精度即可. BigDecimal.ROUND_CEILING//如果有无限循环小数，就会保留 分子 的精度System.out.println(bigDecimal.divide(bigDecimal2, BigDecimal.ROUND_CEILING));&#125;</code></pre><h2 id="日期类"><a href="#日期类" class="headerlink" title="日期类"></a>日期类</h2><h3 id="Date-第一代日期类"><a href="#Date-第一代日期类" class="headerlink" title="Date-第一代日期类"></a>Date-第一代日期类</h3><ol><li><p>Date：精确到毫秒，代表特定的瞬间</p></li><li><p>SimpleDateFormat：格式和解析日期的类</p><p>SimpleDateFormat 格式化和解析日期的具体类</p></li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220613195327856.png" alt="image-20220613195327856"></p><pre><code class="java">//1. 获取当前系统时间//2. 这里的 Date 类是在 java.util 包//3. 默认输出的日期格式是国外的方式, 因此通常需要对格式进行转换Date d1 = new Date(); //获取当前系统时间System.out.println(&quot;当前日期=&quot; + d1);Date d2 = new Date(9234567); //通过指定毫秒数得到时间System.out.println(&quot;d2=&quot; + d2); //获取某个时间对应的毫秒数SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy 年 MM 月 dd 日 hh:mm:ss E&quot;);String format = sdf.format(d1); // format:将日期转换成指定格式的字符串System.out.println(&quot;当前日期=&quot; + format);//1. 可以把一个格式化的 String 转成对应的 Date//2. 得到 Date 仍然在输出时，还是按照国外的形式，如果希望指定格式输出，需要转换//3. 在把 String -&gt; Date ， 使用的 sdf 格式需要和你给的 String 的格式一样，否则会抛出转换异常 String s = &quot;1996 年 01 月 01 日 10:20:30 星期一&quot;;Date parse = sdf.parse(s);System.out.println(&quot;parse=&quot; + sdf.format(parse))</code></pre><h3 id="Calendar-第二代日期类"><a href="#Calendar-第二代日期类" class="headerlink" title="Calendar-第二代日期类"></a>Calendar-第二代日期类</h3><ol><li>第二代日期类，主要就是Calender类（日历）</li><li>Calendar类是一个抽象类，他为特定瞬间与一组诸如Year，MONTH\DAY_OF_MONTH、HOUR等日历字段之间的转换提供了一些方法，并为操作日历字段提供了一些方法</li></ol><pre><code class="java">import java.util.Calendar;/*** @author 韩顺平* @version 1.0*/public class Calendar_ &#123;public static void main(String[] args) &#123;//老韩解读//1. Calendar 是一个抽象类， 并且构造器是 private//2. 可以通过 getInstance() 来获取实例//3. 提供大量的方法和字段提供给程序员//4. Calendar 没有提供对应的格式化的类，因此需要程序员自己组合来输出(灵活)//5. 如果我们需要按照 24 小时进制来获取时间， Calendar.HOUR ==改成=&gt; Calendar.HOUR_OF_DAYCalendar c = Calendar.getInstance(); //创建日历类对象//比较简单，自由System.out.println(&quot;c=&quot; + c);//2.获取日历对象的某个日历字段System.out.println(&quot;年：&quot; + c.get(Calendar.YEAR));// 这里为什么要 + 1, 因为 Calendar 返回月时候，是按照 0 开始编号System.out.println(&quot;月：&quot; + (c.get(Calendar.MONTH) + 1));System.out.println(&quot;日：&quot; + c.get(Calendar.DAY_OF_MONTH));System.out.println(&quot;小时：&quot; + c.get(Calendar.HOUR));System.out.println(&quot;分钟：&quot; + c.get(Calendar.MINUTE));System.out.println(&quot;秒：&quot; + c.get(Calendar.SECOND));//Calender 没有专门的格式化方法，所以需要程序员自己来组合显示System.out.println(c.get(Calendar.YEAR) + &quot;-&quot; + (c.get(Calendar.MONTH) + 1) + &quot;-&quot; +c.get(Calendar.DAY_OF_MONTH) +&quot; &quot; + c.get(Calendar.HOUR_OF_DAY) + &quot;:&quot; + c.get(Calendar.MINUTE) + &quot;:&quot; + c.get(Calendar.SECOND) );&#125;&#125;</code></pre><h3 id="LocalDate-第三代日期类"><a href="#LocalDate-第三代日期类" class="headerlink" title="LocalDate-第三代日期类"></a>LocalDate-第三代日期类</h3><h4 id="前面两代的不足"><a href="#前面两代的不足" class="headerlink" title="前面两代的不足"></a>前面两代的不足</h4><p>JDK1.0包含了一个java.util.Date类，但是他的大多数方法在JDK1.1引入Calendar类之后被弃用了。</p><p><img src="/java/java-xue-xi-wen-dang/image-20220613200338257.png" alt="image-20220613200338257"></p><h4 id="类和方法"><a href="#类和方法" class="headerlink" title="类和方法"></a>类和方法</h4><ol><li>LocalDate（日期&#x2F;年月日）</li><li>LocalTime（时间&#x2F;时分秒）</li><li>LocalDateTime（日期时间&#x2F;年月日时分秒）JDK8加入</li></ol><p>LocalDate只包含日期，可以获取日期字段</p><p>LocalTime只包含时间，可以获取时间字段</p><p>LocalDateTime包含日期+时间，可以获取日期和时间字段</p><p><img src="/java/java-xue-xi-wen-dang/image-20220613201622392.png" alt="image-20220613201622392"></p><h4 id="DateTimeFormatter-格式日期类"><a href="#DateTimeFormatter-格式日期类" class="headerlink" title="DateTimeFormatter 格式日期类"></a>DateTimeFormatter 格式日期类</h4><p>类似于SimpleDateFormat</p><pre><code class="java">DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;);</code></pre><h4 id="Instant时间戳"><a href="#Instant时间戳" class="headerlink" title="Instant时间戳"></a>Instant时间戳</h4><p>类似于Date</p><p>提供了一系列和Date类转换的方式</p><pre><code class="java">//Instant——&gt;DateDate date = Date.from(instant);//Date——&gt;InstantInstant instant = date.toInstant();    public static void main(String[] args) &#123;        Instant now = Instant.now();        System.out.println(now);        Date from = Date.from(now);        System.out.println(from);        Instant instant = from.toInstant();        System.out.println(instant);        &#125;/*2022-06-13T12:15:34.762ZMon Jun 13 20:15:34 CST 20222022-06-13T12:15:34.762Z*/</code></pre><pre><code class="java">import java.time.Instant;import java.time.LocalDate;import java.time.LocalDateTime;import java.time.LocalTime;import java.time.format.DateTimeFormatter;import java.util.ArrayList;import java.util.Collection;/** @author 韩顺平* @version 1.0*/public class LocalDate_ &#123;public static void main(String[] args) &#123;//第三代日期//老韩解读//1. 使用 now() 返回表示当前日期时间的 对象LocalDateTime ldt = LocalDateTime.now(); //LocalDate.now();//LocalTime.now()System.out.println(ldt);//2. 使用 DateTimeFormatter 对象来进行格式化// 创建 DateTimeFormatter 对象DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;);String format = dateTimeFormatter.format(ldt);System.out.println(&quot;格式化的日期=&quot; + format);System.out.println(&quot;年=&quot; + ldt.getYear());System.out.println(&quot;月=&quot; + ldt.getMonth());System.out.println(&quot;月=&quot; + ldt.getMonthValue());System.out.println(&quot;日=&quot; + ldt.getDayOfMonth());System.out.println(&quot;时=&quot; + ldt.getHour());System.out.println(&quot;分=&quot; + ldt.getMinute());System.out.println(&quot;秒=&quot; + ldt.getSecond());LocalDate now = LocalDate.now(); //可以获取年月日LocalTime now2 = LocalTime.now();//获取到时分秒    //提供 plus 和 minus 方法可以对当前时间进行加或者减//看看 890 天后，是什么时候 把 年月日-时分秒LocalDateTime localDateTime = ldt.plusDays(890);System.out.println(&quot;890 天后=&quot; + dateTimeFormatter.format(localDateTime));//看看在 3456 分钟前是什么时候，把 年月日-时分秒输出LocalDateTime localDateTime2 = ldt.minusMinutes(3456);System.out.println(&quot;3456 分钟前 日期=&quot; + dateTimeFormatter.format(localDateTime2));&#125;&#125;</code></pre><h1 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h1><h2 id="数组的不足"><a href="#数组的不足" class="headerlink" title="数组的不足"></a>数组的不足</h2><ol><li>长度开始时必须指定，一旦指定后，不能更改</li><li>保存的必须为同一类型的元素</li><li>使用数组进行增加&#x2F;删除元素的示意代码，比较麻烦</li></ol><h2 id="集合的好处"><a href="#集合的好处" class="headerlink" title="集合的好处"></a>集合的好处</h2><ol><li>可以动态保存任意多个对象，使用比较方便</li><li>提供了一系列方便的操作对象的方法，add、remove、set、get等</li><li>使用集合添加，删除元素，简洁明了</li></ol><h2 id="集合框架体系"><a href="#集合框架体系" class="headerlink" title="集合框架体系"></a>集合框架体系</h2><p>Java 的集合类很多，主要分为两大类，如图 ：[背下来]</p><p><img src="/java/java-xue-xi-wen-dang/image-20220613232138059.png" alt="image-20220613232138059"></p><p><img src="/java/java-xue-xi-wen-dang/image-20220613232151016.png" alt="image-20220613232151016"></p><h2 id="Collection-接口和常用方法"><a href="#Collection-接口和常用方法" class="headerlink" title="Collection 接口和常用方法"></a>Collection 接口和常用方法</h2><pre><code class="java">public interface Collection&lt;E&gt; extends Iterable&lt;E&gt; &#123;&#125;</code></pre><ol><li>Collection 实现子类可以存放多个元素，每个元素可以是Object</li><li>有些Collection的实现类，可以存放重复的元素，有些不可以</li><li>有些Collection的实现类，有些是有序的（List），有些不是有序（Set）</li><li>Collection 接口没有直接的实现子类，是通过他的子接口Set和List实现的</li></ol><pre><code class="java">    public static void main(String[] args) &#123;        List list = new ArrayList();        // add:添加单个元素        list.add(&quot;jack&quot;);        list.add(10);//list.add(new Integer(10))        list.add(true);        System.out.println(&quot;list=&quot; + list);                // remove:删除指定元素        //list.remove(0);//删除第一个元素        list.remove(true);//指定删除某个元素        System.out.println(&quot;list=&quot; + list);                // contains:查找元素是否存在        System.out.println(list.contains(&quot;jack&quot;));//T                // size:获取元素个数        System.out.println(list.size());//2                // isEmpty:判断是否为空        System.out.println(list.isEmpty());//F                // clear:清空        list.clear();        System.out.println(&quot;list=&quot; + list);                // addAll:添加多个元素        ArrayList list2 = new ArrayList();        list2.add(&quot;红楼梦&quot;);        list2.add(&quot;三国演义&quot;);        list.addAll(list2);        System.out.println(&quot;list=&quot; + list);                // containsAll:查找多个元素是否都存在        System.out.println(list.containsAll(list2));//T        // removeAll：删除多个元素        list.add(&quot;聊斋&quot;);        list.removeAll(list2);        System.out.println(&quot;list=&quot; + list);//[聊斋]        // 说明：以 ArrayList 实现类来演示. &#125;    &#125;</code></pre><h3 id="Collection-接口遍历-Iterator（迭代器）"><a href="#Collection-接口遍历-Iterator（迭代器）" class="headerlink" title="Collection 接口遍历-Iterator（迭代器）"></a>Collection 接口遍历-Iterator（迭代器）</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220614001447883.png" alt="image-20220614001447883"></p><ol><li>Iterator对象成为迭代器，主要用于便利Collection集合中的元素</li><li>所有实现了Collection接口的集合类都有一个iterator()方法，用以返回<br>一个实现了Iterator接口的对象，即可以返回一个迭代器。</li><li>Iterator接口，看上图。</li><li>Iterator仅用于遍历集合，Iterator本身并不存放对象</li></ol><h4 id="迭代器执行原理"><a href="#迭代器执行原理" class="headerlink" title="迭代器执行原理"></a>迭代器执行原理</h4><p>Iterator iterator &#x3D; collection.iterator();得到一个集合的迭代器</p><p>hasNext()：判断是否还有下一个元素</p><p>next()：1.下移2.将下移以后集合位置上的元素返回</p><p>在调用next()；方法之前调用hasNext()；方法进行检测。若不检测，如果下一条记录无效，调用next();方法，会抛出NoSuchElementException异常。</p><pre><code class="java">while(iterator.hasNext())&#123;//next()作用1.下移2.将下移以后集合位置上的元素返回&#125;</code></pre><h4 id="迭代器实例"><a href="#迭代器实例" class="headerlink" title="迭代器实例"></a>迭代器实例</h4><pre><code class="java">/*** @author 韩顺平* @version 1.0*/public class CollectionIterator &#123;    @SuppressWarnings(&#123;&quot;all&quot;&#125;)public static void main(String[] args) &#123;Collection col = new ArrayList();col.add(new Book(&quot;三国演义&quot;, &quot;罗贯中&quot;, 10.1));col.add(new Book(&quot;小李飞刀&quot;, &quot;古龙&quot;, 5.1));col.add(new Book(&quot;红楼梦&quot;, &quot;曹雪芹&quot;, 34.6));//System.out.println(&quot;col=&quot; + col);//现在老师希望能够遍历 col 集合//1. 先得到 col 对应的 迭代器Iterator iterator = col.iterator();//2. 使用 while 循环遍历// while (iterator.hasNext()) &#123;//判断是否还有数据// //返回下一个元素，类型是 Object// Object obj = iterator.next();// System.out.println(&quot;obj=&quot; + obj);// &#125;//老师教大家一个快捷键，快速生成 while =&gt; itit//显示所有的快捷键的的快捷键 ctrl + jwhile (iterator.hasNext()) &#123;Object obj = iterator.next();System.out.println(&quot;obj=&quot; + obj);&#125;//3. 当退出 while 循环后 , 这时 iterator 迭代器，指向最后的元素// iterator.next();//NoSuchElementException//4. 如果希望再次遍历，需要重置我们的迭代器iterator = col.iterator();System.out.println(&quot;===第二次遍历===&quot;);while (iterator.hasNext()) &#123;Object obj = iterator.next();System.out.println(&quot;obj=&quot; + obj);&#125;&#125;&#125;class Book &#123;private String name;private String author;private double price;public Book(String name, String author, double price) &#123;this.name = name;this.author = author;this.price = price;&#125;public String getName() &#123;return name;&#125;public void setName(String name) &#123;this.name = name;&#125;public String getAuthor() &#123;return author;&#125;public void setAuthor(String author) &#123;this.author = author;&#125;public double getPrice() &#123;return price;&#125;public void setPrice(double price) &#123;this.price = price;&#125;@Overridepublic String toString()&#123;return &quot;Book&#123;&quot; +&quot;name=&#39;&quot; + name + &#39;\&#39;&#39; +&quot;, author=&#39;&quot; + author + &#39;\&#39;&#39; +&quot;, price=&quot; + price +&#39;&#125;&#39;;&#125;&#125;</code></pre><h3 id="Collection接口遍历2-for循环增强"><a href="#Collection接口遍历2-for循环增强" class="headerlink" title="Collection接口遍历2-for循环增强"></a>Collection接口遍历2-for循环增强</h3><p>增强for循环，可以代替iterator迭代器，特点，增强for就是简化版的iterator，本质一样，之恩那个用于遍历集合或数组。</p><pre><code class="java">for(元素类型 元素名: 集合名或数组名)&#123;访问元素&#125;</code></pre><h2 id="List接口和常用方法"><a href="#List接口和常用方法" class="headerlink" title="List接口和常用方法"></a>List接口和常用方法</h2><p>List接口是Collection接口的子接口</p><ol><li><p>List集合类中元素有序（即添加顺序和取出顺序一致）、且可重复</p></li><li><p>List集合中的每个元素都有其对应的顺序索引，即支持索引</p></li><li><p>List容器中的元素都对应一个整数型的序号记载其在容器中的位置，可以根据序号存取容器中的元素。</p></li><li><p>List接口的实现类有：</p></li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220614005058239.png" alt="image-20220614005058239"></p><h3 id="List常用方法"><a href="#List常用方法" class="headerlink" title="List常用方法"></a>List常用方法</h3><pre><code class="java">public class ListMethod &#123;@SuppressWarnings(&#123;&quot;all&quot;&#125;)public static void main(String[] args) &#123;List list = new ArrayList();list.add(&quot;张三丰&quot;);list.add(&quot;贾宝玉&quot;);// void add(int index, Object ele):在 index 位置插入 ele 元素//在 index = 1 的位置插入一个对象list.add(1, &quot;韩顺平&quot;);System.out.println(&quot;list=&quot; + list                   // boolean addAll(int index, Collection eles):从 index 位置开始将 eles 中的所有元素添加进来List list2 = new ArrayList();list2.add(&quot;jack&quot;);list2.add(&quot;tom&quot;);list.addAll(1, list2);System.out.println(&quot;list=&quot; + list);                   // Object get(int index):获取指定 index 位置的元素//说过                   // int indexOf(Object obj):返回 obj 在集合中首次出现的位置System.out.println(list.indexOf(&quot;tom&quot;));//2                   // int lastIndexOf(Object obj):返回 obj 在当前集合中末次出现的位置list.add(&quot;韩顺平&quot;);System.out.println(&quot;list=&quot; + list);System.out.println(list.lastIndexOf(&quot;韩顺平&quot;));                   // Object remove(int index):移除指定 index 位置的元素，并返回此元素list.remove(0);System.out.println(&quot;list=&quot; + list);                   // Object set(int index, Object ele):设置指定 index 位置的元素为 ele , 相当于是替换. list.set(1, &quot;玛丽&quot;);System.out.println(&quot;list=&quot; + list);                   // List subList(int fromIndex, int toIndex):返回从 fromIndex 到 toIndex 位置的子集合// 注意返回的子集合 fromIndex &lt;= subList &lt; toIndexList returnlist = list.subList(0, 2);System.out.println(&quot;returnlist=&quot; + returnlist);&#125;&#125;/*list=[张三丰, 韩顺平, 贾宝玉]list=[张三丰, jack, tom, 韩顺平, 贾宝玉]2list=[张三丰, jack, tom, 韩顺平, 贾宝玉, 韩顺平]5list=[jack, tom, 韩顺平, 贾宝玉, 韩顺平]list=[jack, tom, 韩顺平, 贾宝玉, 韩顺平]returnlist=[jack, tom]*/</code></pre><h3 id="List-的三种遍历方式-ArrayList，LinkedList，Vector"><a href="#List-的三种遍历方式-ArrayList，LinkedList，Vector" class="headerlink" title="List 的三种遍历方式 [ArrayList，LinkedList，Vector]"></a>List 的三种遍历方式 [ArrayList，LinkedList，Vector]</h3><ol><li>使用iterator</li></ol><pre><code class="java">//遍历//1. 迭代器Iterator iterator = list.iterator();while (iterator.hasNext()) &#123;Object obj = iterator.next();System.out.println(obj);&#125;</code></pre><ol start="2"><li>增强for</li></ol><pre><code class="java">for (Object o : list) &#123;System.out.println(&quot;o=&quot; + o);&#125;</code></pre><ol start="3"><li>使用普通for</li></ol><pre><code class="java">for (int i = 0; i &lt; list.size()&#123;System.out.println(&quot;对象=&quot; + list.get(i));&#125;</code></pre><h3 id="ArrayList注意事项"><a href="#ArrayList注意事项" class="headerlink" title="ArrayList注意事项"></a>ArrayList注意事项</h3><ol><li>permits all elements,including nulll,ArrayList可以加入null，并且多个</li><li>ArrayList是由数组来实现数组存储的</li><li>ArrayList基本等同于Vector，除了ArrayList是线程不安全的（执行效率高），看源码，没有<code>synchronized</code>修饰。在多线程情况下，不建议使用ArrayList。</li></ol><h3 id="ArrayList底层操作即源码分析"><a href="#ArrayList底层操作即源码分析" class="headerlink" title="ArrayList底层操作即源码分析"></a>ArrayList底层操作即源码分析</h3><ol><li><p>ArrayList中维护了一个Object类型的数组elementData</p><pre><code class="java">    /**     * The array buffer into which the elements of the ArrayList are stored.     * The capacity of the ArrayList is the length of this array buffer. Any     * empty ArrayList with elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA     * will be expanded to DEFAULT_CAPACITY when the first element is added.     */    transient Object[] elementData; // non-private to simplify nested class access//transient 表示不会被序列化，短暂的，瞬间的。</code></pre></li><li><p>当创建ArrayList对象时，如果使用的是无参构造器，则初始化容量为0，第一次添加，扩容elementData为10，如果需要再次扩容，则扩容elementData为1.5倍。10*1.5&#x3D;15</p></li><li><p>如果使用的是指定大小的构造器，则初始elementData容量为指定大小，如果需要扩容，则直接扩容elementData为1.5倍。</p></li></ol><h4 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h4><pre><code class="java">@SuppressWarnings(&#123;&quot;all&quot;&#125;)public class ArrayListSource&#123;public static void main(String[] args) &#123;//老韩解读源码//注意，注意，注意，Idea 默认情况下，Debug 显示的数据是简化后的，如果希望看到完整的数据//需要做设置. //使用无参构造器创建 ArrayList 对象//ArrayList list = new ArrayList();ArrayList list = new ArrayList(8);//使用 for 给 list 集合添加 1-10 数据for (int i = 1; i &lt;= 10; i++) &#123;list.add(i);&#125;//使用 for 给 list 集合添加 11-15 数据for (int i = 11; i &lt;= 15; i++) &#123;list.add(i);&#125;list.add(100);list.add(200);list.add(null);&#125;&#125;</code></pre><p>IDEA设置</p><p><img src="/java/java-xue-xi-wen-dang/image-20220614122529062.png" alt="IDEA设置"></p><h4 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h4><p><img src="/java/java-xue-xi-wen-dang/image-20220614121227957.png" alt="image-20220614121227957"></p><h3 id="Vector-底层结构和源码剖析"><a href="#Vector-底层结构和源码剖析" class="headerlink" title="Vector 底层结构和源码剖析"></a>Vector 底层结构和源码剖析</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220614121347236.png" alt="image-20220614121347236"></p><ol><li>Vector类定义如上图</li><li>Vector底层也是一个对象数组，Protected Object[] elementData；</li><li>Vector是线程同步的，及线程安全的，Vector类操作方法带有<code>synchronized</code></li><li>开发中，需要线程同步安全时，使用Vector</li></ol><p>下面是代码的实例，要看具体过程自己查看源码</p><pre><code class="java">/*** @author 韩顺平* @version 1.0*/@SuppressWarnings(&#123;&quot;all&quot;&#125;)public class Vector_ &#123;public static void main(String[] args) &#123;//无参构造器//有参数的构造Vector vector = new Vector(8);for (int i = 0; i &lt; 10; i++) &#123;vector.add(i);&#125;vector.add(100);System.out.println(&quot;vector=&quot; + vector);//老韩解读源码//1. new Vector() 底层/*public Vector() &#123;this(10);&#125;补充：如果是 Vector vector = new Vector(8);走的方法:public Vector(int initialCapacity) &#123;this(initialCapacity, 0);&#125;2. vector.add(i)2.1 //下面这个方法就添加数据到 vector 集合public synchronized boolean add(E e) &#123;modCount++;ensureCapacityHelper(elementCount + 1);elementData[elementCount++] = e;return true;&#125;2.2 //确定是否需要扩容 条件 ： minCapacity - elementData.length&gt;0private void ensureCapacityHelper(int minCapacity) &#123;// overflow-conscious codeif (minCapacity - elementData.length &gt; 0grow(minCapacity);&#125;2.3 //如果 需要的数组大小 不够用，就扩容 , 扩容的算法//newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ?// capacityIncrement : oldCapacity);//就是扩容两倍. private void grow(int minCapacity) &#123;// overflow-conscious codeint oldCapacity = elementData.length;int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ?capacityIncrement : oldCapacity);if (newCapacity - minCapacity &lt; 0)newCapacity = minCapacity;if (newCapacity - MAX_ARRAY_SIZE &gt; 0)newCapacity = hugeCapacity(minCapacity);elementData = Arrays.copyOf(elementData, newCapacity);&#125;*/&#125;&#125;</code></pre><h3 id="Vector底层结构和ArrayList比较"><a href="#Vector底层结构和ArrayList比较" class="headerlink" title="Vector底层结构和ArrayList比较"></a>Vector底层结构和ArrayList比较</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220614121730002.png" alt="image-20220614121730002"></p><h3 id="LinkedList底层结构"><a href="#LinkedList底层结构" class="headerlink" title="LinkedList底层结构"></a>LinkedList底层结构</h3><ol><li><p>LinkedList底层实现了双向链表</p></li><li><p>可以添加任意元素（元素可以重复），包含null</p></li><li><p>线程不安全，没有实现同步</p></li></ol><h4 id="LinkedList-的底层操作机制"><a href="#LinkedList-的底层操作机制" class="headerlink" title="LinkedList 的底层操作机制"></a>LinkedList 的底层操作机制</h4><ol><li><p>LinkedList底层维护了一个双向链表</p></li><li><p>LinkedList中维护了两个属性first和last分别指向 首节点和尾节点</p></li><li><p>每个节点（Node对象）,俩面又维护了prev、next、item三个属性，其中通过prev指向前一个，通过next指向后一个节点。最终实现双向链表</p></li><li><p>所以LinkedList的元素的添加和删除，不是通过数组完成的，相对来说效率较高。</p></li></ol><h4 id="模拟一个简单的双向链表"><a href="#模拟一个简单的双向链表" class="headerlink" title="模拟一个简单的双向链表"></a>模拟一个简单的双向链表</h4><pre><code class="java">//定义一个 Node 类，Node 对象 表示双向链表的一个结点class Node &#123;public Object item; //真正存放数据public Node next; //指向后一个结点public Node pre; //指向前一个结点public Node(Object name) &#123;this.item = name;&#125;public String toString() &#123;return &quot;Node name=&quot; + item;&#125;&#125;</code></pre><pre><code class="java">public class LinkedList01 &#123;public static void main(String[] args) &#123;//模拟一个简单的双向链表Node jack = new Node(&quot;jack&quot;);Node tom = new Node(&quot;tom&quot;);Node hsp = new Node(&quot;老韩&quot;);//连接三个结点，形成双向链表//jack -&gt; tom -&gt; hspjack.next = tom;tom.next = hsp;//hsp -&gt; tom -&gt; jackhsp.pre = tom;tom.pre = jack;Node first = jack;//让 first 引用指向 jack,就是双向链表的头结点Node last = hsp; //让 last 引用指向 hsp,就是双向链表的尾结点//演示，从头到尾进行遍历System.out.println(&quot;===从头到尾进行遍历===&quot;);while (true) &#123;if(first == null) &#123;break;&#125;//输出 first 信息System.out.println(first);first &#125;//演示，从尾到头的遍历System.out.println(&quot;====从尾到头的遍历====&quot;);while (true) &#123;if(last == null) &#123;break;&#125;//输出 last 信息System.out.println(last);last = last.pre;&#125;//演示链表的添加对象/数据，是多么的方便//要求，是在 tom --------- 老韩直接，插入一个对象 smith//1. 先创建一个 Node 结点，name 就是 smithNode smith = new Node(&quot;smith&quot;);//下面就把 smith 加入到双向链表了smith.next = hsp;smith.pre = tom;hsp.pre = smith;tom.next = smith;//让 first 再次指向 jackfirst = jack;//让 first 引用指向 jack,就是双向链表的头结点System.out.println(&quot;===从头到尾进行遍历===&quot;);while (true) &#123;if(first == null) &#123;break;&#125;//输出 first 信息System.out.println(first);first = first.next;&#125;last = hsp; //让 last 重新指向最后一个结点//演示，从尾到头的遍历System.out.println(&quot;====从尾到头的遍历====&quot;);while (true) &#123;if(last == null) &#123;break;&#125;//输出 last 信息System.out.println(last);last = last.pre;&#125;&#125;&#125;</code></pre><h4 id="LinkedList-的增删改查案例"><a href="#LinkedList-的增删改查案例" class="headerlink" title="LinkedList 的增删改查案例"></a>LinkedList 的增删改查案例</h4><pre><code class="java">/*** @author 韩顺平* @version 1.0*/@SuppressWarnings(&#123;&quot;all&quot;&#125;)public class LinkedListCRUD &#123;public static void main(String[] args) &#123;LinkedList linkedList = new LinkedList();linkedList.add(1);linkedList.add(2);linkedList.add(3);System.out.println(&quot;linkedList=&quot; + linkedList);//演示一个删除结点的linkedList.remove(); // 这里默认删除的是第一个结点//linkedList.remove(2);System.out.println(&quot;linkedList=&quot; + linkedList);//修改某个结点对象linkedList.set(1, 999);System.out.println(&quot;linkedList=&quot; + linkedList);//得到某个结点对象//get(1) 是得到双向链表的第二个对象Object o = linkedList.get(1);System.out.println(o);//999//因为 LinkedList 是 实现了 List 接口, 遍历方式System.out.println(&quot;===LinkeList 遍历迭代器====&quot;);Iterator iterator = linkedList.iterator();while (iterator.hasNext()) &#123;Object next = iterator.next();System.out.println(&quot;next=&quot; + next);&#125;System.out.println(&quot;===LinkeList 遍历增强 for====&quot;);for (Object o1 : linkedList) &#123;System.out.println(&quot;o1=&quot; + o1);&#125;System.out.println(&quot;===LinkeList 遍历普通 for====&quot;);for (int i = 0; i &lt; linkedList.size(); i++) &#123;System.out.println(linkedList.get(i));&#125;//老韩源码阅读. /* 1. LinkedList linkedList = new LinkedList();public LinkedList() &#123;&#125;2. 这时 linkeList 的属性 first = null last = null3. 执行 添加public boolean add(E e) &#123;linkLast(e);return true;&#125;4.将新的结点，加入到双向链表的最后void linkLast(E e) &#123;final Node&lt;E&gt; l = last;final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null);last = newNode;if (l == null)first = newNode;elsel.next = newNode;size++;modCount++;&#125;*//*老韩读源码 linkedList.remove(); // 这里默认删除的是第一个结点1. 执行 removeFirstpublic E remove() &#123;return removeFirst();&#125;2. 执行public E removeFirst() &#123;final Node&lt;E&gt; f = first;if (f == null)throw new NoSuchElementException();return unlinkFirst(f);&#125;3. 执行 unlinkFirst, 将 f 指向的双向链表的第一个结点拿掉private E unlinkFirst(Node&lt;E&gt; f) &#123;// assert f == first &amp;&amp; f != null;final E element = f.item;final Node&lt;E&gt; next = f.next;f.item = null;f.next = null; // help GCfirst = next;if (next == null)last = null;elsenext.prev = null;size--;modCount++;return element;&#125;*/&#125;&#125;</code></pre><h3 id="ArrayList和LinkedList比较"><a href="#ArrayList和LinkedList比较" class="headerlink" title="ArrayList和LinkedList比较"></a>ArrayList和LinkedList比较</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220614144502748.png" alt="image-20220614144502748"></p><p>如何选择ArrayList和LinkedList</p><ol><li>如果我们改查的操作多，选择ArrayList</li><li>如果我们增删操作多，选择LinkedList</li><li>一般来说，在程序中，80%-90%都是查询，因此大部分情况下会选择ArrayList</li><li>在一个项目中，根据业务灵活选择，也可能这样，一个模块使用的是ArrayList，另外一个模块是LinkedList，要根据业务来进行选择</li></ol><h2 id="Set接口和常用方法"><a href="#Set接口和常用方法" class="headerlink" title="Set接口和常用方法"></a>Set接口和常用方法</h2><h3 id="Set-接口基本介绍"><a href="#Set-接口基本介绍" class="headerlink" title="Set 接口基本介绍"></a>Set 接口基本介绍</h3><ol><li>无序，（添加和取出的顺序不一致），没有索引</li><li>不允许重复元素，最多包含一个Null</li><li>JDK API中Set接口的实现类有</li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220614145003770.png" alt="image-20220614145003770"></p><h3 id="Set-接口的常用方法"><a href="#Set-接口的常用方法" class="headerlink" title="Set 接口的常用方法"></a>Set 接口的常用方法</h3><p>Set也是Collection接口的子接口，常用方法和Collection接口一样</p><h3 id="Set接口的便利方法"><a href="#Set接口的便利方法" class="headerlink" title="Set接口的便利方法"></a>Set接口的便利方法</h3><p>同Collection的遍历方式一样，因为Set接口是Collection接口的子接口。</p><ol><li>可以使用迭代器</li><li>增强for</li><li>不能使用索引的方式来获取</li></ol><h3 id="Set-接口的常用方法举例"><a href="#Set-接口的常用方法举例" class="headerlink" title="Set 接口的常用方法举例"></a>Set 接口的常用方法举例</h3><pre><code class="java">import java.util.HashSet;import java.util.Iterator;import java.util.Set;/*** @author 韩顺平* @version 1.0*/@SuppressWarnings(&#123;&quot;all&quot;&#125;)public class SetMethod &#123;public static void main(String[] args) &#123;//老韩解读//1. 以 Set 接口的实现类 HashSet 来讲解 Set 接口的方法//2. set 接口的实现类的对象(Set 接口对象), 不能存放重复的元素, 可以添加一个 null//3. set 接口对象存放数据是无序(即添加的顺序和取出的顺序不一致)//4. 注意：取出的顺序的顺序虽然不是添加的顺序，但是他的固定.     Set set = new HashSet();set.add(&quot;john&quot;);set.add(&quot;lucy&quot;);set.add(&quot;john&quot;);//重复set.add(&quot;jack&quot;);set.add(&quot;hsp&quot;);set.add(&quot;mary&quot;);set.add(null);//set.add(null);//再次添加 null//顺序是固定的for(int i = 0; i &lt;10;i ++) &#123;System.out.println(&quot;set=&quot; + set);&#125;//遍历//方式 1： 使用迭代器System.out.println(&quot;=====使用迭代器====&quot;);Iterator iterator = set.iterator();while (iterator.hasNext()) &#123;Object obj = iterator.next();System.out.println(&quot;obj=&quot; + obj);&#125;set.remove(null);//方式 2: 增强 forSystem.out.println(&quot;=====增强 for====&quot;);for (Object o : set) &#123;System.out.println(&quot;o=&quot; + o);&#125;//set 接口对象，不能通过索引来获取&#125;&#125;</code></pre><h3 id="Set接口实现类-HashSet"><a href="#Set接口实现类-HashSet" class="headerlink" title="Set接口实现类-HashSet"></a>Set接口实现类-HashSet</h3><h4 id="HashSet-全面说明"><a href="#HashSet-全面说明" class="headerlink" title="HashSet 全面说明"></a>HashSet 全面说明</h4><ol><li><p>HashSet实现了Set接口</p></li><li><p>HashSet实际上是HashMap</p><p><img src="/java/java-xue-xi-wen-dang/image-20220614150230761.png" alt="image-20220614150230761"></p></li><li><p>可以存放null值，但只能有一个</p></li><li><p>HashSet不保证元素是有序的，取决于hash后，在确定索引的结果（即，不保证存放元素的顺序和取出顺序一致）</p></li><li><p>不能用重复的元素&#x2F;对象。在前面Set接口使用已经讲过。</p></li><li><p>底层是HashMap，HashMap底层使用（数组+单向链表+红黑树）的方式实现的</p></li></ol><h4 id="HashSet案例（难点）"><a href="#HashSet案例（难点）" class="headerlink" title="HashSet案例（难点）"></a>HashSet案例（难点）</h4><pre><code class="java">class Dog &#123; //定义了 Dog 类private String name;public Dog(String name) &#123;this.name = name;&#125;@Overridepublic String toString() &#123;return &quot;Dog&#123;&quot; +&quot;name=&#39;&quot; + name + &#39;\&#39;&#39; +&#39;&#125;&#39;;&#125;&#125;</code></pre><pre><code class="java">/*** @author 韩顺平* @version 1.0*/@SuppressWarnings(&#123;&quot;all&quot;&#125;)public class HashSet01 &#123;public static void main(String[] args) &#123;HashSet set = new HashSet();//说明//1. 在执行 add 方法后，会返回一个 boolean 值//2. 如果添加成功，返回 true, 否则返回 false//3. 可以通过 remove 指定删除哪个对象System.out.println(set.add(&quot;john&quot;));//TSystem.out.println(set.add(&quot;lucy&quot;));//TSystem.out.println(set.add(&quot;john&quot;));//FSystem.out.println(set.add(&quot;jack&quot;));//TSystem.out.println(set.add(&quot;Rose&quot;));//Tset.remove(&quot;john&quot;);System.out.println(&quot;set=&quot; + set);//3 个//set = new HashSet();System.out.println(&quot;set=&quot; + set);//0//4 Hashset 不能添加相同的元素/数据?set.add(&quot;lucy&quot;);//添加成功set.add(&quot;lucy&quot;);//加入不了//这两个对象在这里没有重写hash和equals方法，两个对象返回的是不同的值set.add(new Dog(&quot;tom&quot;));//OKset.add(new Dog(&quot;tom&quot;));//OkSystem.out.println(&quot;set=&quot; + set);    //在加深一下. 非常经典的面试题//看源码，做分析， 先给小伙伴留一个坑，以后讲完源码，你就了然//去看他的源码，即 add 到底发生了什么?=&gt; 底层机制.  //底层hash方法会调用set.add(new String(&quot;hsp&quot;));//okset.add(new String(&quot;hsp&quot;));//加入不了. System.out.println(&quot;set=&quot; + set);&#125;&#125;</code></pre><h4 id="HashSet面试题"><a href="#HashSet面试题" class="headerlink" title="HashSet面试题"></a>HashSet面试题</h4><pre><code class="java">set.add(new Dog(&quot;tom&quot;));//OKset.add(new Dog(&quot;tom&quot;));//OkSystem.out.println(&quot;set=&quot; + set);set.add(new String(&quot;hsp&quot;));//okset.add(new String(&quot;hsp&quot;));//加入不了. 为什么？//hash+equals两个方法返回值相同就无法加入</code></pre><h4 id="HashSet底层机制说明"><a href="#HashSet底层机制说明" class="headerlink" title="HashSet底层机制说明"></a>HashSet底层机制说明</h4><p>看源码！下面是简略总结</p><ol><li><p>HashSet底层是HashMap</p><pre><code class="java">    public HashSet() &#123;        map = new HashMap&lt;&gt;();    &#125;</code></pre></li><li><p>会先创建一个16个长度的Node数组</p><pre><code class="java">static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 1*2*2*2*2=16Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap];</code></pre></li><li><p>添加一个元素时，先得到Hash值，会转成-&gt;索引值</p><pre><code class="java">/*private static final Object PRESENT = new Object();Set相当于只使用了Map的Key，Value使用 PRESENT 填充的。*/public boolean add(E e) &#123;    return map.put(e, PRESENT)==null;&#125;public V put(K key, V value) &#123;        return putVal(hash(key), key, value, false, true);&#125;</code></pre></li><li><p>找到存储数据表table，看这个索引位置是否已经存放的有元素</p></li><li><p>如果没有，直接加入</p></li><li><p>如果有，调用equals比较，如果相同，就放弃添加，如果不相同，则添加到最后</p><pre><code class="java">    final V putVal(int hash, K key, V value, boolean onlyIfAbsent,                   boolean evict) &#123;        Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i;        if ((tab = table) == null || (n = tab.length) == 0)            n = (tab = resize()).length;        if ((p = tab[i = (n - 1) &amp; hash]) == null)            tab[i] = newNode(hash, key, value, null);        else &#123;            Node&lt;K,V&gt; e; K k;            if (p.hash == hash &amp;&amp;                ((k = p.key) == key || (key != null &amp;&amp; key.equals(k))))                e = p;            else if (p instanceof TreeNode)                e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value);            else &#123;                for (int binCount = 0; ; ++binCount) &#123;                    if ((e = p.next) == null) &#123;                        p.next = newNode(hash, key, value, null);                        if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st                            treeifyBin(tab, hash);                        break;                    &#125;                    if (e.hash == hash &amp;&amp;                        ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))                        break;                    p = e;                &#125;            &#125;            if (e != null) &#123; // existing mapping for key                V oldValue = e.value;                if (!onlyIfAbsent || oldValue == null)                    e.value = value;                afterNodeAccess(e);                return oldValue;            &#125;        &#125;        ++modCount;        if (++size &gt; threshold)            resize();        afterNodeInsertion(evict);        return null;    &#125;</code></pre></li><li><p>在Java8中，如果一条链表的元素个数到达TREEIFY_THRESHOLD(默认是8)，并且table的大小 &gt;&#x3D; MIN_TREEIFY_CAPACITY（默认64），就会进行数化（红黑树），不够64先会扩容<code>table</code>数组，</p><pre><code class="java">            else &#123;                for (int binCount = 0; ; ++binCount) &#123;                    if ((e = p.next) == null) &#123;                        p.next = newNode(hash, key, value, null);                        if (binCount &gt;= TREEIFY_THRESHOLD - 1) // 8-1 for 1st                             treeifyBin(tab, hash);                        break;                    &#125;                    if (e.hash == hash &amp;&amp;                        ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))                        break;                    p = e;                &#125;            &#125;</code></pre><p><img src="/java/java-xue-xi-wen-dang/image-20220614174348275.png" alt="image-20220614174348275"></p></li></ol><h4 id="HashSet扩容和转成红黑树机制"><a href="#HashSet扩容和转成红黑树机制" class="headerlink" title="HashSet扩容和转成红黑树机制"></a>HashSet扩容和转成红黑树机制</h4><ol><li>HashSet底层是HashMap，第一次添加时，table数组扩容到16，临界值（threshold）是16*加载因子（loadFactor）0.75 &#x3D; 12</li><li>如果table数组使用到了临界值12，就会扩容到16*2 &#x3D; 32，新的临界值就是32 * 0.75 &#x3D; 24，以此类推</li><li>Java8中，如果一条链表的元素个数到达 TREEIFY_THRESHOLD(默认是8)，并且table的大小&gt;&#x3D;MIN_TREEIFY_CAPACITY（默认64），就会进行树化（红黑树），否则依然采用扩容机制</li><li>转换机制会对数组一个索引位置的元素进行转换，数组其他索引位置的元素不会发生转换操作，直到满足条件。</li></ol><pre><code class="java">if (binCount &gt;= TREEIFY_THRESHOLD - 1) // 8-1 for 1st     treeifyBin(tab, hash);</code></pre><h4 id="HashSet扩容理解实例"><a href="#HashSet扩容理解实例" class="headerlink" title="HashSet扩容理解实例"></a>HashSet扩容理解实例</h4><pre><code class="java">/*1看A类的hashCode方法，Hash值不同，会被分到不同的数组中        if (++size &gt; threshold) 12+1 &gt; 13            resize(); 扩容数组*/public static void main(String[] args) &#123;    HashSet set = new HashSet();        for (int i = 0; i &lt; 13; i++) &#123;            set.add(new A(i));        &#125;    class A &#123;    private Integer aa;    public A() &#123;    &#125;    public A(Integer aa) &#123;        this.aa = aa;    &#125;    @Override    public boolean equals(Object o) &#123;        if (this == o) return true;        if (!(o instanceof A)) return false;        A a = (A) o;        return Objects.equals(aa, a.aa);    &#125;    @Override    public int hashCode() &#123;        return Objects.hash(aa);    &#125;&#125;</code></pre><p><img src="/java/java-xue-xi-wen-dang/image-20220614183854928.png" alt="image-20220614183854928"></p><pre><code class="java">/*2看A类的hashCode方法，hash值一样会被分到同一个数组位置if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st    treeifyBin(tab, hash);    break;*/public static void main(String[] args) &#123;    HashSet set = new HashSet();        for (int i = 0; i &lt; 13; i++) &#123;            set.add(new A(i));        &#125;    class A &#123;    private Integer aa;    public A() &#123;    &#125;    public A(Integer aa) &#123;        this.aa = aa;    &#125;    @Override    public boolean equals(Object o) &#123;        if (this == o) return true;        if (!(o instanceof A)) return false;        A a = (A) o;        return Objects.equals(aa, a.aa);    &#125;    @Override    public int hashCode() &#123;        return 100;    &#125;&#125;</code></pre><p><img src="/java/java-xue-xi-wen-dang/image-20220614184020108.png" alt="image-20220614184020108"></p><h3 id="Set-接口实现类-LinkedHashSet"><a href="#Set-接口实现类-LinkedHashSet" class="headerlink" title="Set 接口实现类-LinkedHashSet"></a>Set 接口实现类-LinkedHashSet</h3><ol><li>LinkedHashSet是HashSet的子类</li><li>LinkedHashSet底层是一个LinkedHashMap，底层维护了一个数组+双向链表</li><li>LinkedHashSet根据元素的hashCode值来决定元素的存储位置。同时使用链表维护元素的&#x3D;&#x3D;次序&#x3D;&#x3D;，这使得元素看起来是以插入顺序保存的。</li><li>LinkedHashSet不允许添加重复元素。</li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220614191124431.png" alt="image-20220614191124431"></p><p>数据存放在数组的索引位置不同，但是他们之间是有指向关系的。保存了插入的顺序。</p><ol><li><p>在LinkedHashSet维护了一个hash表（数组+哈希函数）和双向链表（LinkedHashSet 有 head和tail）</p></li><li><p>每一个节点都有before 和 after 属性，这样可以形成双向链表</p></li><li><p>在添加一个元素时，先求hash值，再求索引，确定该元素在table的位置，然后将添加的元素加入到双向链表（如果已经存在，不添加）</p><pre><code class="java">tail.next = newElementnewElement.pre = tail</code></pre></li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220614200055057.png" alt="image-20220614200055057"></p><h3 id="Set接口实现类-TreeSet"><a href="#Set接口实现类-TreeSet" class="headerlink" title="Set接口实现类-TreeSet"></a>Set接口实现类-TreeSet</h3><ol><li><p>当使用无参构造器创建TreeSet时，他默认是无序的</p></li><li><p>可以调用有参构造器，进行排序，底层是TreeMap$Entry类型</p><p><img src="/java/java-xue-xi-wen-dang/image-20220615132501587.png" alt="image-20220615132501587"></p></li></ol><h4 id="TreeSet底层机制"><a href="#TreeSet底层机制" class="headerlink" title="TreeSet底层机制"></a>TreeSet底层机制</h4><ol><li>TreeSet底层是TreeMap</li><li>具体调用和之前类似，看源码。</li></ol><h2 id="Map-接口和常用方法"><a href="#Map-接口和常用方法" class="headerlink" title="Map 接口和常用方法"></a>Map 接口和常用方法</h2><p><img src="/java/java-xue-xi-wen-dang/image-20220613232151016.png" alt="image-20220613232151016"></p><h3 id="Map-接口实现类的特点-实用"><a href="#Map-接口实现类的特点-实用" class="headerlink" title="Map 接口实现类的特点[实用]"></a>Map 接口实现类的特点[实用]</h3><p>注意：这是JDK8的Map接口特点</p><ol><li>Map与Collection并列存在。用于保存具有映射关系的数据：Key-Value</li><li>Map中的key和value可以是任何引用类型的数据，会封装到HashMap$Node对象中</li><li>Map中的key不允许重复，原因和HashSet一样，前面分析过源码</li><li>Map中的value可以重复</li><li>Map的key 可以为null，value也可以为null，注意key 为null，只能有一个，value为null，可以多个</li><li>常用String类作为Map的key</li><li>key和value之间存在单向一对一关系，即通过指定的key总能找到对应的value</li><li>Map存放数据的Key-Value示意图，一对k-v 是放在一个HashMap$Node中的，因为Node实现了Entry接口，有的书上也说一对kv就是一个Entry</li></ol><p>entrySet类底层存储的还是Node类</p><pre><code class="java">    public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123;        Set&lt;Map.Entry&lt;K,V&gt;&gt; es;        return (es = entrySet) == null ? (entrySet = new EntrySet()) : es;    &#125;</code></pre><p><img src="/java/java-xue-xi-wen-dang/image-20220614220252972.png" alt="image-20220614220252972"></p><pre><code class="java">    public static void main(String[] args) &#123;        HashMap map = new HashMap&lt;Object,Object&gt;();        map.put(null,null);        map.put(&quot;null1&quot;,1);        map.put(&quot;zsf&quot;,2);        map.put(2,&quot;wangwu&quot;);        System.out.println(map.get(2));//wangwu        System.out.println(map);//&#123;null=null, 2=wangwu, zsf=2, null1=1&#125;        Set keySet = map.keySet();        System.out.println(keySet);//[null, 2, zsf, null1]        Set set = map.entrySet();        for(Object obj : set)&#123;            System.out.println(obj.getClass());//class java.util.HashMap$Node            Map.Entry entry = (Map.Entry)obj;            System.out.println(entry.getKey()+&quot;    &quot;+entry.getValue());        &#125;        System.out.println(set);    &#125;</code></pre><p>HashMap是单向链表实现的，HashSet也是使用的这个</p><p><img src="/java/java-xue-xi-wen-dang/image-20220614212220227.png" alt="image-20220614212220227"></p><h3 id="Map的常用方法"><a href="#Map的常用方法" class="headerlink" title="Map的常用方法"></a>Map的常用方法</h3><pre><code class="java">/*** @author 韩顺平* @version 1.0*/@SuppressWarnings(&#123;&quot;all&quot;&#125;)public class MapMethod &#123;public static void main(String[] args) &#123;//演示 map 接口常用方法Map map = new HashMap();map.put(&quot;邓超&quot;, new Book(&quot;&quot;, 100));//OKmap.put(&quot;邓超&quot;, &quot;孙俪&quot;);//替换-&gt; 一会分析源码map.put(&quot;王宝强&quot;, &quot;马蓉&quot;);//OKmap.put(&quot;宋喆&quot;, &quot;马蓉&quot;);//OKmap.put(&quot;刘令博&quot;, null);//OKmap.put(null, &quot;刘亦菲&quot;);//OKmap.put(&quot;鹿晗&quot;, &quot;关晓彤&quot;);//OKmap.put(&quot;hsp&quot;, &quot;hsp 的老婆&quot;);System.out.println(&quot;map=&quot; + map);    // remove:根据键删除映射关系map.remove(null);System.out.println(&quot;map=&quot; + map);    // get：根据键获取值Object val = map.get(&quot;鹿晗&quot;);System.out.println(&quot;val=&quot; + val);//关晓彤    // size:获取元素个数System.out.println(&quot;k-v=&quot; + map.size());    // isEmpty:判断个数是否为 0System.out.println(map.isEmpty());//F    // clear:清除 k-v//map.clear();System.out.println(&quot;map=&quot; + map);// containsKey:查找键是否存在System.out.println(&quot;结果=&quot; + map.containsKey(&quot;hsp&quot;));//T&#125;&#125;class Book &#123;private String name;private int num;public Book(String name, int num) &#123;this.name = name;this.num = num;&#125;&#125;</code></pre><h3 id="Map接口遍历方式"><a href="#Map接口遍历方式" class="headerlink" title="Map接口遍历方式"></a>Map接口遍历方式</h3><ol><li>containsKey：查找健是否存在</li><li>keySet：获取所有键</li><li>entrySet：获取所有关系</li><li>values：获取所有的值</li></ol><pre><code class="java">package hsppedu.Homework.collection_homework;import java.util.*;/** * @author 张文辉 * @version 1.0 */public class MapMethod &#123;    public static void main(String[] args) &#123;        //演示 map 接口常用方法        Map map = new HashMap();        map.put(&quot;邓超&quot;, new Book(&quot;aa&quot;, &quot;aa作者&quot;, 100));//OK        map.put(&quot;邓超&quot;, &quot;孙俪&quot;);//替换-&gt; 一会分析源码        map.put(&quot;王宝强&quot;, &quot;马蓉&quot;);//OK        map.put(&quot;宋喆&quot;, &quot;马蓉&quot;);//OK        map.put(&quot;刘令博&quot;, null);//OK        map.put(null, &quot;刘亦菲&quot;);//OK        map.put(&quot;鹿晗&quot;, &quot;关晓彤&quot;);//OK        map.put(&quot;hsp&quot;, &quot;hsp 的老婆&quot;);        System.out.println(&quot;map=&quot; + map);        Set keySet = map.keySet();        //(1) 增强 for        System.out.println(&quot;-----第一种方式-------&quot;);        for (Object key : keySet) &#123;            System.out.println(key + &quot;-&quot; + map.get(key));        &#125;        //(2) 迭代器        System.out.println(&quot;----第二种方式--------&quot;);        Iterator iterator = keySet.iterator();        while (iterator.hasNext()) &#123;            Object key = iterator.next();            System.out.println(key + &quot;-&quot; + map.get(key));        &#125;        //第二组: 把所有的 values 取出        Collection values = map.values();        //这里可以使用所有的 Collections 使用的遍历方法        //(1) 增强 for        System.out.println(&quot;---取出所有的 value 增强 for----&quot;);        for (Object value : values) &#123;            System.out.println(value);        &#125;        //(2) 迭代器        System.out.println(&quot;---取出所有的 value 迭代器----&quot;);        Iterator iterator2 = values.iterator();        while (iterator2.hasNext()) &#123;            Object value = iterator2.next();            System.out.println(value);        &#125;        //第三组: 通过 EntrySet 来获取 k-v        Set entrySet = map.entrySet();// EntrySet&lt;Map.Entry&lt;K,V&gt;&gt;        //(1) 增强 for        System.out.println(&quot;----使用 EntrySet 的 for 增强(第 3 种)----&quot;);        for (Object entry : entrySet) &#123;        //将 entry 转成 Map.Entry            Map.Entry m = (Map.Entry) entry;            System.out.println(m.getKey() + &quot;-&quot; + m.getValue());        &#125;        //(2) 迭代器        System.out.println(&quot;----使用 EntrySet 的 迭代器(第 4 种)----&quot;);        Iterator iterator3 = entrySet.iterator();        while (iterator3.hasNext()) &#123;            Object entry = iterator3.next();//System.out.println(next.getClass());//HashMap$Node -实现-&gt; Map.Entry (getKey,getValue)//向下转型 Map.Entry            Map.Entry m = (Map.Entry) entry;            System.out.println(m.getKey() + &quot;-&quot; + m.getValue());        &#125;    &#125;&#125;</code></pre><h3 id="HashMap小结"><a href="#HashMap小结" class="headerlink" title="HashMap小结"></a>HashMap小结</h3><ol><li>Map接口常用实现类：HashMap、Hashtable和Properties</li><li>HashMap是Map接口使用频率最高的实现类</li><li>HashMap是以key-val对的方式来存储数据（HashMap$Node类型）</li><li>key不能重复，但是值可以重复，允许使用null键和null值</li><li>如果添加相同的key，则会覆盖原来的key-val，等同于修改（key不会替换，value替换）</li><li>与HashSet一样，不保证映射的顺序，因为底层是hash表的方式来存储。（底层：数组+链表+红黑树（链表和红黑树结构可以同时存在））</li><li>HashMap没有实现同步，因此线程不安全。</li></ol><h3 id="底层示意图"><a href="#底层示意图" class="headerlink" title="底层示意图"></a>底层示意图</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220614230601814.png" alt="image-20220614230601814"></p><h3 id="HashMap扩容机制"><a href="#HashMap扩容机制" class="headerlink" title="HashMap扩容机制"></a>HashMap扩容机制</h3><p>和HashSet相同，HashSet就是HashMap。</p><p><img src="/java/java-xue-xi-wen-dang/image-20220614230651290.png" alt="image-20220614230651290"></p><h3 id="HashMap替换机制"><a href="#HashMap替换机制" class="headerlink" title="HashMap替换机制"></a>HashMap替换机制</h3><p>HashMap相同的Key输入，会把原来位置的Node对象value重新赋值</p><p><img src="/java/java-xue-xi-wen-dang/image-20220614231151540.png" alt="image-20220614231151540"></p><h3 id="HashMap底层机制说明"><a href="#HashMap底层机制说明" class="headerlink" title="HashMap底层机制说明"></a>HashMap底层机制说明</h3><ol><li>执行构造器，初始化加载因子</li></ol><pre><code class="java">    /**     * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity     * (16) and the default load factor (0.75).     */    public HashMap() &#123;        this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted    &#125;</code></pre><ol start="2"><li>执行put方法，计算key的hash值</li></ol><pre><code class="java">map.put(null,null);    public V put(K key, V value) &#123;        return putVal(hash(key), key, value, false, true);    &#125;</code></pre><ol start="3"><li>执行putVal方法</li><li>具体执行流程自己看源码！</li></ol><h3 id="Map接口实现类-HashTable"><a href="#Map接口实现类-HashTable" class="headerlink" title="Map接口实现类-HashTable"></a>Map接口实现类-HashTable</h3><ol><li>存放的是键值对：即K-V</li><li>hashtable的键和值都不能为null，否则会抛出NullPointException</li><li>hashTable 使用方法基本上和HashMap一样</li><li>HashTable是线程安全的（synchronized），hashMap是线程不安全的</li></ol><h4 id="HashTable底层机制"><a href="#HashTable底层机制" class="headerlink" title="HashTable底层机制"></a>HashTable底层机制</h4><ol><li>初始化容量是11</li><li>临界值 11 * 0.75</li><li>扩容是  （当前容量 * 2 ）+ 1</li></ol><pre><code class="java">    public Hashtable() &#123;        this(11, 0.75f);    &#125;</code></pre><ol start="2"><li>剩下看源码，基本上和之前的相同</li></ol><h3 id="HashTable和HashMap对比"><a href="#HashTable和HashMap对比" class="headerlink" title="HashTable和HashMap对比"></a>HashTable和HashMap对比</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220615121958797.png" alt="image-20220615121958797"></p><h3 id="Map接口实现类-Properties"><a href="#Map接口实现类-Properties" class="headerlink" title="Map接口实现类-Properties"></a>Map接口实现类-Properties</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220615123541279.png" alt="image-20220615123541279"></p><ol><li>Properties类继承自HashTable类并且实现了Map接口，并且是HashTable的子类，也是使用一种键值对方式来保存数据</li><li>使用和HashTable类似，k-v不能为Null</li><li>Properties还可以用于从xxx.properties文件中，加载数据到Properties类对象，并进行读取和修改</li><li>xxx.properties文件通常作为配置文件，</li></ol><h4 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h4><pre><code class="java">/*** @author 韩顺平* @version 1.0*/@SuppressWarnings(&#123;&quot;all&quot;&#125;)public class Properties_ &#123;public static void main(String[] args) &#123;//老韩解读//1. Properties 继承 Hashtable//2. 可以通过 k-v 存放数据，当然 key 和 value 不能为 null//增加Properties properties = new Properties();//properties.put(null, &quot;abc&quot;);//抛出 空指针异常//properties.put(&quot;abc&quot;, null); //抛出 空指针异常properties.put(&quot;john&quot;, 100);//k-vproperties.put(&quot;lucy&quot;, 100);properties.put(&quot;lic&quot;, 100);properties.put(&quot;lic&quot;, 88);//如果有相同的 key ， value 被替换System.out.println(&quot;properties=&quot; + properties);//通过 k 获取对应值System.out.println(properties.get(&quot;lic&quot;));//88//删除properties.remove(&quot;lic&quot;);System.out.println(&quot;properties=&quot; + properties);//修改properties.put(&quot;john&quot;, &quot;约翰&quot;);System.out.println(&quot;properties=&quot; + properties);&#125;&#125;/*properties=&#123;john=100, lic=88, lucy=100&#125;88properties=&#123;john=100, lucy=100&#125;properties=&#123;john=约翰, lucy=100&#125;*/</code></pre><h3 id="Map接口实现类-TreeMap"><a href="#Map接口实现类-TreeMap" class="headerlink" title="Map接口实现类-TreeMap"></a>Map接口实现类-TreeMap</h3><ol><li>使用默认无参构造器，创建TreeMap，也是无序的。</li><li>使用 TreeSet 提供的一个构造器，可以传入一个比较器(匿名内部类) 并指定排序规则</li></ol><pre><code class="java">*** @author 韩顺平* @version 1.0*/@SuppressWarnings(&#123;&quot;all&quot;&#125;)public class TreeMap_ &#123;public static void main(String[] args) &#123;//使用默认的构造器，创建 TreeMap, 是无序的(也没有排序)/*老韩要求：按照传入的 k(String) 的大小进行排序*/// TreeMap treeMap = new TreeMap();TreeMap treeMap = new TreeMap(new Comparator() &#123;@Overridepublic int compare(Object o1, Object o2) &#123;//按照传入的 k(String) 的大小进行排序//按照 K(String) 的长度大小排序//return ((String) o2).compareTo((String) o1);return ((String) o2).length() - ((String) o1).length();&#125;&#125;);treeMap.put(&quot;jack&quot;, &quot;杰克&quot;);treeMap.put(&quot;tom&quot;, &quot;汤姆&quot;);treeMap.put(&quot;kristina&quot;, &quot;克瑞斯提诺&quot;);treeMap.put(&quot;smith&quot;, &quot;斯密斯&quot;);treeMap.put(&quot;hsp&quot;, &quot;韩顺平&quot;);//加入不了System.out.println(&quot;treemap=&quot; + treeMap);/*</code></pre><h2 id="集合选型规则"><a href="#集合选型规则" class="headerlink" title="集合选型规则"></a>集合选型规则</h2><p>kai发中，选择什么样的集合实现类，主要取决于业务操作特点，根据集合实现类特性选择：</p><ol><li>先判断存储类型</li><li>一组对象：Collection接口<ul><li>允许重复：&#x3D;&#x3D;List&#x3D;&#x3D;<ul><li>增删多：LinkedList【双向链表】</li><li>改查多：ArrayList【可变数组】</li></ul></li><li>不允许重复：&#x3D;&#x3D;Set&#x3D;&#x3D;<ul><li>无序：HashSet【底层HashMap，维护了一个hash表【数组+单向链表+红黑树】】</li><li>排序：&#x3D;&#x3D;TreeSet&#x3D;&#x3D;</li><li>插入和取出顺序一致：LinkedHashSet，维护数组+双向链表</li></ul></li></ul></li><li>一组键值对：Map<ul><li>键无序：HashMap【底层【数组+单向链表+红黑树】，JDK7只有【数组+单向链表】】</li><li>键排序：TreeMap</li><li>键插入和取出顺序一致：LinkedHashMap</li><li>读取文件 Properties</li></ul></li></ol><h2 id="Collections工具类"><a href="#Collections工具类" class="headerlink" title="Collections工具类"></a>Collections工具类</h2><h3 id="Collections工具类介绍"><a href="#Collections工具类介绍" class="headerlink" title="Collections工具类介绍"></a>Collections工具类介绍</h3><ol><li>Collections是一个操作Set、List和Map等集合的工具类</li><li>Collections中提供了一系列静态方法对集合元素进行排序，查询，和修改操作</li></ol><h3 id="排序操作"><a href="#排序操作" class="headerlink" title="排序操作"></a>排序操作</h3><ol><li>reverse（List）：反转List中元素的顺序</li><li>shuffle（List）：对List集合元素进行随机排序</li><li>sort（List）：根据元素的自然顺序对指定List集合元素按升序排序</li><li>sort(List,Comparator)：根据指定的Comparator产生的顺序对List集合元素进行排序</li><li>swap(List，int，int)：将指定list集合中的i处元素和j处元素进行交换</li></ol><h3 id="查找、替换"><a href="#查找、替换" class="headerlink" title="查找、替换"></a>查找、替换</h3><ol><li>Object max(Collection)：根据元素的自然排序，返回给定集合中的最大元素</li><li>Object max(Collection)：根据Comparator指定的顺序，返回给定集合中的最大元素</li><li>Object min(Collection)：</li><li>Object min(Collection，Comparator)：</li><li>int frequency(Collection，Object)：返回指定集合中指定元素的出现次数</li><li>void copy(List dest，List src)：将src中的内容复制到dest中</li><li>boolean replaceAll(List list，Object oldVal，Object newVal)：使用新值替换List对象的所有旧值。</li></ol><pre><code class="java">@SuppressWarnings(&#123;&quot;all&quot;&#125;)public class Collections_ &#123;public static void main(String[] args) &#123;//创建 ArrayList 集合，用于测试. List list = new ArrayList();list.add(&quot;tom&quot;);list.add(&quot;smith&quot;);list.add(&quot;king&quot;);list.add(&quot;milan&quot;);list.add(&quot;tom&quot;);// reverse(List)：反转 List 中元素的顺序Collections.reverse(list);System.out.println(&quot;list=&quot; + list);// shuffle(List)：对 List 集合元素进行随机排序// for (int i = 0; i &lt; 5; i++) &#123;// Collections.shuffle(list);// System.out.println(&quot;list=&quot; + list);// &#125;// sort(List)：根据元素的自然顺序对指定 List 集合元素按升序排序Collections.sort(list);System.out.println(&quot;自然排序后&quot;);System.out.println(&quot;list=&quot; + list);// sort(List，Comparator)：根据指定的 Comparator 产生的顺序对 List 集合元素进行排序//我们希望按照 字符串的长度大小排序Collections.sort(list, new Comparator() &#123;@Overridepublic int compare(Object o1, Object o2) &#123;//可以加入校验代码. return ((String) o2).length() - ((String) o1).length();&#125;&#125;);System.out.println(&quot;字符串长度大小排序=&quot; + list);// swap(List，int， int)：将指定 list 集合中的 i 处元素和 j 处元素进行交换//比如Collections.swap(list, 0, 1);System.out.println(&quot;交换后的情况&quot;);System.out.println(&quot;list=&quot; + list);//Object max(Collection)：根据元素的自然顺序，返回给定集合中的最大元素System.out.println(&quot;自然顺序最大元素=&quot; + Collections.max(list));//Object max(Collection，Comparator)：根据 Comparator 指定的顺序，返回给定集合中的最大元素//比如，我们要返回长度最大的元素Object maxObject = Collections.max(list, new Comparator() &#123;@Overridepublic int compare(Object o1, Object o2) &#123;return ((String)o1).length() - ((String)o2).length();&#125;&#125;);System.out.println(&quot;长度最大的元素=&quot; + maxObject);//Object min(Collection)//Object min(Collection，Comparator)//上面的两个方法，参考 max 即可//int frequency(Collection，Object)：返回指定集合中指定元素的出现次数System.out.println(&quot;tom 出现的次数=&quot; + Collections.frequency(list, &quot;tom&quot;));//void copy(List dest,List src)：将 src 中的内容复制到 dest 中ArrayList dest = new ArrayList();//为了完成一个完整拷贝，我们需要先给 dest 赋值，大小和 list.size()一样for(int i = 0; i &lt; list.size(); i++) &#123;dest.add(&quot;&quot;);&#125;//拷贝Collections.copy(dest, list);System.out.println(&quot;dest=&quot; + dest);//boolean replaceAll(List list，Object oldVal，Object newVal)：使用新值替换 List 对象的所有旧值//如果 list 中，有 tom 就替换成 汤姆Collections.replaceAll(list, &quot;tom&quot;, &quot;汤姆&quot;);System.out.println(&quot;list 替换后=&quot; + list);&#125;&#125;</code></pre><h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><ol><li>HashSet和TreeSet分别如何实现去重的？</li></ol><p>HashSet去重机制：Hashcode()+equals()，底层存入对象计算hash值，通过hash值得到对应的索引，去找table数组索引所在得位置，没有数据，直接存入，有数据，先进行equals方法比较，相同就不加入</p><p>TreeSet去重机制：如果传入一个Comparator对象，就调用comparator方法去重，返回0，代表相同元素，就不添加。如果没有传入Comparator对象，添加的对象必须实现Compareable接口的compareTo去重。</p><ol start="2"><li>下面代码会报错吗？</li></ol><p>会报错，因为TreeSet调用CompareTo方法，而你没有，就会报错：</p><p><code>.Person cannot be cast to java.lang.Comparable</code></p><pre><code class="java">class Person&#123;&#125;        TreeSet treeSet = new TreeSet();        treeSet.add(new Person());</code></pre><ol start="3"><li>Vector和ArrayList比较？</li></ol><p><img src="/java/java-xue-xi-wen-dang/image-20220615160525782.png" alt="image-20220615160525782"></p><h1 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h1><h2 id="传统方式问题"><a href="#传统方式问题" class="headerlink" title="传统方式问题"></a>传统方式问题</h2><ol><li>不能对加入到集合ArrayList中的数据类型进行约束（不安全）</li><li>便利的时候，需要进行类型转换，如果集合中数据量较大，对效率有影响。</li></ol><h2 id="泛型的理解和好处"><a href="#泛型的理解和好处" class="headerlink" title="泛型的理解和好处"></a>泛型的理解和好处</h2><h3 id="泛型的好处"><a href="#泛型的好处" class="headerlink" title="泛型的好处"></a>泛型的好处</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220615174246909.png" alt="泛型的好处"></p><h2 id="泛型介绍"><a href="#泛型介绍" class="headerlink" title="泛型介绍"></a>泛型介绍</h2><ol><li>泛型又称参数化类型，是JDK5.0出现的新特性，解决数据类型的安全性问题</li><li>在类声明或实例化时，只要制定好需要的具体的类型即可</li><li>Java泛型可以保证如过程序在编译时没有发出警告，运行时就不会产生ClassCastException异常。同时，代码更加简洁，健壮。</li><li>泛型的作用是：可以在类声明时通过一个标识表示类中某个属性的类型，或者是某个方法的返回值类型，或者是参数类型。【下面是实例】</li></ol><pre><code class="java">class Person&lt;E&gt; &#123;    E s;//E 表示 s 的数据类型, 该数据类型在定义 Person 对象的时候指定,即在编译期间，就确定 E 是什么类型    public Person(E s) &#123;//E 也可以是参数类型        this.s = s;    &#125;    public E f() &#123;//返回类型使用 E        return s;    &#125;    public void show() &#123;        System.out.println(s.getClass());//显示 s 的运行类型    &#125;&#125;</code></pre><h2 id="泛型的声明"><a href="#泛型的声明" class="headerlink" title="泛型的声明"></a>泛型的声明</h2><ol><li>T,K,V不代表值，只是表示类型</li><li>任意字母都可以，常用T表示，是Type的缩写。</li></ol><pre><code class="java">interface 接口&lt;T&gt; class 类&lt;K,V&gt;&#123;&#125;</code></pre><h2 id="泛型的实例化"><a href="#泛型的实例化" class="headerlink" title="泛型的实例化"></a>泛型的实例化</h2><pre><code class="java">//要在类名后面指定类型参数的值List&lt;String&gt; strList = new ArrayList&lt;String&gt;();Iterator&lt;CUstomer&gt; iterator = customers.iterator();</code></pre><h2 id="泛型使用的注意事项和细节"><a href="#泛型使用的注意事项和细节" class="headerlink" title="泛型使用的注意事项和细节"></a>泛型使用的注意事项和细节</h2><ol><li><p>泛型只能是&#x3D;&#x3D;引用类型&#x3D;&#x3D;</p><p>interface List<T>{},public class HashSet<E>{}</E></T></p><pre><code class="java">List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();//OKList&lt;int&gt; list = new ArrayList&lt;int&gt;();//NO</code></pre></li><li><p>在给泛型指定具体类型后，可以传入该类型或者其子类类型</p></li><li><p>泛型使用形式</p><pre><code class="java">List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();List&lt;Integer&gt; list = new ArrayList&lt;&gt;();//自动类型推断</code></pre></li><li><pre><code class="java">List list3 = new ArrayList();//默认类型是E：Object</code></pre></li></ol><pre><code class="java">class A &#123;&#125;class B extends A &#123;&#125;class Pig&lt;E&gt; &#123;//E e;public Pig(E e) &#123;this.e = e;&#125;public void f() &#123;System.out.println(e.getClass()); //运行类型&#125;&#125;&#125;class Tiger&lt;E&gt; &#123;//类E e;public Tiger() &#123;&#125;public Tiger(E e) &#123;this.e = e;&#125;&#125;public class GenericDetail &#123;public static void main(String[] args) &#123;//1.给泛型指向数据类型是，要求是引用类型，不能是基本数据类型List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); //OK//List&lt;int&gt; list2 = new ArrayList&lt;int&gt;();//错误//2. 说明//因为 E 指定了 A 类型, 构造器传入了 new A()//在给泛型指定具体类型后，可以传入该类型或者其子类类型Pig&lt;A&gt; aPig = new Pig&lt;A&gt;(new A());aPig.f();Pig&lt;A&gt; aPig2 = new Pig&lt;A&gt;(new B());aPig2.f();//3. 泛型的使用形式ArrayList&lt;Integer&gt; list1 = new ArrayList&lt;Integer&gt;();List&lt;Integer&gt; list2 = new ArrayList&lt;Integer&gt;();//在实际开发中，我们往往简写//编译器会进行类型推断, 老师推荐使用下面写法ArrayList&lt;Integer&gt; list3 = new ArrayList&lt;&gt;();List&lt;Integer&gt; list4 = new ArrayList&lt;&gt;();ArrayList&lt;Pig&gt; pigs = new ArrayList&lt;&gt;();//4. 如果是这样写 泛型默认是 ObjectArrayList arrayList = new ArrayList();//等价 ArrayList&lt;Object&gt; arrayList = new ArrayList&lt;Object&gt;();/*public boolean add(Object e) &#123;ensureCapacityInternal(size + 1); // Increments modCount!!elementData[size++] = e;return true;&#125;*/Tiger tiger = new Tiger();/*class Tiger &#123;//类Object e;public Tiger() &#123;&#125;public Tiger(Object e) &#123;this.e = e;&#125;&#125;*/&#125;</code></pre><h2 id="自定义泛型"><a href="#自定义泛型" class="headerlink" title="自定义泛型"></a>自定义泛型</h2><h3 id="自定义泛型类"><a href="#自定义泛型类" class="headerlink" title="自定义泛型类"></a>自定义泛型类</h3><h4 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h4><pre><code class="java">//... 表示可以有多个class 类名&lt;T,R,...&gt;&#123;成员&#125;</code></pre><h4 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h4><pre><code class="java">Class Tiger&lt;T,R,R&gt;&#123;    String name;    R r;    M m;    T t;&#125;</code></pre><h4 id="注意细节"><a href="#注意细节" class="headerlink" title="注意细节"></a>注意细节</h4><ol><li>普通成员可以使用泛型（属性，方法）</li><li>使用泛型的数组，不能初始化</li><li>静态方法中不能使用类的泛型</li><li>泛型类的类型，是在创建对象时确定的(因为创建对象时，需要指定确定类型)</li><li>如果创建对象时，没有指定类型，默认为Object</li></ol><pre><code class="java">public class CustomGeneric_ &#123;public static void main(String[] args) &#123;//T=Double, R=String, M=IntegerTiger&lt;Double,String,Integer&gt; g = new Tiger&lt;&gt;(&quot;john&quot;);g.setT(10.9); //OK//g.setT(&quot;yy&quot;); //错误，类型不对System.out.println(g);Tiger g2 = new Tiger(&quot;john~~&quot;);//OK T=Object R=Object M=Objectg2.setT(&quot;yy&quot;); //OK ,因为 T=Object &quot;yy&quot;=String 是 Object 子类System.out.println(&quot;g2=&quot; + g2);&#125;&#125;//老韩解读//1. Tiger 后面泛型，所以我们把 Tiger 就称为自定义泛型类//2, T, R, M 泛型的标识符, 一般是单个大写字母//3. 泛型标识符可以有多个. //4. 普通成员可以使用泛型 (属性、方法)//5. 使用泛型的数组，不能初始化//6. 静态方法中不能使用类的泛型class Tiger&lt;T, R, M&gt; &#123;String name;R r; //属性使用到泛型M m;T t;//因为数组在 new 不能确定 T 的类型，就无法在内存开空间T[] ts;public Tiger(String name) &#123;this.name = name;&#125;public Tiger(R r, M m, T t) &#123;//构造器使用泛型this.r = r;this.m = m;this.t = t;&#125;public Tiger(String name, R r, M m, T t) &#123;//构造器使用泛型this.name = name;this.r = r;this.m = m;this.t = t;&#125;//因为静态是和类相关的，在类加载时，对象还没有创建//所以，如果静态方法和静态属性使用了泛型，JVM 就无法完成初始化// static R r2;// public static void m1(M m) &#123;//// &#125;//方法使用泛型public String getName() &#123;return name;&#125;public void setName(String name) &#123;this.name = name;&#125;public R getR() &#123;return r;&#125;public void setR(R r) &#123;//方法使用到泛型this.r = r;&#125;public M getM() &#123;//返回类型可以使用泛型. return m;&#125;public void setM(M m) &#123;this.m = m;&#125;public T getT() &#123;return t;&#125;public void setT(T t) &#123;this.t = t;&#125;@Overridepublic String toString() &#123;return &quot;Tiger&#123;&quot; +&quot;name=&#39;&quot; + name + &#39;\&#39;&#39; +&quot;, r=&quot; + r +&quot;, m=&quot; + m +&quot;, t=&quot; + t +&quot;, ts=&quot; + Arrays.toString(ts) +&#39;&#125;&#39;;&#125;&#125;</code></pre><h3 id="自定义泛型接口"><a href="#自定义泛型接口" class="headerlink" title="自定义泛型接口"></a>自定义泛型接口</h3><h4 id="语法-2"><a href="#语法-2" class="headerlink" title="语法"></a>语法</h4><pre><code class="java">interface 接口名 &lt;T,R...&gt;&#123;&#125;</code></pre><h4 id="实例-3"><a href="#实例-3" class="headerlink" title="实例"></a>实例</h4><pre><code class="java">interface IUsb&lt;U, R&gt; &#123;int n = 10;//U name; 不能这样使用，接口成员都是静态的public static final 修饰的    //普通方法中，可以使用接口泛型R get(U u);void hi(R r);void run(R r1, R r2, U u1, U u2);//在 jdk8 中，可以在接口中，使用默认方法, 也是可以使用泛型default R method(U u) &#123;return null;&#125;&#125;//在继承接口 指定泛型接口的类型interface IA extends IUsb&lt;String, Double&gt; &#123;&#125;//当我们去实现 IA 接口时，因为 IA 在继承 IUsu 接口时，指定了 U 为 String R 为 Double//，在实现 IUsu 接口的方法时，使用 String 替换 U, 是 Double 替换 Rclass AA implements IA &#123;@Overridepublic Double get(String s) &#123;return null;&#125;@Overridepublic void hi(Double aDouble) &#123;&#125;@Overridepublic void run(Double r1, Double r2, String u1, String u2) &#123;&#125;&#125;//实现接口时，直接指定泛型接口的类型//给 U 指定 Integer 给 R 指定了 Float//所以，当我们实现 IUsb 方法时，会使用 Integer 替换 U, 使用 Float 替换 Rclass BB implements IUsb&lt;Integer, Float&gt; &#123;@Overridepublic Float get(Integer integer) &#123;return null;&#125;@Overridepublic void hi(Float aFloat) &#123;&#125;@Overridepublic void run(Float r1, Float r2, Integer u1, Integer u2) &#123;&#125;&#125;//没有指定类型，默认为 Object//建议直接写成 IUsb&lt;Object,Object&gt;class CC implements IUsb &#123; //等价 class CC implements IUsb&lt;Object,Object&gt; &#123;@Overridepublic Object get(Object o) &#123;return null;&#125;@Overridepublic void hi(Object o) &#123;&#125;@Overridepublic void run(Object r1, Object r2, Object u1, Object u2) &#123;&#125;&#125;</code></pre><h4 id="注意细节-1"><a href="#注意细节-1" class="headerlink" title="注意细节"></a>注意细节</h4><ol><li>接口中，静态成员也不能使用泛型（和泛型类规定一样）</li><li>泛型接口的类型，在&#x3D;&#x3D;继承接口&#x3D;&#x3D;或&#x3D;&#x3D;实现接口&#x3D;&#x3D;时确定</li><li>没有指定类型，默认为Object；</li></ol><h3 id="自定义泛型方法"><a href="#自定义泛型方法" class="headerlink" title="自定义泛型方法"></a>自定义泛型方法</h3><h4 id="语法-3"><a href="#语法-3" class="headerlink" title="语法"></a>语法</h4><pre><code class="java">修饰符 &lt;T,R...&gt; 返回类型 方法名(参数列表)&#123;&#125;</code></pre><h4 id="实例-4"><a href="#实例-4" class="headerlink" title="实例"></a>实例</h4><pre><code class="java">/*** @author 韩顺平* @version 1.0* 泛型方法的使用*/@SuppressWarnings(&#123;&quot;all&quot;&#125;)public class CustomMethodGeneric &#123;public static void main(String[] args) &#123;Car car = new Car();car.fly(&quot;宝马&quot;, 100);//当调用方法时，传入参数，编译器，就会确定类型System.out.println(&quot;=======&quot;);car.fly(300, 100.1);//当调用方法时，传入参数，编译器，就会确定类型//测试//T-&gt;String, R-&gt; ArrayListFish&lt;String, ArrayList&gt; fish = new Fish&lt;&gt;();fish.hello(new ArrayList(), 11.3f);&#125;&#125;class Fish&lt;T, R&gt; &#123;//泛型类public void run() &#123;//普通方法&#125;public&lt;U,M&gt; void eat(U u, M m) &#123;//泛型方法&#125;//说明//1. 下面 hi 方法不是泛型方法//2. 是 hi 方法使用了类声明的 泛型public void hi(T t) &#123;&#125;//泛型方法，可以使用类声明的泛型，也可以使用自己声明泛型 K是自己声明的，R时类的声明的public&lt;K&gt; void hello(R r, K k) &#123;System.out.println(r.getClass());//ArrayListSystem.out.println(k.getClass());//Float&#125;&#125;//泛型方法，可以定义在普通类中, 也可以定义在泛型类中class Car &#123;//普通类public void run() &#123;//普通方法&#125;//说明 泛型方法//1. &lt;T,R&gt; 就是泛型//2. 是提供给 fly 使用的public &lt;T, R&gt; void fly(T t, R r) &#123;//泛型方法System.out.println(t.getClass());//StringSystem.out.println(r.getClass());//Integer&#125;&#125;</code></pre><h4 id="注意细节-2"><a href="#注意细节-2" class="headerlink" title="注意细节"></a>注意细节</h4><ol><li>泛型方法，可以定义在&#x3D;&#x3D;普通类&#x3D;&#x3D;中，也可以定义在&#x3D;&#x3D;泛型类&#x3D;&#x3D;中</li><li>但泛型方法被调用时，类型会确定</li><li>public void eat(E e){}，修饰符后没有&lt;T,R…&gt;eat 方法不是泛型方法，而是使用了泛型</li></ol><h2 id="泛型的继承和通配符"><a href="#泛型的继承和通配符" class="headerlink" title="泛型的继承和通配符"></a>泛型的继承和通配符</h2><ol><li><p>泛型不具备继承性</p><pre><code class="java">List&lt;Object&gt; list = new ArrayList&lt;String&gt;();不对</code></pre></li><li><?>：支持任意泛型类型</li><li><? extends A>：支持A类以及A类的子类，规定了泛型的上限</li><li><p>&lt;？super A &gt;：支持A类以及A类的父类，不限于直接父类，规定了泛型的下限</p></li></ol><pre><code class="java">//举例说明下面三个方法的使用List&lt;Object&gt; list1 = new ArrayList&lt;&gt;();List&lt;String&gt; list2 = new ArrayList&lt;&gt;();List&lt;AA&gt; list3 = new ArrayList&lt;&gt;();List&lt;BB&gt; list4 = new ArrayList&lt;&gt;();List&lt;CC&gt; list5 = new ArrayList&lt;&gt;();printCollection1(list1);//√printCollection1(list2);//√printCollection1(list3);//√printCollection1(list4);//√printCollection1(list5);//√//List&lt;? extends AA&gt; c： 表示 上限，可以接受 AA 或者 AA 子类// printCollection2(list1);//×// printCollection2(list2);//×printCollection2(list3);//√printCollection2(list4);//√printCollection2(list5);//√//List&lt;? super AA&gt; c: 支持 AA 类以及 AA 类的父类，不限于直接父类printCollection3(list1);//√//printCollection3(list2);//×printCollection3(list3);//√//printCollection3(list4);//×//printCollection3(list5);//×//说明: List&lt;?&gt; 表示 任意的泛型类型都可以接受public static void printCollection1(List&lt;?&gt; c) &#123;for (Object object : c) &#123; // 通配符，取出时，就是 ObjectSystem.out.println(object);&#125;&#125;// ? extends AA 表示 上限，可以接受 AA 或者 AA 子类public static void printCollection2(List&lt;? extends AA&gt; c) &#123;for (Object object : c) &#123;System.out.println(object);&#125;&#125;// ? super 子类类名 AA:支持 AA 类以及 AA 类的父类，不限于直接父类，//规定了泛型的下限public static void printCollection3(List&lt;? super AA&gt; c) &#123;for (Object object : c) &#123;System.out.println(object);&#125;&#125;class AA &#123;&#125;class BB extends AA &#123;&#125;class CC extends BB &#123;&#125;</code></pre><h1 id="多线程基础"><a href="#多线程基础" class="headerlink" title="多线程基础"></a>多线程基础</h1><h2 id="线程相关概念"><a href="#线程相关概念" class="headerlink" title="线程相关概念"></a>线程相关概念</h2><h3 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h3><p>是为了完成特定任务、用某种语言编写的一组指令的集合。</p><h3 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h3><ol><li>进程是指运行中的程序，比如我们使用QQ，就启动了一个进程，操作系统会为该进程分配内存空间。当我们使用迅雷，又启动了一个进程，操作系统将为迅雷分配新的内存空间</li><li>进程是程序的一次执行过程，或是正在运行的一个程序，是一个动态过程，有它自身的产生、存在和消亡的过程。</li></ol><h3 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h3><ol><li>线程由进程创建的，是进程的一个实体</li><li>一个进程可以拥有多个线程。</li></ol><h3 id="其他相关概念"><a href="#其他相关概念" class="headerlink" title="其他相关概念"></a>其他相关概念</h3><p><strong>单线程</strong>：同一时刻，只允许执行一个线程</p><p><strong>多线程</strong>：同一时刻，可以执行多个线程，比如：一个QQ进程，可以同时打开多个聊天窗口，一个迅雷进程，可以同时下载多个文件。</p><p><strong>并发</strong>：同一个时刻，多个任务交替执行，造成一种“貌似同时”的错觉，简单地说，单核CPU实现多任务就是并发。</p><p><strong>并行</strong>：同一时刻，多个任务同时执行，多核cpu可以实现并行。</p><h2 id="创建线程的两种方式"><a href="#创建线程的两种方式" class="headerlink" title="创建线程的两种方式"></a>创建线程的两种方式</h2><h3 id="继承Thread类，重写run-方法"><a href="#继承Thread类，重写run-方法" class="headerlink" title="继承Thread类，重写run()方法"></a>继承Thread类，重写run()方法</h3><p> 当 main 线程启动一个子线程 Thread-0, 主线程不会阻塞, 会继续执行</p><pre><code class="java">public class Thread01 &#123;    public static void main(String[] args) throws InterruptedException &#123;        threadExercise threadExercise = new threadExercise();        threadExercise.start();        for (int i = 0; i &lt; 50; i++) &#123;            System.out.println(Thread.currentThread().getName());            Thread.sleep(5000);        &#125;    &#125;&#125;class threadExercise extends Thread &#123;    boolean flag = true;    int count = 0;    @Override    public void run() &#123;        while (flag) &#123;            System.out.println(&quot;我是傻逼              &quot; + Thread.currentThread().getName());            count++;            try &#123;                Thread.sleep(500);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;            if (count == 80) &#123;                break;            &#125;        &#125;    &#125;&#125;</code></pre><h3 id="实现Runnable接口，重写run-方法"><a href="#实现Runnable接口，重写run-方法" class="headerlink" title="实现Runnable接口，重写run()方法"></a>实现Runnable接口，重写run()方法</h3><p>使用到了&#x3D;&#x3D;静态代理模式&#x3D;&#x3D;，进程中main线程创建完对象调用方法后就会关闭，创建好的Thread01线程继续执行。</p><pre><code class="java">public class Thread02 &#123;    public static void main(String[] args) &#123;        Thread thread = new Thread(new A());        thread.start();    &#125;&#125;class A implements Runnable &#123;    @Override    public void run() &#123;        while (true) &#123;            System.out.println(&quot;谁是傻逼&quot;+Thread.currentThread().getName());            try &#123;                Thread.sleep(1000);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;&#125;</code></pre><h2 id="程序运行机制"><a href="#程序运行机制" class="headerlink" title="程序运行机制"></a>程序运行机制</h2><p>thread.start();方法执行，&gt; 最终会执行 cat 的 run 方法</p><p><img src="/java/java-xue-xi-wen-dang/image-20220618191218079.png" alt="image-20220618191218079"></p><p>start0() 是本地方法，是 JVM 调用, 底层是 c&#x2F;c++实现。</p><p><img src="/java/java-xue-xi-wen-dang/image-20220618191237103.png" alt="image-20220618191237103"></p><p><img src="/java/java-xue-xi-wen-dang/image-20220618191556467.png" alt="image-20220618191556467"></p><h2 id="继承Thread-VS-实现Runnable区别"><a href="#继承Thread-VS-实现Runnable区别" class="headerlink" title="继承Thread VS 实现Runnable区别"></a>继承Thread VS 实现Runnable区别</h2><ol><li>通过继承Thread或者实现Runnable接口来创建线程本质上没有区别，从jdk帮助文档我们可以看到Thread类本身就实现了Runnable接口</li><li>实现Runnable接口方式更加适合多个线程共享一个资源的情况，并且避免了单继承的限制，建议使用Runnable</li></ol><h2 id="线程终止"><a href="#线程终止" class="headerlink" title="线程终止"></a>线程终止</h2><ol><li><p>当线程完成任务后，会自动退出。</p></li><li><p>还可以通过变量来控制run方法退出的方式停止线程，即通知方式</p></li></ol><pre><code class="java">public class SellTicket &#123;    public static void main(String[] args) throws InterruptedException &#123;        SellTicket01 sellTicket01 = new SellTicket01();        SellTicket01 sellTicket02 = new SellTicket01();        SellTicket01 sellTicket03 = new SellTicket01();        sellTicket01.start();        sellTicket02.start();        sellTicket03.start();        Thread.sleep(1000);        sellTicket01.setflag(false);    &#125;&#125;class SellTicket01 extends Thread &#123;    private static int num = 100;//共享    private boolean flag = true;    @Override    public void run() &#123;        while (flag) &#123;            if (num &lt;= 0) &#123;                System.out.println(&quot;没票了&quot;);                break;            &#125;            System.out.println(Thread.currentThread().getName() + &quot;售票-1   &quot; + num);            num--;            try &#123;                Thread.sleep(500);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;    public void setflag(boolean loop)&#123;        this.flag = loop;    &#125;&#125;</code></pre><h2 id="线程常用方法"><a href="#线程常用方法" class="headerlink" title="线程常用方法"></a>线程常用方法</h2><pre><code class="java">1. setName //设置线程名称，使之于参数name相同2. getName //返回该线程的名称3. start //使线程开始执行；java虚拟机底层调用该线程的start0方法4. run //调用线程对象run方法5. setPriority //更改线程的优先级6. getPriority //获取线程使用优先级7. sleep //在指定毫秒数内让当前正在执行的线程休眠（暂时执行）8. interrupt //中断线程，不是终止，会让线程从休眠状态醒过来    1. yield //线程的礼让。让出cpu，让其他线程执行，但礼让的时间不确定，所以也不一定礼让成功。2. join //线程的插队。插队的线程一旦插队成功，则肯定先执行完插入的线程的所有任务，在执行原来线程的任务（一定会插入成功的）。</code></pre><h3 id="interrupt-实例"><a href="#interrupt-实例" class="headerlink" title="interrupt 实例"></a>interrupt 实例</h3><pre><code class="java">//interrupt 实例public class ThreadMethod01 &#123;    public static void main(String[] args) throws InterruptedException &#123;        T t = new T();        t.start();        //设置优先级        t.setPriority(Thread.MIN_PRIORITY);        Thread.sleep(1000);        System.out.println(&quot;main线程过了1秒调用interrupt方法&quot;);        System.out.println(new Date(System.currentTimeMillis()));        t.interrupt();    &#125;&#125;class T extends Thread &#123;    @Override    public void run() &#123;        while (true) &#123;            for (int i = 0; i &lt; 100; i++) &#123;                System.out.println(&quot;测试interrupt方法&quot; + i);            &#125;            try &#123;                System.out.println(&quot;休眠10秒&quot;);                Thread.sleep(10000);            &#125; catch (InterruptedException e) &#123;                System.out.println(&quot;被唤醒时间：&quot; + new Date(System.currentTimeMillis()));                System.out.println(Thread.currentThread().getName() + &quot;被执行interrupt方法了&quot;);            &#125;        &#125;    &#125;&#125;/*.....测试interrupt方法97测试interrupt方法98测试interrupt方法99休眠10秒main线程过了1秒调用interrupt方法Mon Jun 20 04:24:57 CST 2022被唤醒时间：Mon Jun 20 04:24:57 CST 2022Thread-0被执行interrupt方法了测试interrupt方法0.....*/</code></pre><h3 id="join-实例"><a href="#join-实例" class="headerlink" title="join 实例"></a>join 实例</h3><pre><code class="java">public class ThreadMethod02 &#123;    public static void main(String[] args) throws InterruptedException &#123;        T2 t2 = new T2();        t2.start();        for (int i = 1; i &lt;= 20; i++) &#123;            Thread.sleep(1000);            if (i == 5) &#123;                t2.join();            &#125;            System.out.println(&quot;主线程执行    &quot; + i);        &#125;    &#125;&#125;class T2 extends Thread &#123;    @Override    public void run() &#123;        for (int i = 1; i &lt;= 20; i++) &#123;            try &#123;                Thread.sleep(1000);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;            System.out.println(&quot;子线程执行    &quot; + i);        &#125;    &#125;&#125;/*....子线程执行    3子线程执行    4主线程执行    4子线程执行    5子线程执行    6子线程执行    7子线程执行    8子线程执行    9子线程执行    10子线程执行    11....*/</code></pre><h3 id="注意事项和细节"><a href="#注意事项和细节" class="headerlink" title="注意事项和细节"></a>注意事项和细节</h3><ol><li>start 底层会创建新的线程，调用run，run就是一个简单的方法调用，不会启动新线程</li><li>线程优先级的范围</li><li>interrupt，中断线程，但并没有真正的结束线程。所以一般用于终端正在休眠线程</li><li>sleep现成的静态方法，使当前线程休眠</li></ol><h2 id="用户线程和守护线程"><a href="#用户线程和守护线程" class="headerlink" title="用户线程和守护线程"></a>用户线程和守护线程</h2><ol><li>用户线程：也叫工作线程，当线程的任务执行完或通知方式结束</li><li>守护线程：一般是为工作线程服务的，当所有用户线程结束，守护线程自动结束</li><li>常见守护线程：垃圾回收机制</li></ol><pre><code class="java">public class ThreadMethod02 &#123;    public static void main(String[] args) throws InterruptedException &#123;        T2 t2 = new T2();        t2.start();        //设置t2为守护线程        t2.setDaemon(true);        &#125;        class T2 extends Thread &#123;    @Override    public void run() &#123;        for (int i = 1; i &lt;= 20; i++) &#123;            try &#123;                Thread.sleep(1000);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;            System.out.println(&quot;守护线程    &quot; + i);        &#125;    &#125;&#125;</code></pre><h2 id="线程生命周期"><a href="#线程生命周期" class="headerlink" title="线程生命周期"></a>线程生命周期</h2><p>JDK中用Thread.State枚举表示了线程的几种状态</p><p><img src="/java/java-xue-xi-wen-dang/image-20220620050423270.png" alt="image-20220620050423270"></p><p><img src="/java/java-xue-xi-wen-dang/image-20220620051208451.png" alt="image-20220620051208451"></p><pre><code class="Java">public class ThreadState &#123;    public static void main(String[] args) throws InterruptedException &#123;        T t = new T();        System.out.println(t.getState());        t.start();        System.out.println(t.getState());        while(t.getState()!=Thread.State.TERMINATED)&#123;            System.out.println(t.getState());            Thread.sleep(500);        &#125;        System.out.println(t.getState());    &#125;&#125;class T extends Thread &#123;    @Override    public void run() &#123;        while (true) &#123;            for (int i = 0; i &lt; 10; i++) &#123;//                System.out.println(&quot;i=&quot; + i);                try &#123;                    Thread.sleep(500);//                    System.out.println(&quot;t线程当前状态&quot;+this.getState());                &#125; catch (InterruptedException e) &#123;                    e.printStackTrace();                &#125;            &#125;            break;        &#125;    &#125;&#125;</code></pre><h2 id="Synchronized"><a href="#Synchronized" class="headerlink" title="Synchronized"></a>Synchronized</h2><ol><li>多线程编程，一些敏感数据不允许被多个线程同时访问，此时就使用同步访问技术，保证数据在任何时刻，最多有一个线程访问，以保证数据的完整性。</li><li>线程同步，当有一个线程在对内存进行操作时，其他线程都不可以对这个内存地址进行操作，直到该线程完成操作，其他线程才能对该内存地址进行操作。</li></ol><h3 id="具体使用方法-Synchronized"><a href="#具体使用方法-Synchronized" class="headerlink" title="具体使用方法-Synchronized"></a>具体使用方法-Synchronized</h3><ol><li>同步代码块</li></ol><pre><code class="java">synchronized(对象)&#123;//得到对象的锁，才能操作同步代码//需要被同步代码&#125;</code></pre><ol start="2"><li>synchronized还可以放在方法声明中，表示整个方法-为同步方法</li></ol><pre><code class="java">public synchronized void m(String name)&#123;//需要被同步代码&#125;</code></pre><ol start="3"><li>静态方法</li></ol><pre><code class="java">public static void m2() &#123;        synchronized (类名.class) &#123;            System.out.println(&quot;静态方法的同步块不能使用this，默认当前类.class&quot;);        &#125;    &#125;</code></pre><h3 id="同步原理"><a href="#同步原理" class="headerlink" title="同步原理"></a>同步原理</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220620131546670.png" alt="image-20220620131546670"></p><p>三个线程同时去抢一把锁，抢到后执行自己的逻辑代码，执行完毕后把锁还回去。接着三个线程再次抢这把锁，在执行….</p><h2 id="互斥锁"><a href="#互斥锁" class="headerlink" title="互斥锁"></a>互斥锁</h2><h3 id="基本介绍-1"><a href="#基本介绍-1" class="headerlink" title="基本介绍"></a>基本介绍</h3><ol><li>Java语言中，引入了对象互斥锁的概念，来保证共享数据操作的完整性。</li><li>每个对象都对应于一个可称为“互斥锁”的标记，这个标记用来保证在任意时刻，只能由一个线程访问该对象。</li><li>关键字synchronized来与对象的互斥锁联系。当某个对象用synchronized修饰时，表明该对象在任意时刻只能有一个线程访问。</li><li>同步的局限性：导致程序的执行效率要降低</li><li>同步方法（非静态）的锁可以是this，也可以是其他对象（要求是同一个对象）</li><li>同步方法（静态的）的锁为当前类本身</li></ol><h3 id="实例-5"><a href="#实例-5" class="headerlink" title="实例"></a>实例</h3><pre><code class="java">/** * @author 张文辉 * @version 1.0 * 多线程模拟三个窗口同时售票100张 */public class SellTicket &#123;    public static void main(String[] args) throws InterruptedException &#123;        SellTicket01 sellTicket01 = new SellTicket01();        SellTicket01 sellTicket02 = new SellTicket01();        SellTicket01 sellTicket03 = new SellTicket01();        sellTicket01.start();        sellTicket02.start();        sellTicket03.start();    &#125;&#125;class SellTicket01 extends Thread &#123;    private static int num = 100;//多个线程共享共享    private static Object object = new Object();    private boolean flag = true;    public static void m2() &#123;        synchronized (SellTicket01.class) &#123; //锁加在类对象上            System.out.println(&quot;静态方法的同步块不能使用this，默认当前类.class&quot;);        &#125;        /*        * public synchronized static void m1() &#123;            &#125;        * 静态方法直接加synchronized        */    &#125;    public /*synchronized*/ void m1() &#123; //同步方法, 在同一时刻， 只能有一个线程来执行 sell 方法//            synchronized(this) &#123;    //加在this对象上        synchronized (object) &#123;//加在其他对象上，要求是同一个对象            if (num &lt;= 0) &#123;                System.out.println(&quot;没票了&quot;);                flag = false;                return;            &#125;            System.out.println(Thread.currentThread().getName() + &quot;售票-1   &quot; + num);            num--;            try &#123;                Thread.sleep(100);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;        &#125;//    &#125;    &#125;    @Override    public void run() &#123;        while (flag) &#123;            m1();        &#125;    &#125;    public void setflag(boolean loop) &#123;        this.flag = loop;    &#125;&#125;</code></pre><h3 id="注意事项-7"><a href="#注意事项-7" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li><p>同步方法如果没有使用static修饰：默认锁对象为this</p></li><li><p>如果方法使用static修饰，默认锁对象：当前类.class</p></li></ol><h2 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h2><h3 id="基本介绍-2"><a href="#基本介绍-2" class="headerlink" title="基本介绍"></a>基本介绍</h3><p>多个线程都占用了对方的锁资源，不肯让步，导致了死锁，一定要避免死锁现象的发生。</p><h3 id="实例-6"><a href="#实例-6" class="headerlink" title="实例"></a>实例</h3><pre><code class="java">package hsppedu.Homework.threaduse_;/** * @author 张文辉 * @version 1.0 */public class DeadLock_ &#123;    public static void main(String[] args) &#123;//模拟死锁现象        DeadLockDemo A = new DeadLockDemo(true);        A.setName(&quot;A 线程&quot;);        DeadLockDemo B = new DeadLockDemo(false);        B.setName(&quot;B 线程&quot;);        A.start();        B.start();    &#125;&#125;class DeadLockDemo extends Thread &#123;    static Object o1 = new Object();// 保证多线程，共享一个对象,这里使用 static    static Object o2 = new Object();    boolean flag;    public DeadLockDemo(boolean flag) &#123;//构造器        this.flag = flag;    &#125;    @Override    public void run() &#123;//下面业务逻辑的分析//1. 如果 flag 为 T, 线程 A 就会先得到/持有 o1 对象锁, 然后尝试去获取 o2 对象锁//2. 如果线程 A 得不到 o2 对象锁，就会 Blocked//3. 如果 flag 为 F, 线程 B 就会先得到/持有 o2 对象锁, 然后尝试去获取 o1 对象锁//4. 如果线程 B 得不到 o1 对象锁，就会 Blocked        if (flag) &#123;            synchronized (o1) &#123;//对象互斥锁, 下面就是同步代码                System.out.println(Thread.currentThread().getName() + &quot; 进入 1&quot;);                synchronized (o2) &#123; // 这里获得 li 对象的监视权                    System.out.println(Thread.currentThread().getName() + &quot; 进入 2&quot;);                &#125;            &#125;        &#125; else &#123;            synchronized (o2) &#123;                System.out.println(Thread.currentThread().getName() + &quot; 进入 3&quot;);                synchronized (o1) &#123; // 这里获得 li 对象的监视权                    System.out.println(Thread.currentThread().getName() + &quot; 进入 4&quot;);                &#125;            &#125;        &#125;    &#125;&#125;</code></pre><h3 id="释放锁"><a href="#释放锁" class="headerlink" title="释放锁"></a>释放锁</h3><ol><li>当前线程的同步方法，同步代码块执行结束</li><li>当前线程在同步代码块、同步方法中遇到break、return。</li><li>当前线程在同步代码块、同步方法中出现了未处理的Error或Exception，导致异常结束</li><li>当前线程在同步代码块、同步方法中执行了线程对象的wait()方法，当前线程暂停，并释放锁</li></ol><h3 id="不会释放锁"><a href="#不会释放锁" class="headerlink" title="不会释放锁"></a>不会释放锁</h3><ol><li><p>线程执行同步代码块或同步方法时，程序调用Thread.sleep()、Thread.yield()方法暂停当前线程的执行，不会释放锁</p></li><li><p>线程执行同步代码块时，其他线程调用了该线程的suspend()方法将该线程挂机，该线程不会释放锁。</p><p>应尽量避免使用suspend()和resume()来控制线程，方法不再推荐使用</p></li></ol><h1 id="IO流"><a href="#IO流" class="headerlink" title="IO流"></a>IO流</h1><p>流：数据在数据源（文件）和程序（内存）之间经历的路径</p><p>输入流：数据从数据源（文件）到程序（内存）的路径</p><p>输出流：数据从程序（内存）到数据源（文件）的路径</p><h2 id="常用文件操作"><a href="#常用文件操作" class="headerlink" title="常用文件操作"></a>常用文件操作</h2><h3 id="创建文件对象相关构造器和方法"><a href="#创建文件对象相关构造器和方法" class="headerlink" title="创建文件对象相关构造器和方法"></a>创建文件对象相关构造器和方法</h3><p>new File(String pathname) &#x2F;&#x2F;根据路径构建一个File对象</p><p>new File（File parent，String child） &#x2F;&#x2F;根据附目录文件+子路径构建</p><p>new File（String parent,String child）&#x2F;&#x2F;根据父目录+子路径构建</p><p>createNewFile  创建新文件</p><pre><code class="java">    @Test    //方式 1 new File(String pathnam    public void create01()&#123;        String filePath = &quot;e://testFile.txt&quot;;        File file = new File(filePath);        try &#123;            file.createNewFile();            System.out.println(&quot;文件创建成功&quot;);        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;    &#125;    @Test    //方式 2 new File(File parent,String child) //根据父目录文件+子路径构建    public void create02()&#123;        String filePath = &quot;e://&quot;;        File file = new File(new File(filePath),&quot;testFile2.txt&quot;);        try &#123;            file.createNewFile();            System.out.println(&quot;文件创建成功&quot;);        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;    &#125;    @Test    //方式 3 new File(String parent,String child) //根据父目录+子路径构建    public void create03()&#123;        File file = new File(&quot;e://&quot;,&quot;testFile3.txt&quot;);        try &#123;            file.createNewFile();            System.out.println(&quot;文件创建成功&quot;);        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;    &#125;</code></pre><h3 id="获取文件相关信息"><a href="#获取文件相关信息" class="headerlink" title="获取文件相关信息"></a>获取文件相关信息</h3><p>getName</p><p>getAbsolutePath</p><p>getParent</p><p>Length</p><p>exists</p><p>isFile</p><p>isDirectory</p><pre><code class="java">//获取文件的信息@Testpublic void info() &#123;//先创建文件对象File file = new File(&quot;e:\\news1.txt&quot;);//调用相应的方法，得到对应信息System.out.println(&quot;文件名字=&quot; + file.getName());//getName、getAbsolutePath、getParent、length、exists、isFile、isDirectorySystem.out.println(&quot;文件绝对路径=&quot; + file.getAbsolutePath());System.out.println(&quot;文件父级目录=&quot; + file.getParent());System.out.println(&quot;文件大小(字节)=&quot; + file.length());System.out.println(&quot;文件是否存在=&quot; + file.exists());//TSystem.out.println(&quot;是不是一个文件=&quot; + file.isFile());//TSystem.out.println(&quot;是不是一个目录=&quot; + file.isDirectory());//F&#125;</code></pre><h2 id="IO流原理及流的分类"><a href="#IO流原理及流的分类" class="headerlink" title="IO流原理及流的分类"></a>IO流原理及流的分类</h2><h3 id="IO流原理"><a href="#IO流原理" class="headerlink" title="IO流原理"></a>IO流原理</h3><ol><li>IO是Input&#x2F;Output的缩写，IO技术是非常实用的技术，用于处理数据传输，如读写文件，网络通讯等</li><li>java程序中，对于数据的输入&#x2F;输出操作以流Stream的方式进行</li><li>java.io报下提供了各种流类和接口，用于获取不同种类的数据，并通过方法输入或输出数据</li><li>输入input：读取外部数据，（磁盘，光盘等存储设备的数据）到（程序）中。</li><li>输出output：将程序（内存）数据输出到磁盘、光盘等存储设备中。</li></ol><h3 id="流的分类"><a href="#流的分类" class="headerlink" title="流的分类"></a>流的分类</h3><p>按操作数据单位不同分为：字节流（8 bit） 二进制文件，字符流（按字符）文本文件</p><p>按数据流的流向不同分为：输入流，输出流</p><p>按流的角色不同分为：节点流，处理流&#x2F;包装流</p><p><img src="/java/java-xue-xi-wen-dang/image-20220628051542083.png" alt="image-20220628051542083"></p><h2 id="IO流体系图"><a href="#IO流体系图" class="headerlink" title="IO流体系图"></a>IO流体系图</h2><p><img src="/java/java-xue-xi-wen-dang/image-20220628051736857.png" alt="image-20220628051736857"></p><h2 id="FileInputStream"><a href="#FileInputStream" class="headerlink" title="FileInputStream"></a>FileInputStream</h2><pre><code class="java">@Testpublic void readFile01() &#123;String filePath = &quot;e:\\hello.txt&quot;;int readData = 0;FileInputStream fileInputStream = null;try &#123;//创建 FileInputStream 对象，用于读取 文件fileInputStream = new FileInputStream(filePath);//从该输入流读取一个字节的数据。 如果没有输入可用，此方法将阻止。//如果返回-1 , 表示读取完毕while ((readData = fileInputStream.read()) != -1) &#123;System.out.print((char)readData);//转成 char 显示&#125;&#125; catch (IOException e) &#123;e.printStackTrace();&#125; finally &#123;//关闭文件流，释放资源.try &#123;fileInputStream.close();&#125; catch (IOException e) &#123;e.printStackTrace();&#125;&#125;&#125;/*** 使用 read(byte[] b) 读取文件，提高效率*/@Testpublic void readFile02() &#123;String filePath = &quot;e:\\hello.txt&quot;;//字节数组byte[] buf = new byte[8]; //一次读取 8 个字节. int readLen = 0;FileInputStream fileInputStream = null;try &#123;//创建 FileInputStream 对象，用于读取 文件fileInputStream = new FileInputStream(filePath);//从该输入流读取最多 b.length 字节的数据到字节数组。 此方法将阻塞，直到某些输入可用。//如果返回-1 , 表示读取完毕//如果读取正常, 返回实际读取的字节数while ((readLen = fileInputStream.read(buf)) != -1) &#123;System.out.print(new String(buf, 0, readLen));//显示&#125;&#125; catch (IOException e) &#123;e.printStackTrace();&#125; finally &#123;//关闭文件流，释放资源. try &#123;fileInputStream.close();&#125; catch (IOException e) &#123;e.printStackTrace();&#125;&#125;&#125;&#125;</code></pre><h2 id="FileOutputStream"><a href="#FileOutputStream" class="headerlink" title="FileOutputStream"></a>FileOutputStream</h2><p><img src="/java/java-xue-xi-wen-dang/image-20220628060030244.png" alt="image-20220628060030244"></p><pre><code class="java">public void writeFile() &#123;//创建 FileOutputStream 对象String filePath = &quot;e:\\a.txt&quot;;FileOutputStream fileOutputStream = null;try &#123;//得到 FileOutputStream 对象 对象//老师说明//1. new FileOutputStream(filePath) 创建方式，当写入内容是，会覆盖原来的内容//2. new FileOutputStream(filePath, true) 创建方式，当写入内容是，是追加到文件后面fileOutputStream = new FileOutputStream(filePath, true);//写入一个字节//fileOutputStream.write(&#39;H&#39;);////写入字符串String str = &quot;hsp,world!&quot;;//str.getBytes() 可以把 字符串-&gt; 字节数组//fileOutputStream.write(str.getBytes());/*write(byte[] b, int off, int len) 将 len 字节从位于偏移量 off 的指定字节数组写入此文件输出流*/fileOutputStream.write(str.getBytes(), 0, 3);&#125; catch (IOException e) &#123;e.printStackTrace();&#125; finally &#123;try &#123;fileOutputStream.close();&#125; catch (IOException e) &#123;e.printStackTrace();&#125;&#125;&#125;&#125;</code></pre><pre><code class="java">public class FileCopy &#123;public static void main(String[] args) &#123;//完成 文件拷贝，将 e:\\Koala.jpg 拷贝 e:\\Koala3.jpg//思路分析//1. 创建文件的输入流 , 将文件读入到程序//2. 创建文件的输出流， 将读取到的文件数据，写入到指定的文件. String srcFilePath = &quot;e:\\Koala.jpg&quot;;String destFilePath = &quot;e:\\Koala3.jpg&quot;;FileInputStream fileInputStream = null;FileOutputStream fileOutputStream = null;try &#123;fileInputStream = new FileInputStream(srcFilePath);fileOutputStream = new FileOutputStream(destFilePath);//定义一个字节数组,提高读取效果byte[] buf = new byte[1024];int readLen = 0;while ((readLen = fileInputStream.read(buf)) != -1) &#123;//读取到后，就写入到文件 通过 fileOutputStream//即，是一边读，一边写fileOutputStream.write(buf, 0, readLen);//一定要使用这个方法&#125;System.out.println(&quot;拷贝 ok~&quot;);&#125; catch (IOException e) &#123;e.printStackTrace();&#125; finally &#123;try &#123;//关闭输入流和输出流，释放资源if (fileInputStream != null) &#123;fileInputStream.close();&#125;if (fileOutputStream != null) &#123;fileOutputStream.close();&#125;&#125; catch (IOException e) &#123;e.printStackTrace();&#125;&#125;&#125;&#125;</code></pre><h2 id="FileReader-和-FileWriter介绍"><a href="#FileReader-和-FileWriter介绍" class="headerlink" title="FileReader 和 FileWriter介绍"></a>FileReader 和 FileWriter介绍</h2><p><img src="/java/java-xue-xi-wen-dang/image-20220628075753227.png" alt="image-20220628075753227"></p><h3 id="FileReader-相关方法"><a href="#FileReader-相关方法" class="headerlink" title="FileReader 相关方法"></a>FileReader 相关方法</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220628075825344.png" alt="image-20220628075825344"></p><h3 id="FileWriter-常用方法"><a href="#FileWriter-常用方法" class="headerlink" title="FileWriter 常用方法"></a>FileWriter 常用方法</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220628084314871.png" alt="image-20220628084314871"></p><pre><code class="java">public class FileReader_ &#123;public static void main(String[] args) &#123;&#125;/*** 单个字符读取文件*/@Testpublic void readFile01() &#123;String filePath = &quot;e:\\story.txt&quot;;FileReader fileReader = null;int data = 0;//1. 创建 FileReader 对象try &#123;fileReader = new FileReader(filePath);//循环读取 使用 read, 单个字符读取while ((data = fileReader.read()) != -1) &#123;System.out.print((char) data);&#125;&#125; catch (IOException e) &#123;e.printStackTrace();&#125; finally &#123;try &#123;if (fileReader != null) &#123;fileReader.close();&#125;&#125; catch (IOException e) &#123;e.printStackTrace();&#125;&#125;&#125;/*** 字符数组读取文件*/@Testpublic void readFile02() &#123;System.out.println(&quot;~~~readFile02 ~~~&quot;);String filePath = &quot;e:\\story.txt&quot;;FileReader fileReader = null;int readLen = 0;char[] buf = new char[8];//1. 创建 FileReader 对象try &#123;fileReader = new FileReader(filePath);//循环读取 使用 read(buf), 返回的是实际读取到的字符数//如果返回-1, 说明到文件结束while ((readLen = fileReader.read(buf)) != -1) &#123;System.out.print(new String(buf, 0, readLen));&#125;&#125; catch (IOException e) &#123;e.printStackTrace();&#125; finally &#123;try &#123;if (fileReader != null) &#123;fileReader.close();&#125;&#125; catch (IOException e) &#123;e.printStackTrace();&#125;&#125;&#125;&#125;</code></pre><pre><code class="java">public class FileWriter_ &#123;public static void main(String[] args) &#123;String filePath = &quot;e:\\note.txt&quot;;//创建 FileWriter 对象FileWriter fileWriter = null;char[] chars = &#123;&#39;a&#39;, &#39;b&#39;, &#39;c&#39;&#125;;try &#123;fileWriter = new FileWriter(filePath);//默认是覆盖写入// 3) write(int):写入单个字符fileWriter.write(&#39;H&#39;);// 4) write(char[]):写入指定数组fileWriter.write(chars);// 5) write(char[],off,len):写入指定数组的指定部分fileWriter.write(&quot;韩顺平教育&quot;.toCharArray(), 0, 3);// 6) write（string）：写入整个字符串fileWriter.write(&quot; 你好北京~&quot;);fileWriter.write(&quot;风雨之后，定见彩虹&quot;);// 7) write(string,off,len):写入字符串的指定部分fileWriter.write(&quot;上海天津&quot;, 0, 2);//在数据量大的情况下，可以使用循环操作. &#125; catch (IOException e) &#123;e.printStackTrace();&#125; finally &#123;//对应 FileWriter , 一定要关闭流，或者 flush 才能真正的把数据写入到文件//老韩看源码就知道原因. /*看看代码private void writeBytes() throws IOException &#123;this.bb.flip();int var1 = this.bb.limit();int var2 = this.bb.position();assert var2 &lt;= var1;int var3 = var2 &lt;= var1 ? var1 - var2 : 0;if (var3 &gt; 0) &#123;if (this.ch != null) &#123;assert this.ch.write(this.bb) == var3 : var3;&#125; else &#123;this.out.write(this.bb.array(), this.bb.arrayOffset() + var2,var3);&#125;&#125;this.bb.clear();&#125;*/try &#123;//fileWriter.flush();//关闭文件流，等价 flush() + 关闭fileWriter.close();&#125; catch (IOException e) &#123;e.printStackTrace();&#125;&#125;System.out.println(&quot;程序结束...&quot;);&#125;&#125;</code></pre><h2 id="节点流和处理流"><a href="#节点流和处理流" class="headerlink" title="节点流和处理流"></a>节点流和处理流</h2><h3 id="基本介绍-3"><a href="#基本介绍-3" class="headerlink" title="基本介绍"></a>基本介绍</h3><ol><li><p>节点流可以从一个特定的数据源读写数据，如FileReader、FileWriter</p></li><li><p>处理流，也叫包装流，是连接在已存在的流（节点流或处理流）之上的，为程序提供更为强大的读写功能，也更加灵活，如BufferedReader、BufferedWriter</p></li></ol><h3 id="节点流和处理流一览图"><a href="#节点流和处理流一览图" class="headerlink" title="节点流和处理流一览图"></a>节点流和处理流一览图</h3><p><img src="/java/java-xue-xi-wen-dang/image-20220628085557719.png" alt="image-20220628085557719"></p><h3 id="节点流和处理流的区别和联系"><a href="#节点流和处理流的区别和联系" class="headerlink" title="节点流和处理流的区别和联系"></a>节点流和处理流的区别和联系</h3><ol><li>节点流是底层流&#x2F;低级流，直接跟数据源相接</li><li>处理流（包装流）包装节点流，既可以消除不同节点流的实现差异，也可以提供更方便发的方法来完成输入输出。</li><li>处理流（也叫包装流）对节点流进行包装，使用了修饰器设计模式，不是直接与数据源相连</li></ol><h3 id="处理流的功能主要体现在以下两个方面"><a href="#处理流的功能主要体现在以下两个方面" class="headerlink" title="处理流的功能主要体现在以下两个方面"></a>处理流的功能主要体现在以下两个方面</h3><ol><li>性能的提高：主要以增加缓冲的方式来提高输入输出的效率</li><li>操作的便捷：处理流可以提供一些便捷的方法来依次输入输出大批量的数据，使用更加灵活方便。</li></ol><h3 id="处理流-BufferedReader-和-BufferedWriter"><a href="#处理流-BufferedReader-和-BufferedWriter" class="headerlink" title="处理流-BufferedReader 和 BufferedWriter"></a>处理流-BufferedReader 和 BufferedWriter</h3><p>BufferedReader 和 BufferedWriter 属于字符流，是按照字符来读取数据的</p><p>关闭时处理流，只需要关闭外层流即可。</p><pre><code class="java">public class BufferedReader_ &#123;public static void main(String[] args) throws Exception &#123;String filePath = &quot;e:\\a.java&quot;;//创建 bufferedReaderBufferedReader bufferedReader = new BufferedReader(new FileReader(filePath);//读取String line; //按行读取, 效率高//说明//1. bufferedReader.readLine() 是按行读取文件//2. 当返回 null 时，表示文件读取完毕while ((line = bufferedReader.readLine()) != null) &#123;System.out.println(line);&#125;//关闭流, 这里注意，只需要关闭 BufferedReader ，因为底层会自动的去关闭 节点流//FileReader。/*public void close() throws IOException &#123;synchronized (lock) &#123;if (in == null)return;try &#123;in.close();//in 就是我们传入的 new FileReader(filePath), 关闭了. &#125; finally &#123;in = null;cb = null;&#125;&#125;&#125;*/bufferedReader.close();&#125;&#125;</code></pre><h2 id="Properties类"><a href="#Properties类" class="headerlink" title="Properties类"></a>Properties类</h2>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Static</title>
      <link href="/java/static/"/>
      <url>/java/static/</url>
      
        <content type="html"><![CDATA[<h1 id="Static"><a href="#Static" class="headerlink" title="Static"></a>Static</h1><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><p>在new含有static变量和方法的类时，new出来的对象不包含这些static变量和方法，可以通过类名.方法（或变量）来调用。</p><p><strong>static成员变量是所有该类对象共享的。</strong>创建多个对象后共享在堆内存中的静态变量。</p><p>static属于类，在类被载入时被显示初始化。</p><p>类方法中不允许使用和对象有关的关键字。this和super。</p><p>static不能引用非static变量和方法，静态方法只能访问静态成员。</p><p>在普通方法中可以访问静态和非静态成员。</p><h2 id="main方法的说明："><a href="#main方法的说明：" class="headerlink" title="main方法的说明："></a>main方法的说明：</h2><ol><li>main方法是jvm虚拟机调用的。所以是public的</li><li>java虚拟机调用类的main方法时不会创建对象，所以该方法必须是static。</li><li>该方法接受String类型的数组参数，该数组中保存执行java命令时传递给所运行的类的参数。</li></ol><h2 id="类加载的条件"><a href="#类加载的条件" class="headerlink" title="类加载的条件"></a>类加载的条件</h2><ol><li>new 类的时候会被加载</li><li>调用子类的时候，父类也会被加载。</li><li>使用类中的静态方法是时也会被加载。</li></ol><h2 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h2><p>如果<strong>多个构造器</strong>中有相同的语句，把相同的语句放入到一个代码块中。不管调用那个构造器在其创建对象都会先调用代码块中的内容。代码块的调用优先于构造器的调用。</p><p>普通代码块会被隐式调用，<strong>创建</strong>对象实例多少次就会被调用多少次。如果只是调用static变量是不会执行的，因为没有创建对象。普通代码块中可以执行静态和非静态参数和方法。而静态代码块中只能调用静态的方法和参数。 </p><h3 id="Static代码块"><a href="#Static代码块" class="headerlink" title="Static代码块"></a>Static代码块</h3><p>static代码块叫做静态代码块。对类进行初始化，随着类的加载而加载，<strong>只会执行一次</strong>，先执行。</p><h3 id="构造器"><a href="#构造器" class="headerlink" title="构造器"></a>构造器</h3><p>构造器在方法初始化完成后在会执行。</p><p>new 一个类时：先顺序执行静态代码块或静态属性初始化，然后进行普通方法和代码块的初始化，最后再执行构造器。</p><h1 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h1><p>一个类存在继承关系，一个类的调用关系是什么样的？</p><p>new 一个类时：</p><ol><li><p>加载类信息到方法区，先加载父类然后子类的<strong>静态的</strong>属性、代码块 到堆内存中（只会被加载一次）。（静态方法是调用才会被执行）</p></li><li><p>调用构造器（先调用super()，在调用父类的构造器，一直往上找…..，执行玩最上层super()后，如果本类中有普通代码块就先执行代码块，然后再执行构造器，返回给子类后，子类调用普通代码块，然后执行构造器中的代码）</p></li><li><p>进行普通方法和代码块的初始化时（只调用static变量没有new一个新对象就没有这一步）</p></li><li><p>最后再执行构造器。</p></li></ol><p>1父类构造器，2代码块，3构造器</p><p><img src="/java/static/image-20220226233214947.png" alt="image-20220226233214947"></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>进制转换和位运算</title>
      <link href="/java/jin-zhi-zhuan-huan/"/>
      <url>/java/jin-zhi-zhuan-huan/</url>
      
        <content type="html"><![CDATA[<h1 id="进制"><a href="#进制" class="headerlink" title="进制"></a>进制</h1><p>二进制：0,1 ，满 2 进 1.以 0b 或 0B 开头。 </p><p>十进制：0-9 ，满 10 进 1。 </p><p>八进制：0-7 ，满 8 进 1. 以数字 0 开头表示。 </p><p>十六进制：0-9 及 A(10)-F(15)，满 16 进 1. 以 0x 或 0X 开头表示。此处的 A-F 不区分大小</p><h1 id="进制转换"><a href="#进制转换" class="headerlink" title="进制转换"></a>进制转换</h1><h2 id="二，八，十六进制转十进制"><a href="#二，八，十六进制转十进制" class="headerlink" title="二，八，十六进制转十进制"></a>二，八，十六进制转十进制</h2><h3 id="二转十："><a href="#二转十：" class="headerlink" title="二转十："></a>二转十：</h3><p>从最低位（最右边）开始，将每个位上的数提取出来，乘以<strong>2</strong>的（位数-1）次方，然后求和。<br>$$<br>10010<br>&#x3D; 0<em>2^0 + 1</em>2^1 + 0<em>2^2 + 0</em>2^3 +1*2^4<br>&#x3D; 0 + 2 + 0 + 0 + 16<br>&#x3D; 18<br>$$</p><h3 id="八转十："><a href="#八转十：" class="headerlink" title="八转十："></a>八转十：</h3><p>从最低位（最右边）开始，将每个位上的数提取出来，乘以<strong>8</strong>的（位数-1）次方，然后求和。<br>$$<br>0234<br>&#x3D; 4<em>8^0 + 3</em>8^1 + 2<em>8^2 + 0</em>8^3<br>&#x3D; 4 + 24 + 128 + 0<br>&#x3D; 156<br>$$</p><h3 id="十六转十："><a href="#十六转十：" class="headerlink" title="十六转十："></a>十六转十：</h3><p>从最低位（最右边）开始，将每个位上的数提取出来，乘以<strong>16</strong>的（位数-1）次方，然后求和。<br>$$<br>23A<br>&#x3D; 10<em>16^0 + 3</em>16^1 + 2*16^2<br>&#x3D; 10 + 48 + 512<br>&#x3D; 570<br>$$</p><h2 id="十进制转二，八，十六进制"><a href="#十进制转二，八，十六进制" class="headerlink" title="十进制转二，八，十六进制"></a>十进制转二，八，十六进制</h2><h3 id="十转二"><a href="#十转二" class="headerlink" title="十转二"></a>十转二</h3><p>将该数不断除以二，除到商为0，每步余数倒过来，对应二进制。</p><pre><code>34= 34➗2= 17 .......0=17➗2=8......1=8➗2=4......0=4➗2=2......0=2➗2=1......0=100010</code></pre><p>$$<br>34<br>&#x3D; 34\div2<br>&#x3D; 17 ….0<br>&#x3D;17\div2<br>&#x3D;8…1<br>&#x3D;8\div2<br>&#x3D;4…0<br>&#x3D;4\div2<br>&#x3D;2…0<br>&#x3D;2\div2<br>&#x3D;1…0<br>$$</p><h3 id="十转八"><a href="#十转八" class="headerlink" title="十转八"></a>十转八</h3><p>将该数不断除以八，除到商为0，每步余数倒过来，对应八进制。</p><pre><code>131➗8=16......3=16➗8=2......0=2➗8=0......2=0203</code></pre><h3 id="十转十六"><a href="#十转十六" class="headerlink" title="十转十六"></a>十转十六</h3><p>将该数不断除以十六，除到商为0，每步余数倒过来，对应十六进制。</p><pre><code>237➗16=14......13=14➗16=0......14=01413==0ED</code></pre><h2 id="二进制转八、十六进制"><a href="#二进制转八、十六进制" class="headerlink" title="二进制转八、十六进制"></a>二进制转八、十六进制</h2><h3 id="二转八"><a href="#二转八" class="headerlink" title="二转八"></a>二转八</h3><p>从低位开始，二进制数<strong>每三位</strong>一组，转换成对应的<strong>八进制数</strong></p><pre><code>ob 11010101=101,010,011=523</code></pre><h3 id="二转十六"><a href="#二转十六" class="headerlink" title="二转十六"></a>二转十六</h3><p>从低位开始，二进制数每<strong>四位</strong>一组，转换成对应的<strong>十六进制</strong>数</p><pre><code>ob 11010101=1101,0101=13(D)，5=D5</code></pre><h2 id="八，十六进制转二进制"><a href="#八，十六进制转二进制" class="headerlink" title="八，十六进制转二进制"></a>八，十六进制转二进制</h2><h3 id="八转二"><a href="#八转二" class="headerlink" title="八转二"></a>八转二</h3><p>八进制的每一位数转成对应的一个<strong>3位二进制</strong>即可。</p><pre><code>0237=0,2,3,7=000,010,011,111=000010011111</code></pre><h3 id="十六转二"><a href="#十六转二" class="headerlink" title="十六转二"></a>十六转二</h3><p>十六进制的每一位数转成对应的一个<strong>4位二进制</strong>即可。</p><pre><code>23B=2,3,B=0010，0011，1011=001000111011</code></pre><h1 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h1><p>binary二进制数用0和1来组合并表示任何数，规则是逢2进1，1在不同的位上表示不同的值，从<strong>右到左次序</strong>，以二倍递增。</p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><ol><li><p>做高位（左边）是符号位：0正数，1负数</p></li><li><p>正数的源码，反码，补码都一样(三码合一)</p></li><li><p>负数的反码&#x3D;它的源码符号位不变，其他位取反(0-&gt;1，1-&gt;0)</p></li><li><p>负数的补码&#x3D;她的反码+1，负数的反码&#x3D;负数的补码-1</p></li><li><p>0的反码，补码都是0</p></li><li><p>java没有无符号数，java中的数都是有符号的。</p></li><li><p>计算机运算时，都是以<strong>补码的方式计算</strong>的。</p></li><li><p>但查看运算结果的时候，看它的源码</p></li></ol><h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><h3 id="反码"><a href="#反码" class="headerlink" title="反码"></a>反码</h3><h3 id="补码"><a href="#补码" class="headerlink" title="补码"></a>补码</h3><h2 id="位运算符"><a href="#位运算符" class="headerlink" title="位运算符"></a>位运算符</h2><p>7个位运算符：（&amp;、|、^、~、&gt;&gt;、&lt;&lt;和&gt;&gt;&gt;）</p><h3 id="amp-与"><a href="#amp-与" class="headerlink" title="&amp; 与"></a>&amp; 与</h3><p>两位全为1，结果为1，否则是0；</p><pre><code>1001000111110010=10010000</code></pre><h3 id="或"><a href="#或" class="headerlink" title="| 或"></a>| 或</h3><p>两位只要一个为1，结果为1，否则是0；</p><pre><code>2|3       int：4个字节大小2:00000000 00000000 00000000 000000103:00000000 00000000 00000000 00000011  00000000 00000000 00000000 00000011  =3</code></pre><h3 id="异或"><a href="#异或" class="headerlink" title="^ 异或"></a>^ 异或</h3><p>两个一个为0一个为1，结果是1，否则是0；</p><pre><code>2^32:00000000 00000000 00000000 000000103:00000000 00000000 00000000 00000011  00000000 00000000 00000000 00000001  =1</code></pre><h3 id="取反"><a href="#取反" class="headerlink" title="~ 取反"></a>~ 取反</h3><p>0-&gt;1，1-&gt;0，包括符号位</p><pre><code>~-2源码：         10000000 00000000 00000000 00000010负数反码：         11111111 11111111 11111111 11111101负数补码：         11111111 11111111 11111111 11111110 补码运算：结果补码:00000000 00000000 00000000 00000001正数三码合一</code></pre><h3 id="gt-gt-算术右移"><a href="#gt-gt-算术右移" class="headerlink" title="&gt;&gt;算术右移"></a>&gt;&gt;算术右移</h3><p>低位溢出，符号位不变，使用符号位补溢出的高位</p><pre><code>10&gt;&gt;2;源码：    00000000 00000000 00000000 00001010三码合一右移两位  00000000 00000000 00000000 00000010= 10/2/2=2</code></pre><h3 id="lt-lt-算术左移"><a href="#lt-lt-算术左移" class="headerlink" title="&lt;&lt;算术左移"></a>&lt;&lt;算术左移</h3><p>符号位不变，低位补0</p><pre><code>10&lt;&lt;2;源码：    00000000 00000000 00000000 00001010三码合一右移两位  00000000 00000000 00000000 00101000= 10*2*2=40</code></pre><h3 id="gt-gt-gt-无符号右移"><a href="#gt-gt-gt-无符号右移" class="headerlink" title="&gt;&gt;&gt;无符号右移"></a>&gt;&gt;&gt;无符号右移</h3><p>逻辑右移也叫做无符号右移，运算规则是：<strong>低位溢出，高位补0</strong>。没有&lt;&lt;&lt;符号！！</p><pre><code>10&gt;&gt;&gt;2;源码：    00000000 00000000 00000000 00001010三码合一右移两位  00000000 00000000 00000000 00000010= 10/2/2=2</code></pre>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MysqlMVCC</title>
      <link href="/mysql/mysqlmvcc/"/>
      <url>/mysql/mysqlmvcc/</url>
      
        <content type="html"><![CDATA[<h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><h2 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h2><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>全称：Multi-Version Concurrency Control，多版本并发控制。指维护一个数据的多个版本，使读写操作没有冲突，快照读为MySQL实现MVCC提供了一个非阻塞读功能。MVCC具体实现，还需要依赖于数据库记录中的<strong>三个隐藏字段、undo log日志、readView</strong></p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="当前读"><a href="#当前读" class="headerlink" title="当前读"></a>当前读</h3><p>当前读读取的记录的最新版本，读取还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。</p><p>对于我们日常操作，select…lock in share mode，select … for update，update，insert，delete（排他锁）都是一种当前锁。</p><h3 id="快照读"><a href="#快照读" class="headerlink" title="快照读"></a>快照读</h3><p>简单的select(不加锁)就是快照读，读取的是数据的可见版本，可能是历史数据，不加锁，是非阻塞读。</p><p><strong>Read Committed</strong>：每次select，都生成一个快照读</p><p><strong>Repeatable Read</strong>：开启事务后第一个select语句才是快照读的地方</p><p><strong>Serializable</strong>：快照读会退化为当前读。</p><h2 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h2><h3 id="三个隐式字段"><a href="#三个隐式字段" class="headerlink" title="三个隐式字段"></a>三个隐式字段</h3><p>DB_TRX_ID：事务id，自增型id</p><p>DB_ROLL_PRT：指针，指向上一个版本和undo log 配合使用。</p><p>DB_ROW_ID：隐藏主键，某张表没主键才会生成</p><p><img src="/mysql/mysqlmvcc/image-20220215190337025.png" alt="image-20220215190337025"></p><h4 id="查看-ibd文件"><a href="#查看-ibd文件" class="headerlink" title="查看.ibd文件"></a>查看.ibd文件</h4><p>&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;itcast&#x2F;</p><p>里面查看表中的隐藏字段</p><pre><code class="mysql">ibd2sdi stu.ibd</code></pre><h3 id="undo-log"><a href="#undo-log" class="headerlink" title="undo log"></a>undo log</h3><p>回滚日志，在insert，update，delete的时候便于数据回滚的日志</p><p>当insert的时候，产生的undo log 日志只在回滚时需要，在事务提交后，即可立即删除。</p><p>而update，delete的时候，产生的时候undo log 日志不仅在回滚时需要，在快照时也需要，不会立即删除。</p><h4 id="undo-log-版本链"><a href="#undo-log-版本链" class="headerlink" title="undo log 版本链"></a>undo log 版本链</h4><p>不同事物或者相同事物对同一条记录进行修改，会导致该记录的undolog生成一条记录版本链表，链表的头部是最新的旧记录，链表尾部是最早的旧纪录。</p><p><img src="/mysql/mysqlmvcc/image-20220215195842224.png" alt="image-20220215195842224"></p><h3 id="readview"><a href="#readview" class="headerlink" title="readview"></a>readview</h3><p>ReadView（读视图）是<strong>快照读</strong>SQL执行时MVCC提取数据的根据，记录并维护系统当前活跃的事务（未提交）id。</p><p>ReadView中包含了四个核心字段</p><h4 id="四个核心字段："><a href="#四个核心字段：" class="headerlink" title="四个核心字段："></a>四个核心字段：</h4><p><img src="/mysql/mysqlmvcc/image-20220215200320440.png" alt="image-20220215200320440"></p><p>readview </p><p><img src="/mysql/mysqlmvcc/image-20220215200637554.png" alt="image-20220215200637554"></p><p><img src="/mysql/mysqlmvcc/image-20220215200846951.png" alt="image-20220215200846951"></p><p>Read committed <strong>每一次执行快照读都会生成新的ReadView</strong>。</p><p><img src="/mysql/mysqlmvcc/image-20220215202037793.png" alt="image-20220215202037793"></p><p>根据undo log版本链的事务id进行判断然后选择对应的数据记录读取。</p><p><img src="/mysql/mysqlmvcc/image-20220215202718315.png" alt="image-20220215202718315"></p><p>repeatable read 仅在事务第一次执行快照读时生成ReadView，后续一直复用该ReadView。</p><p>判断规则和Read committed 一样。</p><p><img src="/mysql/mysqlmvcc/image-20220215203216674.png" alt="image-20220215203216674"></p>]]></content>
      
      
      <categories>
          
          <category> Mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mysql事务</title>
      <link href="/mysql/mysql-shi-wu/"/>
      <url>/mysql/mysql-shi-wu/</url>
      
        <content type="html"><![CDATA[<h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>事务(TRANSACTION)是作为单个逻辑工作单元执行的一系列操作,这些操作作为一个整体一起向系统提交，要么都执行、要么都不执行 。<br>事务是一个不可分割的工作逻辑单元，事务处理可以用来维护数据库的完整性，保证成批的 SQL 语句要么全部执行，要么全部不执行。</p><p>2.在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。</p><p>7.<br>   8.<br>      9.<br>         10. 11.</p><h3 id="事务操作"><a href="#事务操作" class="headerlink" title="事务操作"></a>事务操作</h3><p>事务用来管理 insert,update,delete 语句</p><h4 id="查看事务是否自动提交"><a href="#查看事务是否自动提交" class="headerlink" title="查看事务是否自动提交"></a>查看事务是否自动提交</h4><pre><code class="sql">show variables like &#39;autocommit&#39;;</code></pre><h4 id="设置事务自动提交"><a href="#设置事务自动提交" class="headerlink" title="设置事务自动提交"></a>设置事务自动提交</h4><pre><code class="sql">set autocommit = 0(禁止自动提交)/1(自动提交)</code></pre><h4 id="开启事务"><a href="#开启事务" class="headerlink" title="开启事务"></a>开启事务</h4><pre><code class="sql">BEGIN|START TRANSACTION</code></pre><h4 id="提交事务"><a href="#提交事务" class="headerlink" title="提交事务"></a>提交事务</h4><pre><code class="sql">commit</code></pre><h4 id="回滚事务"><a href="#回滚事务" class="headerlink" title="回滚事务"></a>回滚事务</h4><pre><code class="sql">rollbackROLLBACK TO identifier#把事务回滚到标记点；</code></pre><h5 id="设置保存点-标记点"><a href="#设置保存点-标记点" class="headerlink" title="设置保存点(标记点)"></a>设置保存点(标记点)</h5><pre><code class="sql">SAVEPOINT identifier</code></pre><h5 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h5><pre><code class="sql">SET AUTOCOMMIT = 0; -- 禁止自动提交BEGIN;SAVEPOINT point1;INSERT INTO person VALUES (NULL,&#39;牛犇&#39;);SAVEPOINT point2;INSERT INTO person VALUES (NULL,&#39;牛犇犇&#39;);SAVEPOINT point3;INSERT INTO person VALUES (NULL,&#39;牛犇犇犇&#39;);ROLLBACK TO point2;;COMMIT;</code></pre><h3 id="四大特性"><a href="#四大特性" class="headerlink" title="四大特性"></a>四大特性</h3><p><strong>ACID</strong>：原子性，一致性，持久性，隔离性</p><p>事务是必须满足4个条件（ACID）： Atomicity（原子性）、Consistency（稳定性）、Isolation（隔离性）、Durability（可靠性）</p><p><strong>原子性（Atomicity）</strong>：事务是一个完整的操作。事务的各步操作是不可分的（原子的）；要么都执行，要么都不执行</p><p><strong>一致性（Consistency）</strong>：当事务完成时，数据必须处于一致状态</p><p><strong>隔离性（Isolation）</strong>：对数据进行修改的所有并发事务是彼此隔离的，事务不受到外部并发操作影响的独立环境下运行。</p><p><strong>永久性（Durability）</strong>：事务完成后，它对数据库的修改被永久保持，事务日志能够保持事务的永久性</p><h3 id="并发事务问题"><a href="#并发事务问题" class="headerlink" title="并发事务问题"></a>并发事务问题</h3><h4 id="脏读"><a href="#脏读" class="headerlink" title="脏读"></a>脏读</h4><p>一个事务读到另外一个事务还没有提交的数据</p><p><img src="/mysql/mysql-shi-wu/image-20220215144511673.png" alt="image-20220215144511673"></p><h4 id="不可重复读"><a href="#不可重复读" class="headerlink" title="不可重复读"></a>不可重复读</h4><p>一个事务先后读取同一条记录，但两次读取的数据不同，称为不可重复读。</p><p><img src="/mysql/mysql-shi-wu/image-20220215144657019.png" alt="image-20220215144657019"></p><h4 id="幻读"><a href="#幻读" class="headerlink" title="幻读"></a>幻读</h4><p>一个事务按照按条件查询数据时，没有对应的数据行；但是插入数据时，发现这行数据已经存在，好像出现了’幻影’</p><p><img src="/mysql/mysql-shi-wu/image-20220215144957719.png" alt="image-20220215144957719"></p><p><img src="/mysql/mysql-shi-wu/image-20220215151214167.png" alt="image-20220215151214167"></p><h3 id="事务隔离级别"><a href="#事务隔离级别" class="headerlink" title="事务隔离级别"></a>事务隔离级别</h3><p><img src="/mysql/mysql-shi-wu/image-20220215145401535.png" alt="image-20220215145401535"></p><h4 id="1、Read-uncommitted"><a href="#1、Read-uncommitted" class="headerlink" title="1、Read uncommitted"></a>1、Read uncommitted</h4><p>读未提交</p><h4 id="2、Read-committed"><a href="#2、Read-committed" class="headerlink" title="2、Read committed"></a>2、Read committed</h4><p>读已提交</p><h4 id="3、Repeatable-Read-默认）"><a href="#3、Repeatable-Read-默认）" class="headerlink" title="3、Repeatable Read(默认）"></a>3、Repeatable Read(默认）</h4><p>可重复读</p><h4 id="4、Serializable"><a href="#4、Serializable" class="headerlink" title="4、Serializable"></a>4、Serializable</h4><p>串行化</p><h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><h5 id="查看事务隔离级别"><a href="#查看事务隔离级别" class="headerlink" title="查看事务隔离级别"></a>查看事务隔离级别</h5><pre><code class="sql">select @@Transaction_isolation;</code></pre><h5 id="设置事务隔离级别"><a href="#设置事务隔离级别" class="headerlink" title="设置事务隔离级别"></a>设置事务隔离级别</h5><pre><code class="SQL">set [session|Global] TRANSACTION ISOLATION LEVEL &#123;READ UNCOMMITTED |READ COMMITTED|REPEATABLE READ | SERIALIZABLE&#125;</code></pre><h3 id="事务原理"><a href="#事务原理" class="headerlink" title="事务原理"></a>事务原理</h3><p>原子性——undo log</p><p>持久性——redo log</p><p>一致性——undo log + redo log</p><p>隔离性——锁+MVCC</p><p>架构具体查看：<strong>Mysql存储引擎和索引 InnoDB架构</strong> 内容</p><p><img src="/mysql/mysql-shi-wu/image-20220215142048150.png" alt="image-20220215142048150"></p><p><a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?p=139">https://www.bilibili.com/video/BV1Kr4y1i7ru?p=139</a></p><h4 id="redo-log"><a href="#redo-log" class="headerlink" title="redo log"></a>redo log</h4><p>重做日志，记录的是事务提交时数据页的物理修改，用来实现事务的持久性。</p><p>该日志文件由两部分组成：重做日志缓冲（redo log buffer）以及重做日志文件（redo log file），前者是在内存中(redo log buffer)，后面再持久化磁盘中。</p><p>当事务提交之后会把所有修改信息都存到该日志文件中，用于再<strong>刷新赃页到磁盘，发生错误时，进行数据恢复使用</strong>。</p><p>WAL：read buffer写入磁盘.logfile 是顺序读写的，性能很高，在Buffer Pool将脏数据放入.ibd后，定期的删除log file中的日志数据。</p><p>相较Buffer Pool则性能很低，采用随机独写的方式写入到磁盘的.ibd文件</p><p><img src="/mysql/mysql-shi-wu/image-20220215180111302.png" alt="image-20220215180111302"></p><h4 id="undo-log"><a href="#undo-log" class="headerlink" title="undo log"></a>undo log</h4><p>回滚日志，用于记录数据被修改前的信息，作用包含两个：提供回滚和MVCC（多版本并发控制）</p><p>undo long 和 redo log 记录物理日志不一样，他是逻辑日志。可以认为delete一条记录时，undo log会记录一条记录对应的insert语句，反之亦然，当update一条记录时，他记录一条对应相反的update记录。当执行rollback时，就可以从undo log 中的逻辑记录读取到相应的内容并进行回滚。</p><p>Undo log 销毁：undo log 在事务执行时产生，事务提交时，并不会立即删除undo log，因为这些日志可能还用于MVCC。</p><p>Undo log存储：undo log 采用段的方式进行管理和记录，存放在rollback segment 回滚段中，内部包含1024个undo log segment。</p>]]></content>
      
      
      <categories>
          
          <category> Mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构</title>
      <link href="/shu-ju-jie-gou/"/>
      <url>/shu-ju-jie-gou/</url>
      
        <content type="html"><![CDATA[<p>原文链接：<a href="https://mp.weixin.qq.com/s/X3zYwQXxq93P_XUzFmKluQ">https://mp.weixin.qq.com/s/X3zYwQXxq93P_XUzFmKluQ</a></p><h1 id="二叉树-Binary-Search-Trees"><a href="#二叉树-Binary-Search-Trees" class="headerlink" title="二叉树(Binary Search Trees)"></a>二叉树(Binary Search Trees)</h1><p>类似二分查找，但是插入的时候可能会结点不平衡，导致线性查找</p><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><p>1.左子树上所有结点的值均小于或等于它的根结点的值。<br>2.右子树上所有结点的值均大于或等于它的根结点的值。<br>3.左、右子树也分别为二叉排序树。</p><p><img src="/shu-ju-jie-gou/image-20220213142055244.png" alt="image-20220213142055244"></p><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p>运用了二分查找的思想，查找的最大次数等于二叉树的高度</p><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>顺序插入时，形成一个链表，查询性能大大降低。</p><p><img src="/shu-ju-jie-gou/image-20220213142345091.png" alt="image-20220213142345091"></p><h2 id="删除操作"><a href="#删除操作" class="headerlink" title="删除操作"></a>删除操作</h2><p>删除有三种情况</p><h3 id="情况一"><a href="#情况一" class="headerlink" title="情况一"></a><strong>情况一</strong></h3><p><img src="/shu-ju-jie-gou/image-20200730101829657.png" alt="image-20200730101829657"></p><h3 id="情况2"><a href="#情况2" class="headerlink" title="情况2"></a><strong>情况2</strong></h3><p><img src="/shu-ju-jie-gou/image-20200730101853280.png" alt="image-20200730101853280"></p><p><img src="/shu-ju-jie-gou/image-20200730102033782.png" alt="image-20200730102033782"></p><h3 id="情况3"><a href="#情况3" class="headerlink" title="情况3"></a><strong>情况3</strong></h3><p><img src="/shu-ju-jie-gou/image-20200730102137175.png" alt="image-20200730102137175"></p><p><img src="/shu-ju-jie-gou/image-20200730102203744.png" alt="image-20200730102203744"></p><p><img src="/shu-ju-jie-gou/image-20200730102245349.png" alt="image-20200730102245349"></p><h1 id="红黑树-Red-Black-Trees"><a href="#红黑树-Red-Black-Trees" class="headerlink" title="红黑树(Red-Black Trees)"></a>红黑树(Red-Black Trees)</h1><p><a href="https://mp.weixin.qq.com/s/X3zYwQXxq93P_XUzFmKluQ">https://mp.weixin.qq.com/s/X3zYwQXxq93P_XUzFmKluQ</a></p><h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><p>自平衡的二叉查找树（但不是平衡二叉树），<br>除了有二叉树的特性以外还包含一下特点</p><p>1、结点是红色或黑色。<br>2、根节点是黑色的<br>3、每个叶子节点是黑色的空结点 （null结点）<br>4、每个红色结点的两个子节点都是黑色（从每个叶子到跟的所有路径上不能有两个连续的红色节点）<br>5、从任一节点到每个叶子的所有路径都包含相同数目的黑色节点</p><p>6、红黑树从根到叶子的最长路径不会超过路径的2倍<br>但是删除或者新增节点，树的规则可能被打破</p><p>在红黑树当中，我们通过红色结点和黑色结点作为辅助，来判断一颗二叉树是否相对平衡。</p><h2 id="插入操作"><a href="#插入操作" class="headerlink" title="插入操作"></a>插入操作</h2><p><img src="/shu-ju-jie-gou/image-20200730095148180.png" alt="image-20200730095148180"></p><p><img src="/shu-ju-jie-gou/image-20200730095133046.png" alt="image-20200730095133046"></p><h2 id="调整的方法"><a href="#调整的方法" class="headerlink" title="调整的方法"></a>调整的方法</h2><p>变色和旋转  （旋转有分为左旋转和右旋转）</p><h3 id="变色"><a href="#变色" class="headerlink" title="变色"></a>变色</h3><p>变色就是把红的变成黑的，黑的变成红的<br>但是，仅仅把一个结点变色，会导致相关路径凭空多出一个黑色结点，<br>这样就打破了规则5。</p><p><img src="/shu-ju-jie-gou/image-20200730095308728.png" alt="image-20200730095308728"></p><h3 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h3><h5 id="左旋转"><a href="#左旋转" class="headerlink" title="左旋转"></a>左旋转</h5><p>逆时针旋转红黑树的两个结点，使得父结点被自己的右孩子取代，而自己成为自己的左孩子。</p><p><img src="/shu-ju-jie-gou/image-20200730095319373.png" alt="image-20200730095319373"></p><h5 id="右旋转："><a href="#右旋转：" class="headerlink" title="右旋转："></a>右旋转：</h5><p>顺时针旋转红黑树的两个结点，使得父结点被自己的左孩子取代，而自己成为自己的右孩子。</p><h2 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h2><h1 id="平衡二叉搜索树-AVL树"><a href="#平衡二叉搜索树-AVL树" class="headerlink" title="平衡二叉搜索树(AVL树)"></a>平衡二叉搜索树(AVL树)</h1><p><a href="https://mp.weixin.qq.com/s/Tbx-VZxca8Z2U8VpXl6GoA">https://mp.weixin.qq.com/s/Tbx-VZxca8Z2U8VpXl6GoA</a></p><h2 id="特征-1"><a href="#特征-1" class="headerlink" title="特征"></a>特征</h2><p><strong>和红黑树一样能够调整自身的平衡性</strong></p><p><strong>和红黑树的区别是，AVL树遵循高度平衡，任何子节点的两个子树高度差不超过1</strong></p><p>有一个概念叫做平衡因子</p><p>在红黑树当中，我们通过红色结点和黑色结点作为辅助，来判断一颗二叉树是否相对平衡。</p><p>而在AVL树当中，我们通过“<strong>平衡因子</strong>”来判断一颗二叉树是否符合高度平衡。</p><p>对于AVL树的每一个结点，平衡因子是它的<strong>左子树高度和右子树高度的差值</strong>。只有当二叉树所有结点的平衡因子都是-1, 0, 1这三个值的时候，这颗二叉树才是一颗合格的AVL树。</p><p>举个例子，下图就是一颗典型的AVL树，每个节点旁边都标注了平衡因子：</p><p><img src="/shu-ju-jie-gou/image-20200730144551376.png" alt="image-20200730144551376"></p><h1 id="B-Trees（多路平衡查找树）"><a href="#B-Trees（多路平衡查找树）" class="headerlink" title="B-Trees（多路平衡查找树）"></a>B-Trees（多路平衡查找树）</h1><p><a href="https://mp.weixin.qq.com/s/rDCEFzoKHIjyHfI_bsz5Rw">https://mp.weixin.qq.com/s/rDCEFzoKHIjyHfI_bsz5Rw</a></p><p>下面来具体介绍一下B-树（Balance Tree），一个m阶的B树具有如下几个特征：</p><p>（k是b树的阶数，阶数是指一个结点可以存储k个指针和k-1个元素）</p><p>1.根结点至少有两个子女。</p><p>2.每个中间节点都包含k-1个元素和k个孩子，其中 m&#x2F;2 &lt;&#x3D; k &lt;&#x3D; m</p><p>3.每一个叶子节点都包含k-1个元素，其中 m&#x2F;2 &lt;&#x3D; k &lt;&#x3D; m</p><p>4.所有的叶子结点都位于同一层。</p><p>5.每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。</p><h2 id="插入操作-1"><a href="#插入操作-1" class="headerlink" title="插入操作"></a>插入操作</h2><p>每个节点k个区间指针，k-1个key</p><p>key的值到达k数量后，把中间的元素上移，并分割成两个节点。然后子节点再次到达k个key时继续拆分。</p><h1 id="B-Trees"><a href="#B-Trees" class="headerlink" title="B+Trees"></a>B+Trees</h1><p>B+树是基于B-树的一种变体，有着比B-树更高的性能</p><p><strong>一个m阶的B+树具有如下几个特征：</strong></p><p>1.有k个子树的中间节点包含有k个元素（B树中是k-1个元素），<strong>每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。</strong></p><p>2.所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接，形成一个单向链表。</p><p>3.所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。</p><h1 id="B树和B-树的区别"><a href="#B树和B-树的区别" class="headerlink" title="B树和B+树的区别"></a>B树和B+树的区别</h1><p>B树各个节点都会存储数据，B+树只有子节点存储数据，中间节点只启到索引作用。</p><p>B树节点数据没有链接，B+树的子节点之间通过指针连接起来，形成链表</p><h1 id="分库分表"><a href="#分库分表" class="headerlink" title="分库分表"></a>分库分表</h1><h3 id="一致性hash"><a href="#一致性hash" class="headerlink" title="一致性hash"></a>一致性hash</h3><p>为了在分库分表时迁移数据方便和保证数据的可查性</p><p>比如数据通过hash取模方式放入到不同表中后，添加了新的节点，在查询的时候，取模的数就不同了，这样导致了数据根本拿不到！</p><p>一致性hash就是解决这个问题的</p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据治理</title>
      <link href="/shu-ju-cang-ku/shu-ju-zhi-li/"/>
      <url>/shu-ju-cang-ku/shu-ju-zhi-li/</url>
      
        <content type="html"><![CDATA[<h1 id="数据治理"><a href="#数据治理" class="headerlink" title="数据治理"></a>数据治理</h1><p>数据质量管理<strong>不是一时的数据治理手段</strong>，而是循环的管理过程。其终极目标是通过可靠的数据，提升数据在使用中的价值，并最终为企业赢得经济效益</p><h2 id="为什么要数据治理"><a href="#为什么要数据治理" class="headerlink" title="为什么要数据治理"></a><strong>为什么要数据治理</strong></h2><p>l 企业将获得更干净、质量更高的数据，为进一步的数据活动打好基础</p><p>l 标准化的数据资产管理方法、流程和策略，将有效提高数据运营效率</p><p>l 使数据更容易与业务建立紧密连系，推动数据资产的变现</p><p>l 提高数据安全性，保证合规性</p><p>总体来说，数据治理能够带来的好处就在于，更高效地帮助企业将数据价值转化成实际的业务价值。</p><h2 id="怎么做"><a href="#怎么做" class="headerlink" title="怎么做"></a>怎么做</h2><h4 id="统一规范"><a href="#统一规范" class="headerlink" title="统一规范"></a>统一规范</h4><h4 id="模型规范"><a href="#模型规范" class="headerlink" title="模型规范"></a><strong>模型规范</strong></h4><p>l 模型分层</p><p>l 模型数据流向</p><h4 id="主题划分"><a href="#主题划分" class="headerlink" title="主题划分"></a><strong>主题划分</strong></h4><p>l 面向业务：按照业务进行聚焦，降低对业务理解的难度，并能解耦复杂的业务。我们将实体关系模型进行变种处理为实体与业务过程模型。实体定义为业务过程的参与体；业务过程定义是由多个实体作用的结果，实体与业务过程都带有自己特有的属性。根据业务的聚合性，我们把业务进行拆分，形成了几个核心主题。</p><p>l 面向分析：按照分析聚焦，提升数据易用性，提高数据的共享与一致性。按照分析主体对象不同及分析特征，形成分析域主题在DWS 进行应用，例如用户分析域、订单分析域。</p><h4 id="词根"><a href="#词根" class="headerlink" title="词根"></a><strong>词根</strong></h4><h5 id="命名规范"><a href="#命名规范" class="headerlink" title="命名规范"></a><strong>命名规范</strong></h5><p>l 表命名</p><p>l 指标命名</p><h4 id="统一输出"><a href="#统一输出" class="headerlink" title="统一输出"></a><strong>统一输出</strong></h4><h5 id="数据资产管理"><a href="#数据资产管理" class="headerlink" title="数据资产管理"></a><strong>数据资产管理</strong></h5><p>借用大数据平台，我们实现了：</p><p>l 统一指标管理，保证了指标定义、计算口径、数据来源的一致性。</p><p>l 统一维度管理，保证了维度定义、维度值的一致性。</p><p>l 统一维表管理，保证了维表及维表主键编码的唯一性。</p><p>l 统一数据出口，实现了维度和指标元数据信息的唯一出口，维值和指标数据的唯一出口。</p><h4 id="元数据管理"><a href="#元数据管理" class="headerlink" title="元数据管理"></a><strong>元数据管理</strong></h4><h4 id="主数据管理"><a href="#主数据管理" class="headerlink" title="主数据管理"></a><strong>主数据管理</strong></h4><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p>主数据管理的关键就是“管理”。主数据管理不会创建新的数据或新的数据纵向结构。相反，它提供了一种方法，使企业能够有效地管理存储在分布系统中的数据。主数据管理使用现有的系统，它从这些系统中获取最新信息，并提供了先进的技术和流程，用于自动、准确、及时地分发和分析整个企业中的数据，并对数据进行验证。</p><h5 id="主数据功能"><a href="#主数据功能" class="headerlink" title="主数据功能"></a>主数据功能</h5><p>l 匹配与合并，从一个或多个源系统识别重复记录，维护主数据的全局唯一ID</p><p>l 跨数据源的整合，需要提供一个跨系统的信息合并视图，可以后期用来追溯</p><p>l 数据清洗后，可以通过olap引擎或者其他BI系统跨应用系统访问主数据</p><h4 id="数据质量管理"><a href="#数据质量管理" class="headerlink" title="数据质量管理"></a><strong>数据质量管理</strong></h4><p>数据质量管理（Data Quality Management），是指对数据从计划、获取、存储、共享、维护、应用、消亡生命周期的每个阶段里可能引发的各类数据质量问题，进行识别、度量、监控、预警等一系列管理活动，并通过改善和提高组织的管理水平使得数据质量获得进一步提高</p><h4 id="数据安全"><a href="#数据安全" class="headerlink" title="数据安全"></a><strong>数据安全</strong></h4><h5 id="数据脱敏"><a href="#数据脱敏" class="headerlink" title="数据脱敏"></a><strong>数据脱敏</strong></h5><h5 id="数据权限控制"><a href="#数据权限控制" class="headerlink" title="数据权限控制"></a><strong>数据权限控制</strong></h5><h5 id="程序检查"><a href="#程序检查" class="headerlink" title="程序检查"></a><strong>程序检查</strong></h5><h5 id="流程化操作"><a href="#流程化操作" class="headerlink" title="流程化操作"></a><strong>流程化操作</strong></h5><h5 id="敏感SQL实时审查及操作日志分析"><a href="#敏感SQL实时审查及操作日志分析" class="headerlink" title="敏感SQL实时审查及操作日志分析"></a><strong>敏感SQL实时审查及操作日志分析</strong></h5><h4 id="数据血缘"><a href="#数据血缘" class="headerlink" title="数据血缘"></a><strong>数据血缘</strong></h4><h5 id="表血缘"><a href="#表血缘" class="headerlink" title="表血缘"></a><strong>表血缘</strong></h5><p>l sql解析</p><p>l 图数据库</p><h5 id="字段血缘"><a href="#字段血缘" class="headerlink" title="字段血缘"></a><strong>字段血缘</strong></h5><h4 id="数据地图"><a href="#数据地图" class="headerlink" title="数据地图"></a><strong>数据地图</strong></h4><h5 id="表基础信息"><a href="#表基础信息" class="headerlink" title="表基础信息"></a><strong>表基础信息</strong></h5><p>l 表负责人</p><p>l 所属项目</p><p>l 读取次数</p><p>l 创建时间</p><p>l 生命周期</p><p>l 是否分区</p><p>l 存储量</p><p>l 表注释</p><p>l 表类型</p><h5 id="表权限信息"><a href="#表权限信息" class="headerlink" title="表权限信息"></a><strong>表权限信息</strong></h5><h5 id="表明细信息"><a href="#表明细信息" class="headerlink" title="表明细信息"></a><strong>表明细信息</strong></h5><p>l 字段信息</p><p>l 分区信息</p><p>l 变更记录</p><p>l 热度信息</p><h5 id="产出信息"><a href="#产出信息" class="headerlink" title="产出信息"></a><strong>产出信息</strong></h5><p>l 任务名称</p><p>l 运行日志</p><p>l 执行时长</p><p>l 开始时间</p><p>l 结束时间</p><h5 id="使用记录"><a href="#使用记录" class="headerlink" title="使用记录"></a><strong>使用记录</strong></h5><p>l 访问次数</p><p>l 字段热度</p><p>l 访问topN人员</p><h4 id="数据成本治理"><a href="#数据成本治理" class="headerlink" title="数据成本治理"></a><strong>数据成本治理</strong></h4>]]></content>
      
      
      <categories>
          
          <category> 数据治理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据治理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker</title>
      <link href="/docker/docker-bi-ji/"/>
      <url>/docker/docker-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>docker只能运行在64位的系统的上。</p><p>Docker通常的话是web应用的自动化打包和发布，自动化的测试和持续集成发布、服务器环境中部署和调整数据库或其他的后台应用。从头编译或者扩展现有的openshift平台来搭建自己的paas环境。</p><p>Docker是Go语言进行编写的，是一个开源的引擎，可以轻松地为任何引用创建一个轻量级的、可移植的、自给自足的容器</p><p>Docker公司目前有两个版本:Docker CE(社区版)、Docker EE（企业版）</p><p>容器技术</p><p>容器镜像</p><p>容器网络</p><p>容器存储</p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Atlas</title>
      <link href="/atlas/atlas/"/>
      <url>/atlas/atlas/</url>
      
        <content type="html"><![CDATA[<h1 id="Atlas"><a href="#Atlas" class="headerlink" title="Atlas"></a>Atlas</h1><h2 id="Atlas是什么"><a href="#Atlas是什么" class="headerlink" title="Atlas是什么"></a>Atlas是什么</h2><p>Atlas 是一组可扩展和可扩展的核心基础<strong>治理</strong>服务——使企业能够有效且高效地满足其在 Hadoop 中的合规性要求，并允许与整个企业数据生态系统集成。</p><p>Apache Atlas 为组织提供开放的<strong>元数据管理</strong>和<strong>治理功能</strong>，以构建其数据资产的目录，对这些资产进行分类和治理，并为数据科学家、分析师和数据治理团队提供围绕这些数据资产的协作能力。</p><p>如果想要对这些数据做好管理，光用文字、文档等东西是不够的，必须用图。Atlas就是把元数据变成图的工具。</p><h2 id="Atlas架构"><a href="#Atlas架构" class="headerlink" title="Atlas架构"></a>Atlas架构</h2><p>Atlas包括以下组件：</p><p>采用Hbase存储元数据</p><p>采用Solr实现索引</p><p>Ingest&#x2F;Export 采集导出组件 Type System类型系统 Graph Engine图形引擎 共同构成Atlas的核心机制</p><p>所有功能通过API向用户提供，也可以通过Kafka消息系统进行集成</p><p>Atlas支持各种源获取元数据：Hive，Sqoop，Storm。。。</p><p>还有优秀的UI支持</p><p><strong>Atlas High Level Architecture - Overview</strong></p><p><img src="/atlas/atlas/architecture.png" alt="**Atlas High Level Architecture - Overview**"></p><h3 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h3><p>atlas 的核心功能层： core –相当于 javaee 里面的 service 层 atlas 对外提供服务的一层：integration（api 层） </p><p>atlas 的核心功能层： core –相当于 javaee 里面的 service 层 </p><p>atlas 对外提供服务的一层：integration（api 层） </p><p><strong>Core Type System</strong>: Atlas 允许用户为他们想要管理的元数据对象定义一个模型。该模型由称为 “Type” 的定 义组成。”类型” 的 实例被称为 “实体” 表示被管理的实际元数据对象。类型系统是一个组件，允许 用户定义和管理类型和实体。由 Atlas 管理的所有元数据对象（例如 Hive 表）都使用类型进行建模， 并表示为实体。要在 Atlas 中存储新类型的元数据，需要了解类型系统组件的概念。</p><p><strong>Ingest&#x2F;Export</strong>：Ingest 组件允许将元数据添加到 Atlas。类似地，Export 组件暴露由 Atlas 检测到的元 数据更改，以作为事件引发，消费者可以使用这些更改事件来实时响应元数据更改。</p><p><strong>Graph Engine</strong>：在内部，Atlas 通过使用图模型管理元数据对象。以实现元数据对象之间的巨大灵活性 和丰富的关系。图引擎是负责在类型系统的类型和实体之间进行转换的组件，以及基础图形模型。除 了管理图对象之外，图引擎还为元数据对象创建适当的索引，以便有效地搜索它们。 </p><p><strong>JanusGraph</strong>：Atlas 使用 JanusGraph 图数据库来存储元数据对象。</p><p> JanusGraph 使用两个存储：<strong>默认情况下元数据存储配置为 HBase ，索引存储配置为 Solr。</strong></p><h3 id="Integration"><a href="#Integration" class="headerlink" title="Integration"></a>Integration</h3><p>用户可以使用两种方法管理 Atlas 中的元数据： </p><p><strong>API：Atlas</strong> 的所有功能都可以通过 REST API 提供给最终用户，允许创建，更新和删除类型和实体。 它也是查询和发现通过 Atlas 管理的类型和实体的主要方法。 </p><p><strong>Messaging</strong>：除了 API 之外，用户还可以选择使用基于 Kafka 的消息接口与 Atlas 集成。这对于将 元数据对象传输到 Atlas 以及从 Atlas 使用可以构建应用程序的元数据更改事件都非常有用。如果希 望使用与 Atlas 更松散耦合的集成，这可以允许更好的可扩展性，可靠性等，消息传递接口是特别有 用的。</p><p>Atlas 使用 Apache Kafka 作为通知服务器用于钩子和元数据通知事件的下游消费者之间的通 信。事件由钩子(hook)和 Atlas 写到不同的 Kafka 主题: </p><p><strong>ATLAS_HOOK</strong>: 来自各个组件的 Hook 的元数据通知事件通过写入到名为 ATLAS_HOOK 的 Kafka topic 发送到 Atlas； ATLAS_ENTITIES：从 Atlas 到其他集成组件</p><p>（如 Ranger）的事件写入到名为 ATLAS_ENTITIES 的 Kafka topic；</p><h3 id="Metadata-source"><a href="#Metadata-source" class="headerlink" title="Metadata source"></a>Metadata source</h3><p>Atlas 支持与许多元数据源的集成，将来还会添加更多集成。目前，Atlas 支持从以下数据源获取和管理元数据：</p><p><strong>Hive</strong>：通过 hive bridge，可以接入 Hive 的元数据，包括 hive_db&#x2F;hive_table&#x2F;hive_column&#x2F;hive_process </p><p><strong>Sqoop</strong> ： 通 过 sqoop bridge ， 可 以 接 入 关 系 型 数 据 库 的 元 数 据 ， 包 括 sqoop_operation_type&#x2F; sqoop_dbstore_usage&#x2F;sqoop_process&#x2F;sqoop_dbdatastore </p><p><strong>Falcon</strong>：通过 falcon bridge，atlas 可以接入 Falcon 的元数据，包括 falcon_cluster&#x2F;falcon_feed &#x2F;falcon_feed_creation&#x2F;falcon_feed_replication&#x2F; falcon_process </p><p><strong>Storm</strong>:通过 storm bridge，atlas 可以接入流式处理的元数据，包括 storm_topology&#x2F;storm_spout&#x2F;storm_bolt</p><p>Atlas 集成大数据组件的元数据源需要实现以下两点： </p><p>首先，需要基于 atlas 的类型系统定义能够表达大数据组件元数据对象的元数据模型(例如 Hive 的元数据模型实现在 org.apache.atlas.hive.model.HiveDataModelGenerator)； </p><p>然后，需要提供 hook 组件去从大数据组件的元数据源中提取元数据对象，实时侦听元数据的变更并反馈给 atlas；</p><h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><p><strong>Atlas Admin UI</strong>: 该组件是一个基于 Web 的应用程序，允许数据管理员和数据分析师发现和注释元数据。</p><p>Admin UI 提供了搜索界面和 类 SQL 的查询语言，可以用来查询由 Atlas 管理的元数据类型 和对象。Admin UI 使用 Atlas 的 REST</p><h2 id="Atlas特性"><a href="#Atlas特性" class="headerlink" title="Atlas特性"></a>Atlas特性</h2><p>Atlas支持各种Hadoop和非Hadoop元数据类型<br>提供了丰富的REST API进行集成<br>对数据血缘的追溯达到了字段级别，这种技术还没有其实类似框架可以实现<br>对权限也有很好的控制</p><p>atlas 的核心功能层： core –相当于 javaee 里面的 service 层 atlas 对外提供服务的一层：integration（api 层） </p><h2 id="下载安装启动"><a href="#下载安装启动" class="headerlink" title="下载安装启动"></a>下载安装启动</h2><p><a href="https://atlas.apache.org/#/Downloads">https://atlas.apache.org/#/Downloads</a></p><p>1.下载源码包，没有编译好的代码。需要自己编译</p><p>2.解压</p><pre><code class="shell">tar -zxf apache-atlas-2.0.0-sources.tar.gz</code></pre><p>3.linux使用maven来编译项目</p><p>​    3.1 没有maven，就先下载安装好 maven，配好 maven 的环境变量。添加国内下载镜像。</p><p>​    3.2 注意，atlas 可以使用内嵌的 hbase-solr 作为底层索引存储和搜索组件，也可以使用外置的 hbase 和 solr 如果要使用内嵌的 hbase-solr:则使用如下命令进行编译打包:</p><pre><code class="shell">[root@h4 ~]# cd apache-atlas-sources-2.0.0/[root@h4 apache-atlas-sources-2.0.0]# export MAVEN_OPTS=&quot;-Xms2g -Xmx2g&quot;//不适用内嵌hbase-solr[root@h4 apache-atlas-sources-2.0.0]# mvn clean -DskipTests package -Pdist//使用内嵌hbase-solr[root@h4 apache-atlas-sources-2.0.0]# mvn clean -DskipTests package -Pdist,embedded-hbase-solr</code></pre><p>视网络速度，耐心等待，并且可能要反复重试几次，最好是能开一个速度不错的 vpn</p><p>最要命的是，atlas 的 webui 子模块中依赖了 nodejs，会从 nodejs 的中央仓库去下载相关 </p><p>编译完成之后，会产生打包结果，所在位置是：源码目录中的新出现的 distro&#x2F;target 目录</p><p><img src="/atlas/atlas/image-20220128135046413.png" alt="image-20220128135046413"></p><p>如果失败，一定要清干净下载的东西，建议做之前自己电脑搞一个系统快照。</p><p>4.编译好后，安装好相关组件</p><p>​4.1 安装 zookeeper（内嵌版不需要安装）每一台都要起）</p><pre><code>bin/zkServer.sh start</code></pre><p>​4.2 安装安装 kafka（内嵌版不需要安装）每一台都要起）</p><pre><code> bin/kafka-server-start.sh config/server.properties  bin/kafka-topics.sh --list --zookeeper h1:2181,h2:2181,h3:2181</code></pre><p>​4.3 安装 hbase（自带版不需要安装）每一台都要起）</p><pre><code>bin/kafka-topics.sh --list --zookeeper h1:2181,h2:2181,h3:2181</code></pre><p>​4.4 安装 hbase（自带版不需要安装）每一台都要起）</p><pre><code class="shell">bin/start-hbase.shbin/hbase shellbase(main):001:0&gt; status1 active master, 0 backup masters, 3 servers, 0 dead, 0.6667 average load</code></pre><p>​4.5 安装 solr（自带版不需要安装）</p><p>包直接解压就可以</p><pre><code>bin/solr start -c -z h1:2181,h2:2181,h3:2181 -p 8984 -forc</code></pre><pre><code class="shell">$&#123;SOLR&#125;/bin/solr create -c vertex_index -shards 1 -replicationFactor 1 -force$&#123;SOLR&#125;/bin/solr create -c edge_index -shards 1 -replicationFactor 1 -force$&#123;SOLR&#125;/bin/solr create -c fulltext_index -shards 1 -replicationFactor 1 -force// 创建文档集 1个分片  1个副本  强制执行</code></pre><p><img src="/atlas/atlas/image-20220128160155574.png" alt="image-20220128160155574"></p><p>5.上传 atlas 编译好之后的安装包</p><p>6.修改配置文件</p><p><strong>vi atlas-env.sh</strong></p><pre><code class="shell">export JAVA_HOME=/opt/app/jdk1.8.0_191/export MANAGE_LOCAL_HBASE=false (如果要使用内嵌的 zk 和 hbase，则改为 true)export MANAGE_LOCAL_SOLR=false （如果要是用内嵌的 solr，则改为 true）export HBASE_CONF_DIR=/opt/apps/hbase-2.0.6/conf</code></pre><p><strong>vi atlas-application.properties</strong></p><pre><code class="shell"># Hbase 地址配置atlas.graph.storage.hostname=doitedu01:2181,doitedu02:2181,doitedu03:2181（如果使用内嵌 hbase，则填写 localhost:2181）# Solr 地址配置#Solr http mode propertiesatlas.graph.index.search.solr.mode=httpatlas.graph.index.search.solr.http-urls=http://h3:8984/solr（solr 服务地址）# Kafka 地址配置atlas.notification.embedded=false （如果要使用内嵌的 kafka，则改为 true）atlas.kafka.zookeeper.connect=doitedu01:2181,doitedu02:2181,doitedu03:2181atlas.kafka.bootstrap.servers=doitedu01:9092,doitedu02:9092,doitedu03:9092######### Server Properties #########atlas.rest.address=http://doitedu01:21000</code></pre><p>7.启动 atlas</p><pre><code class="shell">[root@h3 apache-atlas-2.0.0]# bin/atlas_start.py</code></pre><p>8.访问web页面</p><p>账号和密码默认都是admin</p><pre><code>ip:21000</code></pre><p><img src="/atlas/atlas/image-20220128195253572.png" alt="image-20220128195253572"></p><h2 id="注入元数据"><a href="#注入元数据" class="headerlink" title="注入元数据"></a>注入元数据</h2><h3 id="1-Hive配置Hook"><a href="#1-Hive配置Hook" class="headerlink" title="1.Hive配置Hook"></a>1.Hive配置Hook</h3><p>在外部组件中安装配置 <strong>Hook</strong> ，配置完毕后，在 hive 中做任何操作，都会被钩子所感应到，并以事件的形式发布到 kafka， 然后，atlas 的 Ingest 模块会消费到 kafka 中的消息，并解析生成相应的 atlas 元数据写入底层的 Janus 图数据库来存储管理；</p><p><strong>修改 hive-env.sh</strong></p><pre><code>export HIVE_AUX_JARS_PATH=/opt/apps/apache-atlas-2.1.0/hook/hive </code></pre><p><strong>修改 hive-site.xml</strong></p><pre><code>&lt;property&gt;&lt;name&gt;hive.exec.post.hooks&lt;/name&gt;&lt;value&gt;org.apache.atlas.hive.hook.HiveHook&lt;/value&gt;&lt;/property&gt;</code></pre><p><strong>同步配置</strong></p><p>拷贝 atlas 配置文件 atlas-application.properties 到 hive 配置目录</p><p>添加两行配置：</p><pre><code>atlas.hook.hive.synchronous=falseatlas.hook.hive.numRetries=3atlas.hook.hive.queueSize=10000//异步//重试次数//队列大小</code></pre><h3 id="2-Hive历史元数据import导入"><a href="#2-Hive历史元数据import导入" class="headerlink" title="2.Hive历史元数据import导入"></a>2.Hive历史元数据import导入</h3><p>在 atlas 安装之前，hive 中已存在的表，钩子是不会自动感应并生成相关元数据的；</p><p>可以通过 atlas 的一个工具，来对已存在的 hive 库或表进行元数据导入；</p><p>它是通过访问Rest Api 来</p><pre><code class="shell">bin/import-hive.sh#默认会全部数据都导入atlasbin/import-hive.sh [-d &lt;database regex&gt; OR --database &lt;database regex&gt;] [-t &lt;table regex&gt; OR --table &lt;table regex&gt;]bin/import-hive.sh [-f &lt;filename&gt;]</code></pre><p><img src="/atlas/atlas/image-20220128195337932.png" alt="image-20220128195337932"></p><h2 id="WEB-UI的使用"><a href="#WEB-UI的使用" class="headerlink" title="WEB-UI的使用"></a>WEB-UI的使用</h2><p>Apache ADMIN_UI 功能包括 4 部分：</p><h3 id="1-create-entity"><a href="#1-create-entity" class="headerlink" title="1.create entity"></a>1.create entity</h3><p>创建实体（创建数据资产的元数据–人工录入元数据） </p><p>（在 atlas 中，有大量的对各类数据资产适配的描述定义：实体类型！ hive_table; hive_db;hive_column;hive_process;hdfs_path,…..，如果我们有一个数据资产，没有对应的内置实体类型，可以自己去定义一个你的实体类型，来描述你的这个数据资产） </p><p><strong>示例</strong>：比如 hdfs 中有一个目录&#x2F;aaa&#x2F;bbb，需要进行描述</p><p><img src="/atlas/atlas/image-20220128205841996.png" alt="image-20220128205841996"></p><h3 id="2-search"><a href="#2-search" class="headerlink" title="2.search"></a>2.search</h3><p>搜索查看</p><h3 id="3-classfication"><a href="#3-classfication" class="headerlink" title="3.classfication"></a>3.classfication</h3><p>分类管理。</p><p>1.先定义好类别</p><p><img src="/atlas/atlas/image-20220128210455008.png" alt="image-20220128210455008"></p><p><img src="/atlas/atlas/image-20220128210512983.png" alt="image-20220128210512983"></p><p>2.类别体系创建好，为数据资产分配类别</p><p><img src="/atlas/atlas/image-20220128210645142.png" alt="image-20220128210645142"></p><p><img src="/atlas/atlas/image-20220128210700162.png" alt="image-20220128210700162"></p><p><img src="/atlas/atlas/image-20220128210715374.png" alt="image-20220128210715374"></p><h3 id="4-glossary"><a href="#4-glossary" class="headerlink" title="4.glossary"></a>4.glossary</h3><p>词汇表：对数据资产标记“术语 term”（标签）</p><p>1.创建<strong>词汇表</strong> </p><p><img src="/atlas/atlas/image-20220128210957012.png" alt="image-20220128210957012"></p><p>2.在创建好的 <strong>glossay</strong>（词汇）中添加 <strong>term</strong>（词）</p><p><img src="/atlas/atlas/image-20220128211154725.png" alt="image-20220128211154725"></p><p><img src="/atlas/atlas/image-20220128211201741.png" alt="image-20220128211201741"></p><p><img src="/atlas/atlas/image-20220128211209810.png" alt="image-20220128211209810"></p><p>3.glossay 创建后，就可以为数据资产打上 glossay 标记</p><p><img src="/atlas/atlas/image-20220128211234874.png" alt="image-20220128211234874"></p><h2 id="查看血缘"><a href="#查看血缘" class="headerlink" title="查看血缘"></a>查看血缘</h2><p><img src="/atlas/atlas/image-20220128211334305.png" alt="image-20220128211334305"></p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
          <category> Atlas </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
            <tag> Atlas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux配置邮件服务器</title>
      <link href="/linux/you-jian-ke-hu-duan/"/>
      <url>/linux/you-jian-ke-hu-duan/</url>
      
        <content type="html"><![CDATA[<h2 id="邮件系统组成"><a href="#邮件系统组成" class="headerlink" title="邮件系统组成"></a>邮件系统组成</h2><p>1.用户代理MUA（Mail User Agent）：“邮件用户代理”MUA是用在客户端的软件 ，主要的功能就是接收邮件主机的电 子邮件，并提供用户浏览与编写邮件的功能<br>2.邮件传输代理MTA（mail Transfer agent） ：将来自于MUA的邮件转发给指定用户<br>3.邮件投递代理MDA（mail delivery agent）：将来MTA的邮件保存到本机的收件箱中</p><table><thead><tr><th>简单邮件传输协议</th><th>STMP</th><th>用来发送或者中转转发的邮件</th><th>tcp占用端口 25</th></tr></thead><tbody><tr><td>第三版邮局协议</td><td>pop3</td><td>用于将服务器上把邮件存储到本地主机</td><td>tcp占用端口110</td></tr><tr><td>第四版互联网信息访问协议</td><td>IMAP4</td><td>用于本地主机上访问邮件</td><td>tcp占用端口143</td></tr></tbody></table><h2 id="安装和使用"><a href="#安装和使用" class="headerlink" title="安装和使用"></a>安装和使用</h2><p>先安装<strong>postfix</strong>和<strong>dovecot</strong>。 </p><h4 id="Mailx"><a href="#Mailx" class="headerlink" title="Mailx"></a>Mailx</h4><p><a href="https://www.cnblogs.com/dingkailinux/p/8723378.html">https://www.cnblogs.com/dingkailinux/p/8723378.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo.pdf文件上传github</title>
      <link href="/si-fang-cai-zi-liao-zheng-li/hexo.pdf-wen-jian-shang-chuan-github-wen-dang/"/>
      <url>/si-fang-cai-zi-liao-zheng-li/hexo.pdf-wen-jian-shang-chuan-github-wen-dang/</url>
      
        <content type="html"><![CDATA[<pre><code>&#123;% pdf  数据仓库（第3版).pdf %&#125;可以用这个来搞pdf，要求文件名称和你的目录名字一样，使用后会把名字对应的文件夹下的所有文件生成，变成静态文件，并在上传github时一起传输。超过100M，Github不让传输，这时使用 Git lfs，但是我不会用安装后没有继续尝试</code></pre>]]></content>
      
      
      <categories>
          
          <category> pdf </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pdf </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据湖</title>
      <link href="/shu-ju-hu/shu-ju-hu/"/>
      <url>/shu-ju-hu/shu-ju-hu/</url>
      
        <content type="html"><![CDATA[<h2 id="数据仓库的缺点"><a href="#数据仓库的缺点" class="headerlink" title="数据仓库的缺点"></a>数据仓库的缺点</h2><p>只能存储结构化数据，无法采集存储非机构化数据</p><p>无法存储原始数据，所有数据须经过ETL清洗过滤</p><p>离线数仓的数据表牵一发而动全身，数据调整工程量大</p><p>实时数仓存储空间有限，无法采集和存储海量实时数据</p><p>回溯效率低下，实时数据和离线数据计算接口难以统一</p><h2 id="数据湖的特点"><a href="#数据湖的特点" class="headerlink" title="数据湖的特点"></a>数据湖的特点</h2><h3 id="1-集中式存储"><a href="#1-集中式存储" class="headerlink" title="1.集中式存储"></a>1.集中式存储</h3><p>无需任何预处理，存储任意规模，任意类型，需求各种速度的数据，结构化半结构化和非结构化的数据</p><h3 id="2-支持各种分析方式"><a href="#2-支持各种分析方式" class="headerlink" title="2.支持各种分析方式"></a>2.支持各种分析方式</h3><p>数据胡基于读取型schema，采用读时模式，能够根据业务需求灵活建表，大大提升了<strong>敏捷性和精准度</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> 数据湖 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据湖 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mysql建表语句变Hive建表语句</title>
      <link href="/cdh/hive/mysql-jian-biao-yu-ju-ti-huan-cheng-hive-jian-biao-yu-ju/"/>
      <url>/cdh/hive/mysql-jian-biao-yu-ju-ti-huan-cheng-hive-jian-biao-yu-ju/</url>
      
        <content type="html"><![CDATA[<pre><code>`                 //删除NOT NULL          //删除DEFAULT NULL      //删除AUTO_INCREMENT    //删除bitint(20)        //替换成bigintvarchar\(.*?\)    //替换成stringint\(.*?\)        //替换成intdecimal\(.*?\)    //替换成stringdatetime          //替换成bigintPRIMARY KEY()     //删除</code></pre>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DataX</title>
      <link href="/datax/datax/"/>
      <url>/datax/datax/</url>
      
        <content type="html"><![CDATA[<h1 id="DataX"><a href="#DataX" class="headerlink" title="DataX"></a>DataX</h1><p>官方地址：<a href="https://github.com/alibaba/DataX"><a href="https://github.com/alibaba/DataX/blob/master/introduction.md">DataX&#x2F;introduction.md at master · alibaba&#x2F;DataX · GitHub</a></a></p><h2 id="DataX是什么"><a href="#DataX是什么" class="headerlink" title="DataX是什么"></a>DataX是什么</h2><p>DataX 是阿里云 <a href="https://www.aliyun.com/product/bigdata/ide">DataWorks数据集成</a> 的开源版本，在阿里巴巴集团内被广泛使用的离线数据同步工具&#x2F;平台。DataX 实现了包括 MySQL、Oracle、OceanBase、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、Hologres、DRDS 等各种异构数据源之间高效的数据同步功能。</p><p><img src="/datax/datax/93b7fc1c-6927-11e6-8cda-7cf8420fc65f.png" alt="datax_why_new"></p><h2 id="DataX3-0框架设计"><a href="#DataX3-0框架设计" class="headerlink" title="DataX3.0框架设计"></a>DataX3.0框架设计</h2><p><img src="/datax/datax/ec7e36f4-6927-11e6-8f5f-ffc43d6a468b.png" alt="datax_framework_new"></p><p>DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader&#x2F;Writer插件，纳入到整个同步框架中。</p><ul><li>Reader：Reader为数据采集模块，负责采集数据源的数据，将数据发送给Framework。</li><li>Writer： Writer为数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。</li><li>Framework：Framework用于连接reader和writer，作为两者的数据传输通道，并处理缓冲，流控，并发，数据转换等核心技术问题。</li></ul><h2 id="DataX3-0核心架构"><a href="#DataX3-0核心架构" class="headerlink" title="DataX3.0核心架构"></a>DataX3.0核心架构</h2><p>DataX 3.0 开源版本支持单机多线程模式完成同步作业运行，本小节按一个DataX作业生命周期的时序图，从整体架构设计非常简要说明DataX各个模块相互关系。</p><p><img src="/datax/datax/aa6c95a8-6891-11e6-94b7-39f0ab5af3b4.png" alt="datax_arch"></p><h4 id="核心模块介绍："><a href="#核心模块介绍：" class="headerlink" title="核心模块介绍："></a>核心模块介绍：</h4><ol><li>DataX完成单个数据同步的作业，我们称之为Job，DataX接受到一个Job之后，将启动一个进程来完成整个作业同步过程。DataX Job模块是单个作业的中枢管理节点，承担了数据清理、子任务切分(将单一作业计算转化为多个子Task)、TaskGroup管理等功能。</li><li>DataXJob启动后，会根据不同的源端切分策略，将Job切分成多个小的Task(子任务)，以便于并发执行。Task便是DataX作业的最小单元，每一个Task都会负责一部分数据的同步工作。</li><li>切分多个Task之后，DataX Job会调用Scheduler模块，根据配置的并发数据量，将拆分成的Task重新组合，组装成TaskGroup(任务组)。每一个TaskGroup负责以一定的并发运行完毕分配好的所有Task，默认单个任务组的并发数量为5。</li><li>每一个Task都由TaskGroup负责启动，Task启动后，会固定启动Reader—&gt;Channel—&gt;Writer的线程来完成任务同步工作。</li><li>DataX作业运行起来之后， Job监控并等待多个TaskGroup模块任务完成，等待所有TaskGroup任务完成后Job成功退出。否则，异常退出，进程退出值非0</li></ol><h4 id="DataX调度流程："><a href="#DataX调度流程：" class="headerlink" title="DataX调度流程："></a>DataX调度流程：</h4><p>举例来说，用户提交了一个DataX作业，并且配置了20个并发，目的是将一个100张分表的mysql数据同步到odps里面。 DataX的调度决策思路是：</p><ol><li>DataXJob根据分库分表切分成了100个Task。</li><li>根据20个并发，DataX计算共需要分配4个TaskGroup。</li><li>4个TaskGroup平分切分好的100个Task，每一个TaskGroup负责以5个并发共计运行25个Task。</li></ol><h2 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h2><h3 id="Download-DataX下载地址"><a href="#Download-DataX下载地址" class="headerlink" title="Download DataX下载地址"></a>Download <a href="http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz">DataX下载地址</a></h3><p>下载后解压至本地某个目录，进入bin目录，即可运行同步作业：</p><p><a href="https://github.com/alibaba/DataX/blob/master/userGuid.md">DataX&#x2F;userGuid.md at master · alibaba&#x2F;DataX · GitHub</a></p><h3 id="配置使用模板"><a href="#配置使用模板" class="headerlink" title="配置使用模板"></a>配置使用模板</h3><p>DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下图，详情请点击：<a href="https://github.com/alibaba/DataX/wiki/DataX-all-data-channels">DataX数据源参考指南</a></p><table><thead><tr><th>类型</th><th>数据源</th><th>Reader(读)</th><th>Writer(写)</th><th>文档</th></tr></thead><tbody><tr><td>RDBMS 关系型数据库</td><td>MySQL</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/mysqlreader/doc/mysqlreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/mysqlwriter/doc/mysqlwriter.md">写</a></td></tr><tr><td></td><td>Oracle</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/oraclereader/doc/oraclereader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/oraclewriter/doc/oraclewriter.md">写</a></td></tr><tr><td></td><td>OceanBase</td><td>√</td><td>√</td><td><a href="https://open.oceanbase.com/docs/community/oceanbase-database/V3.1.0/use-datax-to-full-migration-data-to-oceanbase">读</a> 、<a href="https://open.oceanbase.com/docs/community/oceanbase-database/V3.1.0/use-datax-to-full-migration-data-to-oceanbase">写</a></td></tr><tr><td></td><td>SQLServer</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/sqlserverreader/doc/sqlserverreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/sqlserverwriter/doc/sqlserverwriter.md">写</a></td></tr><tr><td></td><td>PostgreSQL</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/postgresqlreader/doc/postgresqlreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/postgresqlwriter/doc/postgresqlwriter.md">写</a></td></tr><tr><td></td><td>DRDS</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/drdsreader/doc/drdsreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/drdswriter/doc/drdswriter.md">写</a></td></tr><tr><td></td><td>通用RDBMS(支持所有关系型数据库)</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/rdbmsreader/doc/rdbmsreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/rdbmswriter/doc/rdbmswriter.md">写</a></td></tr><tr><td>阿里云数仓数据存储</td><td>ODPS</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/odpsreader/doc/odpsreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/odpswriter/doc/odpswriter.md">写</a></td></tr><tr><td></td><td>ADS</td><td></td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/adswriter/doc/adswriter.md">写</a></td></tr><tr><td></td><td>OSS</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/ossreader/doc/ossreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/osswriter/doc/osswriter.md">写</a></td></tr><tr><td></td><td>OCS</td><td></td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/ocswriter/doc/ocswriter.md">写</a></td></tr><tr><td>NoSQL数据存储</td><td>OTS</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/otsreader/doc/otsreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/otswriter/doc/otswriter.md">写</a></td></tr><tr><td></td><td>Hbase0.94</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/hbase094xreader/doc/hbase094xreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/hbase094xwriter/doc/hbase094xwriter.md">写</a></td></tr><tr><td></td><td>Hbase1.1</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/hbase11xreader/doc/hbase11xreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/hbase11xwriter/doc/hbase11xwriter.md">写</a></td></tr><tr><td></td><td>Phoenix4.x</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/hbase11xsqlreader/doc/hbase11xsqlreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/hbase11xsqlwriter/doc/hbase11xsqlwriter.md">写</a></td></tr><tr><td></td><td>Phoenix5.x</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/hbase20xsqlreader/doc/hbase20xsqlreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/hbase20xsqlwriter/doc/hbase20xsqlwriter.md">写</a></td></tr><tr><td></td><td>MongoDB</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/mongodbreader/doc/mongodbreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/mongodbwriter/doc/mongodbwriter.md">写</a></td></tr><tr><td></td><td>Hive</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md">写</a></td></tr><tr><td></td><td>Cassandra</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/cassandrareader/doc/cassandrareader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/cassandrawriter/doc/cassandrawriter.md">写</a></td></tr><tr><td>无结构化数据存储</td><td>TxtFile</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/txtfilereader/doc/txtfilereader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/txtfilewriter/doc/txtfilewriter.md">写</a></td></tr><tr><td></td><td>FTP</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/ftpreader/doc/ftpreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/ftpwriter/doc/ftpwriter.md">写</a></td></tr><tr><td></td><td>HDFS</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/hdfsreader/doc/hdfsreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md">写</a></td></tr><tr><td></td><td>Elasticsearch</td><td></td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/elasticsearchwriter/doc/elasticsearchwriter.md">写</a></td></tr><tr><td>时间序列数据库</td><td>OpenTSDB</td><td>√</td><td></td><td><a href="https://github.com/alibaba/DataX/blob/master/opentsdbreader/doc/opentsdbreader.md">读</a></td></tr><tr><td></td><td>TSDB</td><td>√</td><td>√</td><td><a href="https://github.com/alibaba/DataX/blob/master/tsdbreader/doc/tsdbreader.md">读</a> 、<a href="https://github.com/alibaba/DataX/blob/master/tsdbwriter/doc/tsdbhttpwriter.md">写</a></td></tr></tbody></table><p>解压后进入bin目录</p><pre><code>$ cd  &#123;YOUR_DATAX_HOME&#125;/bin$ python datax.py &#123;YOUR_JOB.json&#125;</code></pre><h2 id="DataX和Sqoop的对比"><a href="#DataX和Sqoop的对比" class="headerlink" title="DataX和Sqoop的对比"></a>DataX和Sqoop的对比</h2><table><thead><tr><th>功能</th><th>datax</th><th>sqoop</th></tr></thead><tbody><tr><td>运行模式</td><td>单进程多线程</td><td>mr</td></tr><tr><td>hive读写</td><td>单机压力大</td><td>扩展性好</td></tr><tr><td>分布式</td><td>不支持</td><td>支持</td></tr><tr><td>运行信息</td><td>运行时间，数据量，消耗资源，脏数据稽核</td><td>不支持</td></tr><tr><td>流量控制</td><td>支持</td><td>不支持</td></tr><tr><td>社区</td><td>开源不久，不太活跃</td><td>活跃</td></tr></tbody></table><p>对于sqoop和datax，如果只是单纯的数据同步，其实两者都是ok的，但是**如果需要集成在大数据平台，还是比较推荐使用datax，原因就是支持流量控制，支持运行信息收集，及时跟踪数据同步情况。</p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
          <category> DataX </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
            <tag> DataX </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>漏斗模型分析</title>
      <link href="/shu-ju-cang-ku/lou-dou-fen-xi/"/>
      <url>/shu-ju-cang-ku/lou-dou-fen-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是漏斗分析？"><a href="#什么是漏斗分析？" class="headerlink" title="什么是漏斗分析？"></a>什么是漏斗分析？</h2><p>知乎原文链接：<a href="https://zhuanlan.zhihu.com/p/133326442">https://zhuanlan.zhihu.com/p/133326442</a></p><p>漏斗模型概念：</p><p><strong>漏斗模型，是用来分析业务转化率的！</strong></p><p>漏斗分析是一套流程式数据分析，它能够科学反映用户行为状态以及从起点到终点各阶段用户转化率情况的重要分析模型。</p><p>分析师定义的一种业务路径，用户沿着这个路径上的各个步骤，不断走向业务目标 路径上的各个步骤，人数通常是会逐步递减，形如一个漏斗把这种分析形象地称呼为：漏斗模型分析</p><p><img src="/shu-ju-cang-ku/lou-dou-fen-xi/v2-3319735f3069d743945a9dbcc3909c7a_720w.jpg" alt="img"></p><h2 id="漏斗分析模型的特点与价值"><a href="#漏斗分析模型的特点与价值" class="headerlink" title="漏斗分析模型的特点与价值"></a>漏斗分析模型的特点与价值</h2><p>对于业务流程相对规范，周期较长、环节较多的流程进行分析，能够直观地发现和说明问题所在，可以更快地找出某个环节的转化率出现问题。</p><p><strong>1、企业可以监控用户在各个层级的转化情况。</strong></p><p>降低流失是运营人群的重要目标，通过不同层级的情况，迅速定位流失环节，针对性持续分析找到可优化点，如此提升用户留存率</p><p><strong>2、多维度切分与呈现用户转化情况</strong>科学的漏斗分析能够展现转化率趋势的曲线，能帮助企业精细地捕捉用户行为变化，提升了转化分析的精度和效率，对选购流程的异常定位和策略调整效果验证有科学指导意义。</p><p><strong>3、不同属性的用户群体漏斗比较</strong></p><p>漏斗对比分析是科学漏斗分析的重要一环，运营人员可以通过不同属性的用户群体（如新注册用户与老客户）各环节转化率，各流程步骤转化率的差异对比，了解转化率最高的用户群体，分析漏斗合理性，并针对转化率异常环节进行调整。</p><h2 id="漏斗分析封应用场景"><a href="#漏斗分析封应用场景" class="headerlink" title="漏斗分析封应用场景"></a>漏斗分析封应用场景</h2><h4 id="1、AARRR模型："><a href="#1、AARRR模型：" class="headerlink" title="1、AARRR模型："></a>1、AARRR模型：</h4><p>从用户增长各阶段入手，包括Acquisition用户获取，Activation用户激活，Retention用户留存，Revenue用户产生收入，Refer自传播。改模型主要应用于互联网行业</p><p><img src="/shu-ju-cang-ku/lou-dou-fen-xi/v2-a534ec86c91ab8ab6dd281f24be08c13_720w.jpg" alt="img"></p><h4 id="2、消费漏斗模型："><a href="#2、消费漏斗模型：" class="headerlink" title="2、消费漏斗模型："></a>2、消费漏斗模型：</h4><p>一般用于页面结构和内容较为复杂的业务，从用户内容消费和流量走向的角度，宏观层面用于回答用户消费什么内容，微观层面则用于分析影响用户消费的问题是什么。主要流程是从<strong>广告引流—商品介绍—场景打造—下单购买</strong></p><h4 id="3、电商漏斗模型："><a href="#3、电商漏斗模型：" class="headerlink" title="3、电商漏斗模型："></a>3、电商漏斗模型：</h4><p>典型的用户购买行为由以下连续的行为构成：<strong>浏览首页—浏览商品—提交订单—支付订单</strong>。</p><p>当我们期望观察各步骤间及总体转化率，可按以下步骤进行：</p><p><img src="/shu-ju-cang-ku/lou-dou-fen-xi/v2-692feaf5e30214a9b2c96874e74c481d_720w.jpg" alt="img"></p><h4 id="4、AIDMA模型："><a href="#4、AIDMA模型：" class="headerlink" title="4、AIDMA模型："></a>4、AIDMA模型：</h4><p>主要的流程是<strong>注意 → 兴趣 → 欲望 → 记忆 → 行动（购买）</strong>，适用于品牌营销。</p><p><img src="/shu-ju-cang-ku/lou-dou-fen-xi/v2-329b2f47c67846ce8d03eb964dc8ae29_720w.jpg" alt="img"></p><h4 id="5、AISAS模型："><a href="#5、AISAS模型：" class="headerlink" title="5、AISAS模型："></a>5、AISAS模型：</h4><p>主要的流程是<strong>注意-兴趣-搜索-行动-分享</strong>，在AIDMA模型的基础上增加了用户反馈的环节</p><p><img src="/shu-ju-cang-ku/lou-dou-fen-xi/v2-2178a249daeb22b042113c88f86dcd28_720w.jpg" alt="img"></p><h2 id="漏斗分析解决方法"><a href="#漏斗分析解决方法" class="headerlink" title="漏斗分析解决方法"></a>漏斗分析解决方法</h2><p>1.漏斗模型：分析业务转化率，定义模型步骤，有多个业务，业务有多个操作步骤<br>2.Sql可以使用 自定义函数 但是比较麻烦<br>3.正则表达式匹配<br>4.最终采用</p>]]></content>
      
      
      <categories>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新老访客标记</title>
      <link href="/shu-ju-cang-ku/xin-lao-fang-ke-biao-ji/"/>
      <url>/shu-ju-cang-ku/xin-lao-fang-ke-biao-ji/</url>
      
        <content type="html"><![CDATA[<p><img src="/shu-ju-cang-ku/xin-lao-fang-ke-biao-ji/image-20220117185134797.png" alt="image-20220117185134797"></p><p>dwd行为日志的guid对应设备账号绑定表的设备id和账号id，是1对2的，使用到了行转列函数，然后 <strong>今天的日志表 left join  账号设备绑定表</strong>  </p><h4 id="map端算法判断的方式："><a href="#map端算法判断的方式：" class="headerlink" title="map端算法判断的方式："></a>map端算法判断的方式：</h4><p><strong>布隆过滤器</strong>：</p><p>为设备账号绑定表创建一个布隆过滤器，然后广播这个布隆过滤器，然后对比它们的值。</p><p><strong>bitmap</strong>：</p><p>调个api，将所有的id创建在字节数组中</p><p><strong>hyperloglog</strong>：</p><p>在bitmap上的改良，精度会损失一些，但是使用的内存会大大降低。</p>]]></content>
      
      
      <categories>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ID_MAPPING</title>
      <link href="/shu-ju-cang-ku/id-mapping/"/>
      <url>/shu-ju-cang-ku/id-mapping/</url>
      
        <content type="html"><![CDATA[<h1 id="ID-MAPPING是什么？"><a href="#ID-MAPPING是什么？" class="headerlink" title="ID_MAPPING是什么？"></a>ID_MAPPING是什么？</h1><p>数据仓库中为每一个用户生成一个全局唯一标识(给匿名访问，绑定到一个 id 上）</p><p>选取合适的用户标识对于提高用户行为分析的准确性有非常大的影响，尤其是漏斗、留存、Session 等 用户相关的分析功能。</p><p>因此，我们在进行任何数据接入之前，都应当先确定如何来标识用户</p><h2 id="设计难点"><a href="#设计难点" class="headerlink" title="设计难点"></a>设计难点</h2><p>登录状态下，日志中会采集到用户的登录 id（account），可以做到用户身份的精确标识； </p><p>而在匿名状态下，日志中没有采集到用户的登录 id 准确标识用户，成为一件极其棘手的事情；</p><h3 id="困难原因"><a href="#困难原因" class="headerlink" title="困难原因"></a>困难原因</h3><p>在事件日志中，对用户能产生标识作用的字段有： </p><pre><code>app 日志中，有 deviceid，account web 日志中，有 cookieid，ip，account wxapp 日志中，有 openid，acc</code></pre><p>在现实中，一个用户，可能处于如下极其复杂的状态：</p><ul><li>登录状态访问 app</li><li>匿名状态访问 app </li><li>登录状态访问 web </li><li>匿名状态访问 web </li><li>登录状态访问 wx 小程序 </li><li>匿名状态访问 wx 小程序 </li><li>一个用户可能拥有不止一台终端设备</li><li>一台终端设备上可能有多个用户使用 </li><li>一个用户可能一段时间后更换手机 </li><li>……</li></ul><h2 id="方案1"><a href="#方案1" class="headerlink" title="方案1"></a>方案1</h2><p><strong>只使用设备id</strong></p><ul><li>适用场景</li></ul><p>适合没有用户注册体系，或者极少数用户会进行多设备登录的产品，如工具类产品、搜索引擎、部分 小型电商等。 这也是绝大多数数据分析产品唯一提供的方案</p><ul><li>局限性</li></ul><p>同一用户在不同设备使用会被认为不同的用户，对后续的分析统计有影响。 </p><p>不同用户在相同设备使用会被认为是一个用户，也对后续的分析统计有影响。 </p><p>但如果用户跨设备使用或者多用户共用设备不是产品的常见场景的话，可以忽略上述问题</p><h2 id="方案2"><a href="#方案2" class="headerlink" title="方案2"></a>方案2</h2><p>关联设备 ID和登录 ID（一对一）</p><ul><li>适用场景</li></ul><p>成功关联设备 ID 和登录 ID 之后，用户在该设备 ID 上或该登录 ID 下的行为就会贯通，被认为 是一个 全局 ID 发生的。在进行事件、漏斗、留存等用户相关分析时也会算作一个用户</p><p>关联设备 ID 和登录 ID 的方法虽然实现了更准确的用户追踪，但是也会增加复杂度。 </p><p>所以一般来说，我们建议只有当同时满足以下条件时，才考虑进行 ID 关联</p><pre><code>需要贯通一个用户在一个设备上注册前后的行为。需要贯通一个注册用户在不同设备上登录之后的行为。</code></pre><ul><li>局限性</li></ul><p>一个设备 ID 只能和一个登录 ID 关联，而事实上一台设备可能有多个用户使用。 </p><p>一个登录 ID 只能和一个设备 ID 关联，而事实上一个用户可能用一个登录 ID在多台设备上登陆。</p><h2 id="方案3"><a href="#方案3" class="headerlink" title="方案3"></a>方案3</h2><p>关联设备 ID 和登录 ID（多对一）</p><ul><li>适用场景</li></ul><p>一个用户在多个设备上进行登录是一种比较常见的场景，比如 Web 端和 App 端可能都需要进行登 录。支持一个登录 ID 下关联多设备 ID 之后，用户在多设备下的行为就会贯通，被认为是一个 ID 发 生的。</p><ul><li>局限性</li></ul><p>一个设备 ID 只能和一个登录 ID 关联，而事实上一台设备可能有多个用户使用。 </p><p>一个设备 ID 一旦跟某个登录 ID 关联或者一个登录 ID 和一个设备 ID 关联，就不能解除（自动 解除）。 </p><p>而事实上，设备 ID 和登录 ID 的动态关联才应该是更合理的</p><h2 id="方案4"><a href="#方案4" class="headerlink" title="方案4"></a>方案4</h2><p>关联设备 ID 和登录 ID（动态修正）</p><p>基本原则，与方案 3 相同 </p><p>修正之处：一个设备 ID 被绑定到某个登陆 ID（A）之后，如果该设备在后续一段时间（比如一个月 内）被一个新的登陆 ID（B）更频繁使用，则该设备 ID 会被调整至绑定登陆 ID（B）</p><p>（titan项目所使用的就是方案4）</p><h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><p>在方案4的基础上，进行具体算法实现。</p><p>根据每个用户每个设备的登录会话次数进行累加分数，</p><p>如：</p><p>用户1登录了a设备，进行了2次会话，那么就 200分，次日没有登陆就减去一定分数，如果次日登录，则在原来基础上进行累加分数（这里加入次日只有一次会话），则是300分</p><p>如果有过登录用户，但是退出了登录，那么就减去一定分数并把分数高的用户判断为此设备用户。</p><p>如果是新设备，还没有登陆过(没有历史数据)，则不计算分数</p><h2 id="逻辑实现："><a href="#逻辑实现：" class="headerlink" title="逻辑实现："></a>逻辑实现：</h2><p>T-1  昨天的数据是 yestoday 表</p><p>T     表 今天的数据是 today 表</p><ol><li><p>yestoday 表已经算出了账号和设备对应的分数和最早时间戳</p></li><li><p>两张表进行full join.</p></li></ol><blockquote><p>today表有设备id，昨天没有，就是新增设备。t.deviceid</p><p>today表有账号id，昨天没有，就是新增账号。t.accountid</p><p>today表有最早登陆时间，昨天没有，就是新增的账号，获取最小的时间</p><p>today表没有账号，昨天也没有账号，就是null。</p><p>today表没有账号，昨天有账号，就是昨天的分数*0.9。</p><p>today表有账号，昨天有账号，今天分数+昨天的分数（防止昨天没有分数，null+任何数都是0，把null变成0）。</p></blockquote><pre><code class="scala">    // 计算T日的 设备-&gt;账号  绑定得分    val loginCnts = spark.sql(      &quot;&quot;&quot;        |        |select        |deviceid,        |account,        |-- count(distinct sessionid) as login_cnt,        |min(ts) as first_login_ts,        |count(distinct sessionid)*100 as bind_score        |from logdf        |group by deviceid,account        |        |&quot;&quot;&quot;.stripMargin)    loginCnts.createTempView(&quot;today&quot;)</code></pre><pre><code class="scala">    // 全外关联两个绑定得分表    // 并将结果写入hive表的当天分区（T-1日分区就无用了）    val combined = spark.sql(      &quot;&quot;&quot;        |        |insert into table dwd17.id_account_bind partition(dt=&#39;2020-10-07&#39;)        |        |select        |if(today.deviceid is null,yestoday.deviceid,today.deviceid) as deviceid,        |if(today.account is null,yestoday.account,today.account) as account,        |if(yestoday.first_login_ts is not null,yestoday.first_login_ts,today.first_login_ts) as first_login_ts,        |-- if(today.account is null,yestoday.login_cnt,today.login_cnt+yestoday.login_cnt) as login_cnt,        |if(today.account is null,yestoday.bind_score*0.9,today.bind_score+if(yestoday.bind_score is null,0,yestoday.bind_score)) as bind_score        |from        |  today        |full join        |  yestoday        |on today.deviceid=yestoday.deviceid and today.account=yestoday.account        |        |&quot;&quot;&quot;.stripMargin)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ID_Mapping </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink学习笔记</title>
      <link href="/flink/flink/"/>
      <url>/flink/flink/</url>
      
        <content type="html"><![CDATA[<p>尚学堂大数据技术之Flink教案</p><h2 id="Flink是什么？"><a href="#Flink是什么？" class="headerlink" title="Flink是什么？"></a>Flink是什么？</h2><p>Apache Flink 是一个框架和<strong>分布式处理引擎</strong>，用于在<strong>无边界和有边界数据流上</strong>进行<strong>有状态</strong>的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。</p><h3 id="有界流和无界流"><a href="#有界流和无界流" class="headerlink" title="有界流和无界流"></a>有界流和无界流</h3><p><strong>无界流</strong>：有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流 的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理， 因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事 件，例如事件发生的顺序，以便能够推断结果的完整性。</p><p><strong>有界流</strong>：有定义流的开始，也有定义流的结束。有界流可以在<strong>摄取所有数据后</strong>再进行计算。</p><p>有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理</p><h2 id="Flink的优势"><a href="#Flink的优势" class="headerlink" title="Flink的优势"></a>Flink的优势</h2><h3 id="1-高吞吐，低延迟，高性能"><a href="#1-高吞吐，低延迟，高性能" class="headerlink" title="1.高吞吐，低延迟，高性能"></a>1.高吞吐，低延迟，高性能</h3><p>Flink 是目前开源社区中唯一一套集高吞吐、低延迟、高性能三者于一身的分布式 流式数据处理框架。像 Apache Spark 也只能兼顾高吞吐和高性能特性，主要因为在 Spark Streaming 流式计算中无法做到低延迟保障；</p><h3 id="2-事件时间"><a href="#2-事件时间" class="headerlink" title="2.事件时间"></a>2.事件时间</h3><p>流式处理，窗口计算很重要，但是大多数框架窗口计算采用的是系统时间，也就是事件传输到框架计算时系统当前时间。<strong>Flink支持基于事件时间（Event Time）语义进行窗口计算，也就是使用事件产生的时间</strong>，流系统也能够计算出精 确的结果，保持了事件原本产生时的时序性，尽可能避免网络传输或硬件系统的影响。</p><h3 id="3-有状态的计算"><a href="#3-有状态的计算" class="headerlink" title="3.有状态的计算"></a>3.有状态的计算</h3><p>数据产生的过程中进行计算并直接产生统计结果，</p><p>所谓<strong>状态</strong>就是计算过程中产生的中间计算结果，每次计算新的数据进入到流式系统中 都是基于中间状态结果的基础上进行运算，最终产生正确的统计结果。</p><p>基于有状态计算的方 式最大的优势是不需要将原始数据重新从外部存储中拿出来，从而进行全量计算，因为这种 计算方式的代价可能是非常高的。从另一个角度讲，用户无须通过调度和协调各种批量计算 工具，从数据仓库中获取数据统计结果，然后再落地存储，这些操作全部都可以基于流式计 算完成，可以极大地减轻系统对其他框架的依赖，减少数据计算过程中的时间损耗以及硬件存储。</p><h3 id="4-高度灵活的窗口计算"><a href="#4-高度灵活的窗口计算" class="headerlink" title="4.高度灵活的窗口计算"></a>4.高度灵活的窗口计算</h3><p>Flink将窗口函数划分TIME、Count、Session，以及Data-driven等类型的窗口操作，窗口可以用灵活的触发条件定制化来达到对复杂的流传输模式的支持，可以定制不同的窗口出发机制来满足不同的需求。</p><h3 id="5-轻量级分布式快照（CheckPoint）的容错"><a href="#5-轻量级分布式快照（CheckPoint）的容错" class="headerlink" title="5.轻量级分布式快照（CheckPoint）的容错"></a>5.轻量级分布式快照（CheckPoint）的容错</h3><p>Flink 能够分布式运行在上千个节点上，将一个大型计算任务的流程拆解成小的计 算过程，然后将 tesk 分布到并行节点上进行处理。在任务执行过程中，能够自动发现 事件处理过程中的错误而导致数据不一致的问题（节点宕机、网路传输问题，或 是由于用户因为升级或修复问题而导致计算服务重启等），在这些情况下，通过基于分布 式快照技术的 Checkpoints，将执行过程中的状态信息进行持久化存储，一旦任务出现 异常停止，Flink 就能够从 Checkpoints 中进行任务的自动恢复，以确保数据在处理过 程中的一致性（Exactly-Once）。</p><h3 id="6-jvm独立的内存管理"><a href="#6-jvm独立的内存管理" class="headerlink" title="6.jvm独立的内存管理"></a>6.jvm独立的内存管理</h3><p>针对内存管理，Flink 实现了自身管 理内存的机制，尽可能减少 JVM GC 对系统的影响。另外，Flink 通过序列化&#x2F;反序列化 方法将所有的数据对象转换成二进制在内存中存储，降低数据存储的大小的同时，能够 更加有效地对内存空间进行利用降低 GC 带来的性能下降或任务异常的风险，因此 Flink 较其他分布式处理的框架会显得更加稳定，不会因为 JVM GC 等问题而影响整个 应用的运行。</p><h3 id="7-save-points（保存点）"><a href="#7-save-points（保存点）" class="headerlink" title="7.save points（保存点）"></a>7.save points（保存点）</h3><p>对于 7<em>24 小时运行的流式应用，数据源源不断地接入，在一段时间内应用的终止 有可能导致数据的丢失或者计算结果的不准确，例如进行集群版本的升级、停机运维操 作等操作。*<em>Flink 通过 Save Points 技术将任务执行的快照保存在存储 介质上，当任务重启的时候可以直接从事先保存的 Save Points 恢复原有的计算状态， 使得任务继续按照停机之前的状态运行，Save Points 技术可以让用户更好地管理和运 维实时流式应用。</em></em></p><h2 id="Spark-和-Flink对比"><a href="#Spark-和-Flink对比" class="headerlink" title="Spark 和 Flink对比"></a>Spark 和 Flink对比</h2><p><img src="/flink/flink/image-20200806121002796.png" alt="image-20200806121002796"></p><h2 id="Flink集群"><a href="#Flink集群" class="headerlink" title="Flink集群"></a>Flink集群</h2><p>Flink 的安装和部署主要分为本地（单机）模式和集群模式</p><p>集群模式包含： </p><ul><li>Standalone。 </li><li>Flink on Yarn</li><li>Mesos</li><li>Docker</li><li>Kubernetes</li><li>AWS</li><li>Goole Compute Engine。</li></ul><p>目前在企业中使用最多的是 Flink on Yarn 模式。</p><h3 id="集群的组件（架构）"><a href="#集群的组件（架构）" class="headerlink" title="集群的组件（架构）"></a>集群的组件（架构）</h3><p>Flink 整个系统主要由两个组件组成，分别为 <strong>JobManager</strong> 和 <strong>TaskManager</strong>，Flink 架构 也遵循 Master-Slave 架构设计原则，JobManager 为 Master 节点，TaskManager 为 Worker （Slave）节点。所有组件之间的通信都是借助于 Akka Framework，包括任务的状态以及 Checkpoint 触发等信息。</p><p><img src="/flink/flink/image-20210702075748535.png"></p><h4 id="Client客户端"><a href="#Client客户端" class="headerlink" title="Client客户端"></a>Client客户端</h4><p>客户端负责将任务提交到集群，与 JobManager 构建 Akka 连接，然后将任务提交到 JobManager，通过和 JobManager 之间进行交互获取任务执行状态。客户端提交任务可以采 用 CLI 方式或者通过使用 Flink WebUI 提交，也可以在应用程序中指定 JobManager 的 RPC 网络端口构建 ExecutionEnvironment 提交 Flink 应用。</p><h4 id="JobManager"><a href="#JobManager" class="headerlink" title="JobManager"></a>JobManager</h4><p>JobManager 负责整个 Flink 集群任务的调度以及资源的管理，从客户端中获取提交的 应用，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的应用分配相应的 TaskSlots 资源并命令 TaskManger 启动从客户端中获取的应用。JobManager 相当于整个集 群的 Master 节点，且整个集群中有且仅有一个活跃的 JobManager。</p><p>JobManager 和 TaskManager 之间通过 Actor System 进行通信，获取任务执 行的情况并通过 Actor System 将应用的任务执行情况发送给客户端。同时在任务执行过程中，Flink JobManager 会触发 <strong>Checkpoints</strong> 操作，每个 TaskManager 节点收到 Checkpoint 触发指令后，完成 Checkpoint 操作，所有的 Checkpoint 协调过程都是在 Flink JobManager 中完成。当任务完成后，Flink 会将任务执行的信息反馈给客户端，并且释放掉 TaskManager 中的资源以供下一次提交任务使用。</p><h4 id="TaskManager"><a href="#TaskManager" class="headerlink" title="TaskManager"></a>TaskManager</h4><p>TaskManager 相当于整个集群的 Slave 节点，负责具体的任务执行和对应任务在每个节 点上的资源申请与管理。客户端通过将编写好的 Flink 应用编译打包，提交到 JobManager， 然后 JobManager 会根据已经注册在 JobManager 中 TaskManager 的资源情况，将任务分配给 有资源的 TaskManager 节点，然后启动并运行任务。</p><p>TaskManager 从 JobManager 接收需要部署的任务，然后使用 Slot 资源启动 Task，建立数据接入的网络连接，接收数据并开始数 据处理。同时 TaskManager 之间的数据交互都是通过数据流的方式进行的。</p><p><strong>Flink 的任务运行其实是采用多线程的方式，这和 MapReduce 多 JVM 进程的 方式有很大的区别 Fink 能够极大提高 CPU 使用效率，在多个任务和 Task 之间通过 TaskSlot 方式共享系统资源，每个 TaskManager 中通过管理多个 TaskSlot 资源池进行对资源进行有 效管理。</strong></p><h4 id="TaskManager和Slots的关系"><a href="#TaskManager和Slots的关系" class="headerlink" title="TaskManager和Slots的关系"></a>TaskManager和Slots的关系</h4><p>Flink 中每一个 worker(TaskManager)都是一个 JVM 进程，它可能会在独立的线 程上执行一个或多个 subtask。为了控制一个 worker 能接收多少个 task，worker 通 过 task slot 来进行控制（一个 worker 至少有一个 task slot）。</p><p>每个 task slot 表示 TaskManager 拥有资源的一个固定大小的子集。假如一个 TaskManager 有三个 slot，那么它会将其管理的内存分成三份给各个 slot。资源 slot 化意味着一个 subtask 将不需要跟来自其他 job 的 subtask 竞争被管理的内存，取而 代之的是它将拥有一定数量的内存储备。需要注意的是，这里不会涉及到 CPU 的隔 离，slot 目前仅仅用来隔离 task 的受管理的内存。</p><p>通过调整 task slot 的数量，允许用户定义 subtask 之间如何互相隔离。如果一个 TaskManager 一个 slot，那将意味着每个 task group 运行在独立的 JVM 中（该 JVM 可能是通过一个特定的容器启动的），而一个 TaskManager 多个 slot 意味着更多的 subtask 可以共享同一个 JVM。而在同一个 JVM 进程中的 task 将共享 TCP 连接（基 于多路复用）和心跳消息。它们也可能共享数据集和数据结构，因此这减少了每个 task 的负载。</p><p><img src="/flink/flink/image-20200806171834272.png" alt="image-20200806171834272"></p><p><img src="/flink/flink/image-20200806171844824.png" alt="image-20200806171844824"></p><h4 id="Task和subtask"><a href="#Task和subtask" class="headerlink" title="Task和subtask"></a>Task和subtask</h4><ul><li><p>Task（任务）：task是一个阶段多个功能相同subtask的集合，类似于Spark中的TaskSet。</p></li><li><p>subTask（子任务）：subTask是Flink中任务最小执行单元，是一个java类的实例，这个Java类中有属性和方法，完成具体的计算逻辑。</p></li><li><p>Operator Chains（算子链）：没有shuffle的多个算子合并在一个subTask中，就形成了Operator Chains，类似于Spark中的Pipeline。</p></li><li><p>Slot（插槽）：Flink中计算资源进行隔离的单元，一个Slot中可以运行多个subTask，但是这些subTask必须是来自同一个application的不同阶段的subTask。</p></li></ul><h4 id="如何划分task"><a href="#如何划分task" class="headerlink" title="如何划分task"></a>如何划分task</h4><p>Task的并行度发生变化</p><p>调用Keyby这样的产生shuffle算子</p><p>调用startNewChain</p><p>调用disableChaining</p><p>处理分区器 Rebalance shuffle Broadcast Rescale</p><p><img src="/flink/flink/image-20220531125834934.png" alt="image-20220531125834934"></p><p><img src="/flink/flink/image-20220531125902473.png" alt="image-20220531125902473"></p><p>并行数据流，一共有三个task，五个subTask。</p><h3 id="Standalone安装"><a href="#Standalone安装" class="headerlink" title="Standalone安装"></a>Standalone安装</h3><ol><li>下载好Flink包</li><li>三台机器进行解压压缩包</li><li>修改配置文件</li></ol><ul><li>conf目录下，编辑 flink-conf.yaml 配置文件：</li></ul><p><img src="/flink/flink/image-20210702081133879.png"></p><p>其中：taskmanager.numberOfTaskSlot 参数默认值为 1，修改成 3。表示数每一个 TaskManager 上有 3 个 Slot。</p><p><strong>配置文件参数说明</strong> </p><p>下面针对 flink-conf.yaml 文件中的几个重要参数进行分析：</p><ul><li><p>jobmanager.heap.size：JobManager 节点可用的内存大小。 </p></li><li><p>taskmanager.heap.size：TaskManager 节点可用的内存大小。</p></li><li><p>taskmanager.numberOfTaskSlots：每台机器可用的 Slot 数量。</p></li><li><p>parallelism.default：默认情况下 Flink 任务的并行度。 </p><p>上面参数中所说的 Slot 和 parallelism 的区别： </p><ul><li>Slot 是静态的概念，是指 TaskManager 具有的并发执行能力。  </li><li>parallelism 是动态的概念，是指程序运行时实际使用的并发能力。 </li><li>设置合适的 parallelism 能提高运算效率。</li></ul></li><li><p>编辑 conf&#x2F;slaves 配置文件</p></li></ul><p><img src="/flink/flink/image-20210702081505009.png"></p><ul><li>分发给另外两台服务器</li></ul><pre><code>scp -r flink-1.9.1 root@hadoop2</code></pre><ol start="4"><li>启动Flink服务</li></ol><pre><code>bin/start-cluseter.sh</code></pre><ol start="5"><li>访问 WebUI</li></ol><p><img src="/flink/flink/image-20210702081644836.png" alt="访问webUI"></p><ol start="6"><li>测试，提交写好的jar包</li></ol><p><img src="/flink/flink/Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210702081819409.png" alt="运行命令"></p><ol start="7"><li>查看监控页面</li></ol><p><img src="/flink/flink/image-20210702081902697.png" alt="监控页面图"></p><h4 id="Flink-Standalone任务提交流程"><a href="#Flink-Standalone任务提交流程" class="headerlink" title="Flink Standalone任务提交流程"></a>Flink Standalone任务提交流程</h4><p><img src="/flink/flink/image-20200806151415449.png" alt="image-20200806151415449"></p><h3 id="Flink-On-Yarn"><a href="#Flink-On-Yarn" class="headerlink" title="Flink On Yarn"></a>Flink On Yarn</h3><p>Flink on Yarn 模式的原理是依靠 YARN 来调度 Flink 任务，目前在企业中使用较多。 这种模式的好处是可以充分利用集群资源，提高集群机器的利用率，并且<strong>只需要 1 套 Hadoop 集群，就可以执行 MapReduce 和 Spark 任务，还可以执行 Flink 任务等，操作非常方便</strong>，不需要维护多套集群，运维方面也很轻松。Flink on Yarn 模式需要依赖 Hadoop 集群，并且 Hadoop 的版本<strong>需要是 2.2</strong> 及以上。</p><h4 id="Flink-On-Yarn-的任务提交流程："><a href="#Flink-On-Yarn-的任务提交流程：" class="headerlink" title="Flink On Yarn 的任务提交流程："></a>Flink On Yarn 的任务提交流程：</h4><p><img src="/flink/flink/image-20210702082625061.png" alt="Flink on yarn实现原理"></p><ol><li>启动一个新的 Flink YARN Client 会话时，客户端首先会检查所请求的资源（容器和内存）是否可用。之后，它会上传 Flink 配置和 JAR 文件到 HDFS。</li><li>客 户 端 的 下 一 步 是 请 求 一 个 YARN 容 器 启 动 ApplicationMaster 。 JobManager 和 ApplicationMaster(AM)运行在同一个容器中，一旦它们成功地启动了，AM 就能够知道 JobManager 的地址，它会为 TaskManager 生成一个新的 Flink 配置文件（这样它才能连上 JobManager），该文件也同样会被上传到 HDFS。另外，AM 容器还提供了 Flink 的 Web 界面服务。Flink 用来提供服务的端口是由用户和应用程序 ID 作为偏移配置的，这使得用户能够并行执行多个 YARN 会话。 </li><li>之后，AM 开始为 Flink 的 TaskManager 分配容器（Container），从 HDFS 下载 JAR 文件 和修改过的配置文件。一旦这些步骤完成了，Flink 就安装完成并准备接受任务了。</li></ol><p><strong>Flink on Yarn 模式在使用的时候又可以分为两种</strong>： </p><h4 id="第-1-种模式-Session-Cluster-："><a href="#第-1-种模式-Session-Cluster-：" class="headerlink" title="第 1 种模式(Session-Cluster)："></a>第 1 种模式(Session-Cluster)：</h4><p>是在 YARN 中提前初始化一个 Flink 集群(称为 Flink yarn-session)，开辟指定的资源，以后的 Flink 任务都提交到这里。这个 Flink 集群 会常驻在 YARN 集群中，除非手工停止。这种方式创建的 Flink 集群会独占资源，不管 有没有 Flink 任务在执行，YARN 上面的其他任务都无法使用这些资源。</p><p><img src="/flink/flink/image-20210702083543047.png"></p><h4 id="第-2-种模式-Per-Job-Cluster-："><a href="#第-2-种模式-Per-Job-Cluster-：" class="headerlink" title="第 2 种模式(Per-Job-Cluster)："></a>第 2 种模式(Per-Job-Cluster)：</h4><p>每次提交 Flink 任务都会创建一个新的 Flink 集群， 每个 Flink 任务之间相互独立、互不影响，管理方便。任务执行完成之后创建的 Flink 集群也会消失，不会额外占用资源，按需使用，这使资源利用率达到最大，在工作中推荐使用这种模式。</p><p><img src="/flink/flink/image-20210702083611137.png"></p><h4 id="Flink-On-Yarn-安装"><a href="#Flink-On-Yarn-安装" class="headerlink" title="Flink On Yarn 安装"></a>Flink On Yarn 安装</h4><ol><li>配置好hadoop环境变量</li><li>下载 Flink 提交到 Hadoop 的连接器（jar 包），并把 jar 拷贝到 Flink 的 lib 目录下</li></ol><p><img src="/flink/flink/image-20210702084423678.png" alt="连接器jar包下载"></p><h5 id="Session-Cluster-模式（yarn-session）"><a href="#Session-Cluster-模式（yarn-session）" class="headerlink" title="Session-Cluster 模式（yarn-session）"></a>Session-Cluster 模式（yarn-session）</h5><ol><li>先启动 Hadoop 集群，然后通过命令启动一个 Flink 的 yarn-session 集群：</li></ol><pre><code class="shell">bin/yarn-session.sh -n 3 -s 3 -nm bjsxt -d</code></pre><p>其中 yarn-session.sh 后面支持多个参数。下面针对一些常见的参数进行讲解：</p><ul><li>-n,–container  表示分配容器的数量（也就是 TaskManager 的数量）。</li><li>-D  动态属性。 </li><li>-d,–detached 在后台独立运行。</li><li>-jm,–jobManagerMemory ：设置 JobManager 的内存，单位是 MB。 </li><li>-nm,–name：在 YARN 上为一个自定义的应用设置一个名字。 </li><li>-q,–query：显示 YARN 中可用的资源（内存、cpu 核数）。 </li><li>-qu,–queue ：指定 YARN 队列。 </li><li>-s,–slots ：每个 TaskManager 使用的 Slot 数量。</li><li>-tm,–taskManagerMemory ：每个 TaskManager 的内存，单位是 MB。 </li><li>-z,–zookeeperNamespace ：针对 HA 模式在 ZooKeeper 上创建 NameSpace。</li><li>-id,–applicationId ：指定 YARN 集群上的任务 ID，附着到一个后台独 立运行的 yarn session 中。</li></ul><ol start="2"><li>提交 Job : 由于有了之前的配置，所以自动会提交到 Yarn 中。</li></ol><pre><code class="shell">bin/flink run -c com.bjsxt.flink.StreamWordCount /home/Flink-Demo-1.0-SNAPSHOT.jar</code></pre><p><img src="/flink/flink/image-20210702085054905.png" alt="运行监控页面"></p><h5 id="Pre-Job-Cluster-模式（yarn-cluster）"><a href="#Pre-Job-Cluster-模式（yarn-cluster）" class="headerlink" title="Pre-Job-Cluster 模式（yarn-cluster）"></a>Pre-Job-Cluster 模式（yarn-cluster）</h5><ol><li>提交job(如果有启动的yarn-session，先停掉)</li></ol><pre><code class="shell">bin/flink run -m yarn-cluster -yn 3 -ys 3 -ynm bjsxt02 -ccom.bjsxt.flink.StreamWordCount /home/Flink-Demo-1.0-SNAPSHOT.jar</code></pre><p>任务提交参数讲解：相对于 Yarn-Session 参数而言，只是前面加了 y。</p><ul><li>-yn,–container  表示分配容器的数量，也就是 TaskManager 的数量。</li><li>-d,–detached：设置在后台运行。 </li><li>-yjm,–jobManagerMemory:设置 JobManager 的内存，单位是 MB。 </li><li>-ytm，–taskManagerMemory:设置每个 TaskManager 的内存，单位是 MB。</li><li>-ynm,–name:给当前 Flink application 在 Yarn 上指定名称。 </li><li>-yq,–query：显示 yarn 中可用的资源（内存、cpu 核数）</li><li>-yqu,–queue :指定 yarn 资源队列 </li><li>-ys,–slots :每个 TaskManager 使用的 Slot 数量。 </li><li>-yz,–zookeeperNamespace:针对 HA 模式在 Zookeeper 上创建 NameSpace</li><li>-yid,–applicationID : 指定 Yarn 集群上的任务 ID,附着到一个后台独 立运行的 Yarn Session 中。</li></ul><h4 id="Flink-Kubernetes"><a href="#Flink-Kubernetes" class="headerlink" title="Flink Kubernetes"></a>Flink Kubernetes</h4><p>容器化部署时目前业界很流行的一项技术，基于 Docker 镜像运行能够让用户更 加 方 便地 对应 用进 行管 理 和运 维。 容器 管理 工 具中 最为 流行 的就 是 Kubernetes （k8s），而 Flink 也在最近的版本中支持了 k8s 部署模式。</p><p>1）搭建 Kubernetes 集群（略） </p><p>2）配置各组件的 yaml 文件</p><p>在 k8s 上构建 Flink Session Cluster，需要将 Flink 集群的组件对应的 docker 镜像 分别在 k8s 上启动，包括 JobManager、TaskManager、JobManagerService 三个镜像 服务。每个镜像服务都可以从中央镜像仓库中获取。 </p><p>3）启动 Flink Session Cluster</p><pre><code>// 启动 jobmanager-service 服务kubectl create -f jobmanager-service.yaml// 启动 jobmanager-deployment 服务kubectl create -f jobmanager-deployment.yaml// 启动 taskmanager-deployment 服务kubectl create -f taskmanager-deployment.yaml</code></pre><p>4）访问 Flink UI 页面 </p><p>集群启动后，就可以通过 JobManagerServicers 中配置的 WebUI 端口，用浏览器 输入以下 url 来访问 Flink UI 页面了： http:&#x2F;&#x2F;{JobManagerHost:Port}&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;default&#x2F;services&#x2F;flink-jobmanage r:ui&#x2F;proxy</p><h3 id="Flink的HA"><a href="#Flink的HA" class="headerlink" title="Flink的HA"></a>Flink的HA</h3><p>Standalone 模式下，JobManager 的高可用性的基本思想是，任何时候都有<strong>一个 Alive JobManager</strong> 和<strong>多个 Standby JobManager</strong>。</p><p>Standby JobManager 可以在 Alive JobManager 挂掉的情况下接管集群成为 Alive JobManager，这样避免了单点故障，一旦某一个 Standby JobManager 接管集群，程序就可以继续运行。Standby JobManagers 和 Alive JobManager 实例之间没有明确区别，每个 JobManager 都可以成为 Alive 或 Standby。</p><h4 id="Flink-Standalone集群的HA"><a href="#Flink-Standalone集群的HA" class="headerlink" title="Flink Standalone集群的HA"></a>Flink Standalone集群的HA</h4><p>需要使用ZK和HDFS，因此要有一个ZooKeeper集群和Hadoop集群</p><p>首先启动 Zookeeper 集群和 HDFS 集群。</p><h5 id="1-修改配置文件-conf-x2F-masters"><a href="#1-修改配置文件-conf-x2F-masters" class="headerlink" title="1.修改配置文件 conf&#x2F;masters"></a>1.修改配置文件 conf&#x2F;masters</h5><p><img src="/flink/flink/image-20220517180439586.png" alt="image-20220517180439586"></p><h5 id="2-修改配置文件-conf-x2F-flink-conf-yam"><a href="#2-修改配置文件-conf-x2F-flink-conf-yam" class="headerlink" title="2.修改配置文件 conf&#x2F;flink-conf.yam"></a>2.修改配置文件 conf&#x2F;flink-conf.yam</h5><pre><code class="shell">#要启用高可用，设置修改为zookeeperhigh-availability: zookeeper#Zookeeper的主机名和端口信息，多个参数之间用逗号隔开high-availability.zookeeper.quorum:hadoop103:2181,hadoop101:2181,hadoop102:2181# 建议指定HDFS的全路径。如果某个Flink节点没有配置HDFS的话，不指定HDFS的全路径则无法识別到，storageDir存储了恢复一个JobManager所需的所有元数据。high-availability.storageDir: hdfs://hadoop101:9000/flink/h</code></pre><h5 id="3-把修改的配置文件拷贝其他服务器中"><a href="#3-把修改的配置文件拷贝其他服务器中" class="headerlink" title="3.把修改的配置文件拷贝其他服务器中"></a>3.把修改的配置文件拷贝其他服务器中</h5><pre><code class="shell">[root@hadoop101 conf]# scp masters flink-conf.yaml root@hadoop102:`pwd` [root@hadoop101 conf]# scp masters flink-conf.yaml root@hadoop103:`pwd`</code></pre><h5 id="4-启动集群"><a href="#4-启动集群" class="headerlink" title="4.启动集群"></a>4.启动集群</h5><p><img src="/flink/flink/image-20220517180639754.png" alt="image-20220517180639754"></p><h4 id="Flink-On-Yarn-HA安装和配置"><a href="#Flink-On-Yarn-HA安装和配置" class="headerlink" title="Flink On Yarn HA安装和配置"></a>Flink On Yarn HA安装和配置</h4><p>正常基于 Yarn 提交 Flink 程序，无论是使用 yarn-session 模式还是 yarn-cluster 模 式 ， 基 于 yarn 运 行 后 的 application 只 要 kill 掉 对 应 的 Flink 集 群 进 程 “YarnSessionClusterEntrypoint”后，基于 Yarn 的 Flink 任务就失败了，不会自动进行 重试，所以基于 Yarn 运行 Flink 任务，也有必要搭建 HA，这里同样还是需要借助 zookeeper 来完成，步骤如下：</p><h5 id="1-修改所有-Hadoop-节点的-yarn-site-xml"><a href="#1-修改所有-Hadoop-节点的-yarn-site-xml" class="headerlink" title="1.修改所有 Hadoop 节点的 yarn-site.xml"></a>1.修改所有 Hadoop 节点的 yarn-site.xml</h5><p>将所有 Hadoop 节点的 yarn-site.xml 中的提交应用程序最大尝试次数调大</p><pre><code class="properties">#在每台hadoop节点yarn-site.xml中设置提交应用程序的最大尝试次数，建议不低于4，这里重试指的是ApplicationMaster&lt;property&gt;&lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt;&lt;value&gt;4&lt;/value&gt;&lt;/property&gt;</code></pre><h5 id="2-启动-Hadoop-集群"><a href="#2-启动-Hadoop-集群" class="headerlink" title="2.启动 Hadoop 集群"></a>2.启动 Hadoop 集群</h5><p>启动 zookeeper，启动 Hadoop 集群。</p><h5 id="3-修改-Flink-对应-flink-conf-yaml-配置"><a href="#3-修改-Flink-对应-flink-conf-yaml-配置" class="headerlink" title="3.修改 Flink 对应 flink-conf.yaml 配置"></a>3.修改 Flink 对应 flink-conf.yaml 配置</h5><p>配置对应的 conf 下的 flink-conf.yaml，配置内容如下：</p><pre><code class="properties">#配置依赖zookeeper模式进行HA搭建high-availability: zookeeper#配置JobManager原数据存储路径high-availability.storageDir: hdfs://hadoop101:9000/flink/yarnha/#配置zookeeper集群节点high-availability.zookeeper.quorum:hadoop101:2181,hadoop102:2181,hadoop103:2181#yarn停止一个application重试的次数yarn.application-attempts: 10</code></pre><h5 id="4-启动-yarn-session-sh-测试-HA：-yarn-session-sh-n-2-，也可以直接提交-Job-启动之后，可以登录-yarn-中对应的-flink-webui："><a href="#4-启动-yarn-session-sh-测试-HA：-yarn-session-sh-n-2-，也可以直接提交-Job-启动之后，可以登录-yarn-中对应的-flink-webui：" class="headerlink" title="4.启动 yarn-session.sh 测试 HA： yarn-session.sh -n 2 ，也可以直接提交 Job 启动之后，可以登录 yarn 中对应的 flink webui："></a>4.启动 yarn-session.sh 测试 HA： yarn-session.sh -n 2 ，也可以直接提交 Job 启动之后，可以登录 yarn 中对应的 flink webui：</h5><p><img src="/flink/flink/image-20220517181237044.png" alt="image-20220517181237044"></p><p><img src="/flink/flink/image-20220517181531687.png" alt="image-20220517181531687"></p><p>点击job ID，发现对应的重试信息：</p><p><img src="/flink/flink/image-20220517181619547.png" alt="image-20220517181619547"></p><p><img src="/flink/flink/image-20220517181637672.png" alt="image-20220517181637672"></p><h3 id="Flink并行度和Slot"><a href="#Flink并行度和Slot" class="headerlink" title="Flink并行度和Slot"></a>Flink并行度和Slot</h3><p>Flink中每一个worker(TaskManager)都是一个JVM进程，它可能会在独立的线程（Solt） 上执行一个或多个 subtask。Flink 的每个 TaskManager 为集群提供 Solt。Solt 的数量通常 与每个 TaskManager 节点的可用 CPU 内核数成比例，<strong>一般情况下 Slot 的数量就是每个节点 的 CPU 的核数。</strong> </p><p><strong>Slot 的 数 量 由 集 群 中 flink-conf.yaml 配 置 文 件 中 设 置 taskmanager.numberOfTaskSlots 的值为 3，这个值的大小建议和节点 CPU 的数量保持一致。</strong></p><p><img src="/flink/flink/image-20200911164313854.png" alt="image-20200911164313854"></p><h4 id="任务的并行度设置"><a href="#任务的并行度设置" class="headerlink" title="任务的并行度设置"></a>任务的并行度设置</h4><h4 id="1-并行度设置之-Operator-Level（算子层次）"><a href="#1-并行度设置之-Operator-Level（算子层次）" class="headerlink" title="1) 并行度设置之 Operator Level（算子层次）"></a>1) 并行度设置之 Operator Level（算子层次）</h4><p>Operator、Source 和 Sink 目的地的并行度可以通过调用 setParallelism()方法来指定</p><p><img src="/flink/flink/image-20200911164546017-16348157954237.png" alt="image-20200911164546017"></p><h4 id="2-行度设置之-Execution-Environment-Level（执行环境层次）"><a href="#2-行度设置之-Execution-Environment-Level（执行环境层次）" class="headerlink" title="2) 行度设置之 Execution Environment Level（执行环境层次）"></a>2) 行度设置之 Execution Environment Level（执行环境层次）</h4><p>任务的默认并行度可以通过调用 setParallelism()方法指定。为了以并行度 3 来执行 所有的 Operator、Source 和 Sink，可以通过如下方式设置执行环境的并行度</p><p><img src="/flink/flink/image-20200911164657287-16348157954248.png" alt="image-20200911164657287"></p><h4 id="3-并行度设置之-Client-Level-（客户端层次）"><a href="#3-并行度设置之-Client-Level-（客户端层次）" class="headerlink" title="3) 并行度设置之 Client Level （客户端层次）"></a>3) 并行度设置之 Client Level （客户端层次）</h4><p>并行度还可以在客户端提交 Job 到 Flink 时设定。对于 CLI 客户端，可以通过-p 参数指定并行度。</p><p><img src="/flink/flink/image-20200911164731672-16348157954249.png" alt="image-20200911164731672"></p><h4 id="4-并行度设置之-System-Level（系统层次）"><a href="#4-并行度设置之-System-Level（系统层次）" class="headerlink" title="4) 并行度设置之 System Level（系统层次）"></a>4) 并行度设置之 System Level（系统层次）</h4><p>在系统级可以通过设置flink-conf.yaml文件中的parallelism.default属性来指定所 有执行环境的默认并行度。</p><p><img src="/flink/flink/image-20200911164906027-163481579542410.png" alt="image-20200911164906027"></p><h4 id="5-并行度案例分析"><a href="#5-并行度案例分析" class="headerlink" title="5) 并行度案例分析"></a>5) 并行度案例分析</h4><p>Flink 集群中有 3 个 TaskManager 节点，每个 TaskManager 的 Slot 数量为 3</p><p><img src="/flink/flink/image-20200911165041367-163481579542711.png" alt="image-20200911165041367"></p><p><img src="/flink/flink/image-20200911165111817-163481579542712.png" alt="image-20200911165111817"></p><p>并行度优先级顺序：<strong>Operator Level&gt;Execution Environment Level&gt;Client Level&gt;System Level。</strong></p><h3 id="任务调度原理-细节"><a href="#任务调度原理-细节" class="headerlink" title="任务调度原理(细节)"></a>任务调度原理(细节)</h3><p><img src="/flink/flink/image-20200806152207513.png" alt="image-20200806152207513"></p><p><img src="/flink/flink/image-20200911150036473.png" alt="image-20200911150036473"></p><p>客户端不是运行时和程序执行的一部分，但它用于准备并发送 dataflow(JobGraph)给 Master(JobManager)，然后，客户端断开连接或者维持连接以等待接收计算结果。</p><p>当 Flink 集 群 启 动 后 ， 首 先 会 启 动 一 个 JobManger 和一个或多个的TaskManager。</p><p>由 Client 提交任务给 JobManager，JobManager 再调度任务到各个TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。<br>TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。</p><p>Client 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境 连通即可）。提交 Job 后，Client 可以结束进程（Streaming 的任务），也可以不 结束并等待结果返回。</p><p>JobManager 主 要 负 责 调 度 Job 并 协 调 Task 做 checkpoint， 职 责 上 很 像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的 执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。</p><p>TaskManager 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个或多个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自 己的上游建立 Netty 连接，接收数据并处理。</p><h2 id="Flink的常用API"><a href="#Flink的常用API" class="headerlink" title="Flink的常用API"></a>Flink的常用API</h2><p>Flink 根据抽象程度分层，提供了三种不同的 API 和库。每一种 API 在简洁性和表达 力上有着不同的侧重，并且针对不同的应用场景。</p><p><img src="/flink/flink/image-20210929160643776.png" alt="Fink的API"></p><p><strong>- ProcessFunction</strong> </p><p>是 Flink 所提供最底层接口。ProcessFunction 可以处理一或两条 输入数据流中的单个事件或者归入一个特定窗口内的多个事件。它提供了对于时间和状 态的细粒度控制。开发者可以在其中任意地修改状态，也能够注册定时器用以在未来的 某一时刻触发回调函数。因此，你可以利用 ProcessFunction 实现许多有状态的事件 驱动应用所需要的基于单个事件的复杂业务逻辑。</p><p> <strong>- DataStream API</strong> </p><p>为许多通用的流处理操作提供了处理原语。这些操作包括窗口、逐条记录的转换操作，在处理事件时进行外部数据库查询等。DataStream API 支持 Java 和 Scala 语言，预先定义了例如 map()、reduce()、aggregate() 等函数。你可以通过扩 展实现预定义接口或使用 Java、Scala 的 lambda 表达式实现自定义的函数。 </p><p><strong>-  SQL &amp; Table API</strong></p><p>Flink 支持两种关系型的 API，Table API 和 SQL。这两个 API 都 是批处理和流处理统一的 API，这意味着在无边界的实时数据流和有边界的历史记录数据流上，关系型 API 会以相同的语义执行查询，并产生相同的结果。Table API 和 SQL 借助了 Apache Calcite 来进行查询的解析，校验以及优化。它们可以与 DataStream 和 DataSet API 无缝集成，并支持用户自定义的标量函数，聚合函数以及表值函数。</p><p><strong>另外 Flink 具有数个适用于常见数据处理应用场景的扩展库。</strong> </p><ul><li>**复杂事件处理(CEP)**：模式检测是事件流处理中的一个非常常见的用例。Flink 的 CEP 库提供了 API，使用户能够以例如正则表达式或状态机的方式指定事件模式。CEP 库与 Flink 的 DataStream API 集成，以便在 DataStream 上评估模式。CEP 库的应用包括 网络入侵检测，业务流程监控和欺诈检测。 </li><li><strong>DataSet API</strong>：DataSet API 是 Flink 用于批处理应用程序的核心 API。DataSet API 所 提供的基础算子包括 map、reduce、(outer) join、co-group、iterate 等。所有算子 都有相应的算法和数据结构支持，对内存中的序列化数据进行操作。如果数据大小超过 预留内存，则过量数据将存储到磁盘。Flink 的 DataSet API 的数据处理算法借鉴了 传统数据库算法的实现，例如混合散列连接（hybrid hash-join）和外部归并排序 （external merge-sort）。 </li><li><strong>Gelly</strong>: Gelly 是一个可扩展的图形处理和分析库。Gelly 是在 DataSet API 之上实现 的，并与 DataSet API 集成。因此，它能够受益于其可扩展且健壮的操作符。Gelly 提 供了内置算法，如 label propagation、triangle enumeration 和 page rank 算法， 也提供了一个简化自定义图算法实现的 Graph API。</li></ul><h3 id="0-获取执行环境"><a href="#0-获取执行环境" class="headerlink" title="0.获取执行环境"></a>0.获取执行环境</h3><p>创建一个执行环境，表示当前执行程序的上下文。 如果程序是独立调用的，则 此方法返回本地执行环境；如果从命令行客户端调用程序以提交到集群，则此方法 返回此集群的执行环境，也就是说，getExecutionEnvironment 会根据查询运行的方式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。</p><pre><code class="scala">val env = ExecutionEnvironment.getExecutionEnvironmentenv.setParallelism(num int)//这种创建方式会自动检测执行环境，如果是本地就返回local，如果是集群返回remote</code></pre><p>下面两种是底层创建方式，了解就行。</p><pre><code class="scala">val env = StreamExecutionEnvironment.createLocalEnvironment(1)//本地执行环境，可以直接指定并行度</code></pre><pre><code class="scala">val env = ExecutionEnvironment.createRemoteEnvironment(&quot;jobmanage-hostname&quot;,6123,&quot;YOURPATH//wordcount.jar&quot;)//远程的执行环境，jobmanager地址，端口，jar包</code></pre><h3 id="1-DataStream-的编程模型"><a href="#1-DataStream-的编程模型" class="headerlink" title="1.DataStream 的编程模型"></a>1.DataStream 的编程模型</h3><p>DataStream 的编程模型包括四个部分：Environment，DataSource，Transformation，Sink。（上下文环境，数据源，转换类算子，数据输出）</p><p><img src="/flink/flink/image-20210929172737878.png" alt="image-20210929172737878"></p><h3 id="2-Flink-的-DataSource-数据源"><a href="#2-Flink-的-DataSource-数据源" class="headerlink" title="2.Flink 的 DataSource 数据源"></a>2.Flink 的 DataSource 数据源</h3><h4 id="1）-基于文件的Source"><a href="#1）-基于文件的Source" class="headerlink" title="1） 基于文件的Source"></a>1） 基于文件的Source</h4><p>读取本地文件系统的数据，前面的案例已经讲过了。本课程主要讲基于 HDFS 文件系统的 Source。首先需要配置 Hadoop 的依赖</p><pre><code class="xml">&lt;dependency&gt;&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;&lt;version&gt;2.7.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;&lt;version&gt;2.7.2&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>读取 HDFS 上的文件：</p><pre><code class="scala">object FileSource &#123;def main(args: Array[String]): Unit = &#123;//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv = StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._ //读取数据val stream = streamEnv.readTextFile(&quot;hdfs://hadoop101:9000/wc.txt&quot;)//转换计算val result: DataStream[(String, Int)] = stream.flatMap(_.split(&quot;,&quot;)).map((_, 1)).keyBy(0).sum(1)//打印结果到控制台result.print()//启动流式处理，如果没有该行代码上面的程序不会运行streamEnv.execute(&quot;wordcount&quot;)&#125;&#125;</code></pre><h4 id="2）基于集合的Source"><a href="#2）基于集合的Source" class="headerlink" title="2）基于集合的Source"></a>2）基于集合的Source</h4><pre><code class="scala">/*** 通信基站日志数据** @param sid 基站ID* @param callOut 主叫号码* @param callIn 被叫号码* @param callType 通话类型eg:呼叫失败(fail)，占线(busy),拒接（barring），接通(success): * @param callTime 呼叫时间戳，精确到毫秒* @Param duration 通话时长 单位：秒*/case classStationLog(sid:String,callOut:String,callIn:String,callType:String,callTime:Long,duration:Long)object CollectionSource &#123;def main(args: Array[String]): Unit = &#123;//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv = StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._//读取数据var dataStream =streamEnv.fromCollection(Array(    new StationLog(&quot;001&quot;,&quot;186&quot;,&quot;189&quot;,&quot;busy&quot;,1577071519462L,0),     new StationLog(&quot;002&quot;,&quot;186&quot;,&quot;188&quot;,&quot;busy&quot;,1577071520462L,0),     new StationLog(&quot;003&quot;,&quot;183&quot;,&quot;188&quot;,&quot;busy&quot;,1577071521462L,0),     new StationLog(&quot;004&quot;,&quot;186&quot;,&quot;188&quot;,&quot;success&quot;,1577071522462L,32)))dataStream.print()streamEnv.execute()&#125;</code></pre><h4 id="3）-基于Kafka的Source"><a href="#3）-基于Kafka的Source" class="headerlink" title="3） 基于Kafka的Source"></a>3） 基于Kafka的Source</h4><p>首 先 需 要 配 置 Kafka 连 接 器 的 依 赖 ， 另 外 更 多 的 连 接 器 可 以 查 看 官 网 ： <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/zh/dev/connectors/">https://ci.apache.org/projects/flink/flink-docs-release-1.9/zh/dev/connectors/</a></p><pre><code class="xml">&lt;dependency&gt;&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;&lt;version&gt;1.9.1&lt;/version&gt;&lt;/dependency&gt;</code></pre><h6 id="第一种：读取-Kafka-中的普通数据（String）"><a href="#第一种：读取-Kafka-中的普通数据（String）" class="headerlink" title="第一种：读取 Kafka 中的普通数据（String）"></a>第一种：读取 Kafka 中的普通数据（String）</h6><pre><code class="scala">object KafkaSourceByString &#123;def main(args: Array[String]): Unit = &#123;//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv = StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._ val props = new Properties()props.setProperty(&quot;bootstrap.servers&quot;,&quot;hadoop101:9092,hadoop102:9092,hadoop103:9092&quot;)props.setProperty(&quot;group.id&quot;,&quot;fink01&quot;)props.setProperty(&quot;key.deserializer&quot;,classOf[StringDeserializer].getName)props.setProperty(&quot;value.deserializer&quot;,classOf[StringDeserializer].getName)props.setProperty(&quot;auto.offset.reset&quot;,&quot;latest&quot;)//设置kafka为数据源val stream = streamEnv.addSource(new FlinkKafkaConsumer[String](&quot;t_topic&quot;,new SimpleStringSchema(),props))stream.print()streamEnv.execute()&#125;&#125;</code></pre><h6 id="第二种：读取Kafka中KeyValue数据"><a href="#第二种：读取Kafka中KeyValue数据" class="headerlink" title="第二种：读取Kafka中KeyValue数据"></a>第二种：读取Kafka中KeyValue数据</h6><pre><code class="scala">object KafkaSourceByKeyValue &#123;def main(args: Array[String]): Unit = &#123;//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv = StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._ val props = new Properties()props.setProperty(&quot;bootstrap.servers&quot;,&quot;hadoop101:9092,hadoop102:9092,hadoop103:9092&quot;)props.setProperty(&quot;group.id&quot;,&quot;fink02&quot;)props.setProperty(&quot;key.deserializer&quot;,classOf[StringDeserializer].getName)props.setProperty(&quot;value.deserializer&quot;,classOf[StringDeserializer].getName)props.setProperty(&quot;auto.offset.reset&quot;,&quot;latest&quot;)//设置kafka为数据源val stream = streamEnv.addSource(new FlinkKafkaConsumer[(String,String)](&quot;t_topic&quot;, new KafkaDeserializationSchema[(String,String)]&#123;//流是否结束override def isEndOfStream(t: (String, String)) = falseoverride def deserialize(consumerRecord: ConsumerRecord[Array[Byte], Array[Byte]]) = &#123;if(consumerRecord!=null)&#123;var key=&quot;null&quot; var value=&quot;null&quot;if(consumerRecord.key()!=null)key = new String(consumerRecord.key(),&quot;UTF-8&quot;)if(consumerRecord.value()!=null)value = new String(consumerRecord.value(),&quot;UTF-8&quot;)(key,value)&#125;else&#123; //如果kafka中的数据为空返回一个固定的二元组(&quot;null&quot;,&quot;null&quot;)&#125;&#125;//设置返回类型为二元组override def getProducedType = createTuple2TypeInformation(createTypeInformation[String],createTypeInformation[String])&#125;,props).setStartFromEarliest())stream.print()streamEnv.execute()&#125;&#125;</code></pre><h4 id="4）自定义Source"><a href="#4）自定义Source" class="headerlink" title="4）自定义Source"></a>4）自定义Source</h4><p>当然也可以自定义数据源，有两种方式实现： </p><ol><li>通过实现 SourceFunction 接口来自定义无并行度（也就是并行度只能为 1）的 Source。 </li><li>通过实现 ParallelSourceFunction 接口或者继承 RichParallelSourceFunction 来自定义有并行度的数据源。</li></ol><p><strong>SourceFunction接口实现：</strong></p><pre><code class="scala">class MyCustomerSource extends SourceFunction[StationLog]&#123;//是否终止数据流的标记var flag =true;/*** 主要的方法* 启动一个Source* 大部分情况下，都需要在这个run方法中实现一个循环，这样就可以循环产生数据了* @param sourceContext * @throws Exception*/override def run(sourceContext: SourceFunction.SourceContext[StationLog]):Unit = &#123;val random = new Random()var types =Array(&quot;fail&quot;,&quot;busy&quot;,&quot;barring&quot;,&quot;success&quot;)while(flag) &#123; //如果流没有终止，继续获取数据1.to(5).map(i=&gt;&#123;var callOut=&quot;1860000%04d&quot;.format(random.nextInt(10000))var callIn=&quot;1890000%04d&quot;.format(random.nextInt(10000))newStationLog(&quot;station_&quot;+random.nextInt(10),callOut,callIn,types(random.nextInt(4)),System.currentTimeMillis(),0)&#125;).foreach(sourceContext.collect(_)) //发数据Thread.sleep(2000) //每发送一次数据休眠2秒&#125;&#125;//终止数据流override def cancel(): Unit = flag=false&#125;object CustomerSource &#123;def main(args: Array[String]): Unit = &#123;val env: StreamExecutionEnvironment =StreamExecutionEnvironment.getExecutionEnvironmentenv.setParallelism(1)import org.apache.flink.streaming.api.scala._ val stream: DataStream[StationLog] = env.addSource(newMyCustomerSource)stream.print()env.execute()&#125;&#125;</code></pre><h3 id="3-Flink-的-Sink-数据目标"><a href="#3-Flink-的-Sink-数据目标" class="headerlink" title="3.Flink 的 Sink 数据目标"></a>3.Flink 的 Sink 数据目标</h3><p>Flink 针对 DataStream 提供了大量的已经实现的数据目标（Sink），包括文件、Kafka、 Redis、HDFS、Elasticsearch 等等。</p><h4 id="1-基于-HDFS-的-Sink"><a href="#1-基于-HDFS-的-Sink" class="headerlink" title="1)基于 HDFS 的 Sink"></a>1)基于 HDFS 的 Sink</h4><p>首先配置支持 Hadoop FileSystem 的连接器依赖。</p><pre><code class="xml">&lt;dependency&gt;&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&lt;artifactId&gt;flink-connector-filesystem_2.11&lt;/artifactId&gt;&lt;version&gt;1.9.1&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>Streaming File Sink 能把数据写入 HDFS 中，还可以支持分桶写入，每一个分桶就对 应 HDFS 中的一个目录。默认按照小时来分桶，在一个桶内部，会进一步将输出基于滚动策 略切分成更小的文件。这有助于防止桶文件变得过大。滚动策略也是可以配置的，默认 策 略会根据文件大小和超时时间来滚动文件，超时时间是指没有新数据写入部分文件（part file）的时间。</p><pre><code class="scala">object HDFSFileSink &#123;def main(args: Array[String]): Unit = &#123;//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv = StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._ val data: DataStream[StationLog] = streamEnv.addSource(new MyCustomerSource)//创建一个文件滚动规则val rolling: DefaultRollingPolicy[StationLog, String] = DefaultRollingPolicy.create().withInactivityInterval(2000) //不活动的间隔时间。.withRolloverInterval(2000) //每隔两秒生成一个文件 ，重要.build()//创建一个HDFS Sinkvar hdfsSink =StreamingFileSink.forRowFormat[StationLog](new Path(&quot;hdfs://hadoop101:9000/sink001/&quot;), new SimpleStringEncoder[StationLog](&quot;UTF-8&quot;)).withBucketCheckInterval(1000) //检查分桶的间隔时间.withRollingPolicy(rolling).build()data.addSink(hdfsSink)streamEnv.execute()&#125;&#125;</code></pre><h4 id="2-基于Redis的Sink"><a href="#2-基于Redis的Sink" class="headerlink" title="2)基于Redis的Sink"></a>2)基于Redis的Sink</h4><pre><code class="xml">&lt;dependency&gt;&lt;groupId&gt;org.apache.bahir&lt;/groupId&gt;&lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt;&lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>把 WordCount 的结果写入 Redis 中：</p><pre><code class="scala">object RedisSink &#123;def main(args: Array[String]): Unit = &#123;//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv= StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._//读取数据val stream = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888)//转换计算val result = stream.flatMap(_.split(&quot;,&quot;)).map((_, 1)).keyBy(0).sum(1)//连接redis的配置val config = new FlinkJedisPoolConfig.Builder().setDatabase(1).setHost(&quot;hadoop101&quot;).setPort(6379).build()//写入redisresult.addSink(new RedisSink[(String, Int)](config,new RedisMapper[(String, Int)] &#123;override def getCommandDescription = new RedisCommandDescription(RedisCommand.HSET,&quot;t_wc&quot;)override def getKeyFromData(data: (String, Int)) = &#123;data._1 //单词&#125;override def getValueFromData(data: (String, Int)) = &#123;data._2+&quot;&quot; //单词出现的次数&#125;&#125;))streamEnv.execute()&#125;&#125;</code></pre><h4 id="3-基于-Kafka-的-Sink"><a href="#3-基于-Kafka-的-Sink" class="headerlink" title="3)基于 Kafka 的 Sink"></a>3)基于 Kafka 的 Sink</h4><pre><code class="xml">&lt;dependency&gt;&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;&lt;version&gt;1.9.1&lt;/version&gt;&lt;/dependency&gt;</code></pre><pre><code class="scala">object KafkaSink &#123;def main(args: Array[String]): Unit = &#123;val streamEnv: StreamExecutionEnvironment =StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1) //默认情况下每个任务的并行度为1import org.apache.flink.streaming.api.scala._//读取netcat流中数据 （实时流）val stream1: DataStream[String] = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888)//转换计算val result = stream1.flatMap(_.split(&quot;,&quot;)).map((_, 1)).keyBy(0).sum(1)//Kafka生产者的配置val props = new Properties()props.setProperty(&quot;bootstrap.servers&quot;,&quot;hadoop101:9092,hadoop102:9092,hadoop103:9092&quot;)props.setProperty(&quot;key.serializer&quot;,classOf[StringSerializer].getName)props.setProperty(&quot;value.serializer&quot;,classOf[StringSerializer].getName)//数据写入Kafka，并且是KeyValue格式的数据result.addSink(new FlinkKafkaProducer[(String, Int)](&quot;t_topic&quot;,new KafkaSerializationSchema[(String,Int)]&#123;override def serialize(element: (String, Int), timestamp: lang.Long) = &#123;new ProducerRecord(&quot;t_topic&quot;,element._1.getBytes,(element._2+&quot;&quot;).getBytes())&#125;&#125;,props,FlinkKafkaProducer.Semantic.EXACTLY_ONCE)) //EXACTLY_ONCE 精确一次streamEnv.execute()&#125;&#125;</code></pre><h4 id="4-自定义Sink"><a href="#4-自定义Sink" class="headerlink" title="4)自定义Sink"></a>4)自定义Sink</h4><p>当然你可以自己定义 Sink，有两种实现方式：1、实现 SinkFunction 接口。2、实现 RichSinkFunction 类。后者增加了生命周期的管理功能。</p><p>比如需要在 Sink 初始化的时候创建连接对象，则最好使用第二种。</p><p>案例需求：把 StationLog 对象写入 Mysql 数据库中。</p><pre><code class="xml">&lt;dependency&gt;&lt;groupId&gt;mysql&lt;/groupId&gt;&lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;version&gt;5.1.44&lt;/version&gt;&lt;/dependency&gt;</code></pre><pre><code class="scala">//从自定义的Source中读取StationLog数据，通过Flink写入Mysql数据库object CustomJdbcSink &#123;//自定义一个Sink写入Mysqlclass MyCustomSink extends RichSinkFunction[StationLog]&#123;var conn:Connection =_ var pst :PreparedStatement =_//生命周期管理，在Sink初始化的时候调用override def open(parameters: Configuration): Unit = &#123;conn=DriverManager.getConnection(&quot;jdbc:mysql://localhost/test&quot;,&quot;root&quot;,&quot;123123&quot;)pst=conn.prepareStatement(&quot;insert into t_station_log (sid,call_out,call_in,call_type,call_time,duration) values (?,?,?,?,?,?)&quot;)&#125;//把StationLog 写入到表t_station_logoverride def invoke(value: StationLog, context: SinkFunction.Context[_]): Unit =             pst.setString(1,value.sid)pst.setString(2,value.callOut)pst.setString(3,value.callIn)pst.setString(4,value.callType)pst.setLong(5,value.callTime)pst.setLong(6,value.duration)pst.executeUpdate()&#125;override def close(): Unit = &#123;pst.close()conn.close()&#125;&#125;def main(args: Array[String]): Unit = &#123;//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv = StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._ val data: DataStream[StationLog] = streamEnv.addSource(new MyCustomerSource)//数据写入msyqldata.addSink(new MyCustomSink)streamEnv.execute()&#125;&#125;</code></pre><h3 id="4-DataStream转换算子"><a href="#4-DataStream转换算子" class="headerlink" title="4.DataStream转换算子"></a>4.DataStream转换算子</h3><p>从一个或多个DataStream生成新的DataStream的过程称为Transformation操作。在转换过程中，每种操作类型被定义为不同的Operator，Flink将多个Transformation组成一个DataFlow（数据流）的拓扑。</p><h4 id="1）Map（DataStream-gt-DataStream）"><a href="#1）Map（DataStream-gt-DataStream）" class="headerlink" title="1）Map（DataStream-&gt;DataStream）"></a>1）Map（DataStream-&gt;DataStream）</h4><p>调用用户定义的MapFunction对DataStream[T]数据进行处理，形成新的Data-Stream[T]，常用于对数据集内数据的清洗和转换。</p><h4 id="2）FlatMap（DataStream-gt-DataStream）"><a href="#2）FlatMap（DataStream-gt-DataStream）" class="headerlink" title="2）FlatMap（DataStream-&gt;DataStream）"></a>2）FlatMap（DataStream-&gt;DataStream）</h4><p>该算子主要应用处理输入一个元素产生一个或者多个元素的计算场景，比较常见的是在 经典例子 WordCount 中，将每一行的文本数据切割，生成单词序列如在图所示，对于输入 DataStream[String]通过 FlatMap 函数进行处理，字符串数字按逗号切割，然后形成新的整 数数据集。</p><h4 id="3）Filter-DataStream-gt-DataStream"><a href="#3）Filter-DataStream-gt-DataStream" class="headerlink" title="3）Filter[DataStream-&gt;DataStream]"></a>3）Filter[DataStream-&gt;DataStream]</h4><p>该算子将按照条件对输入数据集进行筛选操作，将符合条件的数据集输出，将不符合条 件的数据过滤掉。</p><h4 id="4）KeyBy-DataStream-gt-KeyedStream"><a href="#4）KeyBy-DataStream-gt-KeyedStream" class="headerlink" title="4）KeyBy[DataStream-&gt;KeyedStream]"></a>4）KeyBy[DataStream-&gt;KeyedStream]</h4><p>该算子根据指定的 Key 将输入的 DataStream[T]数据格式转换为 KeyedStream[T]，也就 是在数据集中执行 Partition 操作，将相同的 Key 值的数据放置在相同的分区中。</p><p>如下图所 示，将白色方块和灰色方块通过颜色的 Key 值重新分区，将数据集分为具有灰色方块的数据 集合。</p><p><img src="/flink/flink/image-20220517214547098.png" alt="image-20220517214547098"></p><pre><code class="scala">val dataStream = env.fromElements((1, 5),(2, 2),(2, 4),(1, 3))//指定第一个字段为分区Keyval keyedStream: KeyedStream[(String,Int), Tuple] = dataStream.keyBy(0)</code></pre><h4 id="5）Reduce-KeyedStream-gt-DataStream"><a href="#5）Reduce-KeyedStream-gt-DataStream" class="headerlink" title="5）Reduce[KeyedStream-&gt;DataStream]"></a>5）Reduce[KeyedStream-&gt;DataStream]</h4><p>将keyed数据进行聚合处理 </p><p>将输入的 KeyedStream 通过 传 入 的 用 户 自 定 义 的 ReduceFunction 滚 动 地 进 行 数 据 聚 合 处 理 ， 其 中 定 义 的 ReduceFunciton 必须满足运算结合律和交换律。</p><p>如下代码对传入 keyedStream 数据集中相 同的 key 值的数据独立进行求和运算，得到每个 key 所对应的求和值：</p><pre><code class="scala">val dataStream = env.fromElements((&quot;a&quot;, 3), (&quot;d&quot;, 4), (&quot;c&quot;, 2), (&quot;c&quot;,5), (&quot;a&quot;, 5))//指定第一个字段为分区Keyval keyedStream: KeyedStream[(String,Int), Tuple] = dataStream.keyBy(0)/滚动对第二个字段进行reduce相加求和val reduceStream = keyedStream.reduce &#123; (t1, t2) =&gt;(t1._1, t1._2 + t2._2)&#125;</code></pre><h4 id="6）Aggregations-KeyedStream-gt-DataStream"><a href="#6）Aggregations-KeyedStream-gt-DataStream" class="headerlink" title="6）Aggregations[KeyedStream-&gt;DataStream]"></a>6）Aggregations[KeyedStream-&gt;DataStream]</h4><p>Aggregations 是 KeyedDataStream 接口提供的聚合算子，根据指定的字段进行聚合操 作，滚动地产生一系列数据聚合结果。</p><p>Aggregations其实是将 Reduce 算子中的函数进行了封装，封装的聚合操作有<strong>sum、min、minBy、max、maxBy</strong>等，这样归不需要用户自己定义Reduce函数</p><p>如下代码所示，指定数据集中第一个字段作为 key，用第二个字段作为累加字段，然后滚动 地对第二个字段的数值进行累加并输出:</p><pre><code class="scala">/指定第一个字段为分区Keyval keyedStream: KeyedStream[(Int, Int), Tuple] = dataStream.keyBy(0)//对第二个字段进行sum统计val sumStream: DataStream[(Int, Int)] = keyedStream.sum(1)//输出计算结果sumStream.print()</code></pre><h4 id="7）Union-DataStream-gt-DataStream"><a href="#7）Union-DataStream-gt-DataStream" class="headerlink" title="7）Union[DataStream-&gt;DataStream]"></a>7）Union[DataStream-&gt;DataStream]</h4><p>Union 算子主要是将两个或者多个输入的数据集合并成一个数据集，需要保证两个数据 集的格式一致，输出的数据集的格式和输入的数据集格式保持一致，</p><p>如图所示，将灰色方块 数据集和黑色方块数据集合并成一个大的数据集：</p><pre><code class="scala">//创建不同的数据集val dataStream1: DataStream[(String, Int)] = env.fromElements((&quot;a&quot;, 3), (&quot;d&quot;, 4), (&quot;c&quot;, 2), (&quot;c&quot;, 5), (&quot;a&quot;, 5))val dataStream2: DataStream[(String, Int)] = env.fromElements((&quot;d&quot;, 1), (&quot;s&quot;, 2), (&quot;a&quot;, 4), (&quot;e&quot;, 5), (&quot;a&quot;, 6))val dataStream3: DataStream[(String, Int)] = env.fromElements((&quot;a&quot;, 2), (&quot;d&quot;, 1), (&quot;s&quot;, 2), (&quot;c&quot;, 3), (&quot;b&quot;, 1))//合并两个DataStream数据集val unionStream = dataStream1.union(dataStream_02)//合并多个DataStream数据集val allUnionStream = dataStream1.union(dataStream2, dataStream3)</code></pre><h4 id="8）Connect，CoMap，CoFlatMap-DataStream-gt-ConnectedStream-gt-DataStream"><a href="#8）Connect，CoMap，CoFlatMap-DataStream-gt-ConnectedStream-gt-DataStream" class="headerlink" title="8）Connect，CoMap，CoFlatMap[DataStream -&gt; ConnectedStream-&gt;DataStream]"></a>8）Connect，CoMap，CoFlatMap[DataStream -&gt; ConnectedStream-&gt;DataStream]</h4><p>Connect 算子主要是为了合并两种或者多种不同数据类型的数据集，合并后会保留原来的数据集的数据类型。</p><p>例如：dataStream1 数据集为**(String, Int)元祖<strong>类型，dataStream2 数据集为 <strong>Int 类型</strong>，通过 connect 连接算子将两个不同数据类型的流结合在一起，形成格式 为 <strong>ConnectedStreams</strong> 的数据集，其内部数据为</strong>[(String, Int), Int]**的混合数据类型，保留了两个原始数据集的数据类型。</p><p><strong>Union和Connect的区别</strong></p><p>Union两个流的类型必须是一样的，Connec可以不一样类型的，之后coMap在去调整成一样的</p><p>Connect只能操作两个流，Union可以合并多个。</p><h4 id="9）Split-和-Select-DataStream-gt-SplitStream-gt-DataStream"><a href="#9）Split-和-Select-DataStream-gt-SplitStream-gt-DataStream" class="headerlink" title="9）Split 和 Select [DataStream-&gt;SplitStream-&gt;DataStream]"></a>9）Split 和 Select [DataStream-&gt;SplitStream-&gt;DataStream]</h4><p>Split算子将一个DataStream数据集按照条件进行拆分，形成两个数据集的过程，也是union算子的逆向实现。每个接入的数据都会被路由到一个或者多个输出数据集中。</p><p>使用splist函数中，需要定义split函数中的切分逻辑，通过调用split函数，然后指定条件判断函数。</p><p>如下面的代码所示：将根据第二个字段的奇偶性将数据集标记出来，如 果是偶数则标记为 even，如果是奇数则标记为 odd，然后通过集合将标记返回，最终生成格 式 SplitStream 的数据集：</p><pre><code class="scala">//创建数据集val dataStream1: DataStream[(String, Int)] = env.fromElements((&quot;a&quot;, 3), (&quot;d&quot;, 4), (&quot;c&quot;, 2), (&quot;c&quot;, 5), (&quot;a&quot;, 5))//合并两个DataStream数据集val splitedStream: SplitStream[(String, Int)] = dataStream1.split(t =&gt; if (t._2 % 2 == 0) Seq(&quot;even&quot;) else Seq(&quot;odd&quot;))</code></pre><p>split 函数本身只是对输入数据集进行标记，并没有将数据集真正的实现切分，因此需 要借助 Select 函数根据标记将数据切分成不同的数据集。</p><p>如下代码所示，通过调用 SplitStream 数据集的 select()方法，传入前面已经标记好的标签信息，然后将符合条件的 数据筛选出来，形成新的数据集：</p><pre><code class="scala">//筛选出偶数数据集val evenStream: DataStream[(String, Int)] = splitedStream.select(&quot;even&quot;)//筛选出奇数数据集val oddStream: DataStream[(String, Int)] = splitedStream.select(&quot;odd&quot;)//筛选出奇数和偶数数据集val allStream: DataStream[(String, Int)] = splitedStream.select(&quot;even&quot;, &quot;odd&quot;)</code></pre><h3 id="5-函数类和富函数类"><a href="#5-函数类和富函数类" class="headerlink" title="5.函数类和富函数类"></a>5.函数类和富函数类</h3><p><strong>所有算子几乎都可以自定义一个函数类、富函数类作为参数。</strong></p><p>因为 Flink 暴露了者两种函数类的接口，常见的函数接口有：</p><ul><li>MapFunction </li><li>FlatMapFunction </li><li>ReduceFunction</li><li>。。。</li></ul><p><strong>富函数接口它其他常规函数接口的不同在于：可以获取运行环境的上下文，在上下文环境中可以管理状态（状态下面会讲到），并拥有一些生命周期方法，所以可以实现更复杂的功能</strong>。</p><p>富函数的接口有：</p><ul><li>RichMapFunction </li><li>RichFlatMapFunction </li><li>RichFilterFunction</li><li>。。。</li></ul><h4 id="1）普通函数类举例："><a href="#1）普通函数类举例：" class="headerlink" title="1）普通函数类举例："></a>1）普通函数类举例：</h4><p>按照指定的时间格式输出每个通话的拨号时间和结束时间。数据如下：</p><p><img src="/flink/flink/image-20211021194122433.png" alt="image-20211021194122433"></p><pre><code class="scala">import org.apache.flink.api.common.functions.MapFunctionimport org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment&#125;import java.text.SimpleDateFormatimport java.util.Date//按照指定的时间格式输出每个通话的拨号时间和结束时间object FunctionClassTransformation &#123;  def main(args: Array[String]): Unit = &#123;    //初始化Flink的Streaming（流计算）上下文执行环境    val streamEnv: StreamExecutionEnvironment =      StreamExecutionEnvironment.getExecutionEnvironment    streamEnv.setParallelism(1)    //导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题    import org.apache.flink.streaming.api.scala._    //读取文件数据    val data = streamEnv.readTextFile(&quot;station.log&quot;)      .map(line=&gt;&#123;        var arr =line.split(&quot;,&quot;)        new StationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)      &#125;)    //定义时间输出格式    val format: SimpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;)    //过滤那些通话成功的    data.filter(_.callType.equals(&quot;success&quot;))      .map(new CallMapFunction(format))      .print()    streamEnv.execute()  &#125;  //自定义的函数类  class CallMapFunction(format: SimpleDateFormat) extends    MapFunction[StationLog,String]&#123;    override def map(t: StationLog): String = &#123;      var strartTime=t.callTime;      var endTime =t.callTime + t.duration*1000      &quot;主叫号码:&quot;+t.callOut +&quot;,被叫号码:&quot;+t.callIn+&quot;,呼叫起始时间:&quot;+format.format(new Date(strartTime))+&quot;,呼叫结束时间:&quot;+format.format(new      Date(endTime))    &#125;  &#125;  /* 通信基站日志数据  * @param sid 基站ID  * @param callOut 主叫号码  * @param callIn 被叫号码  * @param callType 通话类型eg:呼叫失败(fail)，占线(busy),拒接（barring），接通  (success): * @param callTime 呼叫时间戳，精确到毫秒  * @Param duration 通话时长 单位：秒  */  case class  StationLog(sid:String,callOut:String,callIn:String,callType:String,callTime:Long,duration:Long)&#125;</code></pre><p><img src="/flink/flink/image-20211021214725044.png" alt="image-20211021214725044"></p><p>Rich Function 有一个生命周期的概念。典型的生命周期方法有：</p><ul><li>open()方法是 rich function 的初始化方法，当一个算子例如 map 或者 filter 被调用 之前 open()会被调用。 </li><li>close()方法是生命周期中的最后一个调用的方法，做一些清理工作。 </li><li>getRuntimeContext()方法提供了函数的 RuntimeContext 的一些信息，例如函数执行的 并行度，任务的名字，以及 state 状态</li></ul><h4 id="2）富函数类举例："><a href="#2）富函数类举例：" class="headerlink" title="2）富函数类举例："></a>2）富函数类举例：</h4><p>把呼叫成功的通话信息转化成真实的用户姓名，通话用户对应的用户表 （在 Mysql 数据中）为：</p><p>由于需要从数据库中查询数据，就需要创建连接，<strong>创建连接的代码必须写在生命周期的 open 方法中</strong>。所以需要使用富函数类。</p><pre><code class="scala">package RichFunctionimport org.apache.flink.api.common.functions.RichMapFunctionimport org.apache.flink.configuration.Configurationimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironmentimport java.sql.&#123;Connection, DriverManager, PreparedStatement, ResultSet&#125;//转换电话号码的真实姓名object RichFunctionClassTransformation &#123;  def main(args: Array[String]): Unit = &#123;    //初始化Flink的Streaming（流计算）上下文执行环境    val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment    streamEnv.setParallelism(1)    //导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题    import org.apache.flink.streaming.api.scala._    //读取文件数据    val data = streamEnv.readTextFile(&quot;station.log&quot;)      .map(line=&gt;&#123;        var arr =line.split(&quot;,&quot;)        new StationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)      &#125;)    //过滤出那些通话成功的    data.filter(_.callType.equals(&quot;success&quot;))      .map(new CallRichMapFunction())      .print()    streamEnv.execute()  &#125;  //自定义的富函数类  class CallRichMapFunction() extends RichMapFunction[StationLog,StationLog]&#123;    var conn:Connection =_    var pst :PreparedStatement =_    //生命周期管理，初始化的时候创建数据连接    override def open(parameters: Configuration): Unit = &#123;      conn=DriverManager.getConnection(&quot;jdbc:mysql://localhost/test&quot;,&quot;root&quot;,&quot;123123&quot;)      pst=conn.prepareStatement(&quot;select name from t_phone where phone_number=?&quot;)    &#125;    override def map(in: StationLog): StationLog = &#123;      //查询主叫用户的名字      pst.setString(1,in.callOut)      val set1: ResultSet = pst.executeQuery()      if(set1.next())&#123;        in.callOut=set1.getString(1)      &#125;      //查询被叫用户的名字      pst.setString(1,in.callIn)      val set2: ResultSet = pst.executeQuery()      if(set2.next())&#123;        in.callIn=set2.getString(1)      &#125;      in    &#125;    //关闭连接    override def close(): Unit = &#123;      pst.close()      conn.close()    &#125;  &#125;  /* 通信基站日志数据* @param sid 基站ID* @param callOut 主叫号码* @param callIn 被叫号码* @param callType 通话类型eg:呼叫失败(fail)，占线(busy),拒接（barring），接通(success): * @param callTime 呼叫时间戳，精确到毫秒* @Param duration 通话时长 单位：秒*/  case class  StationLog(sid: String, var callOut: String, var callIn: String, callType: String, callTime: Long, duration: Long)&#125;</code></pre><h3 id="6-底层-ProcessFunctionAPI"><a href="#6-底层-ProcessFunctionAPI" class="headerlink" title="6.底层 ProcessFunctionAPI"></a>6.底层 ProcessFunctionAPI</h3><p>ProcessFunction 是一个低层次的流处理操作，允许返回所有 Stream 的基础构建模块:</p><ul><li><p>访问 Event 本身数据（比如：Event 的时间，Event 的当前 Key 等） </p></li><li><p>管理状态 State（仅在 Keyed Stream 中） </p></li><li><p>管理定时器 Timer（包括：注册定时器，删除定时器等）</p></li></ul><p>总而言之，ProcessFunction 是 Flink 最底层的 API，也是功能最强大的。</p><p>例如：监控每一个手机，如果在 5 秒内呼叫它的通话都是失败的，发出警告信息。（注意： 这个案例中会用到状态编程，请同学们只要知道状态的意思，不需要掌握。后面的章节中会 详细讲解 State 编程。）：</p><pre><code class="scala">/*** 监控每一个手机号，如果在5秒内呼叫它的通话都是失败的，发出警告信息* 在5秒中内只要有一个呼叫不是fail则不用警告*/object TestProcessFunction &#123;def main(args: Array[String]): Unit = &#123;//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._//读取文件数据val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;)//处理数据data.keyBy(_.callOut).process(new MonitorCallFail()).print()streamEnv.execute()&#125;//监控逻辑class MonitorCallFail() extends KeyedProcessFunction[String,StationLog,String]&#123;//使用一个状态记录时间lazy val timeState :ValueState[Long] =getRuntimeContext.getState(new ValueStateDescriptor[Long](&quot;time&quot;,classOf[Long]))override def processElement(value: StationLog, ctx: KeyedProcessFunction[String, StationLog, String]#Context, out: Collector[String]): Unit = &#123;//从状态中取得时间var time =timeState.value()if(value.callType.equals(&quot;fail&quot;)&amp;&amp; time==0)&#123; //表示第一次发现呼叫当前手机号是失败的//获取当前时间，并注册定时器var nowTime=ctx.timerService().currentProcessingTime()var onTime=nowTime + 5000L //5秒后触发ctx.timerService().registerProcessingTimeTimer(onTime)timeState.update(onTime)&#125;if(!value.callType.equals(&quot;fail&quot;) &amp;&amp; time!=0)&#123;//表示有呼叫成功了，可以取消触发器ctx.timerService().deleteProcessingTimeTimer(time)timeState.clear()&#125;&#125;//时间到了，执行触发器,发出告警override def onTimer(timestamp: Long, ctx: KeyedProcessFunction[String, StationLog, String]#OnTimerContext, out: Collector[String]): Unit = &#123;var warnStr=&quot;触发时间:&quot;+timestamp+&quot; 手机号：&quot;+ctx.getCurrentKeyout.collect(warnStr)timeState.clear()&#125;&#125;&#125;</code></pre><h3 id="7-侧输出流-Side-Output"><a href="#7-侧输出流-Side-Output" class="headerlink" title="7.侧输出流 Side Output"></a>7.侧输出流 Side Output</h3><p>在 flink 处理数据流时，我们经常会遇到这样的情况：在处理一个数据源时，往往需要 将该源中的不同类型的数据做分割处理，如果使用 filter 算子对数据源进行筛选分割的话，势必会造成数据流的多次复制，造成不必要的性能浪费；flink 中的侧输出就是将数据 流进行分割，而不对流进行复制的一种分流机制。flink 的侧输出的另一个作用就是对延时 迟到的数据进行处理，这样就可以不必丢弃迟到的数据。</p><p>Flink中，可以将一个流中的数据根据数据的不同属性进行if判断或者模式匹配，然后给各个流打上标签，以后可以根据标签的名字，取出想要的，类型的数据流，测流输出的优点是比filter效率高，不必对数据进行多次处理，就可以将不同类型的数据拆分。</p><h2 id="Flink-State-管理与恢复"><a href="#Flink-State-管理与恢复" class="headerlink" title="Flink State 管理与恢复"></a>Flink State 管理与恢复</h2><p>Flink 是一个默认就有状态的分析引擎，前面的 WordCount 案例可以做到单词的数量的 累加，其实是因为在内存中保证了每个单词的出现的次数，这些数据其实就是状态数据。但 是如果一个 Task 在处理过程中挂掉了，那么它在内存中的状态都会丢失，所有的数据都需 要重新计算。从容错和消息处理的语义（At -least-once 和 Exactly-once）上来说，Flink 引入了 State 和 CheckPoint。</p><ul><li><p>State 一般指一个具体的 Task&#x2F;Operator 的状态，State 数据默认保存在 Java 的堆内存中</p></li><li><p>CheckPoint（可以理解为 CheckPoint 是把 State 数据持久化存储了）则表示了一个 Flink Job 在一个特定时刻的一份全局状态快照，即包含了所有 Task&#x2F;Operator 的状态</p></li></ul><h3 id="1-常用-Stat"><a href="#1-常用-Stat" class="headerlink" title="1.常用 Stat"></a>1.常用 Stat</h3><p>Flink 有两种常见的 State 类型，分别是</p><h4 id="1）keyed-State-键控状态"><a href="#1）keyed-State-键控状态" class="headerlink" title="1）keyed State(键控状态)"></a>1）keyed State(键控状态)</h4><p>Keyed State：顾名思义就是基于 KeyedStream 上的状态，这个状态是跟特定的 Key 绑 定的。KeyedStream 流上的每一个 Key，都对应一个 State。Flink 针对 Keyed State 提供了 以下可以保存 State 的数据结构：</p><ul><li><p>ValueState: 保存一个可以更新和检索的值（如上所述，每个值都对应到当前的输 入数据的 key，因此算子接收到的每个 key 都可能对应一个值）。 这个值可以通过 update(T) 进行更新，通过 T value() 进行检索。 </p></li><li><p>ListState: 保存一个元素的列表。可以往这个列表中追加数据，并在当前的列表上 进行检索。可以通过 add(T) 或者 addAll(List) 进行添加元素，通过 Iterable get() 获得整个列表。还可以通过 update(List) 覆盖当前的列表。</p></li><li><p>ReducingState: 保存一个单值，表示添加到状态的所有值的聚合。接口与 ListState 类似，但使用 add(T) 增加元素，会使用提供的 ReduceFunction 进行聚合。 </p></li><li><p>AggregatingState: 保留一个单值，表示添加到状态的所有值的聚合。和 ReducingState 相反的是, 聚合类型可能与 添加到状态的元素的类型不同。 接口与 ListState 类似，但使用 add(IN) 添加的元素会用指定的 AggregateFunction 进行聚 合。 </p></li><li><p>FoldingState: 保留一个单值，表示添加到状态的所有值的聚合。 与 ReducingState 相反，聚合类型可能与添加到状态的元素类型不同。接口与 ListState 类似，但使用 add（T）添加的元素会用指定的 FoldFunction 折叠成聚合值。 </p></li><li><p>MapState: 维护了一个映射列表。 你可以添加键值对到状态中，也可以获得 反映当前所有映射的迭代器。使用 put(UK，UV) 或者 putAll(Map) 添加映射。 使用 get(UK) 检索特定 key。 使用 entries()，keys() 和 values() 分别检索映射、 键和值的可迭代视图。</p></li></ul><h5 id="keyed-State案例"><a href="#keyed-State案例" class="headerlink" title="keyed State案例"></a>keyed State案例</h5><p>Flink 中最基础的状态类型是 <a href="https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/datastream/fault-tolerance/state/#using-managed-keyed-state">ValueState</a>，这是一种能够为被其封装的变量添加容错能力的类型。 <code>ValueState</code> 是一种 <em>keyed state</em>，也就是说它只能被用于 <em>keyed context</em> 提供的 operator 中，即所有能够紧随 <code>DataStream#keyBy</code> 之后被调用的operator。 一个 operator 中的 <em>keyed state</em> 的作用域默认是属于它所属的 key 的。</p><ol><li>案例需求：计算每个手机的呼叫间隔时间，单位是毫秒。</li></ol><p><code>ValueState</code> 需要使用 <code>ValueStateDescriptor</code> 来创建，<code>ValueStateDescriptor</code> 包含了 Flink 如何管理变量的一些元数据信息。状态在使用之前需要先被注册。 状态需要使用 <code>open()</code> 函数来注册状态。</p><p><code>ValueState</code> 是一个包装类，类似于 Java 标准库里边的 <code>AtomicReference</code> 和 <code>AtomicLong</code>。 它提供了三个用于交互的方法。<code>update</code> 用于更新状态，<code>value</code> 用于获取状态值，还有 <code>clear</code> 用于清空状态。 如果一个 key 还没有状态，例如当程序刚启动或者调用过 <code>ValueState#clear</code> 方法时，<code>ValueState#value</code> 将会返回 <code>null</code>。 如果需要更新状态，需要调用 <code>ValueState#update</code> 方法，直接更改 <code>ValueState#value</code> 的返回值可能不会被系统识别。 容错处理将在 Flink 后台自动管理，你可以像与常规变量那样与状态变量进行交互。</p><pre><code class="scala">package StateExerciseimport org.apache.flink.api.common.functions.RichFlatMapFunctionimport org.apache.flink.api.common.state.&#123;ValueState, ValueStateDescriptor&#125;import org.apache.flink.configuration.Configurationimport org.apache.flink.util.Collector/** * @author  * @version 1.0 * 案例需求：计算每个手机的呼叫间隔时间，单位是毫秒。 */object KeyedStateExercise &#123;  def main(args: Array[String]): Unit = &#123;    import org.apache.flink.streaming.api.scala._    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment    val data: DataStream[String] = environment.readTextFile(&quot;station.log&quot;)    val value: DataStream[StationLog] = data      .map(a =&gt; &#123;        var arr = a.split(&quot;,&quot;)        new StationLog(arr(0).trim, arr(1).trim, arr(2).trim, arr(3).trim, arr(4).toLong, arr(5).toLong)      &#125;)    value.print(&quot;aaaaaaaaaaaaaaaaaaaaaaaaa&quot;)    value.keyBy(_.callIn) //按照呼叫手机号分组      .flatMap(new CallIntervalFunction())      .print()    environment.execute()  &#125;  /* 通信基站日志数据* @param sid 基站ID* @param callOut 主叫号码* @param callIn 被叫号码* @param callType 通话类型eg:呼叫失败(fail)，占线(busy),拒接（barring），接通(success): * @param callTime 呼叫时间戳，精确到毫秒* @Param duration 通话时长 单位：秒*/  case class  StationLog(sid: String, callOut: String, callIn: String, callType: String, callTime: Long, duration: Long)  class CallIntervalFunction() extends RichFlatMapFunction[StationLog, (String, Long)] &#123;    //定义一个保存前一条呼叫的数据的状态对象    private var preData: ValueState[StationLog] = _    override def open(parameters: Configuration): Unit = &#123;      val stateDescriptor = new          ValueStateDescriptor[StationLog](&quot;pre&quot;, classOf[StationLog])      preData = getRuntimeContext.getState(stateDescriptor)    &#125;    override def flatMap(in: StationLog, collector: Collector[(String, Long)]): Unit = &#123;      var pre: StationLog = preData.value()      if (pre == null) &#123; //如果状态中没有，则存入        preData.update(in)      &#125; else &#123; //如果状态中有值则计算时间间隔        var interval = in.callTime - pre.callTime        collector.collect((in.callIn, interval))      &#125;    &#125;  &#125;&#125;</code></pre><h4 id="2）Operator-State-算子状态"><a href="#2）Operator-State-算子状态" class="headerlink" title="2）Operator State(算子状态)"></a>2）Operator State(算子状态)</h4><p>没有分组，每一个subTask自己维护一个状态。Operator State 与 Key 无关，而是<strong>与 Operator 绑定</strong>，整个 Operator 只对应一个 State。 </p><p>比如：Flink 中的 Kafka Connector 就使用了 Operator State，它会在每个 Connector 实例 中，保存该实例消费 Topic 的所有(partition, offset)映射。</p><p><img src="/flink/flink/image-20220519000446637.png" alt="image-20220519000446637"></p><h4 id="3）Broadcast-state"><a href="#3）Broadcast-state" class="headerlink" title="3）Broadcast state"></a>3）Broadcast state</h4><p>广播state，一个可以通过connect方法获取广播流的数据，广播流的特点是可以动态更新state通常作为字段数据，维度数据关联，广播到属于该任务的所有taskmanager的每个taskslot中，类似于map。</p><h3 id="2-CheckPoint"><a href="#2-CheckPoint" class="headerlink" title="2.CheckPoint"></a>2.CheckPoint</h3><p>当程序出现问题需要恢复 Sate 数据的时候，只有程序提供支持才可以实现 State 的容 错。State 的容错需要依靠 CheckPoint 机制，这样才可以保证 Exactly-once 这种语义，但是注意，它只能保证 Flink 系统内的 Exactly-once，比如 Flink 内置支持的算子。针对 Source 和 Sink 组件，如果想要保证 Exactly-once 的话，则这些组件本身应支持这种语义。</p><h4 id="1-CheckPoint-原理"><a href="#1-CheckPoint-原理" class="headerlink" title="1)CheckPoint 原理"></a>1)CheckPoint 原理</h4><p>Flink 中基于异步轻量级的分布式快照技术提供了 Checkpoints 容错机制，分布式快照 可以将同一时间点 Task&#x2F;Operator 的状态数据全局统一快照处理，包括前面提到的 Keyed State 和 Operator State。Flink 会在输入的数据集上间隔性地生成 checkpoint barrier， 通过栅栏（barrier）将间隔时间段内的数据划分到相应的 checkpoint。</p><p>具体的过程是JobManager定期向TaskManager中的SubTask发送RPC消息，subTask将其计算的state保存到stateBackEnd中，并向JobManager响应checkpointing是否成功，如果程序出现异常或重启，TaskManager中的SubTask可以从上一次成功的checkPointing的state恢复。</p><h5 id="Barrier"><a href="#Barrier" class="headerlink" title="Barrier"></a>Barrier</h5><p>Flink的容错机制主要是通过持续产生快照的方式实现的，对应的快照机制主要由两部分组成，一个是屏障（Barrier），另一个是状态（state）</p><p><img src="/flink/flink/image-20220519153053025.png" alt="image-20220519153053025"></p><h5 id="Barrier对齐机制"><a href="#Barrier对齐机制" class="headerlink" title="Barrier对齐机制"></a>Barrier对齐机制</h5><p>流屏障（barrier）是Flink分布式快照中的核心元素。这些屏障注入到数据流中，并与记录一起作为数据流的一部分流动。他们严格按照顺序进行。每个屏障都带有快照的ID，快照的记录仪推送到块中的前面。屏障不会中断流的流动，非常轻便。来自不同快照的多个障碍可以同时出现在流中，这意味着各种快照可能会同时发生。</p><p>流程<br>当一个算子上游有两条或多条输入时，在进行Checkpoint时可能会出现两条流中数据流速不一样，导致多条流同一批次的Barrier到达下游算子的时间不一致， 此时快的Barrier到达下游算子后，此Barrier之后到达的数据将会放到缓冲区，不会进行处理。等到其他流慢的Barrier到达后，此算子才进行checkpoint，然后把状态保存到状态后端。这就是Barrier的对齐机制。</p><ul><li>优缺点</li></ul><p>1）优点：①状态后端保存数据少。</p><p>2）缺点：①延迟性高(快的Barrier到达后会阻塞此条流的数据处理)</p><p>②当作业出现反压时，会加剧作业的反压(当出现反压时，数据本身就处理不过来，此时某条流的数据又阻塞了所以就会加剧反压。)</p><p>③整体chenkpoint时间变长(因为反压会导致数据流速变慢，导致Barrier流的也慢，所以就会使得整体chenkpoint时间变长)。</p><ul><li>优化</li></ul><p>在Flink1.11后引入了Unaligned Checkpoint的特性，使得当Barrier不对齐的时候也可以实现数据的精准一次消费。<br>————————————————<br>版权声明：本文为CSDN博主「今天好好洗头了嘛」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/qq_42009405/article/details/122850469">https://blog.csdn.net/qq_42009405/article/details/122850469</a></p><p><img src="/flink/flink/image-20220601122700429.png" alt="image-20220601122700429"></p><h6 id="Barrier不对齐-Unaligned-Checkpoint"><a href="#Barrier不对齐-Unaligned-Checkpoint" class="headerlink" title="Barrier不对齐(Unaligned Checkpoint)"></a>Barrier不对齐(Unaligned Checkpoint)</h6><h5 id="恢复流程图"><a href="#恢复流程图" class="headerlink" title="恢复流程图"></a>恢复流程图</h5><img src="/flink/flink/image-20220519153155157.png" alt="image-20220519153155157" style="zoom: 150%;"><h4 id="2-CheckPoint-参数和设置"><a href="#2-CheckPoint-参数和设置" class="headerlink" title="2.CheckPoint 参数和设置"></a>2.CheckPoint 参数和设置</h4><p>默认情况下 Flink 不开启检查点的，用户需要在程序中通过调用方法配置和开启检查 点，另外还可以调整其他相关参数：</p><ul><li><strong>Checkpoint 开启和时间间隔指定：</strong></li></ul><p>开启检查点并且指定检查点时间间隔为 1000ms，根据实际情况自行选择，如果状态比 较大，则建议适当增加该值。</p><pre><code class="scala">streamEnv.enableCheckpointing(1000);</code></pre><ul><li><strong>exactly-ance 和 at-least-once 语义选择</strong></li></ul><p>选择 exactly-once 语义保证整个应用内端到端的数据一致性，这种情况比较适合于数 据要求比较高，不允许出现丢数据或者数据重复，与此同时，Flink 的性能也相对较弱，而 at-least-once 语义更适合于时廷和吞吐量要求非常高但对数据的一致性要求不高的场景。 如 下 通 过 setCheckpointingMode() 方 法 来 设 定 语 义 模 式 ， <strong>默 认 情 况 下 使 用 的 是 exactly-once</strong> 模式。</p><pre><code class="scala">streamEnv.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)；//或者streamEnv.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.AT_LEAST_ONCE)</code></pre><ul><li><strong>Checkpoint 超时时间：</strong></li></ul><p>超时时间指定了每次 Checkpoint 执行过程中的上限时间范围，一旦 Checkpoint 执行时 间超过该阈值，Flink 将会中断 Checkpoint 过程，并按照超时处理。该指标可以通过 setCheckpointTimeout 方法设定，默认为 10 分钟</p><pre><code class="scala">streamEnv.getCheckpointConfig.setCheckpointTimeout(50000)</code></pre><ul><li><strong>检查点之间最小时间间隔：</strong></li></ul><p>该参数主要目的是设定两个 Checkpoint 之间的最小时间间隔，防止出现例如状态数据 过大而导致 Checkpoint 执行时间过长，从而导致 Checkpoint 积压过多，最终 Flink 应用密 集地触发 Checkpoint 操作，会占用了大量计算资源而影响到整个应用的性能。</p><pre><code class="scala">streamEnv.getCheckpointConfig.setMinPauseBetweenCheckpoints(600)</code></pre><ul><li><strong>最大并行执行的检查点数量：</strong></li></ul><p>通过 setMaxConcurrentCheckpoints()方法设定能够最大同时执行的 Checkpoint 数量。 在默认情况下只有一个检查点可以运行，根据用户指定的数量可以同时触发多个 Checkpoint，进而提升 Checkpoint 整体的效率</p><pre><code class="scala">streamEnv.getCheckpointConfig.setMaxConcurrentCheckpoints(1)</code></pre><ul><li><strong>是否删除 Checkpoint 中保存的数据：</strong></li></ul><p>设置为 RETAIN_ON_CANCELLATION：表示一旦 Flink 处理程序被 cancel 后，会保留 CheckPoint 数据，以便根据实际需要恢复到指定的 CheckPoint。</p><p>设置为 DELETE_ON_CANCELLATION：表示一旦 Flink 处理程序被 cancel 后，会删除 CheckPoint 数据，只有 Job 执行失败的时候才会保存 CheckPoint。</p><pre><code class="scala">//删除streamEnv.getCheckpointConfig.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION)//保留streamEnv.getCheckpointConfig.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)</code></pre><ul><li><strong>TolerableCheckpointFailureNumber：</strong></li></ul><p>设置可以容忍的检查的失败数，超过这个数量则系统自动关闭和停止任务。</p><pre><code class="scala">streamEnv.getCheckpointConfig.setTolerableCheckpointFailureNumber(1)</code></pre><h4 id="3-保存机制-StateBackend-状态后端"><a href="#3-保存机制-StateBackend-状态后端" class="headerlink" title="3.保存机制 StateBackend(状态后端)"></a>3.保存机制 StateBackend(状态后端)</h4><p><strong>默认情况下，State 会保存在 TaskManager 的内存中，CheckPoint 会存储在 JobManager 的内存中</strong>。</p><p>State 和 CheckPoint 的存储位置取决于 StateBackend 的配置。Flink 一共提供 了 3 种 StateBackend 。 </p><p>包 括 基 于 内 存 的 MemoryStateBackend 、 基 于 文 件 系 统 的 FsStateBackend，以及基于 RockDB 作为存储介质的 RocksDBState-Backend</p><h5 id="1-MemoryStateBackend"><a href="#1-MemoryStateBackend" class="headerlink" title="1) MemoryStateBackend"></a>1) MemoryStateBackend</h5><p>基于内存的状态管理具有非常快速和高效的特点，但也具有非常多的限制，最主要的就 是内存的容量限制，一旦存储的状态数据过多就会导致系统内存溢出等问题，从而影响整个 应用的正常运行。同时如果机器出现问题，整个主机内存中的状态数据都会丢失，进而无法 恢复任务中的状态数据。因此从数据安全的角度建议用户尽可能地避免在生产环境中使用 MemoryStateBackend</p><pre><code class="scala">streamEnv.setStateBackend(new MemoryStateBackend(10*1024*1024))</code></pre><h5 id="2）FsStateBackend"><a href="#2）FsStateBackend" class="headerlink" title="2）FsStateBackend"></a>2）FsStateBackend</h5><p>和 MemoryStateBackend 有所不同，FsStateBackend 是基于文件系统的一种状态管理器， 这里的文件系统可以是本地文件系统，也可以是 HDFS 分布式文件系统。FsStateBackend 更 适合任务状态非常大的情况，例如应用中含有时间范围非常长的窗口计算，或 Key&#x2F;value State 状态数据量非常大的场景。</p><pre><code class="scala">streamEnv.setStateBackend(new FsStateBackend(&quot;hdfs://hadoop101:9000/checkpoint/cp1&quot;))</code></pre><h5 id="3）RocksDBStateBackend"><a href="#3）RocksDBStateBackend" class="headerlink" title="3）RocksDBStateBackend"></a>3）RocksDBStateBackend</h5><p>RocksDBStateBackend 是 Flink 中内置的第三方状态管理器，和前面的状态管理器不同， RocksDBStateBackend 需要单独引入相关的依赖包到工程中。</p><pre><code class="xml">&lt;dependency&gt;&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&lt;artifactId&gt;flink-statebackend-rocksdb_2.11&lt;/artifactId&gt;&lt;version&gt;1.9.1&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>RocksDBStateBackend 采用异步的方式进行状态数据的 Snapshot，任务中的状态数据首 先被写入本地 RockDB 中，这样在 RockDB 仅会存储正在进行计算的热数据，而需要进行 CheckPoint 的时候，会把本地的数据直接复制到远端的 FileSystem 中。 与 FsStateBackend 相比，RocksDBStateBackend 在性能上要比 FsStateBackend 高一些，主要是因为借助于 RocksDB 在本地存储了最新热数据，然后通过异步的方式再同步到文件系 统中，但 RocksDBStateBackend 和 MemoryStateBackend 相比性能就会较弱一些。RocksDB 克服了 State 受内存限制的缺点，同时又能够持久化到远端文件系统中，<strong>推荐在生产中使用。</strong></p><pre><code class="scala">streamEnv.setStateBackend(new RocksDBStateBackend (&quot;hdfs://hadoop101:9000/checkpoint/cp2&quot;))</code></pre><h5 id="4）全局配置-StateBacken"><a href="#4）全局配置-StateBacken" class="headerlink" title="4）全局配置 StateBacken"></a>4）全局配置 StateBacken</h5><p>以上的代码都是单 job 配置状态后端，也可以全局配置状态后端，需要修改 <strong>flink-conf.yaml</strong> 配置文件：</p><pre><code class="yaml">state.backend: filesystem/***其中：*filesystem 表示使用 FsStateBackend,*jobmanager 表示使用 MemoryStateBackend*rocksdb 表示使用 RocksDBStateBackend**/state.checkpoints.dir: hdfs://hadoop101:9000/checkpoints//默认情况下，如果设置了 CheckPoint 选项，则 Flink 只保留最近成功生成的 1 个 CheckPoint，而当 Flink 程序失败时，可以通过最近的 CheckPoint 来进行恢复。但是，如 果希望保留多个 CheckPoint，并能够根据实际需要选择其中一个进行恢复，就会更加灵活。 添加如下配置，指定最多可以保存的 CheckPoint 的个数。state.checkpoints.num-retained: 2</code></pre><h4 id="4-Checkpoint-案例"><a href="#4-Checkpoint-案例" class="headerlink" title="4.Checkpoint 案例"></a>4.Checkpoint 案例</h4><p>案例：设置 HDFS 文件系统的状态后端，取消 Job 之后再次恢复 Job。</p><pre><code class="scala">object CheckpointOnFsBackend &#123;def main(args: Array[String]): Unit = &#123;//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv: StreamExecutionEnvironment =StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.enableCheckpointing(5000)streamEnv.setStateBackend(new FsStateBackend(&quot;hdfs://hadoop101:9000/checkpoint/cp1&quot;))streamEnv.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)streamEnv.getCheckpointConfig.setCheckpointTimeout(50000)streamEnv.getCheckpointConfig.setMaxConcurrentCheckpoints(1)streamEnv.getCheckpointConfig.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)streamEnv.getCheckpointConfig.setTolerableCheckpointFailureNumber(1)streamEnv.setParallelism(1)import org.apache.flink.streaming.api.scala._//读取数据得到DataStreamval stream = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888)stream.flatMap(_.split(&quot; &quot;)).map((_,1)).keyBy(0).sum(1).print()streamEnv.execute(&quot;wc&quot;) //启动流计算&#125;&#125;</code></pre><p>1.打包运行，接收数据</p><p>2.查看执行结果，cancel掉job</p><p><img src="/flink/flink/image-20220522110948658.png" alt="image-20220522110948658"></p><p><img src="/flink/flink/image-20220522111006190.png" alt="image-20220522111006190"></p><p>3.重新启动job，选择日志目录</p><pre><code class="shell">[root@hadoop101 bin]# ./flink run -d -shdfs://hadoop101:9000/checkpoint/cp1/b38e35788eecf3053d4a87d52e97d22d/chk-272 -c com.bjsxt.flink.state.CheckpointOnFsBackend/home/Flink-Demo-1.0-SNAPSHOT.jar</code></pre><h4 id="5-SavePoint"><a href="#5-SavePoint" class="headerlink" title="5.SavePoint"></a>5.SavePoint</h4><p>Savepoints 是检查点的一种特殊实现，底层实现其实也是使用 Checkpoints 的机制。 Savepoints 是用户以手工命令的方式触发 Checkpoint,并将结果持久化到指定的存储路径 中，其主要目的是帮助用户在升级和维护集群过程中保存系统中的状态数据，避免因为停机 运维或者升级应用等正常终止应用的操作而导致系统无法恢复到原有的计算状态的情况，从 而无法实现从端到端的 Excatly-Once 语义保证</p><h5 id="1-配置-Savepoints"><a href="#1-配置-Savepoints" class="headerlink" title="1.配置 Savepoints"></a>1.配置 Savepoints</h5><p>在 flink-conf.yaml 中配置 SavePoint 存储的位置，设置后，如果要创建指定 Job 的 SavePoint，可以不用在手动执行命令时指定 SavePoint 的位置</p><pre><code class="yaml">state.savepoints.dir: hdfs:/hadoop101:9000/savepoints</code></pre><h5 id="2-在代码中设置算子ID"><a href="#2-在代码中设置算子ID" class="headerlink" title="2.在代码中设置算子ID"></a>2.在代码中设置算子ID</h5><p>为了能够在作业的不同版本之间以及 Flink 的不同版本之间顺利升级，强烈推荐程序员 通过手动给算子赋予 ID，这些 ID 将用于确定每一个算子的状态范围。如果不手动给各算子 指定 ID，则会由 Flink 自动给每个算子生成一个 ID。而这些自动生成的 ID 依赖于程序的结 构，并且对代码的更改是很敏感的。因此，强烈建议用户手动设置 ID</p><pre><code class="scala">object TestSavepoints &#123;def main(args: Array[String]): Unit = &#123;//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv: StreamExecutionEnvironment =StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)import org.apache.flink.streaming.api.scala._//读取数据得到DataStreamval stream: DataStream[String] =streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).uid(&quot;mySource-001&quot;)stream.flatMap(_.split(&quot; &quot;)).uid(&quot;flatMap-001&quot;).map((_,1)).uid(&quot;map-001&quot;).keyBy(0).sum(1).uid(&quot;sum-001&quot;).print()streamEnv.execute(&quot;wc&quot;) //启动流计算&#125;&#125;</code></pre><h5 id="3-触发-SavePoint"><a href="#3-触发-SavePoint" class="headerlink" title="3.触发 SavePoint"></a>3.触发 SavePoint</h5><pre><code class="shell">//先启动Job[root@hadoop101 bin]# ./flink run -c com.bjsxt.flink.state.TestSavepoints -d /home/Flink-Demo-1.0-SNAPSHOT.jar//触发SavePoint，再取消Job [root@hadoop101 bin]# ./flink savepoint 6ecb8cfda5a5200016ca6b01260b94ce[root@hadoop101 bin]# ./flink cancel 6ecb8cfda5a5200016ca6b01260b94ce</code></pre><p><img src="/flink/flink/image-20220519161913041.png" alt="image-20220519161913041"></p><h5 id="4-从-SavePoint-启动-Job"><a href="#4-从-SavePoint-启动-Job" class="headerlink" title="4.从 SavePoint 启动 Job"></a>4.从 SavePoint 启动 Job</h5><pre><code class="shell">[root@hadoop101 bin]# ./flink run -shdfs://hadoop101:9000/savepoints/savepoint-6ecb8c-e56ccb88576a -ccom.bjsxt.flink.state.TestSavepoints -d /home/Flink-Demo-1.0-SNAPSHOT.jar</code></pre><p><img src="/flink/flink/image-20220519161958774.png" alt="image-20220519161958774"></p><h2 id="Flink-Window-窗口-详解"><a href="#Flink-Window-窗口-详解" class="headerlink" title="Flink Window(窗口)详解"></a>Flink Window(窗口)详解</h2><p>Windows 计算是流式计算中非常常用的数据计算方式之一，通过按照固定时间或长度将 数据流切分成不同的窗口，然后对数据进行相应的聚合运算，从而得到一定时间范围内的统 计结果。</p><h3 id="1-Window-分类"><a href="#1-Window-分类" class="headerlink" title="1.Window 分类"></a>1.Window 分类</h3><h4 id="1-Global-Window-和-Keyed-Window"><a href="#1-Global-Window-和-Keyed-Window" class="headerlink" title="1.Global Window 和 Keyed Window"></a>1.Global Window 和 Keyed Window</h4><p>在运用窗口计算时，Flink根据上游数据集是否为KeyedStream类型，对应的Windows 也 会有所不同。 </p><ul><li>Keyed Window：上游数据集如果是 KeyedStream 类型，则调用 DataStream API 的 window() 方法，数据会根据 Key 在不同的 Task 实例中并行分别计算，最后得出针对每个 Key 统 计的结果。 </li><li>Global Window：如果是 Non-Keyed 类型，则调用 WindowsAll()方法，所有的数据都会在窗口算子中由到一个 Task 中计算，并得到全局统计结果。</li></ul><pre><code class="scala">//读取文件数据val data = streamEnv.readTextFile(getClass.getResource(&quot;/station.log&quot;).getPath).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;)//Global Windowdata.windowAll(自定义的WindowAssigner)//Keyed Windowdata.keyBy(_.sid).window(自定义的WindowAssigner)</code></pre><h4 id="2-Time-Window-和-Count-Window"><a href="#2-Time-Window-和-Count-Window" class="headerlink" title="2.Time Window 和 Count Window"></a>2.Time Window 和 Count Window</h4><p>基于业务数据的方面考虑，Flink 又支持两种类型的窗口，一种是基于时间的窗口叫 Time Window。还有一种基于输入数据数量的窗口叫 Count Window</p><h5 id="Time-Window-时间窗口"><a href="#Time-Window-时间窗口" class="headerlink" title="Time Window(时间窗口)"></a>Time Window(时间窗口)</h5><p>根据不同的业务场景，Time Window 也可以分为三种类型，分别是滚动窗口(Tumbling Window)、滑动窗口（Sliding Window）和会话窗口（Session Window）</p><h6 id="滚动窗口-Tumbling-Window"><a href="#滚动窗口-Tumbling-Window" class="headerlink" title="滚动窗口(Tumbling Window)"></a>滚动窗口(Tumbling Window)</h6><p>滚动窗口是根据固定时间进行切分，且窗口和窗口之间的元素互不重叠。这种类型的窗 口的最大特点是比较简单。只需要指定一个窗口长度（window size）</p><p><img src="/flink/flink/image-20220519164544637.png" alt="image-20220519164544637"></p><pre><code class="scala">//每隔5秒统计每个基站的日志数量data.map(stationLog=&gt;((stationLog.sid,1))).keyBy(_._1).timeWindow(Time.seconds(5))//.window(TumblingEventTimeWindows.of(Time.seconds(5))).sum(1) //聚合//其中时间间隔可以是 Time.milliseconds(x)、Time.seconds(x)或 Time.minutes(x)。</code></pre><h6 id="滑动窗口（Sliding-Window"><a href="#滑动窗口（Sliding-Window" class="headerlink" title="滑动窗口（Sliding Window)"></a>滑动窗口（Sliding Window)</h6><p>滑动窗口也是一种比较常见的窗口类型，其特点是在滚动窗口基础之上增加了窗口滑动 时间（Slide Time），且允许窗口数据发生重叠。当 Windows size 固定之后，窗口并不像 滚动窗口按照 Windows Size 向前移动，而是根据设定的 Slide Time 向前滑动。窗口之间的 数据重叠大小根据 Windows size 和 Slide time 决定，当 Slide time 小于 Windows size 便会发生窗口重叠，Slide size 大于 Windows size 就会出现窗口不连续，数据可能不能在 任何一个窗口内计算，Slide size 和 Windows size 相等时，Sliding Windows 其实就是 Tumbling Windows</p><p><img src="/flink/flink/image-20220519165524568.png" alt="image-20220519165524568"></p><pre><code class="scala">//每隔3秒计算最近5秒内，每个基站的日志数量data.map(stationLog=&gt;((stationLog.sid,1))).keyBy(_._1).timeWindow(Time.seconds(5),Time.seconds(3))//.window(SlidingEventTimeWindows.of(Time.seconds(5),Time.seconds(3))).sum(1)</code></pre><h6 id="会话窗口（Session-Window）"><a href="#会话窗口（Session-Window）" class="headerlink" title="会话窗口（Session Window）"></a>会话窗口（Session Window）</h6><p>会话窗口（Session Windows）主要是将某段时间内活跃度较高的数据聚合成一个窗口 进行计算，窗口的触发的条件是 Session Gap，是指在规定的时间内如果没有数据活跃接入， 则认为窗口结束，然后触发窗口计算结果。需要注意的是如果数据一直不间断地进入窗口， 也会导致窗口始终不触发的情况。</p><p>与滑动窗口、滚动窗口不同的是，Session Windows 不需 要有固定 windows size 和 slide time，只需要定义 session gap，来规定不活跃数据的时间上限即可。</p><p><img src="/flink/flink/image-20220519170215749.png" alt="image-20220519170215749"></p><pre><code class="scala">//3秒内如果没有数据进入，则计算每个基站的日志数量data.map(stationLog=&gt;((stationLog.sid,1))).keyBy(_._1).window(EventTimeSessionWindows.withGap(Time.seconds(3))).sum(1)</code></pre><h4 id="Count-Window（数量窗口）"><a href="#Count-Window（数量窗口）" class="headerlink" title="Count Window（数量窗口）"></a>Count Window（数量窗口）</h4><p>Count Window 也有滚动窗口、滑动窗口等。由于使用比较少，在课程中不再赘述了。</p><h3 id="2-Window-的-API"><a href="#2-Window-的-API" class="headerlink" title="2.Window 的 API"></a>2.Window 的 API</h3><p>在以后的实际案例中 Keyed Window 使用最多，所以我们需要掌握 Keyed Window 的算子， 在每个窗口算子中包含了 ：</p><p>Windows Assigner、Windows Trigger（窗口触发器）、Evictor （数据剔除器）、Lateness（时延设定）、Output Tag（输出标签）以及 Windows Funciton 等组成部分，其中 Windows Assigner 和 Windows Funciton 是所有窗口算子必须指定的属性， 其余的属性都是根据实际情况选择指定。</p><pre><code class="scala">stream.keyBy(...) // 是Keyed类型数据集.window(...) //指定窗口分配器类型[.trigger(...)] //指定触发器类型（可选）[.evictor(...)] //指定evictor或者不指定（可选）[.allowedLateness(...)] //指定是否延迟处理数据（可选）[.sideOutputLateData(...)] //指定Output Lag（可选）.reduce/aggregate/fold/apply() //指定窗口计算函数[.getSideOutput(...)] //根据Tag输出数据（可选） Windows Assigner：指定窗口的类型，定义如何将数据流分配到一个或多个窗口； Windows Trigger：指定窗口触发的时机，定义窗口满足什么样的条件触发计算； Evictor：用于数据剔除； allowedLateness：标记是否处理迟到数据，当迟到数据到达窗口中是否触发计算； Output Tag：标记输出标签，然后在通过 getSideOutput 将窗口中的数据根据标签输出； Windows Funciton：定义窗口上数据处理的逻辑，例如对数据进行 sum 操作。</code></pre><h3 id="3-窗口聚合函数"><a href="#3-窗口聚合函数" class="headerlink" title="3.窗口聚合函数"></a>3.窗口聚合函数</h3><p>如果定义了 Window Assigner 之后，下一步就可以定义窗口内数据的计算逻辑，这也就 是 Window Function 的定义。Flink 中提供了四种类型的 Window Function，分别为 ReduceFunction、AggregateFunction 以及 ProcessWindowFunction,（sum 和 max)等</p><p>前三种类型的 Window Fucntion 按照计算原理的不同可以分为两大类：</p><ul><li><p>一类是增量聚合函数：对应有 ReduceFunction、AggregateFunction；</p></li><li><p>另一类是全量窗口函数，对应有 ProcessWindowFunction（还有 WindowFunction）。</p></li></ul><p>增量聚合函数计算性能较高，占用存储空间少，主要因为基于中间状态的计算结果，窗 口中只维护中间结果状态值，不需要缓存原始数据。</p><p>而全量窗口函数使用的代价相对较高， 性能比较弱，主要因为此时算子需要对所有属于该窗口的接入数据进行缓存，然后等到窗口 触发的时候，对所有的原始数据进行汇总计算</p><h4 id="ReduceFunction"><a href="#ReduceFunction" class="headerlink" title="ReduceFunction"></a>ReduceFunction</h4><p>ReduceFunction 定义了对输入的两个相同类型的数据元素按照指定的计算方法进行聚 合的逻辑，然后输出类型相同的一个结果元素。</p><pre><code class="scala">//每隔5秒统计每个基站的日志数量data.map(stationLog=&gt;((stationLog.sid,1))).keyBy(_._1).window(TumblingEventTimeWindows.of(Time.seconds(5))).reduce((v1,v2)=&gt;(v1._1,v1._2+v2._2))</code></pre><h4 id="AggregateFunction"><a href="#AggregateFunction" class="headerlink" title="AggregateFunction"></a>AggregateFunction</h4><p>和 ReduceFunction 相似，AggregateFunction 也是基于中间状态计算结果的增量计算 函数，但 AggregateFunction 在窗口计算上更加通用。AggregateFunction 接口相对 ReduceFunction 更加灵活，实现复杂度也相对较高。</p><p>AggregateFunction 接口中定义了三个 需要复写的方法，其中 add()定义数据的添加逻辑，getResult 定义了根据 accumulator 计算结果的逻辑，merge 方法定义合并 accumulator 的逻辑</p><pre><code class="scala">//每隔3秒计算最近5秒内，每个基站的日志数量data.map(stationLog=&gt;((stationLog.sid,1))).keyBy(_._1).timeWindow(Time.seconds(5),Time.seconds(3)).aggregate(new AggregateFunction[(String,Int),(String,Long),(String,Long)] &#123;override def createAccumulator() = (&quot;&quot;,0)override def add(in: (String, Int), acc: (String, Long)) = &#123;(in._1,acc._2+in._2)&#125;override def getResult(acc: (String, Long)) = accoverride def merge(acc: (String, Long), acc1: (String, Long)) = &#123;(acc._1,acc1._2+acc._2)&#125;&#125;)//* @param &lt;IN&gt;被聚合的值的类型(输入值)//  @param &lt;ACC&gt;累加器的类型(中间聚合状态)。//* @param &lt;OUT&gt;聚合结果的类型</code></pre><h4 id="ProcessWindowFunction"><a href="#ProcessWindowFunction" class="headerlink" title="ProcessWindowFunction"></a>ProcessWindowFunction</h4><p>前面提到的 ReduceFunction 和 AggregateFunction 都是基于中间状态实现增量计算的 窗口函数，虽然已经满足绝大多数场景，但在某些情况下，统计更复杂的指标可能需要依赖 于窗口中所有的数据元素，或需要操作窗口中的状态数据和窗口元数据，这时就需要使用到 ProcessWindowsFunction，ProcessWindowsFunction 能够更加灵活地支持基于窗口全部数 据 元 素 的 结 果 计 算 ， 例 如 对 整 个 窗 口 数 据 排 序 取 TopN， 这 样 的 需 要 就 必 须 使 用 ProcessWindowFunction。</p><pre><code class="scala">//每隔5秒统计每个基站的日志数量data.map(stationLog=&gt;((stationLog.sid,1))).keyBy(_._1).timeWindow(Time.seconds(5)).process(newProcessWindowFunction[(String,Int),(String,Int),String,TimeWindow] &#123;override def process(key: String, context: Context, elements: Iterable[(String, Int)], out: Collector[(String, Int)]): Unit = &#123;println(&quot;-------&quot;)out.collect((key,elements.size))&#125;&#125;).print()</code></pre><h2 id="Flink-Time-详解"><a href="#Flink-Time-详解" class="headerlink" title="Flink Time 详解"></a>Flink Time 详解</h2><p>对于流式数据处理，最大的特点是数据上具有时间的属性特征，Flimk 根据时间产生的位置不同，将时间区分为三种时间语义，分别为事件生成时间（Event Time）、事件接入时 间（Ingestion Time）和事件处理时间（Processing Time）。 </p><ul><li><p>Event Time：事件产生的时间，它通常由事件中的时间戳描述。 </p></li><li><p>Ingestion Time：事件进入 Flink 的时间。 </p></li><li><p>Processing Time：事件被处理时当前系统的时间。</p></li></ul><h3 id="1-时间语义-Time"><a href="#1-时间语义-Time" class="headerlink" title="1.时间语义 Time"></a>1.时间语义 Time</h3><p>数据从终端产生，或者从系统中产生的过程中生成的时间为事件生成时间，当数据经过 消息中间件传入到 Flink 系统中，在 DataSource 中接入的时候会生成事件接入时间，当数据在 Flink 系统中通过各个算子实例执行转换操作的过程中，算子实例所在系统的时间为数据处理时间。Flink 已经支持这三种类型时间概念，用户能够根据需要选择时间类型作为对 流式数据的依据，这种情况极大地增强了对事件数据处理的灵活性和准确性。</p><p><img src="/flink/flink/image-20220519185923161.png" alt="image-20220519185923161"></p><h4 id="1）设置时间语义"><a href="#1）设置时间语义" class="headerlink" title="1）设置时间语义"></a>1）设置时间语义</h4><p>在 Flink 中默认情况下使用是 Process Time 时间语义，如果用户选择使用 Event Time 或 者 Ingestion Time 语 义 ， 则 需 要 在 创 建 的 StreamExecutionEnvironment 中 调 用 setStreamTimeCharacteristic() 方 法 设 定 系 统 的 时 间 概 念 ， 如 下 代 码 使 用 TimeCharacteristic.EventTime 作为系统的时间语义</p><pre><code class="scala">//设置使用EventTimestreamEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)//设置使用IngestionTimestreamEnv.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)//注意：但是上面的代码还没有指定具体的时间到底是什么值，所以后面还有代码需要设置！</code></pre><h3 id="2-WaterMark-水位"><a href="#2-WaterMark-水位" class="headerlink" title="2.WaterMark 水位"></a>2.WaterMark 水位</h3><p>在使用 EventTime 处理 Stream 数据的时候会遇到数据乱序的问题，流处理从 Event（事 件）产生，流经 Source，再到 Operator，这中间需要一定的时间。虽然大部分情况下，传 输到 Operator 的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络延迟等原因而导致乱序的产生，特别是使用 Kafka 的时候，多个分区之间的数据无法保证有序。因此， 在进行 Window 计算的时候，不能无限期地等下去，必须要有个机制来保证在特定的时间后， 必须触发 Window 进行计算，这个特别的机制就是 Watermark（水位线）。Watermark 是用于 处理乱序事件的。</p><p><img src="/flink/flink/image-20220519213500035.png" alt="image-20220519213500035"></p><h4 id="1）Watermark-原理"><a href="#1）Watermark-原理" class="headerlink" title="1）Watermark 原理"></a>1）Watermark 原理</h4><p>在 Flink 的窗口处理过程中，如果确定全部数据到达，就可以对 Window 的所有数据做 窗口计算操作（如汇总、分组等），如果数据没有全部到达，则继续等待该窗口中的数据全 部到达才开始处理。这种情况下就需要用到水位线（WaterMarks）机制，它能够衡量数据处理进度（表达数据到达的完整性），保证事件数据（全部）到达 Flink 系统，或者在乱序及 延迟到达时，也能够像预期一样计算出正确并且连续的结果。当任何 Event 进入到 Flink 系统时，会根据当前最大事件时间产生 Watermarks 时间戳。</p><p>注意：<strong>Watermark 本质可以理解成一个延迟触发机制。</strong></p><p>当Flink接收到每一条数据时，都会产生一条Watermark，这条Watermark就等于当前所有到达数据中的maxEventTime-延迟时长，也就是说Watermark是由数据携带的，一旦数据携带的Watermark比当前未触发的窗口停止时间要晚，就会触发相应窗口的执行。由于Watermark是由数据携带的，因此，如果运行过程中无法获得新的数据，你们没有被触发的窗口将永远都不会被触发。</p><p>1.有序流watermarker：Watermaker设置为0</p><p><img src="/flink/flink/image-20220519215223201.png" alt="image-20220519215223201"></p><p>2.乱序流Watermarker：Watermaker设置为2</p><p><img src="/flink/flink/image-20220519215308959.png" alt="image-20220519215308959"></p><p>上图中，我们设置允许最大延迟到达时间为2s，所以时间戳为7s的事件对应的Watermark是5s，时间戳为12s的事件Watermark是10s，如果窗口1是1s-5s，窗口2是6s-10s，那么时间戳为7s的事件到达时的watermarker恰好触发窗口1，时间戳为12s的事件到达时Watermark恰好触发窗口2.</p><p>3.并行数据流中的 Watermark</p><p>在多并行度的情况下，Watermark 会有一个对齐机制，这个对齐机制会取所有 Channel 中最小的 Watermark。</p><p><img src="/flink/flink/image-20220519220620668.png" alt="image-20220519220620668"></p><h4 id="2）引入-Watermark-和-EventTim"><a href="#2）引入-Watermark-和-EventTim" class="headerlink" title="2）引入 Watermark 和 EventTim"></a>2）引入 Watermark 和 EventTim</h4><h5 id="1-有序数据流中引入-Watermark-和-EventTime"><a href="#1-有序数据流中引入-Watermark-和-EventTime" class="headerlink" title="1.有序数据流中引入 Watermark 和 EventTime"></a>1.有序数据流中引入 Watermark 和 EventTime</h5><p>对于有序的数据，代码比较简洁，主要需要从源Event 中抽取 EventTime</p><pre><code class="scala">//读取文件数据val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;)//根据EventTime有序的数据流data.assignAscendingTimestamps(_.callTime)//StationLog对象中抽取EventTime就是callTime属性</code></pre><h5 id="2-乱序序数据流中引入-Watermark-和-EventTime"><a href="#2-乱序序数据流中引入-Watermark-和-EventTime" class="headerlink" title="2.乱序序数据流中引入 Watermark 和 EventTime"></a>2.乱序序数据流中引入 Watermark 和 EventTime</h5><p>对于乱序数据流，有两种常见的引入方法：周期性和间断性</p><h6 id="1-With-Periodic（周期性的）"><a href="#1-With-Periodic（周期性的）" class="headerlink" title="1.With Periodic（周期性的）"></a>1.With Periodic（周期性的）</h6><p><strong>Watermark 周期性地生成 Watermark 的生成，默认是 100ms</strong>。不论是否有数据的流入都会周期性的调用getCurrentWatermark（）方法。</p><p>每隔 N 毫秒自动向流里注入一个 Watermark，时间间隔由streamEnv.getConfig.setAutoWatermarkInterval()  决定。最简单 的写法如下：</p><pre><code class="scala">//读取文件数据val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;)//如果EventTime是乱序的，需要考虑一个延迟时间t//当前代码设置的延迟时间为3秒data.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[StationLog](Time.seconds(3)) //延迟时间&#123;override def extractTimestamp(element: StationLog) = &#123;element.callTime //设置EventTime的值&#125;&#125;)</code></pre><p>另外还有一种复杂的写法：</p><pre><code class="scala">//读取文件数据val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;)//如果EventTime是乱序的，需要考虑一个延迟时间t//当前代码设置的延迟时间为3秒data.assignTimestampsAndWatermarks(new MyCustomerPeriodicWatermark(3000L)) //自定义延迟3秒&#125;class MyCustomerPeriodicWatermark(delay: Long) extendsAssignerWithPeriodicWatermarks[StationLog]&#123;var maxTime :Long=0override def getCurrentWatermark: Watermark = &#123;new Watermark(maxTime-delay) //创建水位线&#125;override def extractTimestamp(element: StationLog, previousElementTimestamp: Long): Long = &#123;maxTime=maxTime.max(element.callTime) //maxtime永远是最大值element.callTime&#125;&#125;</code></pre><h6 id="2-With-Punctuated（间断性的）-Watermark"><a href="#2-With-Punctuated（间断性的）-Watermark" class="headerlink" title="2.With Punctuated（间断性的） Watermark"></a>2.With Punctuated（间断性的） Watermark</h6><p>间断性的生成 Watermark 一般是<strong>基于某些事件触发</strong> Watermark 的生成和发送，比如：在 我们的基站数据中，有一个基站的 CallTime 总是没有按照顺序传入，其他基站的时间都是 正常的，那我们需要对这个基站来专门生成 Watermark。</p><pre><code class="scala">//读取文件数据val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;)//只有station_1的EventTime是无序的，所以只需要针对station_1做处理//当前代码设置station_1基站的延迟处理时间为3秒data.assignTimestampsAndWatermarks(new MyCustomerPunctuatedWatermarks(3000L)) //自定义延迟｝class MyCustomerPunctuatedWatermarks(delay:Long) extends AssignerWithPunctuatedWatermarks[StationLog]&#123;var maxTime :Long=0override def checkAndGetNextWatermark(element: StationLog, extractedTimestamp: Long): Watermark = &#123;if(element.sid.equals(&quot;station_1&quot;))&#123;//当基站ID为:station_1 才生成水位线maxTime =maxTime.max(extractedTimestamp)new Watermark(maxTime-delay)&#125;else&#123;return null //其他情况下不返回水位线&#125;&#125;override def extractTimestamp(element: StationLog, previousElementTimestamp:Long): Long = &#123;element.callTime //抽取EventTime的值&#125;&#125;</code></pre><h4 id="3-WaterMark案例"><a href="#3-WaterMark案例" class="headerlink" title="3.WaterMark案例"></a>3.WaterMark案例</h4><p>需求：每隔 5 秒中统计一下最近 10 秒内每个基站中通话时间最长的一次通话发生的呼叫时间、主叫号码，被叫号码，通话时长。并且还得告诉我到底是哪个时间范围（10 秒） 内的。 注意：基站日志数据传入的时候是无序的，通过观察发现时间最多延迟了 3 秒。</p><pre><code class="scala">/** 每隔5秒中统计一下最近10秒内每个基站中通话时间最长的一次通话发生的* 呼叫时间、主叫号码，被叫号码，通话时长。* 并且还得告诉我到底是哪个时间范围（10秒）内的。*/object MaxLongCallTime &#123;def main(args: Array[String]): Unit = &#123;//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv: StreamExecutionEnvironment =StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)streamEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._//读取文件数据val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)new StationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;).assignTimestampsAndWatermarks( //引入Watermarknew BoundedOutOfOrdernessTimestampExtractor[StationLog](Time.seconds(3))&#123;//延迟3秒override def extractTimestamp(element: StationLog) = &#123;element.callTime&#125;&#125;)//分组，开窗处理data.keyBy(_.sid).timeWindow(Time.seconds(10),Time.seconds(5))//reduce 函数做增量聚合 ,MaxTimeAggregate能做到来一条数据处理一条，//ReturnMaxTime 在窗口触发的时候调用reduce(new MaxTimeReduce,new ReturnMaxTime).print()streamEnv.execute()&#125;class MaxTimeReduce extends ReduceFunction[StationLog]&#123;override def reduce(t: StationLog, t1: StationLog): StationLog = &#123;//通话时间比较if(t.duration &gt; t1.duration) t else t1&#125;&#125;class ReturnMaxTime extendsWindowFunction[StationLog,String,String,TimeWindow]&#123;override def apply(key: String, window: TimeWindow, input: Iterable[StationLog], out: Collector[String]): Unit = &#123;var sb =new StringBuildersb.append(&quot;窗口范围是：&quot;).append(window.getStart).append(&quot;----&quot;).append(window.getEnd)sb.append(&quot;\n&quot;)sb.append(&quot;通话日志:&quot;).append(input.iterator.next())out.collect(sb.toString())&#125;&#125;&#125;</code></pre><p><img src="/flink/flink/image-20220519232931477.png" alt="image-20220519232931477"></p><p><img src="/flink/flink/image-20220519232953021.png" alt="image-20220519232953021"></p><p>结论：事件事件event time 窗口计算时，当前eventtime+10秒后的数据会触发窗口计算，但不会将这条数据包含在内。</p><h3 id="3-Window-的-allowedLateness和sideoutput"><a href="#3-Window-的-allowedLateness和sideoutput" class="headerlink" title="3.Window 的 allowedLateness和sideoutput"></a>3.Window 的 allowedLateness和sideoutput</h3><p>基于 Event-Time 的窗口处理流式数据，虽然提供了 Watermark 机制，却只能在一定程 度上解决了数据乱序的问题。默认情况下，当watermark通过end-of-window激活window计算结束之后，再有之前的数据到达时，这些数据会被删除。</p><p>为了避免有些迟到的数据被删除，因此产生了allowedLateness，使用allowedLateness延迟销毁窗口，允许有一段时间（也是以event time来衡量）来等待之前的数据到达，以便再次处理这些数据。此时就需要使用 Allowed Lateness 机制来对迟到的数据进行额外的处理。</p><p>allowedLateness的迟到流数据，也是通过.sideOutputLateData(outputTag)和result.getSideOutput(outputTag)的侧输出流方式输出的，通过使用 sideOutputLateData（OutputTag）来标记迟到数据计算的结果，然后使用 getSideOutput（lateOutputTag）从窗口结果中获取 lateOutputTag 标签对应的数据，之后转成独立的 DataStream 数据集进行处理，创建 late-data 的 OutputTag，再通过该标签从窗口结果中将迟到数据筛选出来。<strong>allowedLateness只针对eventTime</strong>，因为processingTime不存在延时的情况。</p><p>注意：如果有 Watermark 同时也有 Allowed Lateness。那么窗口函数再次触发的条件 是：watermark &lt; end-of-window + allowedLatenes</p><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><pre><code class="scala">object LateDataOnWindow &#123;def main(args: Array[String]): Unit = &#123;//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv: StreamExecutionEnvironment =StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)streamEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._//读取文件数据val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;).assignTimestampsAndWatermarks( //引入Watermarknew BoundedOutOfOrdernessTimestampExtractor[StationLog](Time.seconds(2))&#123;//延迟2秒override def extractTimestamp(element: StationLog) = &#123;element.callTime&#125;&#125;)//分组，开窗处理//定义一个侧输出流 的标签var lateTag =new OutputTag[StationLog](&quot;late&quot;)val mainStream: DataStream[String] = data.keyBy(_.sid).timeWindow(Time.seconds(10), Time.seconds(5))//注意：只要符合watermark &lt; end-of-window + allowedLateness之内到达的数据，都会被再次触发窗口的计算//超过之外的迟到数据会被放入侧输出流.allowedLateness(Time.seconds(5)) //允许数据迟到5秒.sideOutputLateData(lateTag)//窗口watermark和allowedLateness之后依然迟到的流数据.aggregate(new AggregateCount, new OutputResult)//自定义操作    mainStream.getSideOutput(lateTag).print(&quot;late&quot;)//迟到很久的数据可以另外再处理mainStream.print(&quot;main&quot;)streamEnv.execute()&#125;//三个参数，in，累加器类型，outclass AggregateCount extends AggregateFunction[StationLog,Long,Long]&#123;override def createAccumulator(): Long = 0override def add(in: StationLog, acc: Long): Long = acc+1override def getResult(acc: Long): Long = accoverride def merge(acc: Long, acc1: Long): Long = acc+acc1&#125;class OutputResult extends WindowFunction[Long,String,String,TimeWindow]&#123;override def apply(key: String, window: TimeWindow, input: Iterable[Long], out:Collector[String]): Unit = &#123;var sb =new StringBuildersb.append(&quot;窗口范围是：&quot;).append(window.getStart).append(&quot;----&quot;).append(window.getEnd)sb.append(&quot;\n&quot;)sb.append(&quot;当前基站是：&quot;).append(key).append(&quot; 呼叫数量是: &quot;).append(input.iterator.next())out.collect(sb.toString())&#125;&#125;&#125;</code></pre><h2 id="TableAPI-和-Flink-SQL"><a href="#TableAPI-和-Flink-SQL" class="headerlink" title="TableAPI 和 Flink SQL"></a>TableAPI 和 Flink SQL</h2><p>Flink 也提供了关系型编程接口 Table API 以及基于 Table API 的 SQL API，让用户能够通过使用结构化编程 接口高效地构建 Flink 应用。同时 Table API 以及 SQL 能够统一处理批量和实时计算业务， 无须切换修改任何应用代码就能够基于同一套 API 编写流式应用和批量应用，从而达到真正 意义的批流统一。</p><h3 id="1-开发环境构建"><a href="#1-开发环境构建" class="headerlink" title="1.开发环境构建"></a>1.开发环境构建</h3><p>在 Flink 1.9 中，Table 模块迎来了核心架构的升级，引入了阿里巴巴 Blink 团队贡献的诸多功能，取名叫： <strong>Blink Planner</strong>。在使用 Table API 和 SQL 开发 Flink 应用之前， 通过添加 Maven 的依赖配置到项目中，在本地工程中引入相应的依赖库，库中包含了 Table API 和 SQL 接口。</p><pre><code class="xml">&lt;dependency&gt;&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&lt;artifactId&gt;flink-table-planner_2.11&lt;/artifactId&gt;&lt;version&gt;1.9.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt;&lt;version&gt;1.9.1&lt;/version&gt;&lt;/dependency&gt;</code></pre><h3 id="2-TableEnvironment"><a href="#2-TableEnvironment" class="headerlink" title="2.TableEnvironment"></a>2.TableEnvironment</h3><p>和 DataStream API 一样，Table API 和 SQL 中具有相同的基本编程模型。首先需要构 建对应的 TableEnviroment 创建关系型编程环境，才能够在程序中使用 Table API 和 SQL 来编写应用程序，另外 Table API 和 SQL 接口可以在应用中同时使用，Flink SQL 基于 Apache Calcite 框架实现了 SQL 标准协议，是构建在 Table API 之上的更高级接口。 首先需要在环境中创建 TableEnvironment 对象，TableEnvironment 中提供了注册内部 表、执行 Flink SQL 语句、注册自定义函数等功能。根据应用类型的不同，TableEnvironment 创建方式也有所不同，但是都是通过调用 create()方法创建。 流计算环境下创建 TableEnviroment：</p><pre><code class="scala">//初始化Flink的Streaming（流计算）上下文执行环境val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment//初始化Table API的上下文环境val tableEvn =StreamTableEnvironment.create(streamEnv)</code></pre><p>在 Flink1.9 之后由于引入了 Blink Planner，还可以为：</p><pre><code class="scala">val bsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build()val bsTableEnv = StreamTableEnvironment.create(streamEnv, bsSettings)</code></pre><p>注意：Flink 社区完整保留原有 Flink Planner (Old Planner)，同时又引入了新的 Blink Planner，用户可以自行选择使用 Old Planner 还是 Blink Planner。官方推荐暂时 使用 Old Planner。</p><h3 id="3-Table-API"><a href="#3-Table-API" class="headerlink" title="3.Table API"></a>3.Table API</h3><p>在 Flink 中创建一张表有两种方法： </p><ol><li>从一个文件中导入表结构（Structure）（常用于批计算）（静态） </li><li>从 DataStream 或者 DataSet 转换成 Table （动态)</li></ol><h4 id="1-创建Table"><a href="#1-创建Table" class="headerlink" title="1.创建Table"></a>1.创建Table</h4><h5 id="从文件中创建-Table（静态表）"><a href="#从文件中创建-Table（静态表）" class="headerlink" title="从文件中创建 Table（静态表）"></a>从文件中创建 Table（静态表）</h5><p>Flink 允许用户从本地或者分布式文件系统中读取和写入数据，在 Table API 中可以通 过 CsvTableSource 类来创建，只需指定相应的参数即可。但是文件格式必须是 CSV 格式的。 其 他 文 件 格 式 也 支 持 （ 在 Flink 还 有 Connector 的 来 支 持 其 他 格 式 或 者 自 定 义 TableSource）。</p><pre><code class="scala">object TableApiExercise &#123;  def main(args: Array[String]): Unit = &#123;    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment    val settings: EnvironmentSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build()    val tableEnvironment: StreamTableEnvironment = StreamTableEnvironment.create(environment, settings)    val tableSouce = new CsvTableSource(      &quot;station.log&quot;,      Array[String](&quot;sid&quot;, &quot;callOut&quot;, &quot;callIn&quot;, &quot;callType&quot;, &quot;callTime&quot;, &quot;duration&quot;),      Array(Types.STRING, Types.STRING, Types.STRING, Types.STRING, Types.LONG, Types.LONG)    )    tableEnvironment.registerTableSource(&quot;t_table&quot;,tableSouce)    tableEnvironment.scan(&quot;t_table&quot;).printSchema()</code></pre><p><img src="/flink/flink/image-20220523001615794.png" alt="image-20220523001615794"></p><h5 id="从-DataStream-中创建-Table（动态表"><a href="#从-DataStream-中创建-Table（动态表" class="headerlink" title="从 DataStream 中创建 Table（动态表)"></a>从 DataStream 中创建 Table（动态表)</h5><p>前面已经知道 Table API 是构建在 DataStream API 和 DataSet API 之上的一层更高级 的抽象，因此用户可以灵活地使用 Table API 将 Table 转换成 DataStream 或 DataSet 数据集，也可以将 DataSteam 或 DataSet 数据集转换成 Table，这和 Spark 中的 DataFrame 和 RDD 的关系类似。</p><p>使用SQL使用这种</p><pre><code class="scala">val data = streamEnv.readTextFile(getClass.getResource(&quot;/station.log&quot;).getPath)// val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)new StationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;)//把DataStream对象变成一个TabletableEvn.registerDataStream(&quot;t_station_log&quot;,data) //注册表val table: Table = tableEvn.scan(&quot;t_station_log&quot;)table.printSchema() //打印表结构streamEnv.execute()</code></pre><p>使用TableAPI注册table</p><pre><code class="scala">val data = streamEnv.readTextFile(getClass.getResource(&quot;/station.log&quot;).getPath)// val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;)//把DataStream对象变成一个Tableval table: Table = tableEvn.fromDataStream(data) //直接变成table对象table.printSchema() //打印表结构streamEnv.execute()</code></pre><h4 id="2-修改-Table-中字段名"><a href="#2-修改-Table-中字段名" class="headerlink" title="2.修改 Table 中字段名"></a>2.修改 Table 中字段名</h4><p>Flink 支持把自定义 POJOs 类的所有 case 类的属性名字变成字段名，也可以通过基于 字段偏移位置和字段名称两种方式重新修改：</p><pre><code class="scala">//导入table库中的隐式转换import org.apache.flink.table.api.scala._// 基于位置重新指定字段名称为&quot;field1&quot;, &quot;field2&quot;, &quot;field3&quot;val table = tStreamEnv.fromDataStream(stream, &#39;field1, &#39;field2, &#39;field3)// 将DataStream转换成Table,并且将字段名称重新成别名val table: Table = tStreamEnv.fromDataStream(stream, &#39;rowtime as &#39;newTime, &#39;id as &#39;newId,&#39;variable as &#39;newVariable)//注意！使用as修改字段时，要修改表中所有的字段。</code></pre><h4 id="3-查询和过滤"><a href="#3-查询和过滤" class="headerlink" title="3.查询和过滤"></a>3.查询和过滤</h4><p>在 Table 对象上使用 select 操作符查询需要获取的指定字段，也可以使用 filter 或 where 方法过滤字段和检索条件，将需要的数据检索出来。</p><p>其中 toAppendStream 函数是吧 Table 对象转换成 DataStream 对象。</p><pre><code class="scala">object TableAPITest &#123;def main(args: Array[String]): Unit = &#123;val streamEnv: StreamExecutionEnvironment =StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)//初始化Table API的上下文环境val tableEvn =StreamTableEnvironment.create(streamEnv)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._import org.apache.flink.table.api.scala._ val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;)val table: Table = tableEvn.fromDataStream(data)//查询tableEvn.toAppendStream[Row](table.select(&#39;sid,&#39;callType as &#39;type,&#39;callTime,&#39;callOut)).print()    //过滤查询   val value: DataStream[Row]=tableEvn.toAppendStream[Row](table.filter(&#39;callType===&quot;success&quot;) //filter.where(&#39;callType===&quot;success&quot;)) //where    .print()tableEvn.execute(&quot;sql&quot;)&#125;</code></pre><h4 id="4-分组聚合"><a href="#4-分组聚合" class="headerlink" title="4.分组聚合"></a>4.分组聚合</h4><pre><code class="scala">// toRetractDstream 得到的第一个boolean型字段标识 true就是最新的数据，false表示过期老数据  DataStream[(Boolean, (String, Long))]//如果使用 groupby table转换为流的时候只能用toRetractDstreamtableEnvironment.toRetractStream[Row](      table1.groupBy(&#39;sid2).select(&#39;sid2,&#39;sid2.count as &#39;num_sid)    ).filter(_._1==true).print()tableEnvironment.registerDataStream(&quot;t_table&quot;,data2)</code></pre><p>在代码中可以看出，使用 toAppendStream 和 toRetractStream 方法将 Table 转换为 DataStream[T]数据集，T 可以是 Flink 自定义的数据格式类型 Row，也可以是用户指定的数 据 格 式 类 型 。</p><h4 id="5-UDF-自定义的函数"><a href="#5-UDF-自定义的函数" class="headerlink" title="5.UDF 自定义的函数"></a>5.UDF 自定义的函数</h4><p>用户可以在 Table API 中自定义函数类，常见的抽象类和接口是：</p><p><strong>ScalarFunction</strong></p><p><strong>TableFunction</strong></p><p><strong>AggregateFunction</strong></p><p><strong>TableAggregateFunction</strong></p><h5 id="案例："><a href="#案例：" class="headerlink" title="案例："></a>案例：</h5><p>使用 Table 完成基于流的 WordCount</p><pre><code class="scala">object TableAPITest2 &#123;def main(args: Array[String]): Unit = &#123;val streamEnv: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironmentstreamEnv.setParallelism(1)//初始化Table API的上下文环境val tableEvn =StreamTableEnvironment.create(streamEnv)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._import org.apache.flink.table.api.scala._ val stream: DataStream[String] =streamEnv.socketTextStream(&quot;hadoop101&quot;,8888)val table: Table = tableEvn.fromDataStream(stream,&#39;words)var my_func =new MyFlatMapFunction()//自定义UDFval result: Table = table.flatMap(my_func(&#39;words)).as(&#39;word, &#39;count).groupBy(&#39;word) //分组.select(&#39;word, &#39;count.sum as &#39;c) //聚合tableEvn.toRetractStream[Row](result).filter(_._1==true).print()tableEvn.execute(&quot;table_api&quot;)&#125;            //自定义UDFclass MyFlatMapFunction extends TableFunction[Row]&#123;//定义类型override def getResultType: TypeInformation[Row] = &#123;Types.ROW(Types.STRING, Types.INT)&#125;//函数主体def eval(str:String):Unit =&#123;str.trim.split(&quot; &quot;).foreach(&#123;word=&gt;&#123;var row =new Row(2)row.setField(0,word)row.setField(1,1)collect(row)&#125;&#125;)&#125;&#125;&#125;</code></pre><h4 id="6-Window"><a href="#6-Window" class="headerlink" title="6.Window"></a>6.Window</h4><p>Flink 支持 ProcessTime、EventTime 和 IngestionTime 三种时间概念，针对每种时间 概念，Flink Table API 中使用 Schema 中单独的字段来表示时间属性，当时间字段被指定 后，就可以在基于时间的操作算子中使用相应的时间属性。 </p><p>在 Table API 中通过使用**.rowtime 来定义 EventTime** 字段，在 ProcessTime 时间字段名后使用**.proctime 后缀来指定 ProcessTime** 时间属性</p><h5 id="tumble案例"><a href="#tumble案例" class="headerlink" title="tumble案例"></a>tumble案例</h5><pre><code class="scala">object TableAPITest &#123;def main(args: Array[String]): Unit = &#123;val streamEnv: StreamExecutionEnvironment =StreamExecutionEnvironment.getExecutionEnvironment//指定EventTime为时间语义streamEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)streamEnv.setParallelism(1)//初始化Table API的上下文环境val tableEvn =StreamTableEnvironment.create(streamEnv)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._import org.apache.flink.table.api.scala._ val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;).assignTimestampsAndWatermarks( //引入Watermarknew BoundedOutOfOrdernessTimestampExtractor[StationLog](Time.seconds(2))&#123;//延迟2秒override def extractTimestamp(element: StationLog) = &#123;element.callTime&#125;&#125;)//注册表并设置时间属性val table: Table = tableEvn.fromDataStream(data,&#39;sid,&#39;callOut,&#39;callIn,&#39;callType,&#39;callTime.rowtime)//滚动Window ,第一种写法val result: Table = table.window(Tumble over 5.second on &#39;callTime as &#39;window)//第二种写法val result: Table = table.window(Tumble.over(&quot;5.second&quot;).on(&quot;callTime&quot;).as(&quot;window&quot;))    .groupBy(&#39;window, &#39;sid).select(&#39;sid, &#39;window.start, &#39;window.end, &#39;window.rowtime, &#39;sid.count)//打印结果tableEvn.toRetractStream[Row](result).filter(_._1==true).print()tableEvn.execute(&quot;sql&quot;)&#125;&#125;</code></pre><h5 id="slide案例"><a href="#slide案例" class="headerlink" title="slide案例"></a>slide案例</h5><pre><code class="scala">object TableAPITest &#123;def main(args: Array[String]): Unit = &#123;val streamEnv: StreamExecutionEnvironment =StreamExecutionEnvironment.getExecutionEnvironment//指定EventTime为时间语义streamEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)streamEnv.setParallelism(1)//初始化Table API的上下文环境val tableEvn =StreamTableEnvironment.create(streamEnv)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._import org.apache.flink.table.api.scala._ val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;).assignTimestampsAndWatermarks( //引入Watermarknew BoundedOutOfOrdernessTimestampExtractor[StationLog](Time.seconds(2))&#123;//延迟2秒override def extractTimestamp(element: StationLog) = &#123;element.callTime&#125;&#125;)//注册表并设置时间属性val table: Table = tableEvn.fromDataStream(data,&#39;sid,&#39;callOut,&#39;callIn,&#39;callType,&#39;callTime.rowtime)//!!!!滑动Window 窗口大小为：10秒，滑动步长为5秒 :第一种写val result: Table = table.window(slide over 10.second every 5.second on &#39;callTime as &#39;window)//第二种写法val result: Table = table.window(slide.over(&quot;10.second&quot;).every(&quot;5.second&quot;).on(&quot;callTime&quot;).as(&quot;window&quot;))    .groupBy(&#39;window, &#39;sid).select(&#39;sid, &#39;window.start, &#39;window.end, &#39;window.rowtime, &#39;sid.count)//打印结果tableEvn.toRetractStream[Row](result).filter(_._1==true).print()tableEvn.execute(&quot;sql&quot;)&#125;&#125;</code></pre><h4 id="Flink-SQL"><a href="#Flink-SQL" class="headerlink" title="Flink  SQL"></a>Flink  SQL</h4><p>SQL 作为 Flink 中提供的接口之一，占据着非常重要的地位，主要是因为 SQL 具有灵活 和丰富的语法，能够应用于大部分的计算场景。<strong>Flink SQL 底层使用 Apache Calcite 框架</strong>， 将标准的 Flink SQL 语句解析并转换成底层的算子处理逻辑，并在转换过程中基于语法规则 层面进行性能优化，比如谓词下推等。另外用户在使用 SQL 编写 Flink 应用时，能够屏蔽底层技术细节，能够更加方便且高效地通过SQL语句来构建Flink应用。Flink SQL构建在Table API 之上，并含盖了大部分的 Table API 功能特性。同时 Flink SQL 可以和 Table API 混用， Flink 最终会在整体上将代码合并在同一套代码逻辑中</p><h5 id="案例-1"><a href="#案例-1" class="headerlink" title="案例"></a>案例</h5><p>通过实例来了解 Flink SQL 整体的使用方式，案例：统计每个基站通话成功的通话时长总和。</p><pre><code class="scala">val data = streamEnv.readTextFile(getClass.getResource(&quot;/station.log&quot;).getPath).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;)val table: Table = tableEvn.fromDataStream(data)//sqlQuery来编写sql语句，可以通过$来引用变量。val result: Table = tableEvn.sqlQuery(s&quot;select sid,sum(duration) as sd from $table where callType=&#39;success&#39; group by sid&quot;)//打印结果tableEvn.toRetractStream[Row](result).filter(_._1==true).print()tableEvn.execute(&quot;sql_api&quot;)</code></pre><p><strong>另外可以有第二种写法：</strong></p><pre><code class="scala">//第二种sql调用方式,注册表后直接表名的方式调用。tableEvn.registerDataStream(&quot;t_station_log&quot;,data)val result: Table = tableEvn.sqlQuery(&quot;select sid ,sum(duration) as sd from t_station_log where callType=&#39;success&#39; group by sid&quot;)tableEvn.toRetractStream[Row](result).filter(_._1==true).print()</code></pre><h5 id="SQL-中的-Window"><a href="#SQL-中的-Window" class="headerlink" title="SQL 中的 Window"></a>SQL 中的 Window</h5><p>Flink SQL 也支持三种窗口类型，分别为 </p><ul><li>Tumble Windows</li><li>HOP Windows（Sliding Window）</li><li>Session Windows</li></ul><p>其中 HOP Windows 对应 Table API 中的 Sliding Window，同时每种窗口分别有相应的使用场景和方法。</p><h6 id="案例-2"><a href="#案例-2" class="headerlink" title="案例"></a>案例</h6><h6 id="Tumble案例："><a href="#Tumble案例：" class="headerlink" title="Tumble案例："></a>Tumble案例：</h6><p>统计最近每 5 秒中内，每个基站的通话成功时间总和：</p><pre><code class="scala">object TestSQL &#123;def main(args: Array[String]): Unit = &#123;val streamEnv: StreamExecutionEnvironment =StreamExecutionEnvironment.getExecutionEnvironment//指定EventTime为时间语义streamEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)streamEnv.setParallelism(1)//初始化Table API的上下文环境val tableEvn =StreamTableEnvironment.create(streamEnv)//导入隐式转换，建议写在这里，可以防止IDEA代码提示出错的问题import org.apache.flink.streaming.api.scala._import org.apache.flink.table.api.scala._// val data = streamEnv.readTextFile(getClass.getResource(&quot;/station.log&quot;).getPath)val data = streamEnv.socketTextStream(&quot;hadoop101&quot;,8888).map(line=&gt;&#123;var arr =line.split(&quot;,&quot;)newStationLog(arr(0).trim,arr(1).trim,arr(2).trim,arr(3).trim,arr(4).trim.toLong,arr(5).trim.toLong)&#125;).assignTimestampsAndWatermarks( //引入Watermarknew BoundedOutOfOrdernessTimestampExtractor[StationLog](Time.seconds(2))&#123;//延迟2秒override def extractTimestamp(element: StationLog) = &#123;element.callTime&#125;&#125;)//滚动窗口,窗口大小为5秒，需求：统计每5秒内，每个基站的成功通话时长总和tableEvn.registerDataStream(&quot;t_station_log&quot;,data,&#39;sid,&#39;callOut,&#39;callIn,&#39;callType,&#39;callTime.rowtime,&#39;duration)var result =tableEvn.sqlQuery( &quot;select sid ,sum(duration) from t_station_log where callType=&#39;success&#39; group bytumble(callTime,interval &#39;5&#39; second),sid&quot;)tableEvn.toRetractStream[Row](result).filter(_._1==true).print()tableEvn.execute(&quot;sql_api&quot;)&#125;&#125;</code></pre><h6 id="slide案例-1"><a href="#slide案例-1" class="headerlink" title="slide案例"></a>slide案例</h6><p>是滑动窗口的话：需求：每隔 5 秒钟，统计最近 10 秒内每个基站的通话成功时间总和</p><pre><code class="scala">//滑动窗口，窗口大小10秒，步长5秒，需求：每隔5秒，统计最近10秒内，每个基站通话成功时长总和tableEvn.registerDataStream(&quot;t_station_log&quot;,data,&#39;sid,&#39;callType,&#39;callTime.rowtime,&#39;duration)//hop_start    滑动窗口开始时间//hop_end   滑动窗口结束时间var result =tableEvn.sqlQuery( &quot;select sid ,sum(duration) , hop_start(callTime,interval &#39;5&#39; second,interval &#39;10&#39; second) as winStart,&quot; +&quot;hop_end(callTime,interval &#39;5&#39; second,interval &#39;10&#39; second) as winEnd &quot; +&quot;from t_station_log where callType=&#39;success&#39; &quot; +&quot;group by hop(callTime,interval &#39;5&#39; second,interval &#39;10&#39; second),sid&quot;)tableEvn.toRetractStream[Row](result) //打印每个窗口的起始时间.filter(_._1==true).print()tableEvn.execute(&quot;sql_api&quot;)</code></pre><h2 id="Flink-CEP（复杂事件处理）"><a href="#Flink-CEP（复杂事件处理）" class="headerlink" title="Flink CEP（复杂事件处理）"></a>Flink CEP（复杂事件处理）</h2><p>复杂事件处理（CEP）是一种基于流处理的技术，将系统数据看作不同类型的事件，通 过分析事件之间的关系，建立不同的事件关系序列库，并利用过滤、关联、聚合等技术，最 终由简单事件产生高级事件，并通过模式规则的方式对重要信息进行跟踪和分析，从实时数 据中发掘有价值的信息。复杂事件处理主要应用于防范网络欺诈、设备故障检测、风险规避 和智能营销等领域。Flink 基于 DataStrem API 提供了 FlinkCEP 组件栈，专门用于对复杂 事件的处理，帮助用户从流式数据中发掘有价值的信息。</p><p>&#x2F;&#x2F;没有写</p><h2 id="Flink-性能优化"><a href="#Flink-性能优化" class="headerlink" title="Flink 性能优化"></a>Flink 性能优化</h2><p>对于构建好的 Flink 集群，如何能够有效地进行集群以及任务方面的监控与优化是非常 重要的，尤其对于 7*24 小时运行的生产环境。重点介绍 Checkpointing 的监控。然后通过分析各种监控指标帮助用户更好地对 Flink 应用进行性能优化，以提高 Flink 任务执行的数 据处理性能和效率。</p><h3 id="1-Checkpoint-页面监控与优化"><a href="#1-Checkpoint-页面监控与优化" class="headerlink" title="1.Checkpoint 页面监控与优化"></a>1.Checkpoint 页面监控与优化</h3><p>Flink Web 页面中也提供了针对 Job Checkpointing 相关的监控信息，Checkpointing 监控页面中共有 Overview、History、Summary 和 Configuration 四个页签，分别对 Checkpointing 从不同的角度进行了监控，每个页面中都包含了与 Checkpointing 相关的指标。</p><h4 id="1）Overview-页签"><a href="#1）Overview-页签" class="headerlink" title="1）Overview 页签"></a>1）Overview 页签</h4><p>Overview 页签中宏观地记录了 Flink 应用中 Checkpoints 的数量以及 Checkpoint 的最 新记录，包括失败和完成的 Checkpoints。</p><img src="/flink/flink/image-20220531160838963.png" alt="image-20220531160838963"><ul><li>Checkpoint Counts：包含了触发、进行中、完成、失败、重置等 Checkpoint 状态数量 统计。 </li><li>Latest Completed Checkpoint：记录了最近一次完成的 Checkpoint 信息，包括结束时 间，端到端时长，状态大小等。 </li><li>Latest Failed Checkpoint：记录了最近一次失败的 Checkpoint 信息。 </li><li>Latest Savepoint：记录了最近一次 Savepoint 触发的信息。 </li><li>Latest Restore：记录了最近一次重置操作的信息，包括从 Checkpoint 和 Savepoint 两种数据中重置恢复任务。</li></ul><h4 id="2）Configuration-页签"><a href="#2）Configuration-页签" class="headerlink" title="2）Configuration 页签"></a>2）Configuration 页签</h4><p>Configuration 页签中包含 Checkpoints 中所有的基本配置，具体的配置解释如下：</p><ul><li>Checkpointing Mode:标记 Checkpointing 是 Exactly Once 还是 At Least Once 的模式。 </li><li>Interval: Checkpointing 触 发 的 时 间 间 隔 ， 时 间 间 隔 越 小 意 味 着 越 频 繁 的 Checkpointing。 </li><li>Timeout: Checkpointing 触发超时时间，超过指定时间 JobManager 会取消当次 Checkpointing，并重新启动新的 Checkpointing。</li><li>Minimum Pause Between Checkpoints:配置两个 Checkpoints 之间最短时间间隔，当上 一次 Checkpointing 结束后，需要等待该时间间隔才能触发下一次 Checkpoints，避触发过多的 Checkpoints 导致系统资源被消耗。 </li><li>Persist Checkpoints Externally:如果开启 Checkpoints，数据将同时写到外部持久 化存储中。</li></ul><p><img src="/flink/flink/image-20220531161226146.png" alt="image-20220531161226146"></p><h3 id="2-Flink-内存优化"><a href="#2-Flink-内存优化" class="headerlink" title="2.Flink 内存优化"></a>2.Flink 内存优化</h3><p>在大数据领域，大多数开源框架（Hadoop、Spark、Storm）都是基于 JVM 运行，但是 JVM 的内存管理机制往往存在着诸多类似 OutOfMemoryError 的问题，主要是因为创建过多 的对象实例而超过 JVM 的最大堆内存限制，却没有被有效回收掉，这在很大程度上影响了系 统的稳定性，尤其对于大数据应用，面对大量的数据对象产生，仅仅靠 JVM 所提供的各种垃 圾回收机制很难解决内存溢出的问题。在开源框架中有很多框架都实现了自己的内存管理， 例如 Apache Spark 的 Tungsten 项目，在一定程度上减轻了框架对 JVM 垃圾回收机制的依赖， 从而更好地使用 JVM 来处理大规模数据集。 </p><p>Flink 也基于 JVM 实现了自己的内存管理，将 JVM 根据内存区分为 <strong>Unmanned Heap、Flink Managed Heap、Network Buffers</strong> 三个区域。</p><p>在 Flink 内部对 Flink Managed Heap 进行管理，在启动集群的过程中直接将堆内存初始化成 Memory Pages Pool，也就是将内存全部以 二进制数组的方式占用，形成虚拟内存使用空间。新创建的对象都是以序列化成二进制数据 的方式存储在内存页面池中，当完成计算后数据对象 Flink 就会将 Page 置空，而不是通过 JVM 进行垃圾回收，保证数据对象的创建永远不会超过 JVM 堆内存大小，也有效地避免了因 为频繁 GC 导致的系统稳定性问题。</p><p><img src="/flink/flink/image-20220531195320118.png" alt="image-20220531195320118"></p><h4 id="1-JobManager-配置"><a href="#1-JobManager-配置" class="headerlink" title="1)JobManager 配置"></a>1)JobManager 配置</h4><p>JobManager 在 Flink 系统中主要承担管理集群资源、接收任务、调度 Task、收集任务 状态以及管理 TaskManager 的功能，JobManager 本身并不直接参与数据的计算过程中，因 此 JobManager 的内存配置项不是特别多，只要指定 JobManager 堆内存大小即可</p><pre><code>jobmanager.heap.size：设定JobManager堆内存大小，默认为1024MB。</code></pre><h4 id="2）TaskManager-配置"><a href="#2）TaskManager-配置" class="headerlink" title="2）TaskManager 配置"></a>2）TaskManager 配置</h4><p>TaskManager作为Flink集群中的工作节点，所有任务的计算逻辑均执行在TaskManager 之上，因此对 TaskManager 内存配置显得尤为重要，可以通过以下参数配置对 TaskManager 进行优化和调整。对应的官方文档是</p><p><img src="/flink/flink/image-20220531195412615.png" alt="image-20220531195412615"></p><ul><li>taskmanager.heap.size：设定 TaskManager 堆内存大小，默认值为 1024M，如果在 Yarn 的集群中，TaskManager 取决于 Yarn 分配给 TaskManager Container 的内存大小，且 Yarn 环境下一般会减掉一部分内存用于 Container 的容错。</li><li>taskmanager.jvm-exit-on-oom：设定 TaskManager 是否会因为 JVM 发生内存溢出而停 止，默认为 false，当 TaskManager 发生内存溢出时，也不会导致 TaskManager 停止。 </li><li>taskmanager.memory.size：设定 TaskManager 内存大小，默认为 0，如果不设定该值 将会使用 taskmanager.memory.fraction 作为内存分配依据。</li><li>taskmanager.memory.fraction：设定 TaskManager 堆中去除 Network Buffers 内存后的内存分配比例。该内存主要用于 TaskManager 任务排序、缓存中间结果等操作。例如， 如果设定为 0.8，则代表 TaskManager 保留 80%内存用于中间结果数据的缓存，剩下 20% 的 内 存 用 于 创 建 用 户 定 义 函 数 中 的 数 据 对 象 存 储 。 注 意 ， 该 参 数 只 有 在 taskmanager.memory.size 不设定的情况下才生效。 </li><li>taskmanager.memory.off-heap：设 置是 否开 启堆 外内 存供 Managed Memory 或 者 Network Buffers 使用。 </li><li>taskmanager.memory.preallocate：设置是否在启动 TaskManager 过程中直接分配 TaskManager 管理内存。 </li><li>taskmanager.numberOfTaskSlots：每个 TaskManager 分配的 slot 数量。</li></ul><h3 id="3-Flink-的网络缓存优化"><a href="#3-Flink-的网络缓存优化" class="headerlink" title="3.Flink 的网络缓存优化"></a>3.Flink 的网络缓存优化</h3><p>Flink 将 JVM 堆内存切分为三个部分，其中一部分为 Network Buffers 内存。Network Buffers 内存是 Flink 数据交互层的关键内存资源，主要目的是缓存分布式数据处理过程中 的输入数据。。通常情况下，比较大的 Network Buffers 意味着更高的吞吐量。如果系统出 现“Insufficient number of network buffers”的错误，一般是因为 Network Buffers 配置过低导致，因此，在这种情况下需要适当调整 TaskManager 上 Network Buffers 的内存 大小，以使得系统能够达到相对较高的吞吐量</p><p>目前 Flink 能够调整 Network Buffer 内存大小的方式有两种：一种是通过直接指定 Network Buffers 内存数量的方式，另外一种是通过配置内存比例的方式。</p><h4 id="1）设定-Network-Buffer-内存数量（过时）"><a href="#1）设定-Network-Buffer-内存数量（过时）" class="headerlink" title="1）设定 Network Buffer 内存数量（过时）"></a>1）设定 Network Buffer 内存数量（过时）</h4><p>直接设定 Nework Buffer 数量需要通过如下公式计算得出： NetworkBuffersNum &#x3D; total-degree-of-parallelism * intra-node-parallelism * n 其 中 total-degree-of-parallelism 表 示 每 个 TaskManager 的 总 并 发 数 量 ， intra-node-parallelism 表示每个 TaskManager 输入数据源的并发数量，n 表示在预估计算 过程中 Repar-titioning 或 Broadcasting 操作并行的数量。intra-node-parallelism 通常 情况下与 Task-Manager 的所占有的 CPU 数一致，且 Repartitioning 和 Broadcating 一般下 不会超过 4 个并发。可以将计算公式转化如下： NetworkBuffersNum &#x3D; ^2 * &lt; TMs&gt;* 4 其中 slots-per-TM 是每个 TaskManager 上分配的 slots 数量，TMs 是 TaskManager 的 总数量。对于一个含有 20 个 TaskManager，每个 TaskManager 含有 8 个 Slot 的集群来说， 总共需要的 Network Buffer 数量为 8^2<em>20</em>4&#x3D;5120 个，因此集群中配置 Network Buffer 内存的大小约为 300M 较为合适。 计算完 Network Buffer 数量后，可以通过添加如下两个参数对 Network Buffer 内存进 行配置。其中 segment-size 为每个 Network Buffer 的内存大小，默认为 32KB，一般不需 要修改，通过设定 numberOfBuffers 参数以达到计算出的内存大小要求。  taskmanager.network.numberOfBuffers：指定 Network 堆栈 Buffer 内存块的数量。  taskmanager.memory.segment-size.：内存管理器和 Network 栈使用的内存 Buffer 大 小，默认为 32K</p><h4 id="2）设定-Network-内存比例"><a href="#2）设定-Network-内存比例" class="headerlink" title="2）设定 Network 内存比例"></a>2）设定 Network 内存比例</h4><p>从 1.3 版本开始，Flink 就提供了通过指定内存比例的方式设置 Network Buffer 内大小。 </p><ul><li>taskmanager.network.memory.fraction: JVM 中用于 Network Buffers 的内存比例。</li><li>taskmanager.network.memory.min: 最小的 Network Buffers 内存大小，默认为 64MB。 </li><li>taskmanager.network.memory.max: 最大的 Network Buffers 内存大小，默认 1GB。 </li><li>taskmanager.memory.segment-size: 内存管理器和 Network 栈使用的 Buffer 大小，默 认为 32KB。</li></ul><h2 id="Flink端口"><a href="#Flink端口" class="headerlink" title="Flink端口"></a>Flink端口</h2><p>6123JobManager的通信端口号</p><p>8081访问JobManager的端口号</p><h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><h3 id="Flink如何保证消费kafka数据的一致性"><a href="#Flink如何保证消费kafka数据的一致性" class="headerlink" title="Flink如何保证消费kafka数据的一致性"></a>Flink如何保证消费kafka数据的一致性</h3><p>Flink有一个checkpoint的机制，挂掉的话可以从这里恢复，</p><p>Flink可以保存状态，在消费kafka数据时，可以将offset当成一个状态保存在flink中，到时候恢复的时候就可以将偏移量恢复出来，重新再提交一遍,Flink是自动的，不需要自己进行手动，非常舒服（如果是Spark，也能保证数据一致性，但是得自己手动的进行）</p><p>Flink在与kafka链接的时候自动保证了状态的一致性</p><h3 id="Flink-如何实现-Exactly-once-语义？"><a href="#Flink-如何实现-Exactly-once-语义？" class="headerlink" title="Flink 如何实现 Exactly-once 语义？"></a>Flink 如何实现 Exactly-once 语义？</h3><p>使用执行exactly-once的数据源，如kafka。</p><p>开启checkpoint，设置checkpointingMode。EXACTLY_ONCE，不让消费者自动提交偏移量存储系统支持覆盖（redis，Hbase，ES），使用其幂等性，将原来的数据覆盖</p><p>Barrier（流屏障）可以保证一个流水线中所有算子都完成了对该条数据做的checkpoint。存储系统支持事务</p><p>Jobmanager定时出发checkpoint的定时器（checkpointCodination）给有状态的subtask做checkpoint</p><p>checkpoint成功后，将数据写入statebackend中，成功后向jobmanager发送ack应答</p><p>jobmanager接收到的所有subtask响应后，jobmanager向所有实现了checkpointlistener的subtask发送notify completed方法成功的消息。</p><p>把数据写入kafka，提交事务，及时提交事务失败，也没关系，会重启从checkpoint恢复后再写。</p><p>Flink 时间类型的分类和各自的实现原理？</p><h3 id="Flink-如何处理数据乱序和延迟？"><a href="#Flink-如何处理数据乱序和延迟？" class="headerlink" title="Flink 如何处理数据乱序和延迟？"></a>Flink 如何处理数据乱序和延迟？</h3><p>waterwark+allowedlateness+sideoutput</p>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>设计模式</title>
      <link href="/java/she-ji-mo-shi/"/>
      <url>/java/she-ji-mo-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="重写和重载的区别"><a href="#重写和重载的区别" class="headerlink" title="重写和重载的区别"></a>重写和重载的区别</h2><p>OverLoad<br>方法的重载<br>一个方法名可以对应多个方法，只是他们的形参列表不同,可以返回值不同。</p><p>override<br>方法的重写<br>子类中隔据需要对基类中继承的方法进行重写<br>重写方法必须和被重写方法具有相同方法名称、参数列表和返回类型</p><h2 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h2><h3 id="什么是设计模式"><a href="#什么是设计模式" class="headerlink" title="什么是设计模式"></a>什么是设计模式</h3><ol><li><p>静态方法和属性的经典使用</p></li><li><p>设计模式是在大量时间中总结的和理论化之后优选的代码结构、编程风格、以及解决问题的思考方式。</p></li></ol><h4 id="单例模式"><a href="#单例模式" class="headerlink" title="单例模式"></a>单例模式</h4><p>在 java.lang.Runtime中就使用到了饿汉式的单例模式.</p><p>单利设计模式特点：<br>　　1、单例类<strong>只</strong>能有一个实例。<br>　　2、单例类必须<strong>自己创建</strong>自己的唯一实例。<br>　　3、单例类必须给所有其他对象提供这一实例。</p><h5 id="饿汉式"><a href="#饿汉式" class="headerlink" title="饿汉式"></a>饿汉式</h5><p>只要类加载就会直接创建对象，即使没有使用到创建的对象。饿汉式可能造成创建对象却没有使用的弊端</p><ol><li>构造器私有化，防止用户直接new</li><li>类的内部创建对象。</li><li>声明一个私有的静态方法，同时创建该对象</li><li>创建一个对外的公共静态方法访问该变量</li></ol><pre><code class="java">public class GirlFriend&#123;private String name;public static int num = 1;    //1.私有化构造器    private GirlFriend(String name)&#123;        this.name = name;    &#125;    //2.类的内部直接创建一个对象，static    private static  GirlFriend instance = new GirlFriend(&quot;222&quot;);        //3.提供static方法返回加载对象    public static GirlFriend GetGirlFriend()&#123;        createGirlFriend();        return gf;    &#125;&#125;</code></pre><pre><code class="java">public static void main(String[] args) &#123; GirlFriend.num; //没有用到类，创建的对象就浪费了.&#125;</code></pre><h5 id="懒汉式"><a href="#懒汉式" class="headerlink" title="懒汉式"></a>懒汉式</h5><p>在你使用到的时候，才会创建这个对象。</p><p>1.构造器私有化，避免外部直接创建对象<br>2.定义一个私有的static静态属性对象。<br>3.创建一个对外的公共静态方法访问该变量，如果变量没有对象，创建该对象 </p><pre><code class="java">class MyJvm&#123;private static  MyJvm instance = null;public static int num = 1;private MyJvm()&#123;    System.out.println(&quot;构造器被调用&quot;);&#125;public static MyJvm getInstance()&#123;if(null ==instance)&#123;  //提高效率synchronized(MyJvm.class)&#123;if(null==instance)&#123;  //安全     instance = new MyJvm();        &#125;        &#125;    &#125;    return instance;&#125;&#125;</code></pre><pre><code class="java">public static void main(String[] args) &#123; MyJvm.num; //不会创建对象,只加载类信息 MyJvm.getInstance(); //才会创建对象&#125;</code></pre><h2 id="final关键字"><a href="#final关键字" class="headerlink" title="final关键字"></a>final关键字</h2><p>final最终的</p><ol><li>类不可以被继承，加final</li><li>不希望父类的某个方法被子类覆盖重写时,使用final关键字修饰.</li><li>某个类的属性值不能被修改,使用final</li><li>局部变量不希望更改，使用final</li></ol><h3 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h3><ol><li>final修饰的属性叫做常量，一般用XXX_XXX_XXX命名（有意义、均大写、长度不宜太长）</li><li>final修饰的属性在定义时，<strong>必须初始化</strong>（构造器，代码块或者定义时都可以），并且不能够再修改。</li><li>final修饰的属性是static静态的，则只能在定义时或者静态代码块中赋值。</li><li>final类不能继承，可以实例化。</li><li>类不是final类，但是含有final方法，虽然不能重写，但是可以被继承。</li><li>类修饰成final类，就不要再final修饰方法了，因为类不能被继承方法怎么可能还会被重写。</li><li>final不能修饰构造器</li><li>final和static搭配使用，效率更高，底层做了优化，<strong>不会导致类的加载</strong>（不会调用）</li><li>包装类包括String 类型都是final修饰的，不能被继承的。</li></ol><h2 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h2><p>是一种模板模式。抽象类为所有子类提供了一个通用模板，子类可以在这个模板基础上进行扩充。<br>通过抽象类，可以避免子类设计的随意性。（继承的子类必须实现抽象类的方法）</p><ol><li><p>abstract修饰类，抽象类</p></li><li><p>abstract修饰方法，抽象方法，没有方法体</p></li><li><p>价值在于设计，设计好后让子类继承并实现抽象类。在框架和设计模式使用比较多。</p></li></ol><h3 id="细节-1"><a href="#细节-1" class="headerlink" title="细节"></a>细节</h3><p>有抽象方法的类只能定义抽象类</p><ol><li>抽象类不能被实列化</li><li>抽象类不一定包含抽象方法，抽象类<strong>可以没有抽象方法</strong>，还可以有实现的方法。</li><li>包含了abstract方法的类必须声明为抽象类。</li><li>abstract只能修饰类和方法<strong>不能修饰属性和其他</strong>。</li><li>抽象类可以 包含非抽象方法、构造器、静态属性等等，构造器不能调用，但可以限制子类构造器的格式（抽象类的本质还是类）</li><li>继承了抽象类，那么所有的抽象方法必须被子类实现。或者子类也声明为抽象类。</li><li>抽象方法不能使用private，static，final来修饰，因为这些关键字都是和重写相违背的。</li><li>抽象方法没有方法体</li></ol><pre><code class="java">abstract   抽象方法  （只声明）public abstract class 类名&#123;    public abstract void 方法名();&#125;</code></pre><h2 id="模板设计模式"><a href="#模板设计模式" class="headerlink" title="模板设计模式"></a>模板设计模式</h2><pre><code class="java">abstract public class Template &#123; //抽象类-模板设计模式public abstract void job();//抽象方法public void calculateTime() &#123;//实现方法，调用 job 方法//得到开始的时间long start = System.currentTimeMillis();job(); //动态绑定机制//得的结束的时间long end = System.currentTimeMillis();System.out.println(&quot;任务执行时间 &quot; + (end - start));&#125;&#125;public class AA extends Template &#123;//计算任务//1+....+ 800000@Overridepublic void job() &#123; //实现 Template 的抽象方法 joblong num = 0;for (long i = 1; i &lt;= 800000; i++) &#123;num += i;&#125;&#125;// public void job2() &#123;// //得到开始的时间// long start = System.currentTimeMillis();// long num = 0public class BB extends Template&#123;public void job() &#123;//这里也去，重写了 Template 的 job 方法long num = 0;for (long i = 1; i &lt;= 80000; i++) &#123;num *= i;&#125;&#125;&#125;public class TestTemplate &#123;public static void main(String[] args)&#123;AA aa = new AA();aa.calculateTime(); //这里还是需要有良好的 OOP 基础，对多态BB bb = new BB();bb.calculateTime();&#125;&#125;</code></pre><h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2><ol><li>接口不能实例化</li><li>接口中所有的public方法，接口中的抽象方法，可以不用abstract修饰</li><li>一个类实现接口就必须将该接口的所有方法都实现。</li><li>抽象类实现接口，可以不实现接口的方法</li><li>类可以实现同时实现多个接口</li><li>接口中的属性都是 public static final 修饰的。</li><li>接口中属性访问形式：接口名.属性名</li><li>接口不能继承其他类，但是可以继承其他接口。</li><li>接口修饰符只能public和默认，和类的修饰符是一样的。</li></ol><pre><code class="java">interface 接口名&#123;    自己的属性；    自己的方法；    必须实现接口的抽象方法&#125;//接口中的抽象方法需要全部实现class 类名 implements 接口&#123;jdk7.0接口的所有方法都没有方法体，及都是抽象方法。jdk8.0接口可以有静态方法，默认方法，可以省略abstract关键字//抽象方法public void ff();    //默认方法default public void ok()&#123;    System.out.prinln(&quot;ok....&quot;);&#125;//静态方法public static void cry()&#123;    System.out.println(&quot;cry....&quot;);&#125;&#125;</code></pre><h2 id="接口和普通类的区别"><a href="#接口和普通类的区别" class="headerlink" title="接口和普通类的区别"></a>接口和普通类的区别</h2><p>接口是普通类单继承机制的一种补充。</p><p>子类继承父类，自动拥有父类的全部功能。</p><p>子类需要扩展功能，通过实现接口的方式扩充。</p><ol><li>继承主要解决代码的复用性和维护性，实现主要价值在于：设计好各种规范，让其他类去实现这些方法。</li><li>接口比继承更加灵活，继承满足is-a 的关系，接口只需要满足like-a的关系。</li></ol><h3 id="接口和普通类的参数冲突"><a href="#接口和普通类的参数冲突" class="headerlink" title="接口和普通类的参数冲突"></a><strong>接口和普通类的参数冲突</strong></h3><p>接口因为是静态属性，可以使用<strong>接口名.属性调用</strong>，父类使用<strong>super.属性</strong> 调用</p><h2 id="内部类"><a href="#内部类" class="headerlink" title="内部类"></a>内部类</h2><p>一个类的内部嵌套了另一个类的结构，被嵌套的类成为内部类，嵌套其他类的类成为外部类。内部类最大的特点就是可以访问私有属性，体现类和类之间的包含关系。重点！</p><h3 id="位置划分"><a href="#位置划分" class="headerlink" title="位置划分"></a>位置划分</h3><h4 id="局部内部类"><a href="#局部内部类" class="headerlink" title="局部内部类"></a>局部内部类</h4><ol><li><p>定义在外部类的局部位置，如方法中，并且有类名。</p></li><li><p>不能添加访问修饰符，地位就是一个<strong>局部变量</strong>，可以使用final修饰类。本质仍然是个类</p></li><li><p>可以直接访问外部类所有属性，包括私有</p></li><li><p>作用域：仅在<strong>定义它的方法或代码块</strong>中。相当于这个方法内的局部变量。</p></li><li><p>局部内部类可以直接访问外部类的成员。</p></li><li><p>外部类其他类不能访问局部内部类，局部类地位是一个局部变量。</p></li><li><p>外部类和局部内部类成员重名时，遵循就近原则，先访问外部类的成员，可以直接使用(外部类.this.成员)去访问（<strong>外部类.this.成员</strong> 代表外部类，直接<strong>this.成员代表局部</strong>）</p></li></ol><pre><code class="java">public class LocalInnerClass &#123;    private int n1 = 100;//私有属性    private void m1() &#123;//私有方法        //可以修饰final        final class test &#123;//方法局部内部类            public void f1() &#123;                //调用父类私有属性和方法                System.out.println(n1);                m1();            &#125;        &#125;        //方法中创建内部类对象，调用方法        //作用域只能在这个方法中        test test = new test();        test.f1();    &#125;&#125;</code></pre><h4 id="匿名内部类-重点！！"><a href="#匿名内部类-重点！！" class="headerlink" title="匿名内部类 重点！！"></a>匿名内部类 重点！！</h4><p>匿名内部类定义在外部类的局部位置，比如方法中，并且没有类名，同时是一个对象！</p><ol><li>不能添加访问修饰符，是一个局部变量</li><li>在定义它的方法或者代码块中</li><li>可以直接访问外部类所有属性方法，包含私有</li><li>不能添加修饰符，因为他本身就是一个局部变量</li><li>仅仅定义在他的方法或者代码块中</li><li>外部其他类不能使用匿名内部类</li><li>就近原则使用，重名使用<strong>外部类.this.属性</strong></li></ol><pre><code class="java">new 类或者接口()&#123;类体&#125;;class outer &#123;    private int n1 = 100;//私有属性    public void m2() &#123;        //接口的匿名内部类         new A() &#123;            @Override            public void cry() &#123;                System.out.println(&quot;接口内部类,只使用一次的类使用这种&quot;);            &#125;        &#125;.cry();    &#125;&#125;//匿名内部类的使用public class AnonymousInnerClass &#123;    public static void main(String[] args) &#123;        outer outer = new outer();        outer.m2();    &#125;&#125;</code></pre><h2 id="成员位置划分"><a href="#成员位置划分" class="headerlink" title="成员位置划分"></a>成员位置划分</h2><h4 id="成员内部类"><a href="#成员内部类" class="headerlink" title="成员内部类"></a>成员内部类</h4><ol><li><p>定义在外部类的成员位置上</p></li><li><p>可以添加任意修饰符，他是一个成员</p></li><li><p>作用域，在外部类中使用</p></li><li><p>可以直接访问外部类成员属性方法</p></li><li><p>创建成员内部类调用相关属性和方法</p></li><li><p>外部类访问成员内部类</p></li><li><p>成员重名时，遵循就近原则，使用<strong>外部类.this.属性</strong>访问外部类属性</p></li></ol><pre><code class="java">class InnerClassExercise01 &#123;    class aa&#123;        private int nn = 11;        public void aa()&#123;            System.out.println(nn);        &#125;    &#125;&#125;public class Cellphone&#123;    public static void main(String[] args) &#123;        InnerClassExercise01 innerClassExercise01 = new InnerClassExercise01();        InnerClassExercise01.aa aa = innerClassExercise01.new aa();    &#125;&#125;</code></pre><h4 id="静态内部类"><a href="#静态内部类" class="headerlink" title="静态内部类"></a>静态内部类</h4><p>外部类成员位置，使用static修饰</p><ol><li>可以直接访问外部类所有静态成员，不能直接访问非静态成员</li><li>可以添加任意修饰符</li><li>作用域：同其他成员为一个整体。</li><li>静态内部类可以直接访问所有静态方法</li><li>外部类访问静态内部类-》创建对象在访问</li><li>外部其他类访问静态内部类 <strong>外部类.静态内部类</strong> （前提满足访问权限）</li><li>外部类和静态内部类重名的时候使用就近原则，访问外部类成员，使用（<strong>外部类.成员</strong>）去访问</li></ol><pre><code class="java">public class StaticInnerClass &#123;    public static void main(String[] args) &#123;        outer01 outer01 = new outer01();        outer01.m1();    &#125;&#125;class outer01 &#123;    //不能访问非static成员    private int nn = 10;    private static int nm = 1;    //static 修饰    static class Inner01 &#123;        public void say() &#123;            System.out.println(nm);        &#125;    &#125;    public void m1() &#123;        Inner01 outer01 = new Inner01();        outer01.say();    &#125;&#125;</code></pre><h2 id="枚举类"><a href="#枚举类" class="headerlink" title="枚举类"></a>枚举类</h2><h3 id="自定义枚举类"><a href="#自定义枚举类" class="headerlink" title="自定义枚举类"></a>自定义枚举类</h3><ol><li>构造器私有化</li><li>本类内部创建一组对象[四个 春夏秋冬]</li><li>对外暴露对象（通过为对象添加 public final static 修饰符）</li><li>可以提供 get 方法，但是不要提供 set</li></ol><pre><code class="java">/*** @author 韩顺平* @version 1.0*/public class Enumeration02 &#123;public static void main(String[] args) &#123;System.out.println(Season.AUTUMN);System.out.println(Season.SPRING);&#125;&#125;//演示字定义枚举实现class Season &#123;//类private String name;private String desc;//描述//定义了四个对象, 固定. public static final Season SPRING = new Season(&quot;春天&quot;, &quot;温暖&quot;);public static final Season WINTER = new Season(&quot;冬天&quot;, &quot;寒冷&quot;);public static final Season AUTUMN = new Season(&quot;秋天&quot;, &quot;凉爽&quot;);public static final Season SUMMER = new Season(&quot;夏天&quot;, &quot;炎热&quot;);//1. 将构造器私有化,目的防止 直接 new//2. 去掉 setXxx 方法, 防止属性被修//3. 在 Season 内部，直接创建固定的对象//4. 优化，可以加入 final 修饰符private Season(String name, String desc) &#123;this.name = name;this.desc = desc;&#125;public String getName() &#123;return name;&#125;public String getDesc() &#123;return desc;&#125;@Overridepublic String toString() &#123;return &quot;Season&#123;&quot; +&quot;name=&#39;&quot; + name + &#39;\&#39;&#39; +&quot;, desc=&#39;&quot; + desc + &#39;\&#39;&#39; +&#39;&#125;&#39;;&#125;&#125;</code></pre><h3 id="枚举类-1"><a href="#枚举类-1" class="headerlink" title="枚举类"></a>枚举类</h3><pre><code class="java">enum Season &#123;//类    SPRING(&quot;春天&quot;,&quot;温暖&quot;),SUMMER(&quot;夏天&quot;,&quot;很热&quot;);    private String name;    private String desc;//描述    private Season(String name, String desc) &#123;        this.name = name;        this.desc = desc;    &#125;    @Override    public String toString() &#123;        return &quot;Season&#123;&quot; +                &quot;name=&#39;&quot; + name + &#39;\&#39;&#39; +                &quot;, desc=&#39;&quot; + desc + &#39;\&#39;&#39; +                &#39;&#125;&#39;;    &#125;&#125;</code></pre><h4 id="细节-2"><a href="#细节-2" class="headerlink" title="细节"></a>细节</h4><ol><li>当我们使用 enum 关键字开发一个枚举类时，默认会继承 Enum 类, 而且是一个static final 类</li></ol><ol start="2"><li>传统的 public static final Season2 SPRING &#x3D; new Season2(“春天”, “温暖”); 简化成 SPRING(“春天”, “温暖”)， 这里必<br>须知道，它调用的是哪个构造器. </li><li>如果使用无参构造器 创建 枚举对象，则实参列表和小括号都可以省略</li><li>当有多个枚举对象时，使用’’，’’间隔，最后有一个分号结尾 </li><li><strong>枚举对象必须放在枚举类的行首</strong></li></ol><p><img src="/java/she-ji-mo-shi/image-20220303092922192.png" alt="image-20220303092922192"></p><h4 id="Enum方法"><a href="#Enum方法" class="headerlink" title="Enum方法"></a>Enum方法</h4><p>有了枚举，可以把相关的常量分组到一个枚举类型里，而且枚举提供了比常量更多的方法。 </p><p>ordinal 从0开始  compareTo 下标-下标的差</p><p><img src="/java/she-ji-mo-shi/image-20220303094132241.png" alt="image-20220303094132241"></p><h4 id="实现接口"><a href="#实现接口" class="headerlink" title="实现接口"></a>实现接口</h4><ol><li>使用enum关键字，不能在继承其他类  enum隐式继承Enum，单继承</li><li>可以实现接口</li></ol><pre><code class="java">interface a&#123;    public void playing();&#125;enum Week implements a&#123;    MONDAY(&quot;星期一&quot;), TUESDAY(&quot;星期二&quot;);    private String name;    Week(String name) &#123;        this.name = name;    &#125;    @Override    public String toString() &#123;        return name;    &#125;    @Override    public void playing() &#123;        System.out.println(&quot;玩tmd&quot;);    &#125;&#125;/** * @author 张文辉 * @version 1.0 */public class EnumExercise02 &#123;    public static void main(String[] args) &#123;        Week[] values = Week.values();        for(Week k : values)&#123;            System.out.println(k);            k.playing();        &#125;    &#125;&#125;</code></pre><h2 id="注解"><a href="#注解" class="headerlink" title="注解"></a>注解</h2><h3 id="常用注解"><a href="#常用注解" class="headerlink" title="常用注解"></a>常用注解</h3><p>使用An’notation时在前面增加@符号，并在Annotation当成一个修饰符使用。</p><p>最基本的Annotation:</p><p>@override：重写父类方法，只能用于方法</p><p>@Deprecated：表示某个程序元素过期，不建议使用，新老版本之间的过渡</p><p>@SuppressWarrings：抑制编译器警告，主要放在方法或者类上面。里面传入一个数组，值去百度，很多</p><p>@Interface：代表此类是一个注解类</p><p>@Target：修饰注解的注解，称为元注解</p><h3 id="元注解"><a href="#元注解" class="headerlink" title="元注解"></a>元注解</h3><p>元注解种类</p><ol><li>Retention &#x2F;&#x2F;指定注解的作用范围，三种 SOURCE,CLASS,RUNTIME</li><li>Target &#x2F;&#x2F; 指定注解可以在哪些地方使用</li><li>Documented &#x2F;&#x2F;指定该注解是否会在 javadoc 体现</li><li>Inherited &#x2F;&#x2F;子类会继承父类注解</li></ol><h4 id="Retention-注解"><a href="#Retention-注解" class="headerlink" title="@Retention 注解"></a>@Retention 注解</h4><p>只能用于修饰一个 Annotation 定义, 用于指定该 Annotation 可以保留多长时间, @Rentention 包含一个 RetentionPolicy<br>类型的成员变量, 使用 @Rentention 时必须为该 value 成员变量指定值:<br>@Retention 的三种值</p><ol><li>RetentionPolicy.SOURCE: 编译器使用后，直接丢弃这种策略的注释</li><li>RetentionPolicy.CLASS: 编译器将把注解记录在 class 文件中. 当运行 Java 程序时, JVM 不会保留注解。 这是默认<br>  值</li><li>RetentionPolicy.RUNTIME:编译器将把注解记录在 class 文件中. 当运行 Java 程序时, JVM 会保留注解. 程序可以<br>  通过反射获取该注解</li></ol><h4 id="Target"><a href="#Target" class="headerlink" title="@Target"></a>@Target</h4><p>修饰Annotation定义，指定annotation被用于修饰哪些程序元素</p><h4 id="Documented"><a href="#Documented" class="headerlink" title="@Documented"></a>@Documented</h4><p>被该注解修饰的Annotation类将被javadoc工具提取成文档，可以看到该注解。</p><p>定义为Documented的注解必须设置Retention为RUNTIME！！</p><h4 id="Inherited"><a href="#Inherited" class="headerlink" title="@Inherited"></a>@Inherited</h4><p>被它修饰的Annotation将具有继承性，如果哪个类使用了被@Inherited修饰的Annotation的类，子类将自动具有该注解</p>]]></content>
      
      
      <categories>
          
          <category> 设计模式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Impala学习笔记</title>
      <link href="/cdh/impala/impala/"/>
      <url>/cdh/impala/impala/</url>
      
        <content type="html"><![CDATA[<h1 id="Impala"><a href="#Impala" class="headerlink" title="Impala"></a>Impala</h1><h2 id="Impala是什么？"><a href="#Impala是什么？" class="headerlink" title="Impala是什么？"></a>Impala是什么？</h2><p>Impala是Cloudera公司推出，提供对HDFS、Hbase数据的高性能、低延迟的交互式SQL查询功能。</p><p>基于Hive使用内存计算，兼顾数据仓库、具有实时、批处理、多并发等优点。</p><p>是CDH平台首选的PB级大数据实时<strong>查询分析引擎</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
            <tag> Impala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java部分面试题</title>
      <link href="/java/mian-shi-ti-wen-dang/"/>
      <url>/java/mian-shi-ti-wen-dang/</url>
      
        <content type="html"><![CDATA[<h2 id="Java中的容器有哪些？"><a href="#Java中的容器有哪些？" class="headerlink" title="Java中的容器有哪些？"></a>Java中的容器有哪些？</h2><p><a href="https://blog.csdn.net/weixin_42574142/article/details/87125363">https://blog.csdn.net/weixin_42574142/article/details/87125363</a></p><ul><li><p>Iterable 接口</p><p>Iterable是一个超级接口，被Collection所继承。它只有一个方法： Iterator<T> iterator() &#x2F;&#x2F;即返回一个迭代器</T></p></li><li><p>Collection 接口</p></li><li><p>List 接口</p><ul><li><p>Arraylist</p><p>线程不安全</p></li><li><h5 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h5><p>线程安全的类。</p><ul><li>Stack</li></ul></li><li><h5 id="LinkedList"><a href="#LinkedList" class="headerlink" title="LinkedList"></a>LinkedList</h5><p>LinkedList是用<strong>链表结构</strong>存储数据的，很适合数据的<strong>动态插入和删除</strong>，随机访问和遍历速度比较慢。</p></li></ul></li><li><p>Set 接口</p><ul><li><h5 id="HashSet"><a href="#HashSet" class="headerlink" title="HashSet"></a>HashSet</h5><ul><li><h5 id="LinkedHashSet"><a href="#LinkedHashSet" class="headerlink" title="LinkedHashSet"></a>LinkedHashSet</h5>LinkedHashSet集合也是根据元素的hashCode值来决定元素的存储位置，但和HashSet不同的是，<strong>它同时使用链表维护元素的次序</strong>，这样使得元素看起来是以插入的顺序保存的。</li></ul></li><li><h5 id="SortedSet-接口"><a href="#SortedSet-接口" class="headerlink" title="SortedSet 接口"></a>SortedSet 接口</h5><p>此接口主要用于<strong>排序</strong>操作，实现了此接口的子类都属于排序的子类</p><ul><li><h5 id="TreeSet"><a href="#TreeSet" class="headerlink" title="TreeSet"></a>TreeSet</h5>TreeSet是SortedSet接口的实现类，TreeSet可以确保集合元素处于排序状态</li></ul></li><li><h5 id="EnumSet"><a href="#EnumSet" class="headerlink" title="EnumSet"></a>EnumSet</h5><p>EnumSet是一个专门为<strong>枚举类</strong>设计的集合类，EnumSet中所有元素都必须是<strong>指定枚举类型</strong>的枚举值，该枚举类型在创建EnumSet时显式、或隐式地指定。EnumSet的集合元素也是有序的，</p></li></ul></li><li><p>Queue 接口</p><ul><li><h5 id="PriorityQueue——-优先队列（类）"><a href="#PriorityQueue——-优先队列（类）" class="headerlink" title="PriorityQueue—— 优先队列（类）"></a>PriorityQueue—— 优先队列（类）</h5><p>其实它并没有按照插入的顺序来存放元素，而是<strong>按照队列中某个属性的大小</strong>来排列的。故而叫优先队列。</p></li><li><h5 id="Deque——双端队列（接口）"><a href="#Deque——双端队列（接口）" class="headerlink" title="Deque——双端队列（接口）"></a>Deque——双端队列（接口）</h5><ul><li><h5 id="ArrayDeque（类）"><a href="#ArrayDeque（类）" class="headerlink" title="ArrayDeque（类）"></a>ArrayDeque（类）</h5></li><li><h5 id="LinkedList-（类）"><a href="#LinkedList-（类）" class="headerlink" title="LinkedList （类）"></a>LinkedList （类）</h5></li></ul></li></ul></li><li><p>Map 接口</p><ul><li><h5 id="HashMap-（类）"><a href="#HashMap-（类）" class="headerlink" title="HashMap （类）"></a>HashMap （类）</h5><ul><li><strong>LinkedHashMap （类）</strong></li></ul></li><li><h5 id="HashTable-（类）"><a href="#HashTable-（类）" class="headerlink" title="HashTable （类）"></a>HashTable （类）</h5><ul><li><strong>Properties（类）</strong></li></ul></li><li><h5 id="SortedMap（接口）"><a href="#SortedMap（接口）" class="headerlink" title="SortedMap（接口）"></a>SortedMap（接口）</h5><ul><li><strong>TreeMap（类）</strong></li></ul></li><li><h5 id="WeakHashMap（类）"><a href="#WeakHashMap（类）" class="headerlink" title="WeakHashMap（类）"></a>WeakHashMap（类）</h5></li><li><h5 id="IdentityHashMap（类）"><a href="#IdentityHashMap（类）" class="headerlink" title="IdentityHashMap（类）"></a>IdentityHashMap（类）</h5></li><li><h5 id="EnumMap（类）"><a href="#EnumMap（类）" class="headerlink" title="EnumMap（类）"></a>EnumMap（类）</h5></li></ul></li></ul><h2 id="HashMap的使用场景？"><a href="#HashMap的使用场景？" class="headerlink" title="HashMap的使用场景？"></a>HashMap的使用场景？</h2><p>kv类型数据，无序不可重复，效率高。</p><p>适用于map中插入，删除和定位元素。</p><h2 id="线程安全的容器有哪些？"><a href="#线程安全的容器有哪些？" class="headerlink" title="线程安全的容器有哪些？"></a>线程安全的容器有哪些？</h2><p><a href="https://blog.csdn.net/u010002184/article/details/74892663">https://blog.csdn.net/u010002184/article/details/74892663</a></p><p>Vector</p><p>HashTable</p><p>ConcurrentHashMap</p><h2 id="线程安全指什么"><a href="#线程安全指什么" class="headerlink" title="线程安全指什么?"></a>线程安全指什么?</h2><p>多线程执行任务，他们的数据是共享的，线程安全保证他们执行过程中通过同步机制都可以正确执行，不会出现数据污染情况。</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis笔记</title>
      <link href="/redis/redis-xue-xi-bi-ji/"/>
      <url>/redis/redis-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="Redis是什么？"><a href="#Redis是什么？" class="headerlink" title="Redis是什么？"></a>Redis是什么？</h2><p>REmote DIctionary Server(Redis) 是一个由Salvatore Sanfilippo写的key-value存储系统。</p><p>Redis 是一个开源的使用 ANSI C 语言编写、遵守 BSD 协议、支持网络、可基于内存、分布式、可选持久性的键值对(Key-Value)存储数据库，并提供多种语言的 API。</p><p>它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Hash), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。是跨平台的非关系型数据库。</p><h2 id="Redis的安装"><a href="#Redis的安装" class="headerlink" title="Redis的安装"></a>Redis的安装</h2><p><a href="http://blog.csdn.net/myrainblues/article/details/25881535">http://blog.csdn.net/myrainblues/article/details/25881535</a></p><p>1.下载redis3的稳定版本，下载地址<a href="http://download.redis.io/releases/redis-3.2.11.tar.gz">http://download.redis.io/releases/redis-3.2.11.tar.gz</a><br>2.上传redis-3.2.11.tar.gz到服务器<br>3.解压redis源码包</p><pre><code>tar -zxvf redis-3.2.11.tar.gz -C /usr/local/src/</code></pre><p>4.进入到源码包中，编译并安装redis</p><pre><code>cd /usr/local/src/redis-3.2.11/make &amp;&amp; make install</code></pre><p>5.报错，缺少依赖的包<br>缺少gcc依赖（c的编译器）<br>6.配置本地YUM源并安装redis依赖的rpm包</p><pre><code>yum -y install gcc-c++</code></pre><p>7.编译并安装</p><pre><code>make &amp;&amp; make install</code></pre><p>8.报错，原因是没有安装jemalloc内存分配器，可以安装jemalloc或直接输入</p><pre><code>make MALLOC=libc &amp;&amp; make install</code></pre><p>9.重新编译安装</p><pre><code>make MALLOC=libc &amp;&amp; make install</code></pre><p>10.在所有机器的&#x2F;usr&#x2F;local&#x2F;下创建一个redis目录，然后拷贝redis自带的配置文件redis.conf到&#x2F;usr&#x2F;local&#x2F;redis</p><pre><code>mkdir /usr/local/rediscp /usr/local/src/redis-3.2.11/redis.conf /usr/local/redis</code></pre><p>11.修改所有机器的配置文件redis.conf</p><pre><code>daemonize yes  #redis后台运行appendonly yes  #开启aof日志，它会每次写操作都记录一条日志bind 192.168.1.207</code></pre><p>12.启动所有的redis节点</p><pre><code>cd /usr/local/redisredis-server redis.conf</code></pre><p>13.查看redis进程状态</p><pre><code>ps -ef | grep redis</code></pre><p>14.使用命令行客户的连接redis</p><pre><code>redis-cli -p 6379</code></pre><p>15.关闭redis</p><pre><code>redis-cli shutdown</code></pre><p>17.配置redis密码</p><pre><code>config set requirepass 123</code></pre><p>添加redis的java依赖</p><pre><code class="xml">  &lt;dependency&gt;    &lt;groupId&gt;redis.clients&lt;/groupId&gt;    &lt;artifactId&gt;jedis&lt;/artifactId&gt;    &lt;version&gt;2.9.0&lt;/version&gt;  &lt;/dependency&gt;</code></pre><h2 id="Redis的数据结构"><a href="#Redis的数据结构" class="headerlink" title="Redis的数据结构"></a>Redis的数据结构</h2><ul><li>String: 字符串</li></ul><p>string 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。</p><ul><li>Hash: 散列</li></ul><p>Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。</p><ul><li>List: 列表</li></ul><p>Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。</p><p>列表最多可存储 232 - 1 元素 (4294967295, 每个列表可存储40多亿)。</p><ul><li>Set: 集合</li></ul><p>Redis 的 Set 是 string 类型的无序集合。</p><p>集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。</p><ul><li>zSet(sorted set): 有序集合</li></ul><p>Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。不同的是<strong>每个元素都会关联一个double类型的分数</strong>。redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的,但分数(score)却可以重复。</p><h3 id="Redis的常用操作"><a href="#Redis的常用操作" class="headerlink" title="Redis的常用操作"></a>Redis的常用操作</h3><p><a href="http://blog.csdn.net/xyang81/article/details/51918129">http://blog.csdn.net/xyang81/article/details/51918129</a></p><h2 id="Redis执行过程底层原理"><a href="#Redis执行过程底层原理" class="headerlink" title="Redis执行过程底层原理"></a>Redis执行过程底层原理</h2><p>Redis底层存储数据使用的是一张全局的Hash表，里面都是一维数组加上链表实现</p><p>hash（key）%数组size，找到对应数组后，再放到对应的链表中。</p><h2 id="Reids优势？"><a href="#Reids优势？" class="headerlink" title="Reids优势？"></a>Reids优势？</h2><p>性能极高 – Redis能读的速度是110000次&#x2F;s,写的速度是81000次&#x2F;s 。<br>丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。<br>原子 – Redis的所有操作都是原子性的，意思就是要么成功执行要么失败完全不执行。单个操作是原子性的。多个操作也支持事务，即原子性，通过MULTI和EXEC指令包起来。<br>丰富的特性 – Redis还支持 publish&#x2F;subscribe, 通知, key 过期等等特性。</p><h2 id="Redis与其他key-value存储有什么不同？"><a href="#Redis与其他key-value存储有什么不同？" class="headerlink" title="Redis与其他key-value存储有什么不同？"></a>Redis与其他key-value存储有什么不同？</h2><p>Redis有更负责的数据结构和原子性操作。</p><p>Redis运行在内存但是<strong>可以持久化</strong>到磁盘，在内存中操作起来比磁盘上操作起来更加简单。磁盘方面都是紧凑的追加方式产生，不需要随机访问</p><h2 id="数据库常见问题"><a href="#数据库常见问题" class="headerlink" title="数据库常见问题"></a>数据库常见问题</h2><h3 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a><strong>缓存穿透</strong></h3><p>查询Redis中没有的数据时，该查询会下沉到数据库层，同时数据库层也没有该数据，当这种情况大量出现或被恶意攻击时，**接口的访问全部透过Redis访问数据库，而数据库中也没有这些数据，我们称这种现象为”缓存穿透”**。缓存穿透会穿透Redis的保护，提升底层数据库的负载压力，同时这类穿透查询没有数据返回也造成了网络和计算资源的浪费。</p><p><img src="/redis/redis-xue-xi-bi-ji/640.webp" alt="img"></p><h3 id="缓存击穿"><a href="#缓存击穿" class="headerlink" title="缓存击穿"></a><strong>缓存击穿</strong></h3><p><strong>穿透表示底层数据库没有数据且缓存内也没有数据，击穿表示底层数据库有数据而缓存内没有数据。</strong></p><h3 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a><strong>缓存雪崩</strong></h3><h3 id="缓存预热"><a href="#缓存预热" class="headerlink" title="缓存预热"></a><strong>缓存预热</strong></h3><h3 id="缓存更新"><a href="#缓存更新" class="headerlink" title="缓存更新"></a><strong>缓存更新</strong></h3><h3 id="缓存降级"><a href="#缓存降级" class="headerlink" title="缓存降级"></a><strong>缓存降级</strong></h3><h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><h3 id="Redis单线程还是多线程？"><a href="#Redis单线程还是多线程？" class="headerlink" title="Redis单线程还是多线程？"></a>Redis单线程还是多线程？</h3><p>Redis6.0之前单线程指的是网络IO和键值对的读写都是由一个线程完成的。</p><p>Redis6.0之后引入的多线程，指的是网络请求过程采用了就多线程，键值对读写命令仍然是单线程处理的。所以Redis依然是并发安全的。</p><p>只有网络请求模块和数据操作模块是单线程的。而其他的持久化，集群数据同步等，其实是由额外的线程执行的。</p><h3 id="Redis单线程为什么还能这么快？"><a href="#Redis单线程为什么还能这么快？" class="headerlink" title="Redis单线程为什么还能这么快？"></a>Redis单线程为什么还能这么快？</h3><p>命令执行基于内存</p><p>单线程执行，没有线程切换开销</p><p>基于IO多路复用机制提升Redis的IO利用率</p><p>高效的数据存储结构，全局Hash表以及多种高效数据结构。</p><h3 id="Reids管道技术？"><a href="#Reids管道技术？" class="headerlink" title="Reids管道技术？"></a>Reids管道技术？</h3><p>在某些高并发的场景下，网络开销成了Redis速度的瓶颈，所以需要使用管道技术来实现突破。Redis管道技术一次可以发送多条命令，并且不用一条条的等待命令的结果。可以一次性获取所有结果，这样仅需要一次网络开销，速度会明显提升。</p><h3 id="Redis单点和集群有什么区别？"><a href="#Redis单点和集群有什么区别？" class="headerlink" title="Redis单点和集群有什么区别？"></a>Redis单点和集群有什么区别？</h3><p>单点：比较简单，作为一个缓存使用。</p><p>集群：</p><p>优点</p><ul><li>负载均衡，读写分离，有了主从以后，查询操作就可以通过查询从节点来完成。</li><li>备份数据，这样当一个节点损坏（指不可恢复的硬件损坏）时，数据因为有备份，可以方便恢复。</li></ul><p>缺点</p><ul><li>master节点挂了以后，redis就不能对外提供写服务了，因为剩下的slave不能成为master</li></ul><h3 id="为什么要用Redis？不用会怎么样？有什么优势？"><a href="#为什么要用Redis？不用会怎么样？有什么优势？" class="headerlink" title="为什么要用Redis？不用会怎么样？有什么优势？"></a>为什么要用Redis？不用会怎么样？有什么优势？</h3>]]></content>
      
      
      <categories>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive调优</title>
      <link href="/cdh/hive/hive-diao-you/"/>
      <url>/cdh/hive/hive-diao-you/</url>
      
        <content type="html"><![CDATA[<h1 id="Hive调优"><a href="#Hive调优" class="headerlink" title="Hive调优"></a>Hive调优</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzI2MDQzOTk3MQ==&amp;mid=2247485048&amp;idx=1&amp;sn=5fc1219f4947bea9743cd938cec510c7&amp;chksm=ea68ecb4dd1f65a2df364d79272e0e472a394c5b13b5d55d848c89d9498ccb7ea78a933fbdea&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzI2MDQzOTk3MQ==&amp;mid=2247485048&amp;idx=1&amp;sn=5fc1219f4947bea9743cd938cec510c7&amp;chksm=ea68ecb4dd1f65a2df364d79272e0e472a394c5b13b5d55d848c89d9498ccb7ea78a933fbdea&amp;scene=21#wechat_redirect</a></p><h2 id="1、存储压缩"><a href="#1、存储压缩" class="headerlink" title="1、存储压缩"></a>1、存储压缩</h2><p><img src="/cdh/hive/hive-diao-you/640" alt="图片"></p><ul><li>map阶段落地压缩算法选择</li></ul><p>一般使用parquet+snappy的方式</p><p><strong>当然，也可以在hive建表时指定表的文件格式和压缩编码</strong></p><pre><code class="sql">set hive.exec.compress.intermediate=trueset mapred.map.output.compression.codec= org.apache.hadoop.io.compress.SnappyCodecset mapred.map.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;</code></pre><ul><li>最终结果压缩选择</li></ul><pre><code class="sql">set hive.exec.compress.output=true set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec ## 当然，也可以在hive建表时指定表的文件格式和压缩编码</code></pre><h2 id="2、Sql优化"><a href="#2、Sql优化" class="headerlink" title="2、Sql优化"></a>2、Sql优化</h2><p>关系型数据库语句会自动优化sql，但是hive不会</p><ol><li>先where在join</li></ol><pre><code class="sql">select * from a join b where a.dt=&#39;****&#39; on a.id=b.id-- 这里会先join成一张新表在进行where过滤优化后：select * from (select * from a where dt=&#39;****&#39;) join b on a.id=b.id</code></pre><ol start="2"><li>union all 替换 union</li></ol><p>要去重就union all 在 group by 不要用union去重</p><ol start="3"><li>group by 替换distinct</li></ol><p>group by 去重效率比distinct高。</p><pre><code class="sql">select distinct aa from a优化后：select aa from a group by aaselect count(distinct aa) from a 优化后：select count(1) from (select aa from a group by aa) </code></pre><ol start="4"><li>尽量不适用产生job的函数</li></ol><p>group by max() min() 等等</p><ol start="5"><li>join优化</li></ol><p>新版本hive会根据join的两张表大小自动进行优化，默认小于10M的表join大表就会使用map join，在内存中进行缓存小表。</p><pre><code class="sql">set hive.auto.convert.join=true;</code></pre><p>两张大表进行join时，使用 <code>sort-merge-bucket Join</code>来进行join。桶表来进行优化。在一个桶内发生笛卡尔积连接。</p><pre><code class="sql"> set hive.auto.convert.sortmerge.join=true;   set hive.optimize.bucketmapjoin = true;   set hive.optimize.bucketmapjoin.sortedmerge = true;   set hive.auto.convert.sortmerge.join.noconditionaltask=true;</code></pre><h2 id="3、hive的参数调优"><a href="#3、hive的参数调优" class="headerlink" title="3、hive的参数调优"></a>3、hive的参数调优</h2><pre><code class="sql">-- 让可以不走mapreduce任务的，就不走mapreduce任务hive&gt; set hive.fetch.task.conversion=more; -- 开启任务并行执行 set hive.exec.parallel=true;-- 解释：当一个sql中有多个job时候，且这多个job之间没有依赖，则可以让顺序执行变为并行执行（一般为用到union all的时候）  -- 同一个sql允许并行任务的最大线程数 set hive.exec.parallel.thread.number=8; -- 设置jvm重用-- JVM重用对hive的性能具有非常大的 影响，特别是对于很难避免小文件的场景或者task特别多的场景，这类场景大多数执行时间都很短。jvm的启动过程可能会造成相当大的开销，尤其是执行的job包含有成千上万个task任务的情况。set mapred.job.reuse.jvm.num.tasks=10;  -- 合理设置reduce的数目-- 方法1：调整每个reduce所接受的数据量大小set hive.exec.reducers.bytes.per.reducer=500000000; （500M）-- 方法2：直接设置reduce数量set mapred.reduce.tasks = 20-- map端聚合，降低传给reduce的数据量set hive.map.aggr=true   -- 开启hive内置的数倾优化机制set hive.groupby.skewindata=true-- 设置hive的非严格模式，支持动态分区set hive.exec.dynamic.partition.mode=nonstrict;set hive.exec.dynamic.partition=true;-- 每个节点可以创建的最大分区数 默认值100set hive.exec.max.dynamic.partitions.pernode=10000-- 每个mr可以创建的最大分区数 默认值1000set hive.exec.max.dynamic.partitions=10000;-- 输出合并小文件SET hive.merge.mapfiles = true; --默认 true，在 map-only 任务结束时合并小文件SET hive.merge.mapredfiles = true; --默认 false，在 map-reduce 任务结束时合并小文件SET hive.merge.size.per.task = 268435456; --默认 256MSET hive.merge.smallfiles.avgsize = 16777216; --当输出文件的平均大小小于 16m 该值时，启动一个独立的 map-reduce 任务进行文件 merge-- 开启 map 端 combiner（不影响最终业务逻辑）set hive.map.aggr=true；</code></pre><h2 id="4、合理分区分桶"><a href="#4、合理分区分桶" class="headerlink" title="4、合理分区分桶"></a>4、合理分区分桶</h2><p>表分区：分区是将表的数据在物理上分成不同的文件夹，以便于在查询时可以精准指定所要读取的分区目录，从来降低读取的数据量</p><p>表分桶：表数据按照hash散列分散在不同文件夹。将来查询时，hive可以根据分桶结构，快速定位到一行数据所在的分桶文件，从来提高读取效率</p><h2 id="5、Hive数据倾斜"><a href="#5、Hive数据倾斜" class="headerlink" title="5、Hive数据倾斜"></a>5、Hive数据倾斜</h2><p><a href="https://blog.csdn.net/zuochang_liu/article/details/105274316">https://blog.csdn.net/zuochang_liu/article/details/105274316</a></p><ol><li>mapjoin</li><li>group by</li></ol><p>group by产生了数据倾斜，k大部分相同，可以添加随机数在group by。如果大量null的key，将key过滤或者转为随机值。</p><ol start="3"><li>不同数据类型join</li></ol><p>不同表字段关联名称一样但是字段类型不一致，可能导致关联不上。使用cast()转换格式</p><ol start="4"><li>开启数据倾斜负载均衡</li></ol><pre><code class="sql">set hive.groupby.skewindata=true;</code></pre><p>思想：就是先随机分发并处理，再按照 key group by 来分发处理。</p><p>操作：当选项设定为 true，生成的查询计划会有两个 MRJob。</p><p>第一个 MRJob 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分</p><p>聚合操作，并输出结果，这样处理的结果是相同的 GroupBy Key 有可能被分发到不同的</p><p>Reduce 中，从而达到负载均衡的目的；</p><p>第二个 MRJob 再根据预处理的数据结果按照 GroupBy Key 分布到 Reduce 中（这个过</p><p>程可以保证相同的原始 GroupBy Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。</p><p>点评：它使计算变成了两个 mapreduce，先在第一个中在 shuffle 过程 partition 时随机</p><p>给 key 打标记，使每个 key 随机均匀分布到各个 reduce 上计算，但是这样只能完成部分计算，因为相同 key 没有分配到相同 reduce 上。</p><p>所以需要第二次的 mapreduce,这次就回归正常 shuffle,但是数据分布不均匀的问题在第一次 mapreduce 已经有了很大的改善，因此基本解决数据倾斜。因为大量计算已经在第一次mr 中随机分布到各个节点完成。</p><h2 id="6、小文件的调优"><a href="#6、小文件的调优" class="headerlink" title="6、小文件的调优"></a>6、小文件的调优</h2><p>小文件的产生有三个地方，map输入，map输出，reduce输出，小文件过多也会影响hive的分析效率：</p><p>设置map输入的小文件合并</p><pre><code>set mapred.max.split.size=256000000;  //一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)set mapred.min.split.size.per.node=100000000;//一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)  set mapred.min.split.size.per.rack=100000000;//执行Map前进行小文件合并set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</code></pre><p>设置map输出和reduce输出进行合并的相关参数：</p><pre><code class="shell">//设置map端输出进行合并，默认为trueset hive.merge.mapfiles = true//设置reduce端输出进行合并，默认为falseset hive.merge.mapredfiles = true//设置合并文件的大小set hive.merge.size.per.task = 256*1000*1000//当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。set hive.merge.smallfiles.avgsize=16000000</code></pre>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Click House</title>
      <link href="/clickhouse/clickhouse/"/>
      <url>/clickhouse/clickhouse/</url>
      
        <content type="html"><![CDATA[<p>Click House 本质是数据库，要用click House需要将数据迁移到ck上。不是单纯的计算引擎</p>]]></content>
      
      
      <categories>
          
          <category> Click House </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Click House </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>presto学习笔记</title>
      <link href="/presto/presto/"/>
      <url>/presto/presto/</url>
      
        <content type="html"><![CDATA[<p>Presto是计算引擎，facebook出的，还很新。</p>]]></content>
      
      
      <categories>
          
          <category> presto </category>
          
      </categories>
      
      
        <tags>
            
            <tag> presto </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库笔记</title>
      <link href="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/"/>
      <url>/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="●-数据仓库是什么？"><a href="#●-数据仓库是什么？" class="headerlink" title="● 数据仓库是什么？"></a>● 数据仓库是什么？</h2><p>数据仓库，英文名称为Data Warehouse，可简写为DW或DWH。数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它出于<strong>分析性报告和决策支持目的</strong>而创建。</p><h2 id="●-数据仓库特点"><a href="#●-数据仓库特点" class="headerlink" title="● 数据仓库特点"></a>● 数据仓库特点</h2><h3 id="1-2-1面向主题"><a href="#1-2-1面向主题" class="headerlink" title="1.2.1面向主题"></a><strong>1.2.1面向主题</strong></h3><p>普通的操作型数据库主要面向事务性处理，而数据仓库中的所有数据一般按照主题进行划分。主题是对业务数据的一种抽象，是从较高层次上对信息系统中的数据进行归纳和整理。</p><p>面向主题的数据可以划分成两部分—-根据原系统业务数据的特点进行主题的抽取和确定每个主题所包含的数据内容。例如客户主题、产品主题、财务主题等；而客户主题包括客户基本信息、客户信用信息、客户资源信息等内容。分析数据仓库主题的时候，一般方法是先确定几个基本的 主题，然后再将范围扩大，最后再逐步求精</p><h3 id="1-2-2集成性"><a href="#1-2-2集成性" class="headerlink" title="1.2.2集成性"></a><strong>1.2.2集成性</strong></h3><p>面向操作型的数据库通常是异构的、并且相互独立，所以无法对信息进行概括和反映信</p><p>息的本质。而数据仓库中的数据是经过数据的抽取、清洗、切换、加载得到的，所以为了保证数据不存在二义性，必须对数据进行编码统一和必要的汇总，以保证数据仓库内数据的一致性。数据仓库在经历数据集成阶段后，使数据仓库中的数据都遵守统一的编码规则，并且消除许多冗余数据。 </p><h3 id="1-2-3稳定性"><a href="#1-2-3稳定性" class="headerlink" title="1.2.3稳定性"></a><strong>1.2.3稳定性</strong></h3><p>数据仓库中的数据反映的都是一段历史时期的数据内容，它的主要操作是查询、分析而</p><p>不进行一般意义上的更新（数据集成前的操作型数据库主要完成数据的增加、修改、删除、查询），一旦某个数据进入到数据仓库后，一般情况下数据会被长期保留，当超过规定的期限才会被删除。通常数据仓库需要做的工作就是加载、查询和分析，一般不进行任何修改操作，是为了企业高层 人员决策分析之用。</p><h3 id="1-2-4反映历史变化"><a href="#1-2-4反映历史变化" class="headerlink" title="1.2.4反映历史变化"></a><strong>1.2.4反映历史变化</strong></h3><p>数据仓库不断从操作型数据库或其他数据源获取变化的数据，从而分析和预测需</p><p>要的历史数据，所以一般数据仓库中数据表的键码（维度）都含有时间键，以表明数据的历史时期信息，然后不断增加新的数据内容。通过这些历史信息可以对企业的发展历程和趋势做出分析和预测。数据仓库的建设需要大量的业务数据作为积累，并将这些宝贵的历史信息经过加工、整理，最后提供给决策分析人员，这是数据仓库建设的根本目的。</p><h2 id="●-数据仓库的技术手段"><a href="#●-数据仓库的技术手段" class="headerlink" title="● 数据仓库的技术手段"></a>● 数据仓库的技术手段</h2><p>传统数仓：一般采用关系型数据库软件，可存可算。</p><p>大数据数仓：使用HDFS来进行存储，hive&#x2F;SparkSql sql引擎，Spark&#x2F;Mapreduce来进行计算，lmpala即是计算引擎也是sql引擎（纯内存的，执行速度比hive和sparksql快）</p><p>项目中使用到的技术</p><p>数据离线采集工具：Flume，sqoop</p><p>数据实时同步工具：FlinkCDC，Canal，Debezium（<a href="https://mp.weixin.qq.com/s/PgYII2PQkfa3WnarPmrL9g%EF%BC%89">https://mp.weixin.qq.com/s/PgYII2PQkfa3WnarPmrL9g）</a></p><p>存储平台：HDFS</p><p>基础设施：Hive</p><p>运算引擎：MapReduce&#x2F;Spark</p><p>资源调度：Yarn</p><p>任务调度：Azkaban，oozie，DolphinScheduler，airflow</p><p>元数据管理：Atlas</p><p>大数据平台安全服务组件：Apache Ranger</p><p>可视化展示工具：Supset</p><p>资源监控报警工具：Zabbix，Grafana，open-falcon</p><p>列式存储数据库：Hbase</p><p>图形化统一交互平台：hue</p><p>数据库设计工具：PowerDesigner</p><p>etl工具：kettle，datastage，informatica</p><h2 id="●-数据研发流程"><a href="#●-数据研发流程" class="headerlink" title="● 数据研发流程"></a>● 数据研发流程</h2><p>需求分析调研（数据调研，需求调研，业务调研）：明确口径，评估排期，需求正规流程提交</p><p>指标管理：完善指标命名规范，指标同名同义，指标与业务强相关，明确指标构成要素。。</p><p>模型设计：完善开发流程规范，标准化业务调研，知识库文档集中管理，建立模型评审机构</p><p>etl开发：ODS&gt;DWD&gt;DWS&gt;ADS</p><p>数据验证：指定数据测试标准</p><p>任务调度：规范化调度参数配置</p><p>上线管理 </p><h2 id="●-数据建模工具"><a href="#●-数据建模工具" class="headerlink" title="● 数据建模工具"></a>● 数据建模工具</h2><p>PowerDesigner</p><p>power designer是能进行数据库设计的强大的软件，是一款开发人员常用的数据库建模工具</p><p>PDManer (国产建模工具)</p><h2 id="●-实时处理架构设计"><a href="#●-实时处理架构设计" class="headerlink" title="● 实时处理架构设计"></a>● 实时处理架构设计</h2><p>实时大数据的架构有两种：</p><ol><li><strong>lambda架构</strong></li></ol><p>在Lambda架构中，数据处理分为 Speed Layer、Batch Layer 和 Serving Layer 三个部分。</p><ul><li>Speed Layer 负责实时处理数据，计算逻辑直接对接流数据，尽量缩短数据处理的延迟，但由于流数据天生的数据质量不可控，尽管缩短了数据处理时间，但可能牺牲数据的正确性和完整性；</li><li>Batch Layer 负责批量规模性处理数据，可以保证数据的正确性和处理规模</li><li>Serving Layer 负责融合 Speed 和 Batch 两个部分的数据能力，对外提供简单一致的数据访问视图。</li></ul><p><strong>企业需要维护两个复杂的分布式系统，即Batch Layer和Speed Layer，并且保证他们逻辑上产生相同的结果输出到服务层中。</strong></p><ol start="2"><li><strong>Kappa 架构</strong></li></ol><p>为了将Batch Layer和Speed Layer进行统一架构，业界由Kafka团队提出了新的Kappa架构，基于Streaming是新的DB（数据库）的设计思想，要将所有的数据消费都基于Kafka，形成统一的数据出口，后续数据处理都基于流式（Kafka）数据源。</p><p>这个架构是随着Kafka上数据加工能力的提升而受到大家关注（特别是Flink框架加持，显著提升流数据处理能力）。Kappa试图解决多个计算引擎带来的开发、运维难题，只保留实时一套代码和数据。<strong>但在实践中，我们发现数据处理的复杂度不完全是一个单向的流式处理可以全部支持的，比如数据模型的演化，历史数据的修补更新，缓慢变化维的处理等等，都需要更灵活的数据建模能力。</strong></p><h2 id="●-数据仓库的设计"><a href="#●-数据仓库的设计" class="headerlink" title="● 数据仓库的设计"></a>● 数据仓库的设计</h2><p><a href="https://my.oschina.net/u/4631230/blog/4664053">https://my.oschina.net/u/4631230/blog/4664053</a></p><p>数据仓库的设计分为两种：</p><ol><li><strong>Inmon</strong></li></ol><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/328e7f90-fb3a-4184-83b1-96e39274d1a7.png" alt="img"></p><p>inmon 提倡企业数据协调与集成，采用第三范式的格式，注重etl过程，这个数据仓库中的数据口径是统一的。数据集市概念有落地的实现：每个数据集市都有独立的物理存储空间，并被单独使用，以部门为中心，为部分决策提供支持。这个架构中用户可以直接访问数据仓库。</p><p>流程：数据抽取–&gt;数据仓库–&gt;数据集市</p><ol start="2"><li><strong>Kimball</strong></li></ol><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/0e8825ca-1733-4fdb-a699-abc9f56e878d.png" alt="img"></p><p>Kimball 架构强调具有一致性维度的企业总线的重要作用，一般采用星型模型，由事实表和维表组成，注重事实表与维表的设计。数据集市只是一个概念，没有具体的物理存储。这个架构中用户不能访问数据仓库，只能访问展示区。</p><p>流程：数据集市(主题划分)–&gt;数据仓库–&gt; 数据抽取</p><h2 id="●-数仓分层"><a href="#●-数仓分层" class="headerlink" title="● 数仓分层"></a>● 数仓分层</h2><p>分为了三层：</p><h3 id="●-ODS-原始数据层"><a href="#●-ODS-原始数据层" class="headerlink" title="● ODS 原始数据层"></a>● ODS 原始数据层</h3><p>主要作用：直接映射操作数据（原始数据），以 json 格式文本文件存储，数据备份； 1）半结构化数据需要设计解析、2）对于增量表，需要设计增量表和全量表，然后合并成全量表数据。</p><ul><li>建模方法：与原始数据结构保持完全一致，不做字段名归一和字段类型统一的操作，如果需要则使用兼容的数据类型</li><li>存储周期：相对来说，存储周期较短；视数据规模，增长速度，以及业务的需求而定；</li><li>对于埋点 日志数据 ODS 层存储，通常可以选择 3 个月或者半年；存 1 年的是土豪公司（或者确有需要）</li><li>一般存储三个月会删除</li></ul><h3 id="●-DW-数据仓库层"><a href="#●-DW-数据仓库层" class="headerlink" title="● DW 数据仓库层"></a>● DW 数据仓库层</h3><p>DW是数据仓库的核心，从ODS层中获得的数据按照主题建立各种数据模型。DW又细分数据明细层DWD 和轻度汇总层DWS。</p><p>这一层和维度建模会有比较深的联系，业务数据是按照<strong>业务流程方便操作的角度</strong>来组织数据的，而统一数仓层是<strong>按照业务易理解的角度或者是业务分析的角度</strong>进行数据组织的，定义了一致的指标、维度，各业务板块、数据域都是按照统一的规范来建设，从而形成统一规范的<strong>标准业务数据体系</strong>，它们通常都是基于Kimball的维度建模理论来构建的，<strong>并通过一致性维度和数据总线来保证各个子主题的维度一致性</strong>。</p><ol><li><p>DWD 数仓明细层</p><p>dwd 主要是对 ods 层做一些数据清洗和规范化的操作，扁平化明细数据），然后加工成面向数仓的基础明细表，这个时候可以加工一些面向分析的大宽表。</p><p>DWD层应该是覆盖所有系统的、完整的、干净的、具有一致性的数据层。在DWD层会根据维度模型，设计事实表和维度表，也就是说DWD层是一个非常规范的、高质量的、可信的数据明细层。</p><ul><li>存储格式：以 orc &#x2F; parquet 文件格式存储 </li><li>存储周期：6 个月</li></ul></li><li><p>DIM 数仓维度层</p><p>维表层，所以其实维度层就是大量维表构成的，为了统一管理这些维度表，所以我们就建设维度层，维度表本身也有很多类型，例如稳定维度维表，渐变维度维表。</p><p>维度表一般为<strong>单一主键</strong>，在ER模型中，实体为客观存在的事务，会带有自己的描述性属性，属性一般为文本性、描述性的，这些描述被称为维度。维度建模的核心是<strong>数据可以抽象为事实和维度</strong>，维度即观察事物的角度，事实某一粒度下的度量词，<strong>维度一定是针对实体而言的</strong></p><p>每个维度表都<strong>包含单一的主键列</strong>。维度表的主键可以作为与之关联的任何事实表的外键，当然，维度表行的描述环境应与事实表行完全对应。维度表通常比较宽，是扁平型非规范表，包含大量的低粒度的文本属性。例如customer（客户表）、goods(商品表)、d_time(时间表)这些都属于维度表，这些表都有一个唯一的主键，然后在表中存放了详细的数据信息。</p><ul><li>存储各种维表</li></ul></li></ol><h4 id="●-设计原则"><a href="#●-设计原则" class="headerlink" title="● 设计原则"></a>● 设计原则</h4><p>维度表通常比较宽，包含多个属性、是扁平的规范表，实际应用中包含几十个或者上百个属性的维度并不少见，所以维度表应该包括一些有意义的描述，方便下游使用。</p><p>维度表的维度属性，应该尽可能的丰富，所以维度表中，经常出现一些反范式的设计，把其他维度属性并到主维度属性中，达到易用少关联的效果。</p><p>维度表的设计包括维度选择，主维表的确定，梳理关联维度，定义维度属性的过程。</p><p>维度的选择一般从报表需求和从业务人员的交谈中发现，主要用于过滤、分组、排序，主维度表一般从业务库直接同步，比如用户表，但是数仓的本身也会有自己的维度，这是因为数仓是面向分析的，所以会有很多从分析的角度出发的维度。</p><p>关联维度主要是不同业务系统或者同一业务系统的表之间存在关联性(范式建模)，根据对业务表的梳理，确定哪些表和主维度表之间存在关联关系，并选择其中的某些表用于生成维度属性。</p><ol start="3"><li><p>DWS 轻度（公共）汇总层（维度退化（大宽表），这一层会进行轻度汇总，粒度比明细数据稍粗，<strong>基于DWD层上的基础数据，整合汇总成分析某一个主题域的服务数据</strong>，一般是也是面向分析宽表或者是面向某个注意的汇总表。DWS层应覆盖80%的应用场景，这样我们才能快速响应数据需求，否则的话，如果很多需求都要从ods开始做的话，那说明我们的数仓建设是不完善的。</p><p><strong>一般采用维度模型方法作为理论基础，更多的采用一些维度退化手法，将维度退化至事实表中，减少维度表与事实表的关联，提高明细数据表的易用性；同时在汇总数据层要加强指标的维度退化，采用更多的宽表化手段构建公共指标数据层，提升公共指标的复用性，减少重复加工</strong>。</p><p>从DWD轻度聚合后的数据（用户连续活跃区间记录表））例如按照业务划分，例如流量，订单，用户等，生成字段比较多的宽表，用于后续的业务查询，OLAP分析，数据分析等。</p><ul><li>存储格式：以 ORC&#x2F;PARQUET 文件格式存储 </li><li>存储周期：1 年</li></ul></li></ol><p><strong>清洗过滤</strong> ：</p><ul><li><p>去除 json 数据体中的废弃字段（前端开发人员在埋点设计方案变更后遗留的无用字段）： </p></li><li><p>过滤掉 json 格式不正确的（脏数据）</p></li><li><p>过滤掉日志中 account 及 deviceid 全为空的记录 </p></li><li><p>过滤掉日志中缺少关键字段（properties&#x2F;eventid&#x2F;sessionid 缺任何一个都不行）的记录！ </p></li><li><p>过滤掉日志中不符合时间段的记录（由于 app 上报日志可能的延迟，有数据延迟到达） </p></li><li><p>对于 web 端日志，过滤爬虫请求数据（通过 useragent 标识来<strong>分析）</strong></p></li></ul><p><strong>数据规范处理：</strong></p><ul><li>Boolean 字段，在数据中有使用 1&#x2F;0&#x2F;-1 标识的，也有使用 true&#x2F;false 表示的，统一为 Y&#x2F;N&#x2F;U </li><li>字符串类型字段，在数据中有空串，有 null 值，统一为 null 值 </li><li>日期格式统一, 2020&#x2F;9&#x2F;2 2020-9-2 2020-09-02 都统一成 YYYY-MM-dd</li></ul><p><strong>维度集成</strong> ：</p><ul><li>将日志中的 GPS 经纬度坐标解析成省、市、县（区）信息；（为了方便后续的地域维度分析） </li><li>将日志中的 IP 地址解析成省、市、县（区）信息；（为了方便后续的地域维度分析）</li></ul><h4 id="●-设计原则-1"><a href="#●-设计原则-1" class="headerlink" title="● 设计原则"></a>● 设计原则</h4><p><strong>一致性维度规范</strong><br>公共层的维度表中相同维度属性在不同物理表中的字段名称、数据类型、数据内容必须保持一致，因为这样可以降低我们在使用过程中犯错误的概率，例如使用了不正确的字段，或者因为数据类型的原因导致了一些奇怪的错误</p><p><strong>维度的组合与拆分</strong><br>将维度所描述业务相关性强的字段在一个物理维表实现。相关性强是指经常需要一起查询或进行报表展现、两个维度属性间是否存在天然的关系等。例如，商品基本属性和所属品牌。</p><h3 id="●-ADS-数据应用层"><a href="#●-ADS-数据应用层" class="headerlink" title="● ADS 数据应用层"></a>● ADS 数据应用层</h3><p>数据应用层ApplicationDataService面向业务定制的应用数据，主要提供给数据产品和数据分析使用的数据，一般会放在ES，MYSQL，Redis等系统供线上系统使用，也可以放在Hive中供数据分析和数据挖掘使用，或者使用一下其他的大数据工具进行存储和使用。</p><p>数据应用层，按照业务的需要，然后从统一数仓层和DIM进行取数，并面向业务的特殊需求对数据进行加工,以满足业务和性能的需求。ADS 层因为面向的实众多的需求，所以这一层没有太多的规范，只需要按照命名规范来进行就可以了。</p><ul><li>数据内容：根据业务人员需求，从 DWS 计算出来的报表 </li><li>存储格式：以 ORC&#x2F;PARQUET 文件格式存储</li><li>存储周期：3 年</li></ul><h3 id="●-DM-数据集市层"><a href="#●-DM-数据集市层" class="headerlink" title="● DM 数据集市层"></a>● DM 数据集市层</h3><p>主要是提供数据产品和数据分析的数据，一般会存放在ES、Mysql、也可能直接存储在hive中或者druid供数据分析和数据挖掘使用。主要<strong>解决部门用户报表和分析需求</strong>而建立数据库，数据集市就代表数据仓库的主题域。</p><p>DM 是面向单个主题的，所以它不会从全局考虑进行建设，只专注于自己的数据、往往是某个业务线，例如流量主题、社交主题、电商主题等等。</p><h2 id="●-为什么要分层？"><a href="#●-为什么要分层？" class="headerlink" title="● 为什么要分层？"></a>● 为什么要分层？</h2><p>分层管理和计算</p><p>进行漏斗式的的统计，将复杂的问题分成多层进行统计简化任务</p><p>减少重复开发，减少重复计算，增加结果复用性</p><p>隔离原始数据，保证数据完整性</p><h2 id="●-DWS主题划分"><a href="#●-DWS主题划分" class="headerlink" title="● DWS主题划分"></a>● DWS主题划分</h2><ul><li><p>用户分布分析主题 ：页面浏览事件的：一次会话，一条记录</p></li><li><p>交互事件概况主题</p><ul><li>分享事件，主题明细表</li><li>点赞事件，主题明细表</li><li>收藏事件，主题明细表</li></ul></li><li><p>广告运营位分析主题</p></li><li><p>站外投放广告分析主题</p></li><li><p>优惠券分析主题</p></li><li><p>红包分析主题</p></li><li><p>用户活跃度分析主题</p><ul><li>用户连续活跃区间记录（类似拉链表）</li></ul></li><li><p>新用户留存分析主题</p></li><li><p>常规固定漏斗模型分析主题：漏斗模型，是用来分析业务转化率的！</p></li></ul><h2 id="●-表是怎么命名的"><a href="#●-表是怎么命名的" class="headerlink" title="● 表是怎么命名的"></a>● 表是怎么命名的</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzI2MDQzOTk3MQ==&amp;mid=2247484635&amp;idx=1&amp;sn=aaec70cb5cf5900e39849ff7b25a8c6a&amp;chksm=ea68ee17dd1f67018f93c96a6fcd00563022589ba0b7913394e132c5ec9791fc2ffd3ac6998f&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzI2MDQzOTk3MQ==&amp;mid=2247484635&amp;idx=1&amp;sn=aaec70cb5cf5900e39849ff7b25a8c6a&amp;chksm=ea68ee17dd1f67018f93c96a6fcd00563022589ba0b7913394e132c5ec9791fc2ffd3ac6998f&amp;scene=21#wechat_redirect</a></p><p>全量表(df),增量表(di),追加表(da)，拉链表(dz)</p><p>ods dwd：**[层次].[业务]<em>[表内容]</em>[周期+处理方式]**</p><p>dw  dws：**[层次].[主题]<em>[表内容]</em>[周期+处理方式]** </p><p>如：ods.test_order_info_df</p><p>wedw_ods表示层次，test表示主题，order_info表示表内容，d表示周期(天)，f表示处理方式(全量抽取)</p><h3 id="●-拆分主题表的意义"><a href="#●-拆分主题表的意义" class="headerlink" title="● 拆分主题表的意义"></a>● 拆分主题表的意义</h3><ol><li>减少了分析时的检索数据量</li><li>方便做成统一二维表结构。方便impala其他组件查询。方便kylin进行cube的构建</li></ol><h2 id="●-Cube是什么？"><a href="#●-Cube是什么？" class="headerlink" title="● Cube是什么？"></a>● Cube是什么？</h2><p>多维数据立方体，可以从不同的角度进行描述数据</p><h2 id="●-建模的方法"><a href="#●-建模的方法" class="headerlink" title="● 建模的方法"></a>● 建模的方法</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzI2MDQzOTk3MQ==&amp;mid=2247485443&amp;idx=1&amp;sn=8307b0922a3ee002446936f83153bcc6&amp;chksm=ea68e2cfdd1f6bd96cd1de5fa46a3191c0a70e4e3ca570ed57c1c647abd624fcf8a0901f08cc&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzI2MDQzOTk3MQ==&amp;mid=2247485443&amp;idx=1&amp;sn=8307b0922a3ee002446936f83153bcc6&amp;chksm=ea68e2cfdd1f6bd96cd1de5fa46a3191c0a70e4e3ca570ed57c1c647abd624fcf8a0901f08cc&amp;scene=21#wechat_redirect</a></p><p>建模的方式有两种：</p><ul><li>关系建模</li></ul><p>关系模型主要应用与OLTP系统中，为了保证数据的一致性以及避免冗余，所以大部分业务系统的表都是遵循第三范式的。</p><ol><li>属性不可切割</li><li>不能存在“ 部分函数依赖 ”</li><li>不能存在传递依赖</li></ol><ul><li>维度建模</li></ul><p>维度模型主要应用于OLAP系统中，因为关系模型虽然冗余少，但是在大规模数据，跨表分析统计查询过程中，会造成多表关联，这会大大降低执行效率。</p><p>把相关各种表整理成两种：事实表和维度表两种。所有维度表围绕着事实表进行解释。</p><p>维度建模分为三种：</p><ol><li>星形模型</li></ol><p> 星形模型中有一张事实表，以及零个或多个维度表，事实表与维度表通过主键外键相关联，维度表之间没有关联，当所有维表都直接连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型。</p><ol start="2"><li>雪花模型</li></ol><p>当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。</p><p>它的优点是 : 通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。缺点是：多表join造成shuffle，大数据不适用这种模型</p><ol start="3"><li>星座模型</li></ol><p>星座模型是由星型模型延伸而来，星型模型是基于一张事实表而星座模式是基于多张事实表，并且共享维度表信息，这种模型往往应用于数据关系比星型模型和雪花模型更复杂的场合。<strong>星座模型需要多个事实表共享维度表</strong>，因而可以视为星形模型的集合，故亦被称为星系模型。</p><p>多张事实表共享维度表，并且维度之间有关联。</p><h2 id="●-遇到了什么问题"><a href="#●-遇到了什么问题" class="headerlink" title="● 遇到了什么问题"></a>● 遇到了什么问题</h2><p>数仓开发中因为开发的不规范问题，业务需求统计口径不一致，导致了数据质量差。</p><h2 id="●-CRM（客户关系管理）"><a href="#●-CRM（客户关系管理）" class="headerlink" title="● CRM（客户关系管理）"></a>● CRM（客户关系管理）</h2><p>客户关系管理简称<a href="https://baike.baidu.com/item/CRM">CRM</a>（Customer Relationship Management）</p><p>为提高核心竞争力，利用相应的信息技术以及<a href="https://baike.baidu.com/item/%E4%BA%92%E8%81%94%E7%BD%91%E6%8A%80%E6%9C%AF/617749">互联网技术</a>协调企业与顾客间在<a href="https://baike.baidu.com/item/%E9%94%80%E5%94%AE/239410">销售</a>、<a href="https://baike.baidu.com/item/%E8%90%A5%E9%94%80/150434">营销</a>和服务上的交互，从而提升其<a href="https://baike.baidu.com/item/%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8F/260899">管理方式</a>，向客户提供创新式的个性化的客户交互和服务的过程。</p><p>最终目标是吸引新客户、保留老客户以及将已有客户转为忠实<a href="https://baike.baidu.com/item/%E5%AE%A2%E6%88%B7/1598984">客户</a>，增加市场。</p><h2 id="●-RFM模型"><a href="#●-RFM模型" class="headerlink" title="● RFM模型"></a>● RFM模型</h2><p>这是电商项目中衡量客户价值和客户创利能力的重要工具和手段。</p><p>RFM在众多的<a href="https://baike.baidu.com/item/%E5%AE%A2%E6%88%B7%E5%85%B3%E7%B3%BB%E7%AE%A1%E7%90%86/254554">客户关系管理</a>(CRM)的分析模式中，RFM模型是被广泛提到的。</p><p>客户数据库中有3个神奇的要素，这3个要素构成了数据分析最好的指标：</p><p>最近一次消费 (Recency)</p><p>消费频率 (Frequency) 次数</p><p>消费金额 (Monetary)</p><h2 id="●-缓慢变化维"><a href="#●-缓慢变化维" class="headerlink" title="● 缓慢变化维"></a>● 缓慢变化维</h2><h3 id="●-关于缓慢变化维"><a href="#●-关于缓慢变化维" class="headerlink" title="● 关于缓慢变化维"></a>● 关于缓慢变化维</h3><pre><code>什么是缓慢变化维？有哪些常见的数据案例？数据仓库与缓慢变维的关系？</code></pre><p>在从 OLTP 业务数据库向 DW 数据仓库抽取数据的过程中，特别是<strong>第一次导入之后的每一次增量抽取往往会遇到这样的问题</strong>、<strong>在新业务上会遇到占比很高的问题</strong>：业务数据库中的一些数据发生了更改，到底要不要将这些变化也反映到数据仓库中？在数据仓库中，<strong>哪些数据应该随之变化</strong>？<strong>哪些可以不用变化？</strong>考虑到这些变化，在数据仓库中的维度表又应该如何设计以满足这些需要。</p><p>在业务数据库中数据的变化是非常自然和正常的，例如：顾客的<strong>联系方式</strong>、<strong>手机号码</strong>等信息可能随着顾客的所在地的更改发生变化；<strong>商品价格</strong>在不同时期有上涨和下降的变化。</p><p>那么在业务数据库中，很自然的就会修改并马上反映到实际业务当中去。但是在<strong>数据仓库</strong>中，其<strong>数据主要的特征</strong>：<strong>①是静态历史数据</strong>，<strong>②是少改变不删除</strong>，<strong>③是定期增长</strong>，其作用主要用来<strong>数据分析</strong>。</p><p>因此分析的过程中对历史数据就提出了要求，有一些数据是需要能够反映出在周期内的变化历史，有一些数据缺不需要，那么这些数据应该如何来控制。</p><h3 id="●-案例解析"><a href="#●-案例解析" class="headerlink" title="● 案例解析"></a>● 案例解析</h3><pre><code>通过案例了解缓慢变化维</code></pre><p>假设在第一次从业务数据库中加载了一批数据到数据仓库中，当时业务数据库有这样的一条顾客的信息。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/image-20200910184044539.png" alt="image-20200910184044539"></p><p>顾客BIWORK，居住在北京，目前是一名BI的开发工程师。假设BIWORK因为北京空气质量 PM2.5 等原因从北京搬到了三亚。那么这条信息在业务数据库中应该被更新了。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/image-20200910184107053.png" alt="image-20200910184107053"></p><p>那么当下次从业务数据库中抽取这类信息的时候，数据仓库又应该如何处理呢？我们假设在数据仓库中实现了与业务数据库之间的同步，数据仓库中也直接将词条数据修改更新。后来我们创建报表做一些简单的数据统计分析，这时在数据仓库中所有对顾客 BIWORK 的销售都指向了 BIWORK 新的所在地 - 城市三亚，但是实际上 BIWORK 在之前所有的购买都发生在 BIWORK 居住在北京的时候。这是一个非常简单的例子，它描述了因一些基本信息的更改可能会引起数据归纳和分析出现的问题。但是有时，这种场景的的确确可能是存在的。</p><p>为了解决类似于这样的问题需要了解数据仓库中的一个非常重要的概念 - 缓慢渐变维度。</p><h3 id="●-解决方案"><a href="#●-解决方案" class="headerlink" title="● 解决方案"></a>● 解决方案</h3><h4 id="●-不记录历史数据"><a href="#●-不记录历史数据" class="headerlink" title="● 不记录历史数据"></a>● 不记录历史数据</h4><pre><code>解决方案说明？该解决方案适用于哪些应用场景？</code></pre><p><strong>不记录历史数据，新数据覆盖旧数据。</strong>在数据仓库中，我们可以保持业务数据和数据仓库中的数据始终处于一致。可以在 Customer 维度中使用来自业务数据库中的 Business Key - CustomerID 来追踪业务数据的变化，一旦发生变化那么就将旧的业务数据覆盖重写。DW中的记录根据业务数据库中的CustomerID 获取了最新的 City 信息，直接更新到 DW 中。针对无业务意义、无分析意义的数据可以直接进行覆盖，例如地区编码、国家名称，具体由实际业务情况决定。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/image-20200910185336595.png" alt="image-20200910185336595"></p><h4 id="●-保存每次变更"><a href="#●-保存每次变更" class="headerlink" title="● 保存每次变更"></a>● 保存每次变更</h4><pre><code>解决方案说明？该解决方案适用于哪些应用场景？</code></pre><p><strong>保存多条记录,直接新添一条记录，同时保留原有记录，并用单独的专用的字段保存区别。</strong>在数据仓库中更多是对相对静态的历史数据进行数据的汇总和分析，因此会<strong>尽可能的维护来自业务系统中的历史数据</strong>，能够真正捕获到这种历史数据的变化。</p><p><strong>（案例）</strong>假设 <code>BIWORK</code> 在 2012年的时候购买额度整体平稳，但是从2013年开始购买额度减少了，出现的原因可能与所在的城市有关系，在北京的门店可能比在三亚的门店相对要多一些。像这种情况，就不能很简单在数据仓库中将 BIWORK 当前所在城市直接更新，而应该新增加一条数据来说明现在 <code>BIWORK</code> 所在地是在 <code>Sanya</code>。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/11161730-a90e8ac2676840d8a1822761bc1e7d78.png" alt="img"></p><h5 id="●-新增代理键-DW唯一键"><a href="#●-新增代理键-DW唯一键" class="headerlink" title="● 新增代理键\DW唯一键"></a>● 新增代理键\DW唯一键</h5><p>但是如果仅仅在 DW 中新增一条新的数据仍然会出现新的问题，因为在 DW 中标识这个顾客是通过<code>CustomerID</code> 来实现的，这条 <code>CustomerID</code> 来源于业务数据库，它是唯一的。然而在 DW 中新增一条数据来保存业务数据库中历史信息，就无法保证这条数据在 DW 中的唯一性了，其它的 DW 数据表关联到这张表就无法知道应该如何引用这个 <code>Customer</code> 的信息。实际上，如果 <code>CustomerID</code> 在 DW 中也作为主键来唯一标识 <code>Customer</code> 的话，在插入新数据的时候就会发生失败。</p><p>因此我们需要继续保持 <code>Business Key</code> 业务键，因为它是关联到业务数据库的唯一纽带。做出改变的部分就是新增加一个 Key，一个数据仓库的键。在数据仓库的术语里面，这个唯一标识数据仓库表记录的键我们称之为 <code>Surrogate Key</code> 代理键，通常设置为DW表的主键。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/image-20200910192338378.png" alt="image-20200910192338378"></p><p>在上面这张表中，其中 <code>CustomerID - Business Key</code> 业务键，用来连接业务数据库和数据仓库的键，注意无论在业务数据库还是数据仓库无论任何时候都不应该发生改变。<code>DWID - Surrogate Key</code> 代理键，一般设置为 DW 维度表的主键，用来在数据仓库内部中的维度表和事实表建立关联。</p><h5 id="●-新增有效性标识字段"><a href="#●-新增有效性标识字段" class="headerlink" title="● 新增有效性标识字段"></a>● 新增有效性标识字段</h5><p>接着上面的表结构讲，光这样设置了新的 Surrogate Key - DWID 是不够的，因为还需要告诉数据仓库哪一条信息是现在正在使用的。当然可以根据 DWID 的顺序来查出最新的记录，但是每次都要比较 CustomerID 然后找出最大的 DWID 这样的查询比较麻烦。因此可以额外一个标志表示这条数据是最新更改的。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/image-20200910192434706.png" alt="image-20200910192434706"></p><p>另外的一种方式就是通过起始时间来标识，<strong>Valid To 为 NULL</strong> 的标识当前数据。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/image-20200911084924957.png" alt="image-20200911084924957"></p><p>当然，也有将两者都综合的。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/image-20200911084945336.png" alt="image-20200911084945336"></p><p>还有一种情况就是混合使用 Type 1 和 Type 2 的，比如说 Occupation 这个字段在业务数据库中发生了变化，但是可以不用维护这个历史信息，因此可能的做法是直接将最新的 Occupation 在数据仓库中覆盖掉。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/11162011-2bff5e451a39412e93fe461c9e0797d2.png" alt="img"></p><p>根据实际情况，还有一种做法就是全部覆盖掉。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/11162017-b197cc4bfd2943438fdb1845cafd0856.png" alt="img"></p><h4 id="●-保存本次和上一次数据"><a href="#●-保存本次和上一次数据" class="headerlink" title="● 保存本次和上一次数据"></a>● 保存本次和上一次数据</h4><p><strong>添加历史列，用不同的字段保存变化痕迹.它只能保存两次变化记录.适用于变化不超过两次的维度。</strong>实际上 Type 1 and 2 可以满足大多数需求了，但是仍然有其它的解决方案，比如说 Type 3 SCD。 Type 3 SCD 希望只维护更少的历史记录。比如说把要维护的历史字段新增一列，然后每次只更新 Current Column 和 Previous Column。这样，只保存了最近两次的历史记录。但是如果要维护的字段比较多，就比较麻烦，因为要更多的 Current 和 Previous 字段。所以 Type 3 SCD 用的还是没有 Type 1 和 Type 2 那么普遍。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/image-20200910192102887.png" alt="image-20200910192102887"></p><h3 id="●-关于代理键的说明"><a href="#●-关于代理键的说明" class="headerlink" title="● 关于代理键的说明"></a>● 关于代理键的说明</h3><p>为什么使用代理键，有什么好处？</p><ul><li>假设业务数据库来自于不同的系统，对这些数据进行整合时有可能出现相同的 <code>Business Key</code>，这时通过<code>Surrogate Key</code>就可以解决这个问题。</li><li>一般来自业务数据库中的 <code>Business Key</code> 可能字段较长，比如 GUID，长字符串标识等，使用 <code>Surrogate Key</code> 可以直接设置成整形的。事实表本身体积就很大，关联 <code>Surrogate Key</code> 与关联 <code>Business Key</code> 相比，<code>Surrogate Key</code> 效率更高，并且节省事实表体积。</li><li>最重要的一点就是上面举到的这个例子，使用 <code>Surrogate Key</code> 可以更好的解决这种缓慢渐变维度，维护历史信息记录。</li></ul><p>什么时候可以不用代理键？我觉得可以结合我们的实际业务，比如像有些业务表本身的 <code>Business Key</code> 就已经是整形的了，并且表中的属性基本上不随着时间或地理发生改变。比如像某些国家名称，地区编码等等基本上不会怎么发生改变，即使改变了也不需要维护历史记录这样的情况下可以直接使用业务数据库中的 <code>Business Key</code> 而不需要设置新的 Surrogate Key。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/11161918-c7e4c8e4c2514526ba2ec55ef97088e2.png" alt="img"></p><h3 id="●-问题总结"><a href="#●-问题总结" class="headerlink" title="● 问题总结"></a>● 问题总结</h3><p>问题的本身并不是在技术实现上有什么难度。最困难的地方在于，应该根据<strong>什么情况</strong>、<strong>业务场景</strong>、<strong>技术成本</strong>，来选择1还是2还是3。</p><ul><li>若遇到涉及关键数据指标的，则应尽可能保留所有记录；</li><li>遇到没有太大业务价值的，可只保留最后一次记录即可；</li><li>如果是就目前看来还没有使用到，但未来会使用到或者还不知道如何使用时，也可以先保留所有记录；</li><li>如果是频繁发生变化、可能存在分析价值、但目前尚未发现其规律性，也可以先单独找个地方存储，以后再拿出来看如何处理；</li></ul><p>缓慢变化维一般常见于数据基础层，若存在上述3类方案的混用，则针对数据使用人员可能会产生困扰。此时应完善数据字典，针对独立的业务分析场景也可以在数据主题层和数据集市层解决这项问题。 </p><pre><code class="mermaid">graph LRA[是否涉及关键数据指标\影响数据分析\是否有业务价值] --&gt; B[选取技术方案]B --&gt; C[A.不记录历史数据]B --&gt; D[B.保存本次和上一次数据]B --&gt; E[C.保存每次变更]      E --&gt; F[&quot;创建代理键(DW主键)&quot;]      E --&gt; G[创建有效性标识]            G --&gt; H[创建有效唯一标识]            G --&gt; I[创建起始日期时间戳]</code></pre><h2 id="●-数据漂移"><a href="#●-数据漂移" class="headerlink" title="● 数据漂移"></a>● 数据漂移</h2><h3 id="●-关于数据漂移"><a href="#●-关于数据漂移" class="headerlink" title="● 关于数据漂移"></a>● 关于数据漂移</h3><p>通常我们把从源系统同步进人数据仓库的第一层数据称为ODS或者staging层数据，阿里巴巴统称为ODS 。数据漂移是ODS 数据的一个顽疾，通常是指 ODS 表的同一个业务日期数据中包含前一天或后凌晨附近的数据或者丢失当天的变更数据。</p><p>由于ODS需要承接面向历史的细节数据查询需求，这就需要物理落地到数据仓库的ODS表按时间段来切分进行分区存储，通常的做法是按某些时间戳字段来切分，而实际上往往由于时间戳字段的准确性问题导致发生数据漂移。</p><h3 id="●-常见的时间戳字段"><a href="#●-常见的时间戳字段" class="headerlink" title="● 常见的时间戳字段"></a>● 常见的时间戳字段</h3><p>常见的4类时间戳字段</p><ul><li><strong>数据库表</strong>中用来标识**”数据记录更新时间”**的时间戳字段（假设这类字段叫 <code>modified time</code> ）。</li><li><strong>数据库日志</strong>中用来标识**”数据记录更新时间”**的时间戳字段·（假设这类宇段叫 <code>log_time</code> ）。</li><li><strong>数据库表</strong>中用来记录具体**”业务过程发生时间”**的时间戳字段 （假设这类字段叫 <code>proc_time</code> ）。</li><li>标识**”数据记录被抽取到时间”**的时间戳字段（假设这类字段 <code>extract time</code> ）。</li></ul><p>理论上这几个时间应该是一致的，但是在实际生产中，这几个时间往往会出现差异，可能的原因有以下几点：</p><ul><li>由于数据抽取是需要时间的 <code>extract_time</code> 往往会晚于前三个时间。</li><li>前台业务系统手工订正数据时未更新 <code>modified_time</code></li><li>由于网络或者系统压力问题，<code>log_time</code> 或者 <code>modified_time</code> 会晚 <code>proc_time</code></li></ul><h3 id="●-常见的数据漂移情况"><a href="#●-常见的数据漂移情况" class="headerlink" title="● 常见的数据漂移情况"></a>● 常见的数据漂移情况</h3><p>通常的做法是根据其中的某个字段来切分ODS表，这就导致产生数据漂移。常见的数据漂移几种场景如下：</p><ul><li>根据 <code>extract_time</code> 来获取数据。这种情况数据漂移的问题最明显</li><li>根据 <code>modified_time</code> 限制。在实际生产中这种情况最常见，但是往往会发生不更新 <code>modified time</code> 而导致的数据遗漏，或者凌晨时间产生的数据记录漂移到后天。</li><li>根据 <code>log_time</code> 限制。由于网络或者系统压力问题， <code>log_time</code> 会晚 <code>proc_time</code>，从而导致凌晨时间产生的数据记录漂移到后一天。例如，在淘宝“双 11 ”大促期间凌晨时间产生的数据量非常大，用户支付需要调用多个接口，从而导致 <code>log time</code> 晚于实际的支付时间。</li><li>根据 <code>proc_time</code> 限制。仅仅根据 <code>proc_time</code> 限制，我们所获取的ODS 表只是包含一个业务过程所产生的记 ，会遗漏很多其他过程的变化记录，这违背了 ODS 和业务系统保持一致的设计原则。</li></ul><h3 id="●-两种处理方式"><a href="#●-两种处理方式" class="headerlink" title="● 两种处理方式"></a>● 两种处理方式</h3><p>处理方法主要有以下两种：</p><ul><li><p><strong>多获取后一天的数据：</strong>既然很难解决数据漂移的问题，那么就在ODS 每个时间分区中向前、向后多冗余一些数据，保障数据只会多不会少，而具体的数据切分让下游根据自身不同的业务场景用不同的业务时间proc_time 来限制。但是这种方式会有一些数据误差，例如一个订单是当天支付的，但是第二天凌晨申请退款关闭了该订单，那么这条记录的订单状态会被更新，下游在统计支付订单状态时会出现错误。</p></li><li><p><strong>通过多个时间戳字段限制时间来获取相对准确的数据</strong></p></li><li><p>首先根据log_time 分别冗余前一天最后15 分钟的数据和后一天凌晨开始15 分钟的数据，并用modified time过滤非当天数据，确保数据不会因为系统问题而遗漏。</p></li><li><p>然后根据log_ time 获取后一天15 分钟的数据z 针对此数据，按照主键根据log_time 做升序排列去重。因为我们需要获取的是最接近当天记录变化的数据（数据库日志将保留所有变化的数据，但是落地到O DS 表的是根据主键去重获取最后状态变化的数据）。</p></li><li><p>最后将前两步的结果数据做全外连</p></li></ul><h3 id="●-淘宝交易订单案例"><a href="#●-淘宝交易订单案例" class="headerlink" title="● 淘宝交易订单案例"></a>● 淘宝交易订单案例</h3><p>下面来看处理淘宝交易订单的数据漂移的实际案例。</p><p>我们在处理“双11”交易订单时发现，有一大批在11月11日23:59:59 左右支付的交易订单漂移到了12日。主要原因是用户下单支付后系统需要调用支付宝的接口而有所延迟，从而导致这些订单最终生成的时间跨天了。即<code>modified_time</code> 和 <code>log_time</code> 都晚于 <code>proc_time</code> 。</p><p>如果订单只有一个支付业务过程，则可以用支付时间来限制就能获取到正确的数据。但是往往实际订单有多个业务过程： 下单、支付、成功，每个业务过程都有相应的时间戳字段，并不只有支付数据会漂移。</p><p>如果直接通过多获取后一天的数据，然后限制这些时间，则可以获取到相关数据，但是后一天的数据可能已经更新多次，我们直接获取到的那条记录已经是更新多次后的状态，数据的准确性存在一定的问题。因此，我们可以根据实际情况获取后一天15 分钟的数据，并限制多个业务过程的时间戳字段（下单、支付、成功）都是“双l l ”当天<br>的，然后对这些数据按照订单的 <code>mo dified_time</code> 做升序排列，获取每个订单首次数据变更的那条记录。</p><p>此外，我们可以根据 <code>log_time</code> 分别冗余前一天最后15 分钟的数据和后一天凌晨开始15 分钟的数据，并用<code>modified_time</code> 过滤非当天数据，针对每个订单按照 <code>log_time</code> 进行降序排列，取每个订单当天最后一次数据变更的那条记录。最后将两份数据根据订单做全外连接，将漂移数据回补到当天数据中。</p><h2 id="●-参考文献资料"><a href="#●-参考文献资料" class="headerlink" title="● 参考文献资料"></a>● 参考文献资料</h2><table><thead><tr><th>资料名称</th><th>涉及内容</th><th>资料来源</th></tr></thead><tbody><tr><td><font size="2">缓慢变化维常见的三种类型及原型设计</font></td><td><font size="2">缓慢变化维；</font></td><td><font size="2"><a href="https://www.cnblogs.com/xqzt/p/4472005.html">CSDN</a></font></td></tr><tr><td><font size="2">数据漂移</font></td><td><font size="2">数据漂移；</font></td><td><font size="2">大数据之路</font></td></tr></tbody></table><h2 id="●-面试题"><a href="#●-面试题" class="headerlink" title="● 面试题"></a>● 面试题</h2><h3 id="DWD做了哪些事？"><a href="#DWD做了哪些事？" class="headerlink" title="DWD做了哪些事？"></a>DWD做了哪些事？</h3><p>清洗过滤，统一数据格式，去除掉废弃字段和json格式不正确的脏数据，过滤去除为空的数据，过滤缺少关键字段的记录，web端日志过滤请求爬虫，规范不一致，命名不规范的数据都会被处理。然后加工成面向数仓的基础明细表，这个时候可以加工一些面向分析的大宽表。DWD层应该是覆盖所有系统的、完整的、干净的、具有一致性的数据层。</p><p><strong>统一数据格式</strong>：</p><p>1&#x2F;-1还有true&#x2F;false  都变成Y&#x2F;N</p><p>空字符串和null都变成null</p><p>日期都统一成YYYY-MM-dd</p><p><strong>地理位置维度集成</strong>：</p><p>GPS经纬度解析成省市县</p><p>ip地址解析成省市县</p><p><strong>新老用户标记</strong></p><p><strong>设备和账号绑定</strong></p><h3 id="DWS做了哪些事？"><a href="#DWS做了哪些事？" class="headerlink" title="DWS做了哪些事？"></a>DWS做了哪些事？</h3><h4 id="主题表和轻度聚合"><a href="#主题表和轻度聚合" class="headerlink" title="主题表和轻度聚合"></a>主题表和轻度聚合</h4><p>根据后续报表需求，做一些轻度聚合计算。按照主题来进行数据的聚合。</p><p>做一些日粒度的，周粒度的，月粒度，季粒度的相关指标聚合计算；</p><p>主题上，主要有流量分析，用户分析，交互事件分析，广告活动分析，转化分析，订单分析，GMV 分析、购物车分析等</p><p>建立一些主题表，并按照不同粒度进行统计。</p><h4 id="用户画像"><a href="#用户画像" class="headerlink" title="用户画像"></a>用户画像</h4><p>用户画像：性别，消费级别，价值等级，流失风险等</p><p>用户的画像标签，分为 3 个层次： </p><p> 事实标签；</p><p>  模型标签；</p><p>  决策标签；</p><h3 id="说一个你负责的统计指标？"><a href="#说一个你负责的统计指标？" class="headerlink" title="说一个你负责的统计指标？"></a>说一个你负责的统计指标？</h3><p>每天的新用户留存统计报表、连续活跃天数统计月报表</p><p><strong>DWS概况主题的会话聚合表</strong> ： 可以支撑的报表：日期，省份，手机型号，跳出页，入口页，时段，新老访客等维度随意组合的 ADS 日\周\月 报表</p><p><strong>DWS用户分布分析主题的流量用户聚合表</strong> : 可以支撑的报表：新老访客，省市，手机型号，app版本，网络类型等 随意组合维度的 ADS 日\周\月 报表</p><p><strong>Dws用户连续活跃区间记录表</strong> ：可以支撑的报表：<strong>每天的新用户留存天数统计报表</strong>，<strong>连续活跃天数统计月报表</strong></p><p><strong>Dws漏斗分析模型：分析用户业务各个阶段的转换率</strong> ：<strong>搜索购买</strong>业务路径、<strong>秒杀购物业务路径</strong> </p><h1 id="GPS-地理位置解析"><a href="#GPS-地理位置解析" class="headerlink" title="GPS 地理位置解析"></a>GPS 地理位置解析</h1><h2 id="经纬度获取地理位置"><a href="#经纬度获取地理位置" class="headerlink" title="经纬度获取地理位置"></a>经纬度获取地理位置</h2><p>使用Geohash编码来把经纬度变成一个字符串，代表被编码 gps 坐标点的一个矩形范围；</p><h3 id="GEOHASH编码"><a href="#GEOHASH编码" class="headerlink" title="GEOHASH编码"></a>GEOHASH编码</h3><p>在地球经纬度范围内，不断通过二分来划分矩形范围，通过观察 gps 坐标点所落的范围，来反复生成 0&#x2F;1 二进制码。</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/image-20211027142819263.png" alt="image-20211027142819263"></p><p>然后在使用Base32来进行编码，把geohash编码生成的字符串，五位变成一位，缩短他的长度</p><p><img src="/shu-ju-cang-ku/shu-ju-cang-ku-xue-xi/image-20211025212241563.png" alt="image-20211025212241563"></p><h2 id="Ip获取地理位置"><a href="#Ip获取地理位置" class="headerlink" title="Ip获取地理位置"></a>Ip获取地理位置</h2><p>通过<strong>开源工具包ip2region（含ip数据库）</strong>来解析</p>]]></content>
      
      
      <categories>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive load脚本</title>
      <link href="/linux/shell-jiao-ben/hive-load-jiao-ben/"/>
      <url>/linux/shell-jiao-ben/hive-load-jiao-ben/</url>
      
        <content type="html"><![CDATA[<pre><code class="sh">#!/bin/bash# 定义变量方便修改APP=gmallhive=/opt/modules/hive/bin/hive# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天if [ -n &quot;$1&quot; ] ;then   do_date=$1else    do_date=`date -d &quot;-1 day&quot; +%F`fi echo &quot;===日志日期为 $do_date===&quot;sql=&quot;load data inpath &#39;/origin_data/gmall/log/topic_start/$do_date&#39; into table &quot;$APP&quot;.ods_start_log partition(dt=&#39;$do_date&#39;);load data inpath &#39;/origin_data/gmall/log/topic_event/$do_date&#39; into table &quot;$APP&quot;.ods_event_log partition(dt=&#39;$do_date&#39;);&quot;$hive -e &quot;$sql&quot;if [ $? -eq 0 ]then echo &quot;congratulations! 任务执行成功！ 邮件已发送至admin@51doit.com&quot;elseecho &quot;节哀顺变! 任务失败! 邮件已发送至admin@51doit.com&quot;fi</code></pre><p>$? 获取执行结果后，可以安装组件发送结果邮件</p><pre><code class="shell">#!/bin/bashexport HIVE_HOME=/opt/apps/hive-3.1.2/DT=`date -d&#39;-1 day&#39; +%Y-%m-%d`if [ $1 ]then#注意赋值的语句不要有空格，否则按照命令执行DT=$1fi$&#123;HIVE_HOME&#125;/bin/hive -e &quot;load data inpath &#39;/logdata/app/$&#123;DT&#125;&#39; into table ODS17.APP_ACTION_LOG partition (dt=&#39;$&#123;DT&#125;&#39;);&quot;if [ $? -eq 0 ]then echo &quot;congratulations! 任务执行成功！ 邮件已发送至admin@51doit.com&quot;elseecho &quot;节哀顺变! 任务失败! 邮件已发送至admin@51doit.com&quot;fi</code></pre><pre><code>-- 配置定时调度（如果是简单的系统，可以用linux自带的crontab）（项目中我们会采用更强大的定时调度系统：azkaban/oozie/airflow）crontab -e10 0 * * * sh /root/taskshells/01.load_ods.app_action_log.sh</code></pre>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux备份</title>
      <link href="/java/spring/spring-cloud/security/springcloud-security/"/>
      <url>/java/spring/spring-cloud/security/springcloud-security/</url>
      
        <content type="html"><![CDATA[<h1 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h1><p>主要的两个功能，认证和授权</p><p>Security的效果是通过过滤器来实现的，他有多个过滤器，对不同的请求进行不同的过滤。</p><h3 id="过滤器是如何加载的？"><a href="#过滤器是如何加载的？" class="headerlink" title="过滤器是如何加载的？"></a>过滤器是如何加载的？</h3><p>使用SpringSecurity配置过滤器</p><p>DelegationFilterProxy</p><h3 id="设置用户名和密码的方式"><a href="#设置用户名和密码的方式" class="headerlink" title="设置用户名和密码的方式"></a>设置用户名和密码的方式</h3><ol><li><strong>通过配置文件设置</strong></li></ol><p>在application.yml文件中配置</p><pre><code>spring.security.user.name=账号spring.security.user.password=密码</code></pre><ol><li><strong>通过配置类设置</strong></li></ol><p>继承WebSecurityConfigurationAdapter类，重写方法configure方法</p><p>在里面通过auth对象设置账号和密码以及保存的位置</p><p>这里面有多个configurate方法，http的config方法可以设置登陆页面的跳转和设置访问是否需要拦截、auth的config方法可以定义userdatails类和password地址</p><pre><code class="java">@EnableWebSecurity//开启security注解支持@EnableGlobalMethodSecurity(prePostEnabled = true)public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123;    @Autowired    PasswordEncoder passwordEncoder;    /**     * userDetailsService 获取token的时候对用户进行一些自定义过滤，并将保存用户信息（用户名，密码，角色等），这个是userdetailsservice的实现类     */    @Autowired    @Qualifier(&quot;userDetailsServiceImpl&quot;)    private UserDetailsService userDetailsService;    @Override    @Bean    public AuthenticationManager authenticationManagerBean() throws Exception &#123;        return super.authenticationManagerBean();    &#125;    /**     * 配置用户签名服务 主要是user-details 机制，     *     * @param auth 签名管理器构造器，用于构建用户具体权限控制     * @throws Exception     */    @Override    protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123;        auth.userDetailsService(userDetailsService)                .passwordEncoder(passwordEncoder);    &#125;    @Override    public void configure(HttpSecurity http) throws Exception &#123;        http                .cors()                .and()                .csrf().disable()                .requestMatchers().antMatchers(&quot;/oauth/**/&quot;, &quot;/login/**/&quot;, &quot;/logout/**/&quot;)                .and()                .authorizeRequests()                .antMatchers(&quot;/oauth/**/&quot;).authenticated();    &#125;&#125;</code></pre><ol start="3"><li><strong>通过自定义实现类设置</strong></li></ol><p>实现接口UserDetailsService接口并重写方法</p><p>上面的两种方式都不常用，因为他们都是写死的并且不安全</p><p>下面是使用的实例：</p><ol><li>service层的userdetailservice实现类编写，这里调用的servcie方法查数据库就不写进来了</li></ol><pre><code class="java">@Servicepublic class UserDetailsServiceImpl implements UserDetailsService &#123;    @Autowired    AdminService adminService;    @Override    public UserDetails loadUserByUsername(final String username)            throws UsernameNotFoundException &#123;        Admin admin = adminService.findAdminByName(username);// 如果用户不存在则认证失败        if (admin == null) &#123;            throw new UsernameNotFoundException(username + &quot; not found&quot;);        &#125;        List&lt;SimpleGrantedAuthority&gt; list = new ArrayList&lt;SimpleGrantedAuthority&gt;();        //授权        list.add(new SimpleGrantedAuthority(&quot;admin&quot;));        return new User(username, admin.getPwd(), list);    &#125;&#125;</code></pre><ol start="2"><li>webSecurityConfig配置类</li></ol><pre><code class="java">package com.caeri.largescreen.oauth.config.auth;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.context.annotation.Bean;import org.springframework.security.authentication.AuthenticationManager;import org.springframework.security.config.annotation.authentication.builders.AuthenticationManagerBuilder;import org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;import org.springframework.security.core.userdetails.UserDetailsService;import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;import org.springframework.security.crypto.password.PasswordEncoder;@EnableWebSecurity@EnableGlobalMethodSecurity(prePostEnabled = true)public class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123;    @Autowired    PasswordEncoder passwordEncoder;    /**     * userDetailsService 获取token的时候对用户进行一些自定义过滤，并将保存用户信息（用户名，密码，角色等）     */    @Autowired    @Qualifier(&quot;userDetailsServiceImpl&quot;)    private UserDetailsService userDetailsService;        @Override    @Bean    public AuthenticationManager authenticationManagerBean() throws Exception &#123;        return super.authenticationManagerBean();    &#125;    /**     * 配置用户签名服务 主要是user-details 机制，     *     * @param auth 签名管理器构造器，用于构建用户具体权限控制     * @throws Exception     */    @Override    protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123;        auth.userDetailsService(userDetailsService)                .passwordEncoder(passwordEncoder);    &#125;    @Override    public void configure(HttpSecurity http) throws Exception &#123;        http                .cors()                .and()                .csrf().disable()                .requestMatchers().antMatchers(&quot;/oauth/**/&quot;, &quot;/login/**/&quot;, &quot;/logout/**/&quot;)                .and()                .authorizeRequests()                .antMatchers(&quot;/oauth/**/&quot;).authenticated();    &#125;&#125;</code></pre><h3 id="基于角色或权限进行访问控制"><a href="#基于角色或权限进行访问控制" class="headerlink" title="基于角色或权限进行访问控制"></a>基于角色或权限进行访问控制</h3><ol><li>hasAuthority方法  针对指定权限进行控制</li></ol><p>![image-20210220195224472](Springcloud Security&#x2F;image-20210220195224472.png)</p><ol start="2"><li>hasAnyAuthority方法  针对多个权限进行设置</li></ol><p>![image-20210220195546172](Springcloud Security&#x2F;image-20210220195546172.png)</p><h2 id="UserDetailsService接口"><a href="#UserDetailsService接口" class="headerlink" title="UserDetailsService接口"></a>UserDetailsService接口</h2><p>loadUserByUsername方法 返回<code>UserDetails</code> 类型的数据。</p><h2 id="UserDetails接口"><a href="#UserDetails接口" class="headerlink" title="UserDetails接口"></a>UserDetails接口</h2><p>里面的方法返回用户的明细信息</p><p>UserDetails 也是一个接口，源码：</p><pre><code class="java">/** * Provides core user information. * * &lt;p&gt; * Implementations are not used directly by Spring Security for security purposes. They * simply store user information which is later encapsulated into &#123;@link Authentication&#125; * objects. This allows non-security related user information (such as email addresses, * telephone numbers etc) to be stored in a convenient location. * &lt;p&gt; * Concrete implementations must take particular care to ensure the non-null contract * detailed for each method is enforced. See * &#123;@link org.springframework.security.core.userdetails.User&#125; for a reference * implementation (which you might like to extend or use in your code). * * @see UserDetailsService * @see UserCache * * @author Ben Alex */public interface UserDetails extends Serializable &#123;   // ~ Methods   // ========================================================================================================   /**    * Returns the authorities granted to the user. Cannot return &lt;code&gt;null&lt;/code&gt;.    *    * @return the authorities, sorted by natural key (never &lt;code&gt;null&lt;/code&gt;)    * 获取权限列表    */   Collection&lt;? extends GrantedAuthority&gt; getAuthorities();   /**    * Returns the password used to authenticate the user.    *    * @return the password    * 获取用户密码    */   String getPassword();   /**    * Returns the username used to authenticate the user. Cannot return &lt;code&gt;null&lt;/code&gt;.    *    * @return the username (never &lt;code&gt;null&lt;/code&gt;)    * 获取用户名称    */   String getUsername();   /**    * Indicates whether the user&#39;s account has expired. An expired account cannot be    * authenticated.    *    * @return &lt;code&gt;true&lt;/code&gt; if the user&#39;s account is valid (ie non-expired),    * &lt;code&gt;false&lt;/code&gt; if no longer valid (ie expired)    * 凭证是否过期    */   boolean isAccountNonExpired();   /**    * Indicates whether the user is locked or unlocked. A locked user cannot be    * authenticated.    *    * @return &lt;code&gt;true&lt;/code&gt; if the user is not locked, &lt;code&gt;false&lt;/code&gt; otherwise    * 账号是否被锁定    */   boolean isAccountNonLocked();   /**    * Indicates whether the user&#39;s credentials (password) has expired. Expired    * credentials prevent authentication.    *    * @return &lt;code&gt;true&lt;/code&gt; if the user&#39;s credentials are valid (ie non-expired),    * &lt;code&gt;false&lt;/code&gt; if no longer valid (ie expired)    * 凭证是否过期    */   boolean isCredentialsNonExpired();   /**    * Indicates whether the user is enabled or disabled. A disabled user cannot be    * authenticated.    *    * @return &lt;code&gt;true&lt;/code&gt; if the user is enabled, &lt;code&gt;false&lt;/code&gt; otherwise    * 是否启用    */   boolean isEnabled();&#125;</code></pre><p>UserDetails是个接口，我们主要使用它的实现类 User</p><p>![实现类](Springcloud Security&#x2F;image-20210218162256716.png)</p><p>User需要对比password </p><h2 id="PasswordEncoder接口"><a href="#PasswordEncoder接口" class="headerlink" title="PasswordEncoder接口"></a>PasswordEncoder接口</h2><p>这是个接口，用来加密和解密，需要提前注入到容器中使用</p><p>![passwordencoder实现类](Springcloud Security&#x2F;image-20210218163230375.png)</p><p>实现类很多，官方主要推荐使用<code>BCryptPasswordEncoder</code></p><h3 id="BCryptPasswordEncoder类"><a href="#BCryptPasswordEncoder类" class="headerlink" title="BCryptPasswordEncoder类"></a>BCryptPasswordEncoder类</h3><p>![实现类的使用](Springcloud Security&#x2F;image-20210218163835905.png)</p><p>可以自定义加密算法</p>]]></content>
      
      
      <categories>
          
          <category> Spring Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring Security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云服务</title>
      <link href="/yun-fu-wu/"/>
      <url>/yun-fu-wu/</url>
      
        <content type="html"><![CDATA[<h2 id="DOS攻击"><a href="#DOS攻击" class="headerlink" title="DOS攻击"></a>DOS攻击</h2><p>多台计算机同时发送大量数据包，导致服务器无法提供正常服务</p><h3 id="三种类型"><a href="#三种类型" class="headerlink" title="三种类型"></a>三种类型</h3><h4 id="连接耗尽型"><a href="#连接耗尽型" class="headerlink" title="连接耗尽型"></a>连接耗尽型</h4><p>SYN Flood</p><p>SYN(TCP&#x2F;IP建立连接时使用的握手信号) </p><p><a href="https://baike.baidu.com/item/TCP/33012">TCP</a>连接的第一个包，非常小的一种<a href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%8C%85/489739">数据包</a>。SYN 攻击包括大量此类的包，由于这些包看上去来自实际不存在的站点，因此无法有效进行处理。每个机器的欺骗包都要花几秒钟进行尝试方可放弃提供正常响应。利用了TCP协议的缺陷，发送大量半连接请求，耗尽你的资源和内存。</p><h4 id="带宽耗尽型"><a href="#带宽耗尽型" class="headerlink" title="带宽耗尽型"></a>带宽耗尽型</h4><p><strong>ACK Flood</strong></p><p><strong>UDP Flood</strong></p><p>UDP（[Internet ](<a href="https://baike.baidu.com/item/Internet">https://baike.baidu.com/item/Internet</a> &#x2F;272794)协议集支持一个无连接的<a href="https://baike.baidu.com/item/%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AE/8048821">传输协议</a>）</p><p>通过大量伪造UDP小包然后冲击DNS服务器</p><p><strong>ICMP Flood</strong></p><p>ICMP（是<a href="https://baike.baidu.com/item/TCP%2FIP%E5%8D%8F%E8%AE%AE%E7%B0%87">TCP&#x2F;IP协议簇</a>的一个子协议）</p><p>短时间向特定目标不断请求ICMP回应，通过伪装目的主机的IP地址，仙多格ip网络广播地址发送ICMP Echo Requets数据包，消耗大量CPU资源和有效带宽来处理节点的数据包</p><h4 id="应用层攻击"><a href="#应用层攻击" class="headerlink" title="应用层攻击"></a>应用层攻击</h4><p>Http GET Flood，Http Post慢速攻击，</p><p>DNS Query Flood</p><p>DNS服务器攻击</p><p>操纵大量机器向被攻击的服务器发送大量域名解析请求。被攻击的DNS服务器在接收到域名解析请求的时候首先查找缓存，找不到就服务器解析，进行递归查询。</p><h2 id="DDos防御"><a href="#DDos防御" class="headerlink" title="DDos防御"></a>DDos防御</h2><h3 id="攻击源消除"><a href="#攻击源消除" class="headerlink" title="攻击源消除"></a>攻击源消除</h3><p>DDos攻击需要傀儡机。防范计算机成为傀儡机，必须对主机的硬件或者系统进行修复。</p><h3 id="攻击缓解"><a href="#攻击缓解" class="headerlink" title="攻击缓解"></a>攻击缓解</h3><h4 id="报文过滤"><a href="#报文过滤" class="headerlink" title="报文过滤"></a>报文过滤</h4><p>DDos攻击，通过对报文源ip地址进行检测，进行ip地址的过滤</p><h4 id="速率限制"><a href="#速率限制" class="headerlink" title="速率限制"></a>速率限制</h4><p>限制一些数据流减少拥塞而达到阻止DDoS攻击的目的</p><h3 id="攻击预防"><a href="#攻击预防" class="headerlink" title="攻击预防"></a>攻击预防</h3><p>减少公开暴露</p><p>提升网络带宽</p><p>分布式资源共享服务器</p><p>监控系统性能</p><h2 id="证书和加密"><a href="#证书和加密" class="headerlink" title="证书和加密"></a>证书和加密</h2><h3 id="加密技术"><a href="#加密技术" class="headerlink" title="加密技术"></a>加密技术</h3><p>密码体制包括：明文空间，密文空间，密匙空间，加密算法，解密算法</p><p>算法分为两种</p><p>1.对称加密算法</p><p>加密和解密使用相同的密钥</p><p>常见对称加密算法：DES、3DES、Blowfish、IDEA、RC4、RC5、RC6和AES。</p><p>2.非对称加密算法</p><p>加密和解密使用不同的密钥，每个用户有一对密钥，公钥和私钥</p><p>常见的非对称加密算法：RSA、ECC（移动设备用）、Diffie-Hellman、EL Gamal、DSA（数字签名用）</p><p>一般一起使用这两种加密算法，密钥使用非对称加密算法，其他内容使用对称加密。</p><p>非对称加密效率比对称加密低</p><h3 id="数字签名"><a href="#数字签名" class="headerlink" title="数字签名"></a>数字签名</h3><p>数字签名是写在纸上的普通的物理签名，使用了公钥加密领域的技术实现，用于鉴别数字信息的方法</p><p>包括签名算法和验证算法。</p><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>签名，hash运算，加密签名，网络传输，接受、hash运算、解密签名、验证签名。</p><h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>服务器认证</p><p>代码认证</p><h2 id="数字证书"><a href="#数字证书" class="headerlink" title="数字证书"></a>数字证书</h2><p>数字证书是标志了通讯各方身份信息的一系列数据，保存了你的密钥信息，提供了一种在Internet上验证你的身份的方式。</p><p>数字证书是一个经<strong>证书授权中心</strong>数字签名的<strong>包含是公开密钥拥有着信息以及公开密钥的文件</strong>。</p><p>数字证书是一种权威性的电子文档，有权威第三方机构，即CA中心签发的证书，也可以由企业级CA系统进行签发</p><h4 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h4><p>一个证书包含：</p><ul><li>证书的发布机构</li><li>证书有效期</li><li>公钥</li><li>证书所有者</li><li>签名所使用的算法</li><li>签名所使用的算法</li><li>指纹以及指纹算法</li></ul><h2 id="云安全架构"><a href="#云安全架构" class="headerlink" title="云安全架构"></a>云安全架构</h2><h3 id="CAS安全参考模型"><a href="#CAS安全参考模型" class="headerlink" title="CAS安全参考模型"></a>CAS安全参考模型</h3><p>云安全联盟（Cloud Security Alliance，CSA）是云安全研究论坛</p><p><img src="/yun-fu-wu/1151055-20170630104509071-1166579073.png" alt="img"></p><p><strong>云计算</strong>可以认为包括以下几<strong>个层次的服务</strong>：基础设施即服务（IaaS），平台即服务（PaaS）和软件即服务（SaaS）。</p><p>云计算的三个层次</p><p>基础设施即服务（IaaS），平台即服务（PaaS）和软件即服务（SaaS）。</p><p>SaaS: Software-as-a-Service（软件即服务）</p><ul><li>提供给客户的服务是运营商运行在云计算基础设施上的应用程序，用户可以在各种设备上通过客户端界面访问。客户不需要管理或控制任何云计算基础设施。</li></ul><p>PaaS: Platform-as-a-Service（平台即服务）</p><ul><li>提供给客户的服务是把客户开发或收购的应用程序部署到供应商的云计算基础设施上。客户不需要管理或控制底层的云基础设施，包括网络、服务器、操作系统、存储等，但客户能控制部署的应用程序，也可能控制运行应用程序的托管环境配置。</li></ul><p>IaaS: Infrastructure-as-a-Service（基础设施即服务）</p><ul><li>提供给客户的服务是对所有设施的利用，包括处理、存储、网络和其他基本的计算资源。客户能够部署和运行任意软件，包括操作系统和应用程序。客户不管理或控制任何云计算基础设施，但能控制操作系统的选择、储存空间、部署的应用，也有可能获得有限制的网络组件（例如防火墙、负载均衡器等）的控制。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 云服务 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Oauth2笔记</title>
      <link href="/java/spring/spring-cloud/security/oauth2/"/>
      <url>/java/spring/spring-cloud/security/oauth2/</url>
      
        <content type="html"><![CDATA[<h2 id="Oauth2"><a href="#Oauth2" class="headerlink" title="Oauth2"></a>Oauth2</h2><p>官方文档：<a href="https://projects.spring.io/spring-security-oauth/docs/oauth2.html">https://projects.spring.io/spring-security-oauth/docs/oauth2.html</a></p><p>入门文档：<a href="https://docs.spring.io/spring-security-oauth2-boot/docs/2.2.0.RELEASE/reference/html5/#boot-features-security-oauth2-authorization-server">https://docs.spring.io/spring-security-oauth2-boot/docs/2.2.0.RELEASE/reference/html5/#boot-features-security-oauth2-authorization-server</a></p><p>OAuth 2.0本质上是一个框架，用于指定将长寿命令牌交换为短寿命令牌的策略。</p><h2 id="1、Authorization-Server-授权-服务器"><a href="#1、Authorization-Server-授权-服务器" class="headerlink" title="1、Authorization Server(授权 服务器)"></a>1、Authorization Server(授权 服务器)</h2><h3 id="依赖关系"><a href="#依赖关系" class="headerlink" title="依赖关系"></a>依赖关系</h3><p>要使用此库中的自动配置功能，您需要<code>spring-security-oauth2</code>具有OAuth 2.0原语和<code>spring-security-oauth2-autoconfigure</code>。请注意，您需要指定的版本<code>spring-security-oauth2-autoconfigure</code>，因为它不再受Spring Boot的管理，尽管它仍应与Boot的版本匹配。</p><p>对于JWT支持，您还需要<code>spring-security-jwt</code>。</p><pre><code class="xml">    &lt;properties&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;        &lt;spring-cloud.version&gt;2.2.4.RELEASE&lt;/spring-cloud.version&gt;        &lt;mybatis.version&gt;2.1.3&lt;/mybatis.version&gt;    &lt;/properties&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-security&lt;/artifactId&gt;            &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-oauth2&lt;/artifactId&gt;            &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt;    &lt;/dependency&gt;</code></pre><h3 id="最小OAuth2使用配置"><a href="#最小OAuth2使用配置" class="headerlink" title="最小OAuth2使用配置"></a>最小OAuth2使用配置</h3><p>创建最小的Spring Boot授权服务器包括三个基本步骤：</p><ol><li>包括依赖项。</li><li>包括<code>@EnableAuthorizationServer</code>注释。</li><li>指定至少一个客户端ID和密码。</li></ol><p>默认情况下，<code>@EnableAuthorizationServer</code>授予客户端访问客户端凭据的权限，这意味着您可以执行以下操作：</p><h3 id="添加最终用户登录流程"><a href="#添加最终用户登录流程" class="headerlink" title="添加最终用户登录流程"></a>添加最终用户登录流程</h3><p>顺便说一句，添加的实例<code>WebSecurityConfigurerAdapter</code>只是我们现在为最终用户添加表单登录流程所需要的。但是，请注意，这是有关Web应用程序本身的所有其他配置（而不是OAuth 2.0 API）的地方。</p><p>如果您想自定义登录页面，不仅为用户提供表单登录，还可以添加其他支持（如密码恢复），因此请<code>WebSecurityConfigurerAdapter</code>自行选择</p><h2 id="运行流程"><a href="#运行流程" class="headerlink" title="运行流程"></a>运行流程</h2><p><img src="/java/spring/spring-cloud/security/oauth2/15209061-98feec3dbfd354bf.png" alt="OAuth 2.0的运行流程"> </p><p>（A）用户打开客户端以后，客户端要求用户给予授权。<br> （B）用户同意给予客户端授权。<br> （C）客户端使用上一步获得的授权，向授权服务器申请令牌。<br> （D）授权服务器对客户端进行认证以后，确认无误，同意发放令牌。<br> （E）客户端使用令牌，向资源服务器申请获取资源。<br> （F）资源服务器确认令牌无误，同意向客户端开放资源。</p><p><a href="https://www.jianshu.com/p/50083c74e5b3">https://www.jianshu.com/p/50083c74e5b3</a></p><h3 id="1、授权码模式（authorization-code）"><a href="#1、授权码模式（authorization-code）" class="headerlink" title="1、授权码模式（authorization code）"></a>1、授权码模式（authorization code）</h3><p><img src="/java/spring/spring-cloud/security/oauth2/11176703-a4c3084ea58e1530.png" alt="img"></p><p>（A）用户访问客户端，后者将前者导向认证服务器。</p><p>（B）用户选择是否给予客户端授权。</p><p>（C）假设用户给予授权，认证服务器将用户导向客户端事先指定的”重定向URI”（redirection URI），同时附上一个授权码。（一般有效时间是10分钟）</p><p>（D）客户端收到授权码，附上早先的”重定向URI”，向认证服务器申请令牌。这一步是在客户端的后台的服务器上完成的，对用户不可见。</p><p>（E）认证服务器核对了授权码和重定向URI，确认无误后，向客户端发送访问令牌（access token）和更新令牌（refresh token）。</p><h3 id="2、简化模式（implicit）"><a href="#2、简化模式（implicit）" class="headerlink" title="2、简化模式（implicit）"></a>2、简化模式（implicit）</h3><p>简化模式（implicit grant type）不通过第三方应用程序的服务器，直接在浏览器中向认证服务器申请令牌，跳过了”授权码”这个步骤，因此得名。所有步骤在浏览器中完成，令牌对访问者是可见的，且客户端不需要认证。</p><p><img src="/java/spring/spring-cloud/security/oauth2/15209061-2d1bb6aa3925ca0d.png" alt="img"></p><p>（A）客户端将用户导向认证服务器。<br> （B）用户决定是否给于客户端授权。<br> （C）假设用户给予授权，认证服务器将用户导向客户端指定的”重定向URI”，并在URI的Hash部分包含了访问令牌。<br> （D）浏览器向资源服务器发出请求，其中不包括上一步收到的Hash值。<br> （E）资源服务器返回一个网页，其中包含的代码可以获取Hash值中的令牌。<br> （F）浏览器执行上一步获得的脚本，提取出令牌。<br> （G）浏览器将令牌发给客户端。</p><h3 id="3、密码模式（resource-owner-password-credentials）"><a href="#3、密码模式（resource-owner-password-credentials）" class="headerlink" title="3、密码模式（resource owner password credentials）"></a>3、密码模式（resource owner password credentials）</h3><p>密码模式（Resource Owner Password Credentials Grant）中，用户向客户端提供自己的用户名和密码。客户端使用这些信息，向”服务商提供商”索要授权。在这种模式中，用户必须把自己的密码给客户端，但是客户端不得储存密码。这通常用在用户对客户端高度信任的情况下。一般不支持refresh token。</p><p><img src="/java/spring/spring-cloud/security/oauth2/15209061-38d92855ebb979d1.png" alt="img"></p><p>（A）用户向客户端提供用户名和密码。<br>（B）客户端将用户名和密码发给认证服务器，向后者请求令牌。<br>（C）认证服务器确认无误后，向客户端提供访问令牌。</p><h3 id="4、客户端模式（client-credentials）"><a href="#4、客户端模式（client-credentials）" class="headerlink" title="4、客户端模式（client credentials）"></a>4、客户端模式（client credentials）</h3><p>指客户端以自己的名义，而不是以用户的名义，向”服务提供商”进行认证。严格地说，客户端模式并不属于OAuth框架所要解决的问题。在这种模式中，用户直接向客户端注册，客户端以自己的名义要求”服务提供商”提供服务，其实不存在授权问题。</p><p><img src="/java/spring/spring-cloud/security/oauth2/15209061-193021c225466558.png" alt="img"></p><p>（A）客户端向认证服务器进行身份认证，并要求一个访问令牌。<br>（B）认证服务器确认无误后，向客户端提供访问令牌。</p><h2 id="Oauth和Secutiry的使用"><a href="#Oauth和Secutiry的使用" class="headerlink" title="Oauth和Secutiry的使用"></a>Oauth和Secutiry的使用</h2><p>1 、授权服务器配置</p><p><code>EnableAuthorizationServer</code>注解的使用</p><pre><code class="java">package com.xxxx.springsecurityoauth2demo.config;import com.xxxx.springsecurityoauth2demo.service.UserService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.context.annotation.Configuration;import org.springframework.security.authentication.AuthenticationManager;import org.springframework.security.crypto.password.PasswordEncoder;import org.springframework.security.oauth2.config.annotation.configurers.ClientDetailsServiceConfigurer;import org.springframework.security.oauth2.config.annotation.web.configuration.AuthorizationServerConfigurerAdapter;import org.springframework.security.oauth2.config.annotation.web.configuration.EnableAuthorizationServer;import org.springframework.security.oauth2.config.annotation.web.configurers.AuthorizationServerEndpointsConfigurer;import org.springframework.security.oauth2.provider.token.TokenStore;/** * @author zhoubin * @since 1.0.0 */@Configuration@EnableAuthorizationServerpublic class AuthorizationServerConfig extends AuthorizationServerConfigurerAdapter &#123;    @Autowired    private PasswordEncoder passwordEncoder;    @Autowired    private AuthenticationManager authenticationManager;    @Autowired    private UserService userService;    @Autowired    @Qualifier(&quot;redisTokenStore&quot;)    private TokenStore tokenStore;    /**     * 密码模式     *     * @param endpoints     * @throws Exception     */    @Override    public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123;        endpoints.authenticationManager(authenticationManager)                .userDetailsService(userService)                .tokenStore(tokenStore);    &#125;    //这里的client代表我们的应用，就是用户第一个访问的应用    @Override    public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123;        clients.inMemory()                //客户端ID                .withClient(&quot;client&quot;)                //秘钥                .secret(passwordEncoder.encode(&quot;112233&quot;))                //重定向地址                .redirectUris(&quot;http://www.baidu.com&quot;)                //授权范围                .scopes(&quot;all&quot;)                /**                 * 授权类型                 * authorization_code：授权码模式                 * password:密码模式                 */                .authorizedGrantTypes(&quot;authorization_code&quot;, &quot;password&quot;);    &#125;&#125;</code></pre><p>2、资源服务器</p><pre><code class="java">package com.caeri.largescreen.oauth.config.auth;import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.oauth2.config.annotation.web.configuration.EnableResourceServer;import org.springframework.security.oauth2.config.annotation.web.configuration.ResourceServerConfigurerAdapter;import javax.servlet.http.HttpServletResponse;@Configuration@EnableResourceServerpublic class ResourceServerConfig extends ResourceServerConfigurerAdapter &#123;    @Override    public void configure(HttpSecurity http) throws Exception &#123;        http            //禁用csrf                .csrf().disable()            //                .exceptionHandling()                .authenticationEntryPoint((request, response, authException) -&gt; response.sendError(HttpServletResponse.SC_UNAUTHORIZED))                .and()            //                .authorizeRequests()            //拦截所有请求                .anyRequest().authenticated()                .and()                .httpBasic();    &#125;&#125;</code></pre><p>jwt的组成</p><p>head</p><pre><code>iss: jwt签发者sub: jwt所面向的用户aud: 接收jwt的一方exp: jwt的过期时间，这个过期时间必须要大于签发时间nbf: 定义在什么时间之前，该jwt都是不可用的.iat: jwt的签发时间jti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。</code></pre>]]></content>
      
      
      <categories>
          
          <category> Spring Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Oauth </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux命令</title>
      <link href="/linux/linux-ming-ling/"/>
      <url>/linux/linux-ming-ling/</url>
      
        <content type="html"><![CDATA[<h2 id="Linux常用命令"><a href="#Linux常用命令" class="headerlink" title="Linux常用命令"></a>Linux常用命令</h2><table><thead><tr><th>序号</th><th>命令</th><th>命令解释</th></tr></thead><tbody><tr><td>1</td><td>top</td><td>查看内存</td></tr><tr><td>2</td><td>df -h</td><td>显示磁盘分区上可以使用的磁盘空间#使用-h选项以KB、MB、GB的单位来显示，可读性高~~~（最常用）</td></tr><tr><td>3</td><td>du -sh  &#x2F;</td><td>显示每个文件和目录的磁盘使用空间，也就是文件的大小。-h       #以K M G为单位显示，提高可读性（最常用的一个）</td></tr><tr><td>4</td><td>iotop</td><td>查看磁盘IO读写(yum  install iotop安装）</td></tr><tr><td>5</td><td>iotop -o</td><td>直接查看比较高的磁盘读写程序</td></tr><tr><td>6</td><td>netstat -tunlp | grep 端口号</td><td>查看端口占用情况</td></tr><tr><td>7</td><td>uptime</td><td>查看报告系统运行时长及平均负载</td></tr><tr><td>8</td><td>ps aux</td><td>查看进程</td></tr><tr><td>9</td><td>export</td><td>用于设置或显示环境变量。</td></tr><tr><td>10</td><td>pa -ef</td><td>查看进程</td></tr><tr><td>11</td><td>grep</td><td>查找符合条件的字符串</td></tr><tr><td>12</td><td>grep -v</td><td>反向过滤文本行的搜索 <br>grep name# 表示只查看包含name这个关键字的行内容<br>grep -v name # 表示查看除了含有name之外的行内容</td></tr><tr><td>13</td><td>wc -l</td><td>统计行数</td></tr></tbody></table><h2 id="Linux基础命令"><a href="#Linux基础命令" class="headerlink" title="Linux基础命令"></a>Linux基础命令</h2><table><thead><tr><th align="left">命令</th><th>命令解释</th></tr></thead><tbody><tr><td align="left">cd 路径</td><td>进入到路径下面 (..&#x2F;返回上一级目录)</td></tr><tr><td align="left">pwd</td><td>查看当前路径</td></tr><tr><td align="left">clear</td><td>清空屏幕</td></tr><tr><td align="left">ls</td><td>查看数据列表</td></tr><tr><td align="left">ls -l</td><td>显示文件列表及详情</td></tr><tr><td align="left">ls-la</td><td>显示所有文件（包含隐藏文件</td></tr><tr><td align="left">ls-l</td><td>等于 ll</td></tr><tr><td align="left">ls -R&#x2F;data0</td><td>递归显示目录中的文件列表</td></tr><tr><td align="left">uname -r</td><td>显示操作系统的发行版号</td></tr><tr><td align="left">uname -a</td><td>显示系统名、节点名称、操作系统的发行版号、操作系统版本、运行系统的机器 ID 号。</td></tr><tr><td align="left">uname -m</td><td>显示电脑类型</td></tr><tr><td align="left">cat  文件名</td><td>查看具体的文件信息，默认不显示行号</td></tr><tr><td align="left">cat -n 文件名</td><td>查看文件内容显示行号</td></tr><tr><td align="left">tail   尾部</td><td></td></tr><tr><td align="left">head   头部</td><td></td></tr><tr><td align="left">tail -n &#x2F;路径</td><td>查看尾部多少行</td></tr><tr><td align="left">head -n &#x2F;路径</td><td>查看头部多少行</td></tr><tr><td align="left">vim</td><td>修改文件内容或新建一个不为空的文件</td></tr><tr><td align="left">shift+:</td><td>输入 wq 保存退出  q！强制退出</td></tr><tr><td align="left">shift+dd</td><td>删除此行</td></tr><tr><td align="left">shift+zz</td><td>保存退出 等同于 wq</td></tr><tr><td align="left">touch</td><td>可以新建一个空的文件</td></tr><tr><td align="left">poweroff</td><td>关机</td></tr><tr><td align="left">reboot</td><td>重启机器</td></tr><tr><td align="left">mkdir  路径</td><td>创建一个空的文件夹</td></tr></tbody></table><h3 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h3><p>useradd  添加用户 </p><p>自定义： 创建用户自定义<br>自定义用户信息：<br>    -u指定uid<br>    -g指定基本组    —-》基本组也存在<br>    -G指定附加组 —–》附加组存在<br>Groups 用户名 ：查看这个用户所属的用户组（所有组）</p><p>useradd openlab   add</p><p>gpasswd -a openlab gropenlab  &#x2F;&#x2F;将用户加入到组中  加入的附加组</p><p>gpasswd -d openlab gropenlab&#x2F;&#x2F;将用户从组中删除 删除的附加组 </p><p>userdel &#x2F;&#x2F;删除用户 不删除用户文件 </p><p>userdel -r &#x2F;&#x2F;连主目录一起删除</p><p>id openlab  &#x2F;&#x2F;显示用户信息 </p><p>Passwd:修改密码<br>cat &#x2F;etc&#x2F;shadow   查看用户密码 密码是乱码说明有密码<br>cat &#x2F;etc&#x2F;passwd   前面的用户的数字必须大于等于500和后面必须是&#x2F;bin&#x2F;bash此账号才能够登录</p><p>保存用户信息的文件：&#x2F;etc&#x2F;passwd</p><p>用户名 密码 UID GID 描述信息 宿主目录 执行脚本(bin&#x2F;bash)</p><p>0    root</p><p>1-499   系统用户</p><p>500以上才能够使用</p><p>保存密码的文件：&#x2F;etc&#x2F;shadow<br>保存用户组的文件：&#x2F;etc&#x2F;group<br>保存用户组密码的文件：&#x2F;etc&#x2F;shadow<br>用户配置文件：&#x2F;etc&#x2F;default&#x2F;useradd</p><h3 id="修改主机名称"><a href="#修改主机名称" class="headerlink" title="修改主机名称"></a>修改主机名称</h3><p>hostname   查看主机名<br>hostname name 临时改变主机名<br>在etc下面修改&#x2F;etc&#x2F;sysconfig&#x2F;network文件 可以达到永久修改主机名称的目的 </p><h3 id="时间查看和设置"><a href="#时间查看和设置" class="headerlink" title="时间查看和设置"></a>时间查看和设置</h3><p>date     查看系统当前时间<br>date -s “2015-05-05 15:00:15” 设置当前系统时间</p><h3 id="网卡查看"><a href="#网卡查看" class="headerlink" title="网卡查看"></a>网卡查看</h3><p>ifconfig    查看当前网卡信息 （有eth0和io两个）</p><p>修改网卡信息</p><p><img src="/linux/linux-ming-ling/1565691729973.png" alt="1565691729973"></p><p><img src="/linux/linux-ming-ling/1565691740313.png" alt="1565691740313"></p><p>克隆的机器要删除70-persistent-net.rules第一行(eth0)，把eth1变成eth0,然后复制物理地址 </p><p>为了方便后续写代码，把window端的host文件也修改了 </p><p>C:\Windows\System32\drivers\etc\hosts</p><pre><code>192.168.237.10 linux01192.168.237.11 linux02192.168.237.12 linux03</code></pre><h3 id="copy、移动和删除文件"><a href="#copy、移动和删除文件" class="headerlink" title="copy、移动和删除文件"></a>copy、移动和删除文件</h3><p>cp &#x2F;路径  &#x2F;路径  复制文件 </p><p>移动，剪切，重命名<br>    23.1 mv &#x2F; 文件 文件夹<br>    23.2 mv &#x2F; 文件<br>    23.4 mv &#x2F;原文件地址文件名称 &#x2F;新文件名称</p><p>删除<br>rm     删除文件<br>rm -rf 强制删除 删除文件和文件夹</p><h3 id="Linux-gt-gt-gt-重定向"><a href="#Linux-gt-gt-gt-重定向" class="headerlink" title="Linux &gt;,&gt;&gt;重定向"></a>Linux &gt;,&gt;&gt;重定向</h3><pre><code>&gt;重定向 先清空内容再进行添加内容 例: ls -l root&gt;tmp/file1.txt&gt;&gt;追加新的内容，旧的内容不会消除例: ls -l root&gt;&gt;tmp/file1.txt</code></pre><h3 id="Linux解压"><a href="#Linux解压" class="headerlink" title="Linux解压"></a>Linux解压</h3><p>tar -zxvf 包名</p><h3 id="Linux防火墙"><a href="#Linux防火墙" class="headerlink" title="Linux防火墙"></a>Linux防火墙</h3><p>关闭虚拟机的防火墙<br>service iptables stop<br>重启配置文件<br>service network restart<br>查看防火墙状态<br>service iptables status<br>永久关闭防火墙（在临时关闭防火墙的基础之上的）<br>chkconfig iptables off </p><h4 id="dirname"><a href="#dirname" class="headerlink" title="dirname"></a>dirname</h4><p><a href="https://blog.csdn.net/lyricsias_java/article/details/18705107">https://blog.csdn.net/lyricsias_java/article/details/18705107</a></p><p>dirname 命令读取指定路径名删除最后一个“&#x2F;”（ 斜杠）及其后面的字符，保留其他部分，并写结果到标准输出。如果最后一个“&#x2F;”后无 字符，dirname 命令使用倒数第二个“&#x2F;”，并忽略其后的所有字符。dirname 命令在创建路径名的时候遵从以下规则：<br>如果 Path 参数为“&#x2F;&#x2F;”（双 斜杠），或者参数 Path 全部由 斜杠组成，将其转换为单斜杠“&#x2F;”。 从指定路径删除尾部的“&#x2F;” 字符。 如果参数 Path 中没有剩下的“&#x2F;”，则将路径转换成 . （点）。 从 路径中删除尾部的所有非斜杠 字符。 如果剩下的路径为“&#x2F;&#x2F;”（双 斜杠），删除路径尾部的斜杠 字符。 如果剩下的路径为空，则转换成单 斜杠“&#x2F;”。例如，输入：</p><pre><code class="shell">dirname //    结果为 /（ 斜杠）。输入：dirname /a/b/ 结果为：/a。输入：dirname a     结果为 . （点）。输入：dirname a/b   结果为路径名 a。命令 dirname 和 basename 通常在 shell 内部命令替换使用，以指定一个与指定输入文件名略有差异的输出文件名。</code></pre><h3 id="显示当前登录系统的用户"><a href="#显示当前登录系统的用户" class="headerlink" title="显示当前登录系统的用户"></a>显示当前登录系统的用户</h3><p>Linux who命令用于显示系统中有哪些使用者正在上面，显示的资料包含了使用者 ID、使用的终端机、从哪边连上来的、上线时间、呆滞时间、CPU 使用量、动作等等。</p><pre><code>who</code></pre><h3 id="查看磁盘大小"><a href="#查看磁盘大小" class="headerlink" title="查看磁盘大小"></a>查看磁盘大小</h3><pre><code>df -THdf -TH 查看磁盘大小，解决：删除比较大无用的文件-H 1H=1000-h 1h=1024</code></pre><h3 id="查看inode-文件的描述信息"><a href="#查看inode-文件的描述信息" class="headerlink" title="查看inode(文件的描述信息)"></a>查看inode(文件的描述信息)</h3><p>df -i</p><p>df -i 查看inode：文件的字节数，拥有者id，组id，权限，改动时间，链接数，数据block的位置，解决：删除数量过多的小文件</p><h3 id="查看内存大小"><a href="#查看内存大小" class="headerlink" title="查看内存大小"></a>查看内存大小</h3><p>查看内存大小，  m 是兆字节  -h 是 1021M -H 是 1000M</p><pre><code>free</code></pre><h3 id="查看服务器的系统版本"><a href="#查看服务器的系统版本" class="headerlink" title="查看服务器的系统版本"></a>查看服务器的系统版本</h3><pre><code>dmidecode|grep &quot;System Information&quot; -A9|egrep  &quot;Manufacturer|Product&quot;    Manufacturer: VMware, Inc.    Product Name: VMware Virtual Platform</code></pre><h3 id="查看centos系统的版本"><a href="#查看centos系统的版本" class="headerlink" title="查看centos系统的版本"></a>查看centos系统的版本</h3><pre><code> cat  /etc/redhat-releaseCentOS release 6.8 (Final)</code></pre><h3 id="查看cpu的型号"><a href="#查看cpu的型号" class="headerlink" title="查看cpu的型号"></a>查看cpu的型号</h3><pre><code>查看cpu的型号cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz</code></pre><h3 id="Linux-expect-介绍和用法"><a href="#Linux-expect-介绍和用法" class="headerlink" title="Linux expect 介绍和用法"></a><a href="https://www.cnblogs.com/saneri/p/10819348.html">Linux expect 介绍和用法</a></h3><p>expect是一个自动化交互套件，主要应用于执行命令和程序时，系统以交互形式要求输入指定字符串，实现交互通信</p><h3 id="more"><a href="#more" class="headerlink" title="more"></a>more</h3><p>Linux more 命令类似 cat ，不过会以一页一页的形式显示，更方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示，而且还有搜寻字串的功能（与 vi 相似），使用中的说明文件，请按 h 。</p><h3 id="less"><a href="#less" class="headerlink" title="less"></a>less</h3><p>less 与 more 类似，less 可以随意浏览文件，支持翻页和搜索，支持向上翻页和向下翻页。</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>拉链表</title>
      <link href="/shu-ju-cang-ku/la-lian-biao/"/>
      <url>/shu-ju-cang-ku/la-lian-biao/</url>
      
        <content type="html"><![CDATA[<p><em>*<em>*</em>*2020-04-06 mysql对应的user_info表****</em>*</p><table><thead><tr><th>用户id</th><th>姓名</th><th>年龄</th><th>手机号</th><th>创建时间</th><th>修改时间</th></tr></thead><tbody><tr><td>0001</td><td>张三</td><td>20</td><td>111111</td><td>2020-04-06</td><td>2020-04-06</td></tr><tr><td>0002</td><td>小红</td><td>21</td><td>222222</td><td>2020-04-06</td><td>2020-04-06</td></tr></tbody></table><p><em>*<em>*</em>*2020-04-07 mysql对应的user_info表****</em>*</p><table><thead><tr><th>用户id</th><th>姓名</th><th>年龄</th><th>手机号</th><th>创建时间</th><th>修改时间</th></tr></thead><tbody><tr><td>0001</td><td>张三</td><td>20</td><td>111222</td><td>2020-04-06</td><td>2020-04-07</td></tr><tr><td>0002</td><td>小红</td><td>21</td><td>222222</td><td>2020-04-06</td><td>2020-04-06</td></tr><tr><td>0003</td><td>李四</td><td>22</td><td>333333</td><td>2020-04-07</td><td>2020-04-07</td></tr><tr><td>0004</td><td>王五</td><td>44</td><td>444444</td><td>2020-04-07</td><td>2020-04-07</td></tr></tbody></table><p><em>*<em>*</em>*2020-04-08 mysql对应的user_info表****</em>*</p><table><thead><tr><th>用户id</th><th>姓名</th><th>年龄</th><th>手机号</th><th>创建时间</th><th>修改时间</th></tr></thead><tbody><tr><td>0001</td><td>张三</td><td>20</td><td>111233</td><td>2020-04-06</td><td>2020-04-08</td></tr><tr><td>0002</td><td>小红</td><td>21</td><td>222221</td><td>2020-04-06</td><td>2020-04-08</td></tr><tr><td>0003</td><td>李四</td><td>22</td><td>333333</td><td>2020-04-07</td><td>2020-04-07</td></tr><tr><td>0004</td><td>王五</td><td>44</td><td>444445</td><td>2020-04-07</td><td>2020-04-08</td></tr><tr><td>005</td><td>德玛</td><td>55</td><td>555555</td><td>2020-04-08</td><td>2020-04-08</td></tr></tbody></table><p>抽取到ods之后，现在要保留用户的历史数据和新增数据，负责后期我要查看这个用户历史使用的手机号没法查看</p><p>所以我们要变成这样的结构：</p><table><thead><tr><th>用户id</th><th>姓名</th><th>年龄</th><th>手机号</th><th>创建时间</th><th>修改时间</th><th>start_time</th><th>end_time</th><th>date_id</th></tr></thead><tbody><tr><td>0001</td><td>张三</td><td>20</td><td>111111</td><td>2020-04-06</td><td>2020-04-06</td><td>2020-04-06</td><td>2020-04-06</td><td>2020-04-08</td></tr><tr><td>0001</td><td>张三</td><td>20</td><td>111222</td><td>2020-04-06</td><td>2020-04-07</td><td>2020-04-07</td><td>2020-04-07</td><td>2020-04-08</td></tr><tr><td>0001</td><td>张三</td><td>20</td><td>111233</td><td>2020-04-06</td><td>2020-04-08</td><td>2020-04-08</td><td>9999-12-31</td><td>2020-04-08</td></tr><tr><td>0002</td><td>小红</td><td>21</td><td>222222</td><td>2020-04-06</td><td>2020-04-06</td><td>2020-04-06</td><td>2020-04-07</td><td>2020-04-08</td></tr><tr><td>0002</td><td>小红</td><td>21</td><td>222221</td><td>2020-04-06</td><td>2020-04-08</td><td>2020-04-08</td><td>9999-12-31</td><td>2020-04-08</td></tr><tr><td>0003</td><td>李四</td><td>22</td><td>333333</td><td>2020-04-07</td><td>2020-04-07</td><td>2020-04-07</td><td>9999-12-31</td><td>2020-04-08</td></tr><tr><td>0004</td><td>王五</td><td>44</td><td>444444</td><td>2020-04-07</td><td>2020-04-07</td><td>2020-04-07</td><td>2020-04-07</td><td>2020-04-08</td></tr><tr><td>0004</td><td>王五</td><td>44</td><td>444445</td><td>2020-04-07</td><td>2020-04-08</td><td>2020-04-08</td><td>9999-12-31</td><td>2020-04-08</td></tr><tr><td>005</td><td>德玛</td><td>55</td><td>555555</td><td>2020-04-08</td><td>2020-04-08</td><td>2020-04-08</td><td>9999-12-31</td><td>2020-04-08</td></tr></tbody></table><p>保留历史可以选择的方式：</p><p>1、可以选择全量</p><p>但是每次都全量，肯定行不通，极大的浪费空间和造成数据的重复性</p><p>2、增量</p><p>可以获取到增量数据，但是改变的数据，你没有办法获取到</p><p>3、拉链</p><p>这里使用拉链表，是最适合解决这个问题的</p><p>要保留每个用户的历史手机号和修改时间</p><ol><li>创建一个拉链表</li></ol><pre><code class="sql">alter table wedw_dwd.test_user_info_dz drop if EXISTS PARTITION(date_id=&#39;2020-04-07&#39;);</code></pre><ol start="2"><li>插入数据</li></ol><p>a表是建的拉链表，b表是ods层的数据表</p><pre><code class="sql">insert overwrite table wedw_dwd.test_user_info_dz PARTITION(date_id=&#39;2020-04-07&#39;)select      a.user_id    ,a.user_name    ,a.user_age    ,a.user_cellphone    ,a.create_time    ,a.update_time    ,a.start_time    ,a.end_time    from(select      a.user_id    ,a.user_name    ,a.user_age    ,a.user_cellphone    ,a.create_time    ,a.update_time    ,a.start_time    ,a.end_timefrom wedw_dwd.test_user_info_dz a left join wedw_ods.test_user_info_$&#123;DATA_DATE&#125; b on b.user_id=a.user_id and b.create_time &lt; &#39;2020-04-08&#39;   where a.date_id = &#39;2020-04-06&#39; and (b.user_id is null       or (b.user_id is not null and a.end_time &lt;=&#39;2020-04-06&#39;)  ) tmp1  union all  (select     a.user_id    ,a.user_name    ,a.user_age    ,a.user_cellphone    ,a.create_time    ,a.update_time    ,to_date(update_time) as start_time    ,&#39;9999-12-31&#39; as end_timefromwedw_ods.test_user_info_$&#123;DATA_DATE&#125; ) tmp2 </code></pre><ol start="3"><li><img src="/shu-ju-cang-ku/la-lian-biao/640.webp" alt="图片" style="zoom:67%;"></li></ol><table><thead><tr><th>用户id</th><th>姓名</th><th>年龄</th><th>手机号</th><th>创建时间</th><th>修改时间</th><th>start_time</th><th>end_time</th><th>date_id</th></tr></thead><tbody><tr><td>0001</td><td>张三</td><td>20</td><td>111233</td><td>2020-04-06</td><td>2020-04-08</td><td>2020-04-08</td><td>9999-12-31</td><td>2020-04-08</td></tr><tr><td>0002</td><td>小红</td><td>21</td><td>222221</td><td>2020-04-06</td><td>2020-04-08</td><td>2020-04-08</td><td>9999-12-31</td><td>2020-04-08</td></tr><tr><td>0003</td><td>李四</td><td>22</td><td>333333</td><td>2020-04-07</td><td>2020-04-07</td><td>2020-04-07</td><td>9999-12-31</td><td>2020-04-08</td></tr><tr><td>0004</td><td>王五</td><td>44</td><td>444445</td><td>2020-04-07</td><td>2020-04-08</td><td>2020-04-08</td><td>9999-12-31</td><td>2020-04-08</td></tr><tr><td>005</td><td>德玛</td><td>55</td><td>555555</td><td>2020-04-08</td><td>2020-04-08</td><td>2020-04-08</td><td>9999-12-31</td><td>2020-04-08</td></tr></tbody></table><p>拉链表实例：<a href="https://mp.weixin.qq.com/s/ZFe-Eghn6syX0BGEzYzOxQ">https://mp.weixin.qq.com/s/ZFe-Eghn6syX0BGEzYzOxQ</a></p><h2 id="缓慢变化纬"><a href="#缓慢变化纬" class="headerlink" title="缓慢变化纬"></a>缓慢变化纬</h2><p>原文链接：<a href="https://mp.weixin.qq.com/s/mrM2x-YH2Bs69JGCXLBRdA">https://mp.weixin.qq.com/s/mrM2x-YH2Bs69JGCXLBRdA</a></p><p>Slowly Changing Dimensions are dimensions that have data that slowly changes. 意思就是说数据会发生缓慢变化的维度就叫”缓慢变化维”。</p><p>处理缓慢变化维度是Kimball数仓体系中永恒的话题，因为数据仓库的本质，以及维度表在维度建模中的基础作用，我们几乎总是要跟踪维度的变更（change tracking），以保留历史，并提供准确的查询和分析结果。在《The Data Warehouse Toolkit, 3rd Edition》一书的第5章，Kimball提出了多种缓慢变化维度的类型和处理方法，其中前五种是原生的，后面的方法都是混合方法（hybrid techniques），因此下面来看看前五种，即Type 0~Type 4。</p><h4 id="Tpye0-保留原始值"><a href="#Tpye0-保留原始值" class="headerlink" title="Tpye0 保留原始值"></a>Tpye0 保留原始值</h4><p>某一个属性值绝不会变化。事实表始终按照该原始值进行分组。比如在用户维度表中，用户注册时使用的原始用户名（original_user_name）。如果它发生变化，那么变化后的值是无效的，会被抛弃，始终按照用户第一次填写的数据为准。很明显这种方式是不推荐的。</p><p>说白话就是只要第一次的数据，不要变化后的数据</p><h4 id="Type1-覆盖更新"><a href="#Type1-覆盖更新" class="headerlink" title="Type1 覆盖更新"></a>Type1 覆盖更新</h4><p>与业务数据保持一致，同样为直接update。这样就难以记录历史变化，例如如果周杰伦于15年7月调入北京，那么我们想要知道北京销售员在15年的销售数据时，就会将周杰伦的业绩算入北京分公司下，实际上周杰伦7月份以前的销售数据均应算在台北，所以为了避免这样的问题就有了TYPE2的处理方式。</p><p>这样的方式就是Mysql中一般表的处理方式，如果变中的值发生了变化就直接update，不保留历史数据，这样显然是不符合我们需求的。</p><h4 id="Type2-增加新的列"><a href="#Type2-增加新的列" class="headerlink" title="Type2 增加新的列"></a>Type2 增加新的列</h4><p>数据仓库系统的目标之一是正确地表示历史记录。我们在生产环境中的基于Hive的数仓建设过程中，拉链表就是直接的体现。</p><p>这种类型在维度表中<strong>添加两个辅助列</strong>：该行的有效日期（effective date）和过期日期（expiration date），分别指示该行从哪个时间点开始生效，以及在哪个时间点过后会变为无效。<strong>每当一个或多个维度发生更改时，就创建一个新的行</strong>，新行包含有修改后的维度值，而旧行包含有修改前的维度值，且旧行的过期日期也会同步修改。书中的例子如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2MNlco9pTMxjCHkDStQYtm6g4ftCUE2aCXeuBwYmguksjwnR0SJ7W4MXmKHcrR1e1svztIWiaTheew/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>在上图中，当前有效列（current列）的过期日期会被记录为9999-12-31。当Department Name维度变化时，旧有的Product Key为12345的行的过期日期被更新为修改日期，并且新建了一个Key为25984的行，包含新的数据。</p><p>需要注意的是，这里的Product Key是所谓代理键（surrogate key），即不表示具体业务含义，而只是代表表内数据行的唯一ID。在处理SCD时，代理键可以直接用来区分同一自然键（natural key）的数据的新旧版本。上图中的SKU就是自然键。</p><p>这种类型的SCD处理方式能够非常有效且精确地保留历史与反映变更，但缺点是会造成数据的膨胀，因为即使只有一个维度变化，也要创建新行。</p><h4 id="Type3-新增属性列"><a href="#Type3-新增属性列" class="headerlink" title="Type3 新增属性列"></a>Type3 新增属性列</h4><p>用不同的字段来保存不同的值，就是在表中增加一个字段，这个字段用来保存变化后的当前值，而原来的值则被称为变化前的值。我们举个很简单的例子，例如我们在用户表中的用户住址这一列会变化，那么我们可以通过新增一个列来表示曾经的地址：</p><p>这么做虽然解决上面的数据膨胀的问题，但是如果很多个列都会变化，那么我们要新增很多系列，显然这是不合理的。另外，这种做法只能保留上一次的数据，那么更久远的变化就丢失了。</p><p>这种方式是我第一次了解拉链表时，最快想到的方式，但是这样并不理想，如果某一列持续update 那么会一直新增column，如果column没有了那么就会覆盖上次的值，有这上面两种类型的缺点</p><h4 id="Type4-新增维度表"><a href="#Type4-新增维度表" class="headerlink" title="Type4 新增维度表"></a>Type4 新增维度表</h4><p>如果我们的表规模非常大，数据量千万以上，大量的列变化非常频繁，那么这时候就不能用上面的办法来支撑了，我们需要将那些快速变化的维度从原来的大维度表中拆分出来单独处理，是为微维度（mini-dimension）。</p><p>我们用书中的内容举例，如果顾客维度中有一部分人口统计学（demographic）维度是RCD，就将它们拆成单独的维度表：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2MNlco9pTMxjCHkDStQYtm6jRhnoDkSk0Vul1Tibv26TmyqQDSBmbLk4OAsxAUQ2afib2b3ibtxFEfhQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p><p>最后给出一张《The Data Warehouse Toolkit, 3rd Edition》中的这几种方式的比较图：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2MNlco9pTMxjCHkDStQYtm6Eia8ZWvH3J25UmIj5uyau7vHSesHuSoMSBmlsmzrBGnNCNOnb4QeibeA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>]]></content>
      
      
      <categories>
          
          <category> 数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 拉链表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux环境搭建</title>
      <link href="/linux/centos-huan-jing-da-jian/"/>
      <url>/linux/centos-huan-jing-da-jian/</url>
      
        <content type="html"><![CDATA[<p>纯净版Centos系统需要自己进行安装环境和组件以及需要的库。一面是一些常用命令。</p><p>安装gcc</p><pre><code>yum install gcc</code></pre><p>安装wget</p><pre><code>yum -y install wget</code></pre><p>安装vim</p><pre><code>yum install vim </code></pre>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Cloud Config</title>
      <link href="/java/spring/spring-cloud/fu-wu-pei-zhi/config/"/>
      <url>/java/spring/spring-cloud/fu-wu-pei-zhi/config/</url>
      
        <content type="html"><![CDATA[<h2 id="Config-Server是什么？"><a href="#Config-Server是什么？" class="headerlink" title="Config Server是什么？"></a>Config Server是什么？</h2><p>Spring Cloud Config为分布式系统中的<strong>外部化配置提供服务器和客户端支持</strong>。使用Config Server，您可以集中管理所有环境中应用程序的外部属性。客户端和服务器上的概念与Spring<code>Environment</code>和<code>PropertySource</code>抽象，因此它们非常适合Spring应用程序，但可以与以任何语言运行的任何应用程序一起使用。当应用程序从开发人员迁移到测试人员并进入生产过程时，您可以管理这些环境之间的配置，并确保应用程序具有迁移时所需的一切。服务器存储后端的默认实现使用git，因此它轻松支持配置环境的标记版本，并且可以由各种工具访问以管理内容。添加替代实现并将其插入Spring配置很容易。</p><h3 id="配置中心的不同架构"><a href="#配置中心的不同架构" class="headerlink" title="配置中心的不同架构"></a>配置中心的不同架构</h3><p>1、config+bus</p><p>2、nacos</p><p>3、携程的apollo(阿波罗)</p><h2 id="Config概念"><a href="#Config概念" class="headerlink" title="Config概念"></a>Config概念</h2><p>Config分为服务端和客户端</p><p>服务端也称为分布式配置中心，它是一个独立的微服务应用，用来连接配置服务器并为客户端提供获取配置信息，加密&#x2F;解密信息等访问接口</p><p>客户端是通过指定的配置中心来管理引用资源，以及和业务相关的配置内容，并在启动的时候从配置中心互殴和加载配置信息配置服务器默认才用git来存储配置信息，这样有助于对环境进行版本控制，并且可以通过git客户端工具来方便的管理和访问配置内容。</p><h2 id="配置中心可以做什么"><a href="#配置中心可以做什么" class="headerlink" title="配置中心可以做什么"></a>配置中心可以做什么</h2><ul><li>集中管理配置文件</li><li>不同环境不同配置，动态化的配置更新，分环境部署</li><li>运行期间动态调整配置，不再需要在每个服务部署的机器上编写配置文件，服务会向配置中心统一拉取配置自己的信息，但配置发生变动时，服务不需要重启即可感知到配置的变化并应用新的配置</li><li>将配置信息以REST接口的形式暴露</li></ul><p>推荐和GitHub整合</p><h2 id="配置中心搭建"><a href="#配置中心搭建" class="headerlink" title="配置中心搭建"></a>配置中心搭建</h2><h3 id="服务端搭建"><a href="#服务端搭建" class="headerlink" title="服务端搭建"></a>服务端搭建</h3><ol><li>pom文件修改</li></ol><p>主要的包：spring-cloud-config-server 、 spring-cloud-starter-bus-amqp  和  spring-boot-starter-actuator</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;cloud-config-center-3344&lt;/artifactId&gt;    &lt;properties&gt;        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--添加消息总线RbbitMQ支持--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-bus-amqp&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><ol start="2"><li>yml 文件修改</li></ol><p>这里没有跟着视频使用github，这里使用的gitee，并且指定了用户名和密码 </p><pre><code class="yaml">server:  port: 3344spring:  application:    name: cloud-config-center  cloud:    config:      server:        git:          uri: https://gitee.com/zhangwehui/springcloud-config.git   #github仓库上面的git仓库名字          ##搜索目录   写具体的仓库名称          search-paths:            - springcloud-config          skip-ssl-validation: true          username: ************          password: ************      #读取分支      label: master  #rabbit相关配置#  rabbitmq:#    host: localhost#    port: 5672#    username: guest#    password: guesteureka:  client:    service-url:      defaultZone: http://localhost:7001/eureka #注册进eureka##rabbitmq相关配置，暴露bus刷新配置的端点#这里使用bus 来实现动态刷新#management:#  endpoints:  #暴露bus刷新配置的端点#    web:#      exposure:#        include: &#39;bus-refresh&#39;  #凡是暴露监控、刷新的都要有actuator依赖，bus-refresh就是actuator的刷新操作</code></pre><ol start="3"><li>启动类</li></ol><p>添加注解 <code>@EnableConfigServer</code></p><pre><code class="java">package com.exercise.springcloud;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.config.server.EnableConfigServer;@SpringBootApplication@EnableConfigServerpublic class ConfigCenterMain3344 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(ConfigCenterMain3344.class,args);    &#125;&#125;</code></pre><ol start="4"><li>启动访问结果</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-pei-zhi/config/image-20210203155631856.png" alt="image-20210203155631856"></p><h3 id="客户端搭建"><a href="#客户端搭建" class="headerlink" title="客户端搭建"></a>客户端搭建</h3><ol><li>pom文件修改</li></ol><p>主要导入 ：spring-cloud-starter-bus-amqp  和  spring-cloud-starter-config</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;cloud-config-client-3355&lt;/artifactId&gt;    &lt;properties&gt;        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--添加消息总线rabbitMQ支持--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-bus-amqp&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--不带server了，说明是客户端--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><ol start="2"><li><code>bootstrap.yml</code>文件修改</li></ol><p>注意点：</p><h4 id="bootstrap与application"><a href="#bootstrap与application" class="headerlink" title="bootstrap与application"></a>bootstrap与application</h4><p>只有springcloud有bootstrap项目，而springboot只有application</p><p>这里主要是说明application和bootstrap的<strong>加载顺序</strong>。</p><ul><li>bootstrap.yml（bootstrap.properties）先加载</li><li>application.yml（application.properties）后加载</li></ul><p><strong>配置区别</strong></p><p>bootstrap.yml 用于应用程序上下文的引导阶段，bootstrap.yml 可以理解成系统级别的一些参数配置，这些参数一般是不会变动的。</p><p>而application.yml 可以用来定义应用级别的，如果搭配 spring-cloud-config 使用 application.yml 里面定义的文件可以实现动态替换。</p><pre><code class="yaml">server:  port: 3355spring:  application:    name: config-client  cloud:    #Config客户端配置    config:      label: master #分支名称      name: config #配置文件名称      profile: dev #读取后缀名称 上述3个综合：master分支上 config-dev.yml 的配置文件被读取 http://config-3344.com:3344/master/config-dev.yml      uri: http://localhost:3344    #配置中心地址  #rabbit相关配置 15672是web管理界面的端口，5672是MQ访问的端口#  rabbitmq:#    host: localhost#    port: 5672#    username: guest#    password: guest#服务注册到eureka地址eureka:  client:    service-url:      defaultZone: http://localhost:7001/eureka#暴露监控端点management:  endpoints:    web:      exposure:        include: &quot;*&quot;</code></pre><p><img src="/java/spring/spring-cloud/fu-wu-pei-zhi/config/image-20210203163214802.png" alt="image-20210203163214802"></p><ol start="3"><li>启动类编写</li></ol><pre><code class="java">@SpringBootApplication@EnableEurekaClientpublic class ConfigClientMain3355 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(ConfigClientMain3355.class,args);    &#125;&#125;</code></pre><ol start="4"><li>Controller层代码</li></ol><pre><code class="java">@RestController@RefreshScopepublic class ConfigClientController &#123;    //这里获取的config.info 是gitee上配置文件里面的参数    @Value(&quot;$&#123;config.info&#125;&quot;)    private String configInfo;  //要访问的3344上的信息    @GetMapping(&quot;/configInfo&quot;)    public String getConfigInfo()&#123;        return configInfo;    &#125;&#125;</code></pre><ol start="5"><li>查看结果</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-pei-zhi/config/image-20210203163002446.png" alt="image-20210203163002446"></p><p>3344结果</p><p><img src="/java/spring/spring-cloud/fu-wu-pei-zhi/config/image-20210203163039336.png" alt="3344结果"></p><h2 id="Config动态刷新"><a href="#Config动态刷新" class="headerlink" title="Config动态刷新"></a>Config动态刷新</h2><p>问题描述：</p><p>gitee上文件已经修改，服务端连接gitee可以获取到改变后的最新结果，但是3355却不能获取到最新结果，要获取到最新结果只能够重启或者重新加载。</p><h3 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h3><ol><li>pom配置</li></ol><p>和上面的服务端pom文件一致，主要的包：spring-cloud-config-server 、 spring-cloud-starter-bus-amqp  和  spring-boot-starter-actuator</p><ol start="2"><li>bootstrap.yml</li></ol><p>主要配置了rabbitmq</p><pre><code class="yaml">server:  port: 3355spring:  application:    name: config-client  cloud:    #Config客户端配置    config:      label: master #分支名称      name: config #配置文件名称      profile: dev #读取后缀名称 上述3个综合：master分支上config-dev.yml的配置文件被读取 http://config-3344.com:3344/master/config-dev.yml      uri: http://localhost:3344    #配置中心地址  #rabbit相关配置 15672是web管理界面的端口，5672是MQ访问的端口  rabbitmq:    host: localhost    port: 5672    username: guest    password: guest#服务注册到eureka地址eureka:  client:    service-url:      defaultZone: http://localhost:7001/eureka#暴露监控端点management:  endpoints:    web:      exposure:        include: &quot;*&quot;</code></pre><ol start="3"><li>业务类配置</li></ol><p>添加  <code>@RefreshScope</code> 注解</p><pre><code class="java">package com.exercise.springcloud.controller;import org.springframework.beans.factory.annotation.Value;import org.springframework.cloud.context.config.annotation.RefreshScope;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RefreshScopepublic class ConfigClientController &#123;    //这里获取的config.info 是gitee上配置文件里面的参数    @Value(&quot;$&#123;config.info&#125;&quot;)    private String configInfo;  //要访问的3344上的信息    @GetMapping(&quot;/configInfo&quot;)    public String getConfigInfo()&#123;        return configInfo;    &#125;&#125;</code></pre><ol start="4"><li><p>再次修改gitee上的配置文件</p></li><li><p>查看结果，会发现客户端还是没有变化。</p></li></ol><p>需要发送一个刷新请求</p><pre><code class="powershell">C:\Users\HP&gt;curl -X POST &quot;http://localhost:3355/actuator/refresh&quot;</code></pre><p>每次修改完配置文件都必须发送请求才能够起作用</p><p>这里是手动的，如果服务过多，那就会太过于繁琐。为了解决这个问题，使用了<code>Bus</code>来解决这个问题</p>]]></content>
      
      
      <categories>
          
          <category> Spring </category>
          
          <category> Spring Config </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring Config </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Gateway</title>
      <link href="/java/spring/spring-cloud/fu-wu-wang-guan/gateway/"/>
      <url>/java/spring/spring-cloud/fu-wu-wang-guan/gateway/</url>
      
        <content type="html"><![CDATA[<h2 id="Gateway是什么？"><a href="#Gateway是什么？" class="headerlink" title="Gateway是什么？"></a>Gateway是什么？</h2><blockquote><p>Spring Cloud Gateway是Spring官方基于Spring 5.0，Spring Boot 2.0和Project Reactor等技术开发的网关，Spring Cloud Gateway旨在为微服务架构提供一种简单而有效的统一的API路由管理方式。Spring Cloud Gateway作为Spring Cloud生态系中的网关，目标是替代ZUUL，其不仅提供统一的路由方式，并且基于Filter链的方式提供了网关基本的功能，例如：安全，监控&#x2F;埋点，和限流等。</p></blockquote><p>Gateway提供一种简单而有效的方式来对API。Gateway使用的Webflux中的reactor-netty 响应式变成组件，底层使用了Netty通讯框架。</p><p>Spring cloud2.0 以上版本，没有对新版本的Zuul2.0以上最新高性能版本进行集成，仍然还是使用Zuul1.*非Reacor模式的老版本。而为了提升网关的性能，SpringCloud Gateway基于WebFlux的框架实现的，而WebFlux框架底层则使用了高性能的Reactor模式通信框架的Netty。</p><h2 id="网关可以干什么？"><a href="#网关可以干什么？" class="headerlink" title="网关可以干什么？"></a>网关可以干什么？</h2><p>反向代理、鉴权、流量控制、熔断、日志监控。。。。</p><p>在使用的时候可以不显示实际的地址和端口，都显示gateway的端口</p><h2 id="Geteway三大核心"><a href="#Geteway三大核心" class="headerlink" title="Geteway三大核心"></a>Geteway三大核心</h2><p><strong>Route（路由）</strong></p><ul><li>构建网关的基本模块，它由ID，目标URI，一系列的断言和过滤器组成，如果断言为true则匹配该路由</li></ul><p><strong>Predicate（断言）</strong></p><ul><li>参考的是Java8的 java.util.function.Predicate</li><li>开发人员可以匹配HTTP请求中的所有内容（例如请求头或请求参数），如果请求与断言相匹配则进行路由</li></ul><p><strong>Filter（过滤）</strong></p><p><img src="/java/spring/spring-cloud/fu-wu-wang-guan/gateway/aHR0cDovL2Nvcy5yYWluMTAyNC5jb20vbWFya2Rvd24vaW1hZ2UtMjAxOTEwMDgxNjA3MTM4MjIucG5n-1613999397470" alt="三大核心"></p><p><img src="/java/spring/spring-cloud/fu-wu-wang-guan/gateway/image-20210202190926365-1613999397471.png" alt="image-20210202190926365"></p><h2 id="Gateway的使用"><a href="#Gateway的使用" class="headerlink" title="Gateway的使用"></a>Gateway的使用</h2><h3 id="1、yaml方式配置"><a href="#1、yaml方式配置" class="headerlink" title="1、yaml方式配置"></a>1、yaml方式配置</h3><ol><li>创建module</li><li>修改pom文件</li></ol><p>这里的pom注意不需要之前的web和actuator这两个依赖</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;cloud-provider-hystrix-payment8001&lt;/artifactId&gt;    &lt;dependencies&gt;        &lt;!--hystrix--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--eureka client--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--web--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--引入自定义的api通用包，可用使用Payment支付Entity--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;            &lt;artifactId&gt;cloud-api-commons&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><ol start="3"><li>修改yaml文件</li></ol><pre><code class="yaml">server:  port: 9527spring:  application:    name: cloud-gateway  cloud:    gateway:      routes:        - id: payment_routh #payment_routh    #路由的ID，没有固定规则但要求唯一，简易配合服务名          uri: http://localhost:8001         #匹配后提供服务的路由地址          predicates:            - Path=/payment/get/**          #断言，路径相匹配的进行路由  这里的** 指的是&#123;id&#125;        - id: payment_routh2 #payment_routh   #路由的ID，没有固定规则但要求唯一，简易配合服务名          uri: http://localhost:8001          #匹配后提供服务的路由地址          predicates:            - Path=/payment/lb/**             #断言，路径相匹配的进行路由eureka:  instance:    hostname: cloud-gateway-service  client:    register-with-eureka: true    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka</code></pre><p>这里指定了端口和名称，但是要实现多个服务负载均衡。那需要配置多个端口和路径，很麻烦。可以直接写微服务名称来实现</p><p>增加了 discovery 开启注册中心动态创建路由功能</p><p>改变了uri地址，从指定地址变成了微服务名称。<code>uri: lb://微服务名称</code> </p><p>lb代表从注册中心获取服务负载均衡loadbalance</p><pre><code class="yaml">server:  port: 9527spring:  application:    name: cloud-gateway  cloud:    gateway:      discovery:        locator:          enabled: true   #开启从注册中心动态创建路由的功能，利用微服务名进行路由      routes:        - id: payment_routh #payment_routh    #路由的ID，没有固定规则但要求唯一，简易配合服务名          #uri: http://localhost:8001         #匹配后提供服务的路由地址          uri: lb://cloud-payment-service   #匹配后提供服务的路由地址          predicates:            - Path=/payment/get/**          #断言，路径相匹配的进行路由        - id: payment_routh2 #payment_routh   #路由的ID，没有固定规则但要求唯一，简易配合服务名          #uri: http://localhost:8001          #匹配后提供服务的路由地址          uri: lb://cloud-payment-service     #匹配后提供服务的路由地址          predicates:            - Path=/payment/lb/**             #断言，路径相匹配的进行路由            #- After=2020-03-15T15:35:07.412+08:00[GMT+08:00]      #可以设置在什么时间后生效，在生效时间之前都无法访问            #- Before=                                             #在这个时间之前生效            #- Between=,                                           #设置时间范围内生效#            - Cookie=username,zzyy                                #访问过程中带cookie    username=zzyy#            - Header=X-Request-Id, \d+                            #请求头要有X-Request-Id属性并且值为整数的正则表达式#            - Host=10.130.212.8                                   #请求头Host属性  参数为10.130.212.8            #- Method=GET                                          #只有Get方式请求可以进来            #- Query=username, \d+                                 #要有参数名username并且值还要整数才能路由eureka:  instance:    hostname: cloud-gateway-service  client:    register-with-eureka: true    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka</code></pre><ol start="4"><li>启动服务调用结果查看结果</li></ol><p>下面两种都可以调用到结果，并返回结果</p><p><img src="/java/spring/spring-cloud/fu-wu-wang-guan/gateway/image-20210202201432751-1613999397471.png" alt="image-20210202201432751"></p><p><img src="/java/spring/spring-cloud/fu-wu-wang-guan/gateway/image-20210202201414673-1613999397471.png" alt="image-20210202201414673"></p><h3 id="2、编码方式进行配置"><a href="#2、编码方式进行配置" class="headerlink" title="2、编码方式进行配置"></a>2、编码方式进行配置</h3><ol><li>pom文件 和上方一样</li><li>yml文件 和上方一样</li><li>建立配置类，进行编写</li></ol><pre><code class="java">package com.exercise.springcloud.config;import org.springframework.cloud.gateway.route.RouteLocator;import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class GateWayConfig &#123;    /**     * 配置一个id为route-name的路由规则，     * 当访问地址http://localhost:9527/guonei时会自动转发到地址：http://news.baidu.com/guonei     *     * @param routeLocatorBuilder     * @return     */    @SuppressWarnings(&quot;JavaDoc&quot;)    @Bean    public RouteLocator customRouteLocator(RouteLocatorBuilder routeLocatorBuilder) &#123;        //路由构建器        RouteLocatorBuilder.Builder routes = routeLocatorBuilder.routes();        //id唯一   path路径填写gateway的访问url  uri 填写uri        routes.route(&quot;path_route_atguigu&quot;,                r -&gt; r.path(&quot;/guonei&quot;)                        .uri(&quot;http://news.baidu.com/guonei&quot;)).build();        return routes.build();    &#125;&#125;</code></pre><ol start="4"><li>启动后进行访问</li></ol><p>可以看到通过9527端口指定路径转到了百度行为页面</p><p><img src="/java/spring/spring-cloud/fu-wu-wang-guan/gateway/image-20210202202834173-1613999397471.png" alt="image-20210202202834173"></p><h2 id="Predicate的使用"><a href="#Predicate的使用" class="headerlink" title="Predicate的使用"></a>Predicate的使用</h2><p>使用 Predicate（断言） 可以实现不同的功能，请求方式拦截，地址拦截，时间配置，查询方式配置等等。</p><p>下面yaml文件中注释掉的部分有详细的说明</p><pre><code class="yaml">server:  port: 9527spring:  application:    name: cloud-gateway  cloud:    gateway:      discovery:        locator:          enabled: true   #开启从注册中心动态创建路由的功能，利用微服务名进行路由      routes:        - id: payment_routh #payment_routh    #路由的ID，没有固定规则但要求唯一，简易配合服务名          #uri: http://localhost:8001         #匹配后提供服务的路由地址          uri: lb://cloud-payment-service   #匹配后提供服务的路由地址          predicates:            - Path=/payment/get/**          #断言，路径相匹配的进行路由        - id: payment_routh2 #payment_routh   #路由的ID，没有固定规则但要求唯一，简易配合服务名          #uri: http://localhost:8001          #匹配后提供服务的路由地址          uri: lb://cloud-payment-service     #匹配后提供服务的路由地址          predicates:            - Path=/payment/lb/**             #断言，路径相匹配的进行路由            #- After=2020-03-15T15:35:07.412+08:00[GMT+08:00]      #可以设置在什么时间后生效，在生效时间之前都无法访问            #- Before=                                             #在这个时间之前生效            #- Between=,                                           #设置时间范围内生效#            - Cookie=username,zzyy                                #访问过程中带cookie    username=zzyy#            - Header=X-Request-Id, \d+                            #请求头要有X-Request-Id属性并且值为整数的正则表达式#            - Host=10.130.212.8                                   #请求头Host属性  参数为10.130.212.8            #- Method=GET                                          #只有Get方式请求可以进来            #- Query=username, \d+                                 #要有参数名username并且值还要整数才能路由eureka:  instance:    hostname: cloud-gateway-service  client:    register-with-eureka: true    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka</code></pre><h2 id="Filter的使用"><a href="#Filter的使用" class="headerlink" title="Filter的使用"></a>Filter的使用</h2><p>官方文档上提供了31种filter过滤器，选择自己需要的过滤器使用</p><p><a href="https://docs.spring.io/spring-cloud-gateway/docs/2.2.6.RELEASE/reference/html/#gatewayfilter-factories">https://docs.spring.io/spring-cloud-gateway/docs/2.2.6.RELEASE/reference/html/#gatewayfilter-factories</a></p><p>在yaml文件中配置过滤器</p><pre><code class="yaml">server:  port: 9527spring:  application:    name: cloud-gateway  cloud:    gateway:      discovery:        locator:          enabled: true   #开启从注册中心动态创建路由的功能，利用微服务名进行路由      routes:        - id: payment_routh #payment_routh    #路由的ID，没有固定规则但要求唯一，简易配合服务名          #uri: http://localhost:8001         #匹配后提供服务的路由地址          uri: lb://cloud-payment-service   #匹配后提供服务的路由地址          predicates:            - Path=/payment/get/**          #断言，路径相匹配的进行路由        - id: payment_routh2 #payment_routh   #路由的ID，没有固定规则但要求唯一，简易配合服务名          #uri: http://localhost:8001          #匹配后提供服务的路由地址          uri: lb://cloud-payment-service     #匹配后提供服务的路由地址          predicates:            - Path=/payment/lb/**             #断言，路径相匹配的进行路由            #- After=2020-03-15T15:35:07.412+08:00[GMT+08:00]      #可以设置在什么时间后生效，在生效时间之前都无法访问            #- Before=                                             #在这个时间之前生效            #- Between=,                                           #设置时间范围内生效#            - Cookie=username,zzyy                                #访问过程中带cookie    username=zzyy#            - Header=X-Request-Id, \d+                            #请求头要有X-Request-Id属性并且值为整数的正则表达式#            - Host=10.130.212.8                                   #请求头Host属性  参数为10.130.212.8            #- Method=GET                                          #只有Get方式请求可以进来            #- Query=username, \d+                                 #要有参数名username并且值还要整数才能路由          filters:            - AddRequestHeader=X-Request-red, blueeureka:  instance:    hostname: cloud-gateway-service  client:    register-with-eureka: true    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka</code></pre><h3 id="自定义全局过滤器GlobalFilter"><a href="#自定义全局过滤器GlobalFilter" class="headerlink" title="自定义全局过滤器GlobalFilter"></a>自定义全局过滤器GlobalFilter</h3><p>实现两个接口，过滤器可以同时配置多个，并注入容器</p><p>implements GlobalFilter, Ordered</p><pre><code class="java">package com.exercise.springcloud.filter;import lombok.extern.slf4j.Slf4j;import org.springframework.cloud.gateway.filter.GatewayFilterChain;import org.springframework.cloud.gateway.filter.GlobalFilter;import org.springframework.core.Ordered;import org.springframework.http.HttpStatus;import org.springframework.stereotype.Component;import org.springframework.web.server.ServerWebExchange;import reactor.core.publisher.Mono;import java.util.Date;@Component@Slf4jpublic class MyLogGateWayFilter implements GlobalFilter, Ordered &#123;    @Override    public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123;        log.info(&quot;***********come in MyLogGateWayFilter: &quot; + new Date());        String uname = exchange.getRequest().getQueryParams().getFirst(&quot;uname&quot;);//每次进来后判断带不带uname这个key        if (uname == null) &#123;            log.info(&quot;*********用户名为null ，非法用户，o(╥﹏╥)o&quot;);            exchange.getResponse().setStatusCode(HttpStatus.NOT_ACCEPTABLE);    //uname为null非法用户            System.out.println(exchange.getResponse().setComplete());            return exchange.getResponse().setComplete();        &#125;        //过滤链，传给下一个过滤器        return chain.filter(exchange);    &#125;    //加载过滤器的优先级大小，数字越小优先级越高    @Override    public int getOrder() &#123;        return 0;    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> Spring Gateway </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring Gateway </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Zuul</title>
      <link href="/java/spring/spring-cloud/fu-wu-wang-guan/zuul/"/>
      <url>/java/spring/spring-cloud/fu-wu-wang-guan/zuul/</url>
      
        <content type="html"><![CDATA[<h1 id="zuul"><a href="#zuul" class="headerlink" title="zuul"></a>zuul</h1><p>官网文档：<a href="https://github.com/Netflix/zuul/wiki">https://github.com/Netflix/zuul/wiki</a></p><h2 id="什么是API网关"><a href="#什么是API网关" class="headerlink" title="什么是API网关"></a>什么是API网关</h2><p>在微服务架构中，通常会有多个服务提供者。设想一个电商系统，可能会有商品、订单、支付、用户等多个类型的服务，而每个类型的服务数量也会随着整个系统体量的增大也会随之增长和变更。作为UI端，在展示页面时可能需要从多个微服务中聚合数据，而且服务的划分位置结构可能会有所改变。网关就可以对外暴露聚合API，屏蔽内部微服务的微小变动，保持整个系统的稳定性。</p><p>当然这只是网关众多功能中的一部分，它还可以做负载均衡，统一鉴权，协议转换，监控监测等一系列功能。</p>]]></content>
      
      
      <categories>
          
          <category> Spring Zuul </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring Zuul </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>配置ssh免密</title>
      <link href="/linux/centos7-pei-zhi-ssh/"/>
      <url>/linux/centos7-pei-zhi-ssh/</url>
      
        <content type="html"><![CDATA[<p>centos配置ssh的时候可以修改配置文件</p><pre><code>vim /etc/ssh/sshd_config</code></pre><p>进行配置 </p><pre><code>Port 22PermitRootLogin no //禁止root账户远程登录PasswordAuthentication yes//启用口令认证方式PermitEmptyPasswords no//禁止使用空密码LoginGraceTime 2m//重复验证时间为2分钟MaxAuthTries 6//最大重试次数</code></pre>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux系统备份</title>
      <link href="/linux/linux-xi-tong-bei-fen/"/>
      <url>/linux/linux-xi-tong-bei-fen/</url>
      
        <content type="html"><![CDATA[<p>今天尝试备份linux的数据，并转移到其他的机器上使用</p><ol><li>先执行的命令，将文件打包成 <code>.tgz</code> 格式保存在当前路径上。</li></ol><pre><code>tar cvpzf backup.tgz / --exclude=/proc --exclude=/lost+found --exclude=/backup.tgz --exclude=/mnt --exclude=/sys</code></pre>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Hystrix</title>
      <link href="/java/spring/spring-cloud/fu-wu-jiang-ji/hystrix/"/>
      <url>/java/spring/spring-cloud/fu-wu-jiang-ji/hystrix/</url>
      
        <content type="html"><![CDATA[<h2 id="Hystrix是什么"><a href="#Hystrix是什么" class="headerlink" title="Hystrix是什么?"></a>Hystrix是什么?</h2><p>Hystrix断路器，目前已经进入了维护阶段，不会再更新</p><p>在分布式系统中，每个服务都可能会调用很多其他服务，被调用的那些服务就是依赖服务，有的时候某些依赖服务出现故障也是很常见的。</p><p>Hystrix是一个用于处理分布式系统的延迟和容错的开源库，在分布式系统里，许多以来不可避免的会调用失败，比如超时，异常等，Hystrix 能够保证在一个依赖出问题的情况下，不会导致整体服务失败，避免级联故障，提高分布式系统的容错性。</p><p>Hystrix  通过将依赖服务进行资源隔离，进而阻止某个依赖服务出现故障时在整个系统所有的依赖服务调用中进行蔓延；同时Hystrix 还提供故障时的  fallback 降级机制。向调用方返回一个符合预期、可处理的备选响应(fallback)，而不是长时间的等待或者抛出异常，<strong>保证了调用方线程不会被长时间、不必要的占用</strong>，避免了分布式系统中的蔓延，乃至雪崩。</p><p>总而言之，Hystrix 通过这些方法帮助我们提升分布式系统的可用性和稳定性。</p><h2 id="Hystrix干什么事？"><a href="#Hystrix干什么事？" class="headerlink" title="Hystrix干什么事？"></a>Hystrix干什么事？</h2><p>服务降级、服务熔断、接近实时的监控，限流、隔离</p><h3 id="服务限流"><a href="#服务限流" class="headerlink" title="服务限流"></a><strong>服务限流</strong></h3><ul><li>这里的限流与 Guava 的 RateLimiter 的限流差异比较大，一个是为了“保护自我”，一个是“保护下游”</li><li>当对服务进行限流时，超过的流量将直接 Fallback，即熔断。而 RateLimiter 关心的其实是“流量整形”，将不规整流量在一定速度内规整</li></ul><h3 id="服务熔断"><a href="#服务熔断" class="headerlink" title="服务熔断"></a><strong>服务熔断</strong></h3><ul><li>当我的应用无法提供服务时，我要对上游请求熔断，避免上游把我压垮</li><li>当我的下游依赖成功率过低时，我要对下游请求熔断，避免下游把我拖垮</li></ul><h3 id="服务降级"><a href="#服务降级" class="headerlink" title="服务降级"></a><strong>服务降级</strong></h3><ul><li>降级与熔断紧密相关，熔断后业务如何表现，约定一个快速失败的 Fallback(较有好的提示)，即为服务降级</li></ul><p>程序运行异常、超时，服务熔断触发服务降级，线程池&#x2F;信号量打满</p><h3 id="服务隔离"><a href="#服务隔离" class="headerlink" title="服务隔离"></a><strong>服务隔离</strong></h3><ul><li>业务之间不可互相影响，不同业务需要有独立的运行空间</li><li>最彻底的，可以采用物理隔离，不同的机器部</li><li>次之，采用进程隔离，一个机器多个 Tomcat</li><li>次之，请求隔离</li><li>由于 Hystrix 框架所属的层级为代码层，所以实现的是请求隔离，线程池或信号量</li></ul><h2 id="Hystrix的使用"><a href="#Hystrix的使用" class="headerlink" title="Hystrix的使用"></a>Hystrix的使用</h2><ol><li>创建一个项目</li><li>修改pom依赖关系</li></ol><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;cloud-provider-hystrix-payment8001&lt;/artifactId&gt;    &lt;dependencies&gt;        &lt;!--hystrix--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--eureka client--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--web--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--引入自定义的api通用包，可用使用Payment支付Entity--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;            &lt;artifactId&gt;cloud-api-commons&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><ol start="3"><li>修改yml文件</li></ol><pre><code class="yaml">server:  port: 8001#服务的名称spring:  application:    name: cloud-provider-hystrix-paymenteureka:  client:    register-with-eureka: true    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka/</code></pre><ol start="4"><li>service层</li></ol><pre><code class="java">package com.exercise.springcloud.service;import org.springframework.stereotype.Service;import java.util.concurrent.TimeUnit;@Servicepublic class PaymentServiceImpl &#123;    //正常的方法，    public String paymentInfo_OK(Integer id) &#123;        return &quot;线程池：  &quot; + Thread.currentThread().getName() + &quot;  paymentInfo_OK,id:&quot; + id + &quot;\t&quot; + &quot;哈哈哈哈哈哈&quot;;    &#125;    //可能会引起超时的方法    public String paymentInfo_TimeOut(Integer id) &#123;        try &#123;            TimeUnit.SECONDS.sleep(3);        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;        return &quot;线程池：  &quot; + Thread.currentThread().getName() + &quot;  paymentInfo_TimeOut,id:&quot; + id + &quot;\t&quot; + &quot;呜呜呜呜呜&quot;+&quot;耗时3秒钟&quot;;    &#125;&#125;</code></pre><ol start="5"><li>controller</li></ol><pre><code class="java">package com.exercise.springcloud.controller;import com.exercise.springcloud.service.PaymentServiceImpl;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Value;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RestController;import javax.annotation.Resource;@RestController@Slf4jpublic class PaymentController &#123;    @Resource    private PaymentServiceImpl paymentService;    @Value(&quot;$&#123;server.port&#125;&quot;)    private String serverport;    @GetMapping(&quot;/payment/hystrix/ok/&#123;id&#125;&quot;)    public String paymentInfo_OK(@PathVariable(&quot;id&quot;) Integer id) &#123;        String result = paymentService.paymentInfo_OK(id);        log.info(&quot;****result&quot; + result);        return result;    &#125;    @GetMapping(&quot;/payment/hystrix/timeout/&#123;id&#125;&quot;)    public String paymentInfo_TimeOut(@PathVariable(&quot;id&quot;) Integer id)&#123;        String result = paymentService.paymentInfo_TimeOut(id);        log.info(&quot;****result&quot; + result);        return result;    &#125;&#125;</code></pre><ol start="6"><li>正常启动后，使用<code>Jmeter</code> 工具进行并发测试</li></ol><p>会发现很卡，并且可能会报连接超时或卡死</p><h3 id="Hystrix解决高并发导致的问题"><a href="#Hystrix解决高并发导致的问题" class="headerlink" title="Hystrix解决高并发导致的问题"></a>Hystrix解决高并发导致的问题</h3><p><strong>超时导致服务器变慢</strong></p><ul><li>超时不再等待</li></ul><p><strong>出错（宕机或者程序出错）</strong></p><ul><li>出错要兜底</li></ul><p><strong>解决</strong></p><ul><li>对方服务超时，调用者不能够一直卡死</li><li>对方宕机了，调用者不能卡死</li><li>对方服务正常，但是调用者自己有问题</li></ul><h2 id="服务降级-1"><a href="#服务降级-1" class="headerlink" title="服务降级"></a>服务降级</h2><blockquote><p> 这里的Hystrix配置可以放在服务端也可以放在客户端，一般是配置在客户端的！这样就可以设置客户端不同的配置了，服务端是对所有的客户端生效</p></blockquote><h3 id="服务端配置"><a href="#服务端配置" class="headerlink" title="服务端配置"></a>服务端配置</h3><h4 id="超时和Running-Error案例"><a href="#超时和Running-Error案例" class="headerlink" title="超时和Running Error案例"></a>超时和Running Error案例</h4><h5 id="超时案例"><a href="#超时案例" class="headerlink" title="超时案例"></a>超时案例</h5><p>官方上的案例是代码实现的，在这里使用注解实现。</p><blockquote><p> <code>@HystrixCommand</code>  注解</p><p>HystrixCommand 可以同时处理运行时异常和超时异常！</p></blockquote><p><strong>配置服务端，设定超时时间峰值，超过后就降级 <code>fallback</code></strong></p><ol><li>Service 配置超时时间 和 超时后执行的方法</li></ol><pre><code class="java">package com.exercise.springcloud.service;import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;import com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty;import org.springframework.stereotype.Service;import java.util.concurrent.TimeUnit;@Servicepublic class PaymentServiceImpl &#123;    //正常的方法，    public String paymentInfo_OK(Integer id) &#123;        return &quot;线程池：  &quot; + Thread.currentThread().getName() + &quot;  paymentInfo_OK,id:&quot; + id + &quot;\t&quot; + &quot;哈哈哈哈哈哈&quot;;    &#125;    /**     * @HystrixCommand报异常后如何处理：     * 一旦调用服务方法失败并抛出了错误信息后，     * 会自动调用@HystrixCommand标注好的fallbackMethod调用类中的指定方法     *     * @param id     * @return     */    @HystrixCommand(fallbackMethod = &quot;paymentInfo_TimeOutHandler&quot;,commandProperties = &#123;            //设置这个线程的超时时间是3s，3s内是正常的业务逻辑，超过3s调用fallbackMethod指定的方法进行处理            @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;,value = &quot;3000&quot;)    &#125;)    public String paymentInfo_Timeout(Integer id) &#123;        //这里配置的超时时间是5秒        int timeNumber = 5;        try &#123;            TimeUnit.SECONDS.sleep(timeNumber);        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;        return &quot;线程池：  &quot; + Thread.currentThread().getName() + &quot;  paymentInfo_TimeOut,id:&quot; + id + &quot;\t&quot; + &quot;呜呜呜呜呜&quot; + &quot;耗时3秒钟&quot;;    &#125;    //这是一个降级方法，超过三秒就调用这个方法    public String paymentInfo_TimeOutHandler(Integer id)&#123;        return &quot;线程池：&quot;+Thread.currentThread().getName()+&quot;   系统繁忙，请稍后再试,id：&quot;+id+&quot;\t&quot;+&quot;o(╥﹏╥)o&quot;;    &#125;&#125;</code></pre><ol start="2"><li>在服务端启动类上添加注解 <code>@EnableCircuitBreaker</code>，代表断路器的意思</li></ol><pre><code class="java">package com.exercise.springcloud;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker;import org.springframework.cloud.netflix.eureka.EnableEurekaClient;@SpringBootApplication@EnableEurekaClient@EnableCircuitBreakerpublic class PaymentHystrixMain8001 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(PaymentHystrixMain8001.class,args);    &#125;&#125;</code></pre><h5 id="Running-Error案例"><a href="#Running-Error案例" class="headerlink" title="Running Error案例"></a>Running Error案例</h5><ol><li>下面的案例就改了下service层 *&#x2F;0能够报运行时异常，HystrixCommand 可以同时处理运行时异常和超时异常。</li></ol><pre><code class="java">package com.exercise.springcloud.service;import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;import com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty;import org.springframework.stereotype.Service;import java.util.concurrent.TimeUnit;@Servicepublic class PaymentServiceImpl &#123;    //正常的方法，    public String paymentInfo_OK(Integer id) &#123;        return &quot;线程池：  &quot; + Thread.currentThread().getName() + &quot;  paymentInfo_OK,id:&quot; + id + &quot;\t&quot; + &quot;哈哈哈哈哈哈&quot;;    &#125;    /**     * @HystrixCommand报异常后如何处理：     * 一旦调用服务方法失败并抛出了错误信息后，     * 会自动调用@HystrixCommand标注好的fallbackMethod调用类中的指定方法     *     * @param id     * @return     */    @HystrixCommand(fallbackMethod = &quot;paymentInfo_TimeOutHandler&quot;,commandProperties = &#123;            //设置这个线程的超时时间是3s，3s内是正常的业务逻辑，超过3s调用fallbackMethod指定的方法进行处理            @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;,value = &quot;3000&quot;)    &#125;)    public String paymentInfo_Timeout(Integer id) &#123;//        int timeNumber = 5;        int age = 10/0;//        try &#123;//            TimeUnit.SECONDS.sleep(timeNumber);//        &#125; catch (InterruptedException e) &#123;//            e.printStackTrace();//        &#125;        return &quot;线程池：  &quot; + Thread.currentThread().getName() + &quot;  paymentInfo_TimeOut,id:&quot; + id + &quot;\t&quot; + &quot;呜呜呜呜呜&quot; + &quot;耗时3秒钟&quot;;    &#125;    //这是一个降级方法    public String paymentInfo_TimeOutHandler(Integer id)&#123;        return &quot;线程池：&quot;+Thread.currentThread().getName()+&quot;   系统繁忙，请稍后再试,id：&quot;+id+&quot;\t&quot;+&quot;o(╥﹏╥)o&quot;;    &#125;&#125;</code></pre><ol start="2"><li>启动类和上面没有区别</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-jiang-ji/hystrix/image-20210202110727181.png" alt="image-20210202110727181"></p><h3 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h3><h4 id="超时和Running-Error案例-1"><a href="#超时和Running-Error案例-1" class="headerlink" title="超时和Running Error案例"></a>超时和Running Error案例</h4><h5 id="超时案例-1"><a href="#超时案例-1" class="headerlink" title="超时案例"></a>超时案例</h5><p>在class 中设置的，并没有在interface中设置，如果客户端和服务端都设置了Hystrix超时时间，服务端3秒超时，客户端1秒超时，那么1秒后就会进入到客户端的超时方法。</p><pre><code class="java">package com.exercise.springcloud.controller;import com.exercise.springcloud.service.PaymentHystrixService;import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;import com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty;import lombok.extern.slf4j.Slf4j;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RestController;import javax.annotation.Resource;@RestController@Slf4jpublic class OrderHystrixController &#123;    @Resource    private PaymentHystrixService paymentHystrixService;    @GetMapping(&quot;/consumer/payment/hystrix/ok/&#123;id&#125;&quot;)    public String paymentInfo_OK(@PathVariable(&quot;id&quot;) Integer id) &#123;        String result = paymentHystrixService.paymentInfo_OK(id);        return result;    &#125;    @GetMapping(&quot;/consumer/payment/hystrix/timeout/&#123;id&#125;&quot;)    @HystrixCommand(fallbackMethod = &quot;paymentTimeOutFallBackMethod&quot;,commandProperties = &#123;            //设置这个线程的超时时间是3s，3s内是正常的业务逻辑，超过3s调用fallbackMethod指定的方法进行处理            @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;,value = &quot;1500&quot;)&#125;)    public String paymentInfo_TimeOut(@PathVariable(&quot;id&quot;) Integer id) &#123;        String result = paymentHystrixService.paymentInfo_TimeOut(id);        return result;    &#125;    public String paymentTimeOutFallBackMethod(@PathVariable(&quot;id&quot;) Integer id) &#123;        return &quot;我是消费者80，对方支付系统繁忙，请稍后再试，o(╥﹏╥)o&quot;;    &#125;&#125;</code></pre><p><img src="/java/spring/spring-cloud/fu-wu-jiang-ji/hystrix/image-20210202113641144.png" alt="访问结果"></p><h5 id="Running-Error案例-1"><a href="#Running-Error案例-1" class="headerlink" title="Running Error案例"></a>Running Error案例</h5><ol><li>也是只更改了下 *&#x2F;0 ，剩下的没变</li></ol><pre><code class="java">package com.exercise.springcloud.service;import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;import com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty;import org.springframework.stereotype.Service;import java.util.concurrent.TimeUnit;@Servicepublic class PaymentServiceImpl &#123;    //正常的方法，    public String paymentInfo_OK(Integer id) &#123;        return &quot;线程池：  &quot; + Thread.currentThread().getName() + &quot;  paymentInfo_OK,id:&quot; + id + &quot;\t&quot; + &quot;哈哈哈哈哈哈&quot;;    &#125;    /**     * @HystrixCommand报异常后如何处理：     * 一旦调用服务方法失败并抛出了错误信息后，     * 会自动调用@HystrixCommand标注好的fallbackMethod调用类中的指定方法     *     * @param id     * @return     */    @HystrixCommand(fallbackMethod = &quot;paymentInfo_TimeOutHandler&quot;,commandProperties = &#123;            //设置这个线程的超时时间是3s，3s内是正常的业务逻辑，超过3s调用fallbackMethod指定的方法进行处理            @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;,value = &quot;3000&quot;)    &#125;)    public String paymentInfo_Timeout(Integer id) &#123;//        int timeNumber = 5;        int age = 10/0;//        try &#123;//            TimeUnit.SECONDS.sleep(timeNumber);//        &#125; catch (InterruptedException e) &#123;//            e.printStackTrace();//        &#125;        return &quot;线程池：  &quot; + Thread.currentThread().getName() + &quot;  paymentInfo_TimeOut,id:&quot; + id + &quot;\t&quot; + &quot;呜呜呜呜呜&quot; + &quot;耗时3秒钟&quot;;    &#125;    //这是一个降级方法    public String paymentInfo_TimeOutHandler(Integer id)&#123;        return &quot;线程池：&quot;+Thread.currentThread().getName()+&quot;   系统繁忙，请稍后再试,id：&quot;+id+&quot;\t&quot;+&quot;o(╥﹏╥)o&quot;;    &#125;&#125;</code></pre><h5 id="服务端宕机案例"><a href="#服务端宕机案例" class="headerlink" title="服务端宕机案例"></a>服务端宕机案例</h5><p>直接停掉服务端程序，客户端连接不到服务端也不会报错，执行fallback方法。</p><h3 id="全局FallBack"><a href="#全局FallBack" class="headerlink" title="全局FallBack"></a>全局FallBack</h3><p>上方配置不管是在客户端配置还是在服务端配置，如果每个方法都要配置，那么会严重的代码膨胀。这里设置全局降级配置。 </p><p>配置之后，如果触发Hystrix，方法没有专门的 fallback ，就进入全局配置，如果有专门配置的fallback，就进入专门的fallback</p><ol><li>在客户端的controller 层编写全局方法</li><li>在controller上添加<code>@DefaultProperties</code> 注解</li><li>将原来指明的<code>@HystrixCommand</code> 注解注释掉，以免冲突</li></ol><p>注意事项：全局的fallback方法不能带参数</p><pre><code class="java">package com.exercise.springcloud.controller;import com.exercise.springcloud.service.PaymentHystrixService;import com.netflix.hystrix.contrib.javanica.annotation.DefaultProperties;import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;import com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty;import lombok.extern.slf4j.Slf4j;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RestController;import javax.annotation.Resource;@RestController@Slf4j@DefaultProperties(defaultFallback = &quot;payment_Global_FallbackMethod&quot;)public class OrderHystrixController &#123;    @Resource    private PaymentHystrixService paymentHystrixService;    @GetMapping(&quot;/consumer/payment/hystrix/ok/&#123;id&#125;&quot;)    public String paymentInfo_OK(@PathVariable(&quot;id&quot;) Integer id) &#123;        String result = paymentHystrixService.paymentInfo_OK(id);        return result;    &#125;    @GetMapping(&quot;/consumer/payment/hystrix/timeout/&#123;id&#125;&quot;)//    @HystrixCommand(fallbackMethod = &quot;paymentTimeOutFallBackMethod&quot;,commandProperties = &#123;//            //设置这个线程的超时时间是3s，3s内是正常的业务逻辑，超过3s调用fallbackMethod指定的方法进行处理//            @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;,value = &quot;1500&quot;)&#125;)    @HystrixCommand    public String paymentInfo_TimeOut(@PathVariable(&quot;id&quot;) Integer id) &#123;        String result = paymentHystrixService.paymentInfo_TimeOut(id);        return result;    &#125;    public String paymentTimeOutFallBackMethod(@PathVariable(&quot;id&quot;) Integer id) &#123;        return &quot;我是消费者80，对方支付系统繁忙，请稍后再试，o(╥﹏╥)o&quot;;    &#125;    /**     * 全局 fallback 方法     * @return     */    public String payment_Global_FallbackMethod()&#123;        return &quot;Global异常处理信息，请稍后再试。/(╥﹏╥)/~~&quot;;    &#125;&#125;</code></pre><p>但是上面的写法只能够针对在同一个类中，不是同一个类就没有办法引用了，这种写法和业务逻辑代码在一起，看起来很混乱</p><h3 id="fallback方法和业务代码分开"><a href="#fallback方法和业务代码分开" class="headerlink" title="fallback方法和业务代码分开"></a>fallback方法和业务代码分开</h3><p>客户端调用服务端接口的service层，是通过feign来调用的。客户端的service 接口<code>@fiegn</code>配置注解，可以配置他们fallback类</p><ol><li>创建客户端接口的实现类，并重写他里面的方法，作为每个方法的fallback返回值</li></ol><pre><code class="java">package com.exercise.springcloud.service;import org.springframework.stereotype.Component;@Componentpublic class PaymentFallbackService implements PaymentHystrixService &#123;    @Override    public String paymentInfo_OK(Integer id) &#123;        return &quot;PaymentFallbackService fall back , paymentInfo_OK&quot;;    &#125;    @Override    public String paymentInfo_TimeOut(Integer id) &#123;        return &quot;PaymentFallbackService fall back , paymentInfo_TimeOut&quot;;    &#125;&#125;</code></pre><ol start="2"><li>service接口上进行编写注释</li></ol><pre><code class="java">package com.exercise.springcloud.service;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.stereotype.Service;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;@Service@FeignClient(value = &quot;CLOUD-PROVIDER-HYSTRIX-PAYMENT&quot;, fallback = PaymentFallbackService.class)public interface PaymentHystrixService &#123;    @GetMapping(&quot;/payment/hystrix/ok/&#123;id&#125;&quot;)    public String paymentInfo_OK(@PathVariable(&quot;id&quot;) Integer id);    @GetMapping(&quot;/payment/hystrix/timeout/&#123;id&#125;&quot;)    public String paymentInfo_TimeOut(@PathVariable(&quot;id&quot;) Integer id);&#125;</code></pre><ol start="3"><li>在客户端 yml 文件中配置</li></ol><pre><code class="yaml">server:  port: 80eureka:  client:    register-with-eureka: true    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka/#自定义ribbon的超时时间ribbon:  ReadTimeout: 5000  #处理请求的超时时间，默认为5秒  ConnectTimeout: 5000  #连接建立的超时时长，默认5秒  MaxAutoRetries: 1  #同一台实例的最大重试次数，但是不包括首次调用，默认为1次  MaxAutoRetriesNextServer: 0  #重试负载均衡其他实例的最大重试次数，不包括首次调用，默认为0次  OkToRetryOnAllOperations: false  #是否对所有操作都重试，默认false#开启feign的hystrix支持feign:  hystrix:    enabled: true</code></pre><ol start="4"><li>controller 层不变,保留全局fallback方法</li></ol><pre><code class="java">package com.exercise.springcloud.controller;import com.exercise.springcloud.service.PaymentHystrixService;import com.netflix.hystrix.contrib.javanica.annotation.DefaultProperties;import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;import lombok.extern.slf4j.Slf4j;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RestController;import javax.annotation.Resource;@RestController@Slf4j@DefaultProperties(defaultFallback = &quot;payment_Global_FallbackMethod&quot;)public class OrderHystrixController &#123;    @Resource    private PaymentHystrixService paymentHystrixService;    @GetMapping(&quot;/consumer/payment/hystrix/ok/&#123;id&#125;&quot;)    public String paymentInfo_OK(@PathVariable(&quot;id&quot;) Integer id) &#123;        String result = paymentHystrixService.paymentInfo_OK(id);        return result;    &#125;    @GetMapping(&quot;/consumer/payment/hystrix/timeout/&#123;id&#125;&quot;)//    @HystrixCommand(fallbackMethod = &quot;paymentTimeOutFallBackMethod&quot;,commandProperties = &#123;//            //设置这个线程的超时时间是3s，3s内是正常的业务逻辑，超过3s调用fallbackMethod指定的方法进行处理//            @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;,value = &quot;1500&quot;)&#125;)    @HystrixCommand    public String paymentInfo_TimeOut(@PathVariable(&quot;id&quot;) Integer id) &#123;        String result = paymentHystrixService.paymentInfo_TimeOut(id);        return result;    &#125;    public String paymentTimeOutFallBackMethod(@PathVariable(&quot;id&quot;) Integer id) &#123;        return &quot;我是消费者80，对方支付系统繁忙，请稍后再试，o(╥﹏╥)o&quot;;    &#125;    /**     * 全局 fallback 方法     * @return     */    public String payment_Global_FallbackMethod()&#123;        return &quot;Global异常处理信息，请稍后再试。/(╥﹏╥)/~~&quot;;    &#125;&#125;</code></pre><p>访问结果：</p><p>访问时timeout方法，由于客户端配置了全局fallback，继承的那个方法并不会起作用，以全局的为主。客户端和服务端都有fallback方法，就会以先触发的返回</p><p>访问ok方法，没有设置全局的fallback ，所以继承重写的方法起作用。</p><h2 id="服务熔断-1"><a href="#服务熔断-1" class="headerlink" title="服务熔断"></a>服务熔断</h2><p>熔断机制是应对雪崩效应的一种微服务链路保护机制。当扇出链路的某个微服务出错不肯用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回错误的响应信息。当检测到该节点微服务调用响应正常后，<strong>恢复调用链路</strong>。</p><p>在Springcloud 框架中，熔断机制通过Hystrix实现。Hystrix会监控微服务间调用的状态，但失败的调用到一定阈值的时候，缺省5秒内20此调用失败，就会启动熔断机制。熔断机制的注解是<code>@HystrixCommand</code>。</p><h3 id="熔断类型"><a href="#熔断类型" class="headerlink" title="熔断类型"></a>熔断类型</h3><p><strong>熔断打开</strong></p><p>请求不在进行调用当前服务，内部设置时钟MTTR（平均故障处理时间），当打开时长到达所设时钟则进入半熔断状态</p><p><strong>熔断关闭</strong></p><p>熔断关闭不会对服务进行熔断</p><p><strong>熔断半开</strong></p><p>部分请求根据规则调用当前服务，如果请求成功且符合规则则认为当前服务恢复正常，关闭熔断</p><h3 id="服务端配置-1"><a href="#服务端配置-1" class="headerlink" title="服务端配置"></a>服务端配置</h3><ol><li>这里在服务端service 中配置</li></ol><pre><code class="java">package com.exercise.springcloud.service;import cn.hutool.core.util.IdUtil;import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;import com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty;import org.springframework.stereotype.Service;import org.springframework.web.bind.annotation.PathVariable;import java.util.concurrent.TimeUnit;@Servicepublic class PaymentServiceImpl &#123;    //正常的方法，    public String paymentInfo_OK(Integer id) &#123;        return &quot;线程池：  &quot; + Thread.currentThread().getName() + &quot;  paymentInfo_OK,id:&quot; + id + &quot;\t&quot; + &quot;哈哈哈哈哈哈&quot;;    &#125;    /**     * @param id     * @return     * @HystrixCommand报异常后如何处理： 一旦调用服务方法失败并抛出了错误信息后，     * 会自动调用@HystrixCommand标注好的fallbackMethod调用类中的指定方法     */    @HystrixCommand(fallbackMethod = &quot;paymentInfo_TimeOutHandler&quot;, commandProperties = &#123;            //设置这个线程的超时时间是3s，3s内是正常的业务逻辑，超过3s调用fallbackMethod指定的方法进行处理            @HystrixProperty(name = &quot;execution.isolation.thread.timeoutInMilliseconds&quot;, value = &quot;3000&quot;)    &#125;)    public String paymentInfo_Timeout(Integer id) &#123;//        int timeNumber = 5;        int age = 10 / 0;//        try &#123;//            TimeUnit.SECONDS.sleep(timeNumber);//        &#125; catch (InterruptedException e) &#123;//            e.printStackTrace();//        &#125;        return &quot;线程池：  &quot; + Thread.currentThread().getName() + &quot;  paymentInfo_TimeOut,id:&quot; + id + &quot;\t&quot; + &quot;呜呜呜呜呜&quot; + &quot;耗时3秒钟&quot;;    &#125;    //这是一个服务降级方法    public String paymentInfo_TimeOutHandler(Integer id) &#123;        return &quot;线程池：&quot; + Thread.currentThread().getName() + &quot;   系统繁忙，请稍后再试,id：&quot; + id + &quot;\t&quot; + &quot;o(╥﹏╥)o&quot;;    &#125;    //触发熔断的方法    @HystrixCommand(fallbackMethod = &quot;paymentCircuitBreaker_fallback&quot;, commandProperties = &#123;            @HystrixProperty(name = &quot;circuitBreaker.enabled&quot;, value = &quot;true&quot;),   //是否开启断路器            @HystrixProperty(name = &quot;circuitBreaker.requestVolumeThreshold&quot;, value = &quot;10&quot;),  //请求次数            @HystrixProperty(name = &quot;circuitBreaker.sleepWindowInMilliseconds&quot;, value = &quot;10000&quot;),    //时间窗口期  10秒            @HystrixProperty(name = &quot;circuitBreaker.errorThresholdPercentage&quot;, value = &quot;60&quot;),    //失败率达到多少后跳闸  超过10次访问次数中失败率超过60    &#125;)    public String paymentCircuitBreaker(@PathVariable(&quot;id&quot;) Integer id) &#123;        if (id &lt; 0) &#123;            //抛出异常            throw new RuntimeException(&quot;******id 不能为负数&quot;);        &#125;        String serialNumber = IdUtil.simpleUUID();  //等价于UUID.randomUUID();        return Thread.currentThread().getName() + &quot;\t&quot; + &quot;调用成功，流水号：&quot; + serialNumber;    &#125;    //================服务熔断方法    public String paymentCircuitBreaker_fallback(@PathVariable(&quot;id&quot;) Integer id) &#123;        return &quot;id 不能负数，请稍后再试，o(╥﹏╥)o  id：&quot; + id;    &#125;&#125;</code></pre><ol start="2"><li>服务端controller 调用service</li></ol><pre><code class="java">    //服务熔断    @GetMapping(&quot;/payment/circuit/&#123;id&#125;&quot;)    public String paymentCircuitBreaker(@PathVariable(&quot;id&quot;) Integer id)&#123;        String result = paymentService.paymentCircuitBreaker(id);        log.info(&quot;****result：&quot;+result);        return result;    &#125;</code></pre><ol start="3"><li>启动微服务，调用方法查看结果</li></ol><p>在客户端访问链接地址，只要在10秒内访问次数超过10次并达到60%的失败率，就会启动熔断机制。我先一直输入负数，到达次数后熔断触发，接下来就算输入正确的结果也不会去访问原来的接口，直接走熔断方法。过一段时间后恢复正常。</p><h3 id="断路器什么时候起作用？"><a href="#断路器什么时候起作用？" class="headerlink" title="断路器什么时候起作用？"></a>断路器什么时候起作用？</h3><p>断路器的三个重要参数：<strong>快照时间窗</strong>，<strong>请求总数阈值</strong>，<strong>错误百分比阈值</strong></p><p>1、快照时间窗：断路器确定是否打开需要统计一些请求和错误数据，而统计的时间范围就是快照时间窗，默认为最近的10秒</p><p>2、请求总数阈值：在快照时间窗内，必须满足请求总数阈值才有资格熔断，默认20，意味着10秒内，如果hystrix调用次数不足20次，即使全部请求都超时或者其他原因失效，也不会启动断路器</p><p>3、错误百分比阈值，当请求总数在快照时间窗内超过了阈值，比如发生了30次调用，在这30次中，有15次发生了超时异常，也就是超过50%错误百分比，默认设定50%阈值情况下，就会将断路器打开。</p><h2 id="Hystrix的dashBoard"><a href="#Hystrix的dashBoard" class="headerlink" title="Hystrix的dashBoard"></a>Hystrix的dashBoard</h2><p>进行数据的可视化展示</p><ol><li>pom文件导入依赖</li></ol><p>主要导入<code>dashboard</code>和<code>actuator</code>这两个依赖</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;cloud-consumer-hystrix-dashboard9001&lt;/artifactId&gt;    &lt;properties&gt;        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><ol start="2"><li>yml 文件修改</li></ol><pre><code class="yaml">server:  port: 9001</code></pre><ol start="3"><li>启动类修改</li></ol><p>添加注解 <code>@EnableHystrixDashboard</code></p><pre><code class="java">package com.exercise.springcloud;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.hystrix.dashboard.EnableHystrixDashboard;@SpringBootApplication@EnableHystrixDashboardpublic class HystrixDashboardMain9001 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(HystrixDashboardMain9001.class, args);    &#125;&#125;</code></pre><ol start="4"><li>启动服务后进行访问</li></ol><blockquote><p>localhost:9001&#x2F;hystrix</p></blockquote><p><img src="/java/spring/spring-cloud/fu-wu-jiang-ji/hystrix/image-20210202165539741.png" alt="image-20210202165539741"></p><p>这里只是启动了hystrix的监控，但是没有监控任务微服务</p><p>5.配置被检控的微服务</p><p>pom文件配置,凡是被监控的都得包含下面这两个依赖</p><pre><code class="xml">        &lt;!--web--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;</code></pre><ol start="6"><li>启动类配置</li></ol><p>添加 <code>@EnableCircuitBreaker</code> 注解</p><pre><code class="java">@SpringBootApplication@EnableEurekaClient@EnableCircuitBreakerpublic class PaymentHystrixMain8001 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(PaymentHystrixMain8001.class,args);    &#125;&#125;</code></pre><ol start="7"><li>启动类配置</li></ol><p>视频中说时因为spring版本的原因导致，所以需要这些额外的配置。</p><p>经过测试，这些配置是必须的。</p><pre><code class="java">    /**     * 此配置是为了服务监控而配置，与服务容错本身无关,SpringCloud升级后的坑     * ServletRegistrationBean因为springboot的默认路径不是&quot;/hystrix.stream&quot;，     * 只要在自己的项目里配置上下面的servlet就可以了     */    @Bean    public ServletRegistrationBean getServlet()&#123;        HystrixMetricsStreamServlet streamServlet = new HystrixMetricsStreamServlet();        ServletRegistrationBean registrationBean = new ServletRegistrationBean(streamServlet);        registrationBean.setLoadOnStartup(1);        registrationBean.addUrlMappings(&quot;/hystrix.stream&quot;);        registrationBean.setName(&quot;HystrixMetricsStreamServlet&quot;);        return registrationBean;    &#125;</code></pre><ol start="8"><li>Hystrix页面进行指定微服务访问</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-jiang-ji/hystrix/image-20210202171827449.png" alt="History监控页面"></p><p><img src="/java/spring/spring-cloud/fu-wu-jiang-ji/hystrix/image-20210202171945337.png" alt="具体微服务监控页面"></p><p><img src="/java/spring/spring-cloud/fu-wu-jiang-ji/hystrix/2020081013181955.png" alt="监控指标含义"></p><p>具体指标查看含义可查看:<a href="https://blog.csdn.net/chen134225/article/details/107911009">https://blog.csdn.net/chen134225/article/details/107911009</a></p>]]></content>
      
      
      <categories>
          
          <category> Spring Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring Cloud </tag>
            
            <tag> Hystrix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring OpenFeign</title>
      <link href="/java/spring/spring-cloud/fu-wu-diao-yong-2/openfeign/"/>
      <url>/java/spring/spring-cloud/fu-wu-diao-yong-2/openfeign/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是OpenFeign？"><a href="#什么是OpenFeign？" class="headerlink" title="什么是OpenFeign？"></a>什么是OpenFeign？</h2><p>Feign 是一种声明式、模板化的 HTTP （WebService）客户端(仅在 consumer 中使用)。</p><p>声明式调用就像调用本地方法一样调用远程方法</p><p>无感知远程 http 请求</p><p>Spring Cloud 的声明式调用, 可以做到使用 HTTP 请求远程服务时能就像调用本地方法一样的体验，开发者完全感知不到这是远程方法，更感知不到这是个 HTTP 请求。</p><p>它像 Dubbo 一样，consumer 直接调用接口方法调用 provider，而不需要通过常规的 Http Client 构造请求再解析返回数据。</p><p>它解决了让开发者调用远程接口就跟调用本地方法一样，无需关注与远程的交互细节，更无需关注分布式环境开发。</p><h2 id="OpenFeign干了什么事？"><a href="#OpenFeign干了什么事？" class="headerlink" title="OpenFeign干了什么事？"></a>OpenFeign干了什么事？</h2><p>Feign在使用java http客户端变得容易</p><p>之前使用方式是Ribbon+RestTemplate，利用RestTemplate对http请求的封装处理，形成了一套模板化的调用方法。但是对于服务依赖调用可能不止一处，往往一个接口会被多处调用，通常会针对每个微服务自行封装一些客户端来包装这些依赖服务的调用。Feign进行了进一步的封装，他来定义和实现依赖服务接口的定义。在Feign帮助下，只需要创建一个接口并使用注解的方式来配置它，即可完成对服务的接口绑定，简化了Spring cloud Ribbon调用客户端的开发量。</p><p><img src="/java/spring/spring-cloud/fu-wu-diao-yong-2/openfeign/image-20210201134515999.png" alt="之前的方式"></p><h2 id="OpenFeign的具体使用"><a href="#OpenFeign的具体使用" class="headerlink" title="OpenFeign的具体使用"></a>OpenFeign的具体使用</h2><p>OpenFeign需要在接口上使用注解</p><ol><li>pom文件配置,导入openfeign依赖</li></ol><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;cloud-consumer-feign-order80&lt;/artifactId&gt;    &lt;properties&gt;        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--openfeign--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--eureka client--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--引入自定义的api通用包，可以使用Payment支付Entity--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;            &lt;artifactId&gt;cloud-api-commons&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;!--web--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--一般基础通用配置--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><ol start="2"><li>yml文件配置</li></ol><p>这里不用任何特殊配置</p><pre><code class="yaml">server:  port: 80eureka:  client:    #因为是客户端，此服务不注册到Eureka中    register-with-eureka: false    service-url: http://localhost:7001/eureka,http://localhost:7002/eureka</code></pre><ol start="3"><li>在启动类上添加注解<code>@EnableFeignClients</code></li></ol><pre><code class="java">package springcloud;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration;import org.springframework.cloud.netflix.eureka.EnableEurekaClient;import org.springframework.cloud.openfeign.EnableFeignClients;@SpringBootApplication//pom文件引入了mybatis-spring-boot-starter，这里会自动加载 DataSourceAutoConfiguration，下面的注解是为了阻止自动注入的@EnableAutoConfiguration(exclude=&#123;DataSourceAutoConfiguration.class&#125;)@EnableEurekaClient@EnableFeignClients//这里服务名大写//@RibbonClient(name=&quot;CLOUD-PAYMENT-SERVICE&quot;,configuration = MySelfRule.class)public class OrderFeignMain80 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(OrderFeignMain80.class, args);    &#125;&#125;</code></pre><ol start="4"><li>在 service 层添加 <code>@FeignClient</code>注解,并指定微服务的名称</li></ol><pre><code class="java">package com.exercise.springcloud.service;import com.exercise.springcloud.entities.CommonResult;import com.exercise.springcloud.entities.Payment;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.stereotype.Service;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;@Service@FeignClient(&quot;CLOUD-PAYMENT-SERVICE&quot;)public interface PaymentFeignService &#123;    @GetMapping(&quot;/payment/get/&#123;id&#125;&quot;)    public CommonResult&lt;Payment&gt; getPaymentById(@PathVariable(&quot;id&quot;) Long id);&#125;</code></pre><ol start="5"><li>controller 层调用service</li></ol><pre><code class="java">package com.exercise.springcloud.controller;import com.exercise.springcloud.entities.CommonResult;import com.exercise.springcloud.entities.Payment;import com.exercise.springcloud.service.PaymentFeignService;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RestController;@RestController@Slf4jpublic class OrderFeignController &#123;    @Autowired    private PaymentFeignService paymentFeignService;    @GetMapping(&quot;/consumer/payment/get/&#123;id&#125;&quot;)    public CommonResult&lt;Payment&gt; getPaymentById(@PathVariable(&quot;id&quot;) Long id) &#123;        return paymentFeignService.getPaymentById(id);    &#125;&#125;</code></pre><ol start="6"><li>调用接口查看结果</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-diao-yong-2/openfeign/image-20210201151007944.png" alt="image-20210201151007944"></p><p><img src="/java/spring/spring-cloud/fu-wu-diao-yong-2/openfeign/image-20210201151020811.png" alt="image-20210201151020811"></p><p>是轮询方式的负载均衡，但是想一想这里是Eureka起作用的负载均衡吗，并不是的</p><p>OpenFeign集成了Ribbon，底层调用时会自动的按照ribbon负载均衡策略来调用微服务</p><p><img src="/java/spring/spring-cloud/fu-wu-diao-yong-2/openfeign/image-20210201150928110.png" alt="Openfeign中包含Ribbon"></p><p>利用Ribbon维护了Payment的服务列表信息，并且通过轮询的方式实现了客户端的负载均衡。</p><h2 id="Feign和OpenFeign的区别"><a href="#Feign和OpenFeign的区别" class="headerlink" title="Feign和OpenFeign的区别"></a>Feign和OpenFeign的区别</h2><p><img src="/java/spring/spring-cloud/fu-wu-diao-yong-2/openfeign/image-20210201132142779.png" alt="Feign和OpenFeign的区别"></p><h2 id="OpenFeign超时控制"><a href="#OpenFeign超时控制" class="headerlink" title="OpenFeign超时控制"></a>OpenFeign超时控制</h2><p>OpenFeign默认<code>1 second</code> 没有返回结果就会报 <code>read timeout </code></p><p><img src="/java/spring/spring-cloud/fu-wu-diao-yong-2/openfeign/image-20210201154035051.png" alt="超时web页面"></p><p><img src="/java/spring/spring-cloud/fu-wu-diao-yong-2/openfeign/image-20210201153759137.png" alt="超过1Secnod 客户端就报错"></p><p>但是我们业务需要超过1S时间去获取结果。这时候需要配置他的超时时间</p><ol><li>在yml文件中配置</li></ol><p>OpenFeign中包含着Ribbon，底层的超时时间控制也是由Ribbon控制的，所以要修改Ribbon的配置</p><p>注意事项：注意yaml语法，注意空格和和位置</p><pre><code class="yaml">server:  port: 80eureka:  client:    #因为是客户端，此服务不注册到Eureka中    register-with-eureka: false    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka,http://localhost:7002/eureka#自定义ribbon的超时时间ribbon:  ReadTimeout: 5000  #处理请求的超时时间，默认为5秒  ConnectTimeout: 5000  #连接建立的超时时长，默认5秒  MaxAutoRetries: 1  #同一台实例的最大重试次数，但是不包括首次调用，默认为1次  MaxAutoRetriesNextServer: 0  #重试负载均衡其他实例的最大重试次数，不包括首次调用，默认为0次  OkToRetryOnAllOperations: false  #是否对所有操作都重试，默认false</code></pre><h2 id="OpenFeign日志打印功能"><a href="#OpenFeign日志打印功能" class="headerlink" title="OpenFeign日志打印功能"></a>OpenFeign日志打印功能</h2><h3 id="日志级别"><a href="#日志级别" class="headerlink" title="日志级别"></a>日志级别</h3><p><code>NONE</code>:默认，不显示任何日志</p><p><code>BASIC</code>: 仅记录请求方法，URL，响应状态码和执行时间</p><p><code>HEADERS</code>：除了BASIC中定义的信息之外，华友请求和响应的头信息</p><p><code>FULL</code>：除了HEADERS中定义的信息之外，还有请求和响应的正文及元数据</p><h3 id="配置日志输出"><a href="#配置日志输出" class="headerlink" title="配置日志输出"></a>配置日志输出</h3><ol><li>创建config目录并创建配置文件</li></ol><pre><code class="java">package com.exercise.springcloud.config;import feign.Logger;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class FeignConfig &#123;    @Bean    Logger.Level feignLoggerLevel() &#123;        //最小粒度的日志级别        return Logger.Level.FULL;    &#125;&#125;</code></pre><ol start="2"><li>修改yml配置文件进行配置</li></ol><pre><code class="yaml">server:  port: 80eureka:  client:    #因为是客户端，此服务不注册到Eureka中    register-with-eureka: false    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka,http://localhost:7002/eureka#自定义ribbon的超时时间ribbon:  ReadTimeout: 5000  #处理请求的超时时间，默认为5秒  ConnectTimeout: 5000  #连接建立的超时时长，默认5秒  MaxAutoRetries: 1  #同一台实例的最大重试次数，但是不包括首次调用，默认为1次  MaxAutoRetriesNextServer: 0  #重试负载均衡其他实例的最大重试次数，不包括首次调用，默认为0次  OkToRetryOnAllOperations: false  #是否对所有操作都重试，默认falselogging:  level:    #指定要监控的类和日志的级别    com.exercise.springcloud.service.PaymentFeignService: debug</code></pre><ol start="3"><li>尝试访问接口</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-diao-yong-2/openfeign/image-20210201162142740.png" alt="访问接口"></p><p><img src="/java/spring/spring-cloud/fu-wu-diao-yong-2/openfeign/image-20210201161303732.png" alt="日志在控制台的输出"></p>]]></content>
      
      
      <categories>
          
          <category> Spring OpenFeign </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring OpenFeign </tag>
            
            <tag> OpenFeign </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Cloud组件说明</title>
      <link href="/java/spring/spring-cloud/fu-wu-diao-yong/ribbon/"/>
      <url>/java/spring/spring-cloud/fu-wu-diao-yong/ribbon/</url>
      
        <content type="html"><![CDATA[<h2 id="Ribbon是什么"><a href="#Ribbon是什么" class="headerlink" title="Ribbon是什么"></a>Ribbon是什么</h2><p>Spring Cloud Ribbon是基于Netfix Ribbon实现的一套客户端 </p><p>Ribbon是Netfix发布的开源项目，主要功能是提供客户端的负载均衡算法和服务调用.Ribbon客户端组件提供一系列完善的配置项，如连接超时、重试等。简单地说，就是在配置文件中列出Load Balancer 后面所有的机器。Ribbon会自动帮助你与于某种规则去链接这些机器。我们很容易使用Ribbon实现自定义的负载均衡算法。</p><p>Ribbon 目前也已经是维护模式，但是目前还在大规模使用</p><h2 id="Ribbon负载均衡"><a href="#Ribbon负载均衡" class="headerlink" title="Ribbon负载均衡"></a>Ribbon负载均衡</h2><p>就是将用户请求均匀的分配到多个服务上，到达系统的HA(高可用)</p><p>工作流程：</p><ol><li><p>先选择EurekaServer，他优先选择同一个区域内负载较少的server</p></li><li><p>根据用户指定的策略，再从server取到的服务注册列表中选择一个地址</p></li></ol><p>Ribbon的负载均衡策略：轮询，随机和根据响应时间加权重。他的实现方式有很多种。</p><p>常见的负载均衡组件有：Nginx，LVS，硬件F5等</p><h3 id="Ribbon核心组件IRule"><a href="#Ribbon核心组件IRule" class="headerlink" title="Ribbon核心组件IRule"></a>Ribbon核心组件IRule</h3><p>IRule：根据特定算法从服务列表中选取一个要访问的服务。</p><p><img src="/java/spring/spring-cloud/fu-wu-diao-yong/ribbon/image-20210131144236556.png" alt="image-20210131144236556"></p><h3 id="Ribbon替换负载均衡算法"><a href="#Ribbon替换负载均衡算法" class="headerlink" title="Ribbon替换负载均衡算法"></a>Ribbon替换负载均衡算法</h3><p><img src="/java/spring/spring-cloud/fu-wu-diao-yong/ribbon/image-20210131145712484.png" alt="image-20210131145712484"></p><ol><li>建立一个IRule的的返回类，并将它注入spring</li></ol><p>注意事项：这个类的位置不能再ComponentScan注解（在SpringApplication注解中包含此注解）可以扫描掉的包下面，否则就会被所有Ribbon客户端共享，达不到特殊化定制的目的</p><p>这里建了一个和启动类所在的包同级别的包</p><p><img src="/java/spring/spring-cloud/fu-wu-diao-yong/ribbon/image-20210131150137351.png" alt="image-20210131150137351"></p><pre><code class="java">package myrule;import com.netflix.loadbalancer.IRule;import com.netflix.loadbalancer.RandomRule;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class MySelfRule &#123;    @Bean    //定义为随机    public IRule myRule() &#123;        return new RandomRule();    &#125;&#125;</code></pre><ol start="2"><li>在启动类上面添加注解</li></ol><pre><code class="java">package com.exercise.springcloud;import myrule.MySelfRule;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration;import org.springframework.cloud.netflix.eureka.EnableEurekaClient;import org.springframework.cloud.netflix.ribbon.RibbonClient;import org.springframework.cloud.netflix.ribbon.RibbonClients;@SpringBootApplication//pom文件引入了mybatis-spring-boot-starter，这里会自动加载 DataSourceAutoConfiguration，下面的注解是为了阻止自动注入的@EnableAutoConfiguration(exclude=&#123;DataSourceAutoConfiguration.class&#125;)@EnableEurekaClient//这里服务名大写@RibbonClient(name=&quot;CLOUD-PAYMENT-SERVICE&quot;,configuration = MySelfRule.class)public class OrderMain80 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(OrderMain80.class, args);    &#125;&#125;</code></pre><h3 id="Ribbon和Nginx负载均衡的区别"><a href="#Ribbon和Nginx负载均衡的区别" class="headerlink" title="Ribbon和Nginx负载均衡的区别"></a>Ribbon和Nginx负载均衡的区别</h3><p>一个是集中式的Load Balance</p><p>如：Nginx服务器负载均衡，客户端的所有请求都会交给Nginx，然后统一由Nginx来进行转发请求，及负载均衡是由服务器实现的</p><p>一个是进程内Load Balance</p><p>Ribbon本地负载均衡。在调用微服务接口时候，会在注册中心上获取注册信息服务列表之后缓存到 JVM 本地，从而在本地实现RPC远程服务调用技术。</p><p>进程内负载均衡，将负载均衡逻辑集成到 consumer，consumer 从服务注册中 心获知有哪些地址可用，然后自己再从这些地址中选择出一个合适的 provider。</p><h3 id="自定义负载均衡的实现"><a href="#自定义负载均衡的实现" class="headerlink" title="自定义负载均衡的实现"></a>自定义负载均衡的实现</h3><ol><li>创建源码中类似的接口</li></ol><pre><code class="java">package com.exercise.springcloud.lb;import org.springframework.cloud.client.ServiceInstance;import java.util.List;public interface LoadBalance &#123;    ServiceInstance instances(List&lt;ServiceInstance&gt; serviceInstances);&#125;</code></pre><ol start="2"><li>创建实现类，进行重写</li></ol><pre><code class="java">package com.exercise.springcloud.lb;import org.springframework.cloud.client.ServiceInstance;import org.springframework.stereotype.Component;import java.util.List;import java.util.concurrent.atomic.AtomicInteger;@Componentpublic class MyLB implements LoadBalance &#123;    //模拟轮询，这是一个线程安全的Integer，每次进行叠加这个参数    private AtomicInteger atomicInteger = new AtomicInteger(0);    public final int getAndIncrement() &#123;        int current;        int next;        do &#123;            current = this.atomicInteger.get();            next = current &gt;= Integer.MAX_VALUE ? 0 : current+1;            //这个方式是两个值相等就会更换并返回 boolean            System.out.println(&quot;aaaaaaaaaaaa&quot;+next);        &#125; while (!this.atomicInteger.compareAndSet(current,next));        System.out.println(next);    return next;    &#125;    @Override    public ServiceInstance instances(List&lt;ServiceInstance&gt; serviceInstances) &#123;        int index = getAndIncrement()% serviceInstances.size();        return serviceInstances.get(index);    &#125;&#125;</code></pre><ol start="3"><li>写好后进行调用测试</li></ol><pre><code class="java">    //自定义的轮询算法实现    @GetMapping(&quot;/consumer/payment/lb&quot;)    public String getPaymentLB() &#123;        List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(&quot;CLOUD-PAYMENT-SERVICE&quot;);        //如果没有对应名字并运行中的微服务就返回null        if (instances == null || instances.size() &lt;= 0) &#123;            return null;        &#125;        //返回本次负载均衡的实例        ServiceInstance serviceInstance = loadBalance.instances(instances);        //获取uri        URI uri = serviceInstance.getUri();        return restTemplate.getForObject(uri+&quot;/payment/lb&quot;,String.class);    &#125;</code></pre><h2 id="Ribbon的使用"><a href="#Ribbon的使用" class="headerlink" title="Ribbon的使用"></a>Ribbon的使用</h2><h3 id="服务端配置"><a href="#服务端配置" class="headerlink" title="服务端配置"></a>服务端配置</h3><p>这里并没有添加Ribbon的依赖。因为前面引入了一个Eureka的依赖，里面自带了Ribbon。所以就没有引入这个依赖。</p>]]></content>
      
      
      <categories>
          
          <category> Spring </category>
          
          <category> Spring Ribbon </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring Ribbon </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Eureka</title>
      <link href="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/"/>
      <url>/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/</url>
      
        <content type="html"><![CDATA[<h2 id="Eureka"><a href="#Eureka" class="headerlink" title="Eureka"></a>Eureka</h2><h3 id="Eureka是什么"><a href="#Eureka是什么" class="headerlink" title="Eureka是什么?"></a>Eureka是什么?</h3><p>Eureka是spring cloud中的一个负责服务注册与发现的组件。Eureka是Netflix中的一个开源框架。它和 zookeeper、Consul一样，都是用于服务注册管理的，同样，Spring-Cloud 还集成了Zookeeper和Consul。Eureka还可以配合ribbon实现负载均衡。</p><p>Eureka由多个instance(服务实例)组成，这些服务实例可以分为两种：Eureka Server和Eureka Client。为了便于理解，我们将Eureka client再分为Service Provider和Service Consumer。</p><ul><li>Eureka Server 提供服务注册和发现</li><li>Service Provider 服务提供方，将自身服务注册到Eureka，从而使服务消费方能够找到</li><li>Service Consumer服务消费方，从Eureka获取注册服务列表，从而能够消费服务</li></ul><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/1117146-20190510153141906-1290481315-1612189062907.png" alt="img"></p><h3 id="什么是服务注册与发现？"><a href="#什么是服务注册与发现？" class="headerlink" title="什么是服务注册与发现？"></a>什么是服务注册与发现？</h3><p>Eureka采用了CS的设计架构，Eureka Server 作为服务注册功能的服务器。而系统的其他微服务，使用Eureka客户端连接到了Eureka Server 并维持心跳链接。这蓝系统的维护人员就可以通过Eureka Server来监控系统中各个微服务是否正常运行。</p><p>在服务注册与发现中，有一个注册中心。当服务器启动时，会把当前自己的服务器信息，比如 服务地址通讯地址等以及别名方式注册到注册到注册中心上。另一方（消费者|服务提供者），以该别名的方式去注册中心上获取到实际的服务通讯地址，然后再实现本地RPC调用RPC远程调用框架核心设计思想：在于注册中心，因为使用注册中心管理每个服务与服务之间的依赖关系（服务治理概念）。在任何RPC远程框架中，都会有一个注册中心（存放服务地址相关信息（接口地址））</p><h3 id="Eureka和Zookeeper的对比"><a href="#Eureka和Zookeeper的对比" class="headerlink" title="Eureka和Zookeeper的对比"></a>Eureka和Zookeeper的对比</h3><p>参考文章：<a href="http://www.heartthinkdo.com/?p=1933">http://www.heartthinkdo.com/?p=1933</a></p><p>首先介绍下cap原理，可以参考：<a href="http://www.ruanyifeng.com/blog/2018/07/cap.html%E3%80%82">http://www.ruanyifeng.com/blog/2018/07/cap.html。</a></p><ul><li>P:Partition tolerance,网络分区容错。类似多机房部署，保证服务稳定性。</li><li>A: Availability，可用性。</li><li>C:Consistency ，一致性。</li></ul><p>CAP定理：CAP三个属性对于分布式系统不同同时做到。如AP&#x2F;CP&#x2F;AC。再来看Zookeepr区别：</p><p>（1）Zookeeper是CP，分布式协同服务，突出一致性。对ZooKeeper的的每次请求都能得到一致的数据结果，但是无法保证每次访问服务可用性。如请求到来时，zookeer正在做leader选举，此时不能提供服务，即不满足A可用性。</p><p>（2）Euere是AP，高可用与可伸缩的Service发现服务，突出可用性。相对于Zookeeper而言，可能返回数据没有一致性，但是保证能够返回数据，服务是可用的。</p><h3 id="Eureka和Dubbo的对比"><a href="#Eureka和Dubbo的对比" class="headerlink" title="Eureka和Dubbo的对比"></a>Eureka和Dubbo的对比</h3><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210125192449469.png" alt="image-20210125192449469"></p><h3 id="Eureka搭建使用"><a href="#Eureka搭建使用" class="headerlink" title="Eureka搭建使用"></a>Eureka搭建使用</h3><ol><li>新增一个moudel，并更改pom文件</li></ol><blockquote><p>注意点：这里引入Eureka的依赖时，他还会自动地引入Ribbon的依赖，并实现负载均衡</p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210129193851239-1612189062907.png" alt="image-20210129193851239"></p></blockquote><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;groupId&gt;com.exercise.springcloud&lt;/groupId&gt;    &lt;artifactId&gt;demo&lt;/artifactId&gt;    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;    &lt;name&gt;cloud-eureka-server7001&lt;/name&gt;    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;    &lt;properties&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;!--eureka server--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;!--            &lt;version&gt;2.0.2.RELEASE&lt;/version&gt;--&gt;&lt;!--            这里没有设置默认导入的是spring对应版本的eureka server--&gt;        &lt;/dependency&gt;        &lt;!--引入自己定义的api通用包--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;            &lt;artifactId&gt;cloud-api-commons&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;!--spring boot web--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--actuator是监控系统健康情况的工具--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;excludes&gt;                        &lt;exclude&gt;                            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;                            &lt;artifactId&gt;lombok&lt;/artifactId&gt;                        &lt;/exclude&gt;                    &lt;/excludes&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><ol start="2"><li>更改yml文件</li></ol><p><strong>注意事项：这里的缩进一定要按照下面的格式，要不然识别不到你的配置。</strong></p><pre><code class="yaml">server:  port: 7001eureka:  instance:      hostname: localhost #eureka服务端的实例名称  client:    register-with-eureka: false # 实例是否在eureka服务器上注册自己的信息以供其他服务发现，默认为true    fetch-registry: false  # 此客户端是否获取eureka服务器注册表上的注册信息，默认为true    service-url:      defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/      #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址      #defaultZone: http://eureka7002.com:7002/eureka/  #互相注册#        defaultZone: http://eureka7001.com:7001/eureka/ #单机模式#        defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/</code></pre><ol start="3"><li>在主启动类中添加注解<code>@EnableEurekaServer</code></li></ol><p>不用写业务类，直接启动。eurekaServer服务就起来了</p><pre><code class="java">package com.exercise.springcloud;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServerpublic class CloudEurekaServer7001Application &#123;    public static void main(String[] args) &#123;        SpringApplication.run(CloudEurekaServer7001Application.class, args);    &#125;&#125;</code></pre><ol start="4"><li>配置eureka Client 端</li></ol><p>client端项目配置pom,引入了eurekaClient端的数据</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;cloud-provider-payment8001&lt;/artifactId&gt;    &lt;!--子模块不用写gv--&gt;    &lt;!--&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;--&gt;    &lt;!--&lt;name&gt;cloud-provider-payment8001&lt;/name&gt;--&gt;    &lt;!--&lt;description&gt;Demo project for Spring Boot&lt;/description&gt;--&gt;    &lt;!--&lt;packaging&gt;jar&lt;/packaging&gt;--&gt;    &lt;dependencies&gt;        &lt;!-- 包含了sleuth zipkin 数据链路追踪--&gt;        &lt;!--&lt;dependency&gt;--&gt;        &lt;!--&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;--&gt;        &lt;!--&lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;--&gt;        &lt;!--&lt;/dependency&gt;--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;            &lt;artifactId&gt;cloud-api-commons&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--图形化监控--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt;            &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--eureka Client--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt;            &lt;version&gt;1.1.16&lt;/version&gt;            &lt;!--如果没写版本,从父层面找,找到了就直接用,全局统一--&gt;        &lt;/dependency&gt;        &lt;!--mysql-connector-java--&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--jdbc--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--热部署--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;!--&lt;dependency&gt;--&gt;        &lt;!--&lt;groupId&gt;org.yaml&lt;/groupId&gt;--&gt;        &lt;!--&lt;artifactId&gt;snakeyaml&lt;/artifactId&gt;--&gt;        &lt;!--&lt;version&gt;1.23&lt;/version&gt;--&gt;        &lt;!--&lt;/dependency&gt;--&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><ol start="5"><li>配置client端的application.yml</li></ol><p>配置了eureka的地址和当前服务注册的名称</p><pre><code class="yaml">#端口号配置server:  port: 8001#服务的名称spring:  application:    name: cloud-payment-service  #jdbc配置  datasource:    url: jdbc:mysql://localhost:3306/exercise?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=UTC    # druid  德鲁伊的数据源    type: com.alibaba.druid.pool.DruidDataSource    # mysql驱动类    driver-class-name: com.mysql.cj.jdbc.Driver    username: root    password: 123456#指定map文件的位置mybatis:  mapper-locations: classpath*:mapper/*.xml  #指定实体包下面类的别名  type-aliases-package: com.exercise.springcloud.entitieseureka:  client:    #是否将自己注册到eureka    register-with-eureka: true    #是否获取eureka注册信息，默认为true，单节点无所谓，集群必须设置true才能够配合ribbon使用负载均衡    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka</code></pre><ol start="6"><li>启动微服务进行查看结果</li></ol><p>这里已经启动并注册成功，访问localhost:7001的web页面，这个名称就是yml配置的微服务名称</p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128111258008-1612189062907.png" alt="image-20210128111258008"></p><h3 id="Eureka删除微服务信息"><a href="#Eureka删除微服务信息" class="headerlink" title="Eureka删除微服务信息"></a>Eureka删除微服务信息</h3><p>现在要删除order服务</p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128113114910-1612189062908.png" alt="image-20210128113114910"></p><p>删除的时候需要使用postman模拟delete请求来删除无用的微服务</p><p>1、<code>http://localhost:7001/</code>是请求Eureka管理界面的url</p><p>2、<code>eureka/apps</code>是Eureka api 的固定url</p><p>3、<code>CLOUD-ORDER-SERVICE</code>是服务注册的服务名一般指application:name，可公用因分布式</p><p>4、<code>DESKTOP-K8EF8D9:cloud-order-service:80</code>服务的唯一标识，具体体现在哪个服务，可为分布式中的一个。</p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128113144680-1612189062908.png" alt="image-20210128113144680"></p><p>这样就成功删除了</p><h2 id="Eureka集群搭建"><a href="#Eureka集群搭建" class="headerlink" title="Eureka集群搭建"></a>Eureka集群搭建</h2><ol><li>多个eureka module，更改pom文件引入依赖，和上方单机版引入相同的pom</li><li>更改他们的<code>application.yml</code>文件，这里配置单机版不同</li></ol><p>更改他们的<code>defaultZone</code> 设置成另一台节点的ip和端口，因为这里都是本地的，就没有改变他们的地址</p><p><strong>注意</strong>：如果有三台或以上的节点数，<code>defaultZone</code>需要写成</p><pre><code class="yaml">defaultZone: http://eureka7001.com:7001/eureka,http://eureka7001.com/eureka:7002,......</code></pre><p>这里第一个设置访问第二个节点的端口和地址</p><pre><code class="yaml">server:  port: 7001eureka:  instance:      hostname: eureka7001   #eureka服务端的实例名称  client:    register-with-eureka: false # 实例是否在eureka服务器上注册自己的信息以供其他服务发现，默认为true    fetch-registry: false  # 此客户端是否获取eureka服务器注册表上的注册信息，默认为true    service-url:      defaultZone: http://localhost:7002/eureka/      #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址      #defaultZone: http://eureka7002.com:7002/eureka/  #互相注册#        defaultZone: http://eureka7001.com:7001/eureka/ #单机模式#        defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/</code></pre><p>第二个设置访问第一个节点的端口和地址</p><pre><code class="yaml">server:  port: 7002eureka:  instance:    hostname: eureka7002 #eureka服务端的实例名称  client:    register-with-eureka: false # 实例是否在eureka服务器上注册自己的信息以供其他服务发现，默认为true    fetch-registry: false  # 此客户端是否获取eureka服务器注册表上的注册信息，默认为true    service-url:      defaultZone: http://localhost:7001/eureka/      #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址      #defaultZone: http://eureka7002.com:7002/eureka/  #互相注册#        defaultZone: http://eureka7001.com:7001/eureka/ #单机模式#        defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/</code></pre><ol start="3"><li>修改微服务的yml文件配置</li></ol><p>修改eureka配置</p><p><strong>注意事项</strong>：<code>defaultZone</code> 必须写成 <code>http://localhost:7002</code>+<code>/eureka</code> 的形式，否则微服务找不到你的Eureka服务，这里不晓得为啥</p><pre><code class="yaml">eureka:  client:    #是否将自己注册到eureka    register-with-eureka: true    #是否获取eureka注册信息，默认为true，单节点无所谓，集群必须设置true才能够配合ribbon使用负载均衡    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka,http://localhost:7002/eureka</code></pre><ol start="4"><li>顺序启动服务</li></ol><p>先启动Eureka服务，然后启动提供者和消费者的微服务</p><p>启动后的监控图：可以访问不同节点的</p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128135609078-1612189062908.png" alt="image-20210128135609078"></p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128135651540-1612189062908.png" alt="image-20210128135651540"></p><h3 id="Eureka集群原理"><a href="#Eureka集群原理" class="headerlink" title="Eureka集群原理"></a>Eureka集群原理</h3><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128131654138-1612189062908.png" alt="image-20210128131654138"></p><h3 id="Eureka负载均衡"><a href="#Eureka负载均衡" class="headerlink" title="Eureka负载均衡"></a>Eureka负载均衡</h3><ol><li>这里微服务配置在yml中的<code>服务名称是一致的</code>，Eureka是通过名称来实现负载均衡</li></ol><blockquote><p> 他是通过轮询方式进行负载均衡的。1 2 1 2 1 2 1…</p></blockquote><p>这里有两个服务端微服务，yml文件配置如下</p><p>一：8001</p><pre><code class="yaml">#端口号配置server:  port: 8001#服务的名称spring:  application:    name: cloud-payment-service  #jdbc配置  datasource:    url: jdbc:mysql://localhost:3306/exercise?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=UTC    # druid  德鲁伊的数据源    type: com.alibaba.druid.pool.DruidDataSource    # mysql驱动类    driver-class-name: com.mysql.cj.jdbc.Driver    username: root    password: 123456#指定map文件的位置mybatis:  mapper-locations: classpath*:mapper/*.xml  #指定实体包下面类的别名  type-aliases-package: com.exercise.springcloud.entitieseureka:  client:    #是否将自己注册到eureka    register-with-eureka: true    #是否获取eureka注册信息，默认为true，单节点无所谓，集群必须设置true才能够配合ribbon使用负载均衡    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka,http://localhost:7002/eureka</code></pre><p>二：8002</p><pre><code class="yaml">#端口号配置server:  port: 8002#服务的名称spring:  application:    name: cloud-payment-service  #jdbc配置  datasource:    url: jdbc:mysql://localhost:3306/exercise?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=UTC    # druid  德鲁伊的数据源    type: com.alibaba.druid.pool.DruidDataSource    # mysql驱动类    driver-class-name: com.mysql.cj.jdbc.Driver    username: root    password: 123456#指定map文件的位置mybatis:  mapper-locations: classpath*:mapper/*.xml  #指定实体包下面类的别名  type-aliases-package: com.exercise.springcloud.entitieseureka:  client:    #是否将自己注册到eureka    register-with-eureka: true    #是否获取eureka注册信息，默认为true，单节点无所谓，集群必须设置true才能够配合ribbon使用负载均衡    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka,http://localhost:7002/eureka</code></pre><ol start="2"><li>服务端微服务的其他配置不变，把调用的服务的端口打印一下，方便查看结果</li></ol><pre><code class="java">@RestController@Slf4jpublic class PaymentController &#123;    @Resource    private PaymentService paymentService;    @Value(&quot;$&#123;server.port&#125;&quot;)    private String serverPort;    @PostMapping(&quot;/payment/create&quot;)    public CommonResult create(@RequestBody Payment payment) &#123;        log.debug(&quot;aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaapayment结果:&quot; + payment);        int result = paymentService.create(payment);        log.debug(&quot;插入的结果:&quot; + result);        if (result &gt; 0) &#123;            return new CommonResult(200, &quot;插入数据库成功,serverPort&quot;+serverPort, result);        &#125; else &#123;            return new CommonResult(444, &quot;插入数据库失败&quot;);        &#125;    &#125;    @GetMapping(&quot;/payment/get/&#123;id&#125;&quot;)    public CommonResult getPaymentById(@PathVariable(&quot;id&quot;) Long id) &#123;        Payment paymentById = paymentService.getPaymentById(id);        log.debug(&quot;查询的:&quot; + paymentById);        if (paymentById != null) &#123;            return new CommonResult(200, &quot;查询成功,serverPort&quot;+serverPort, paymentById);        &#125; else &#123;            return new CommonResult(444, &quot;查询失败,没有对应记录&quot;);        &#125;    &#125;&#125;</code></pre><ol start="3"><li>配置消费者服务端</li></ol><p>这里的微服务地址不能写死，写成你上面负载均衡的微服务名称</p><pre><code class="java">package com.exercise.springcloud.controller;import com.exercise.springcloud.entities.CommonResult;import com.exercise.springcloud.entities.Payment;import lombok.extern.slf4j.Slf4j;import org.springframework.web.bind.annotation.*;import org.springframework.web.client.RestTemplate;import javax.annotation.Resource;@RestController@Slf4jpublic class OrderController &#123;    //public static final String PAYMENT_URL = &quot;http://localhost:8001&quot;;    public static final String PAYMENT_URL = &quot;http://CLOUD-PAYMENT-SERVICE&quot;;    @Resource    private RestTemplate restTemplate;    @PostMapping(&quot;/consumer/payment/create&quot;)    public CommonResult&lt;Payment&gt; create(Payment payment) &#123;        log.info(&quot;试用info&quot;+&quot;aaaaaaaaaa/&quot;+payment);        return restTemplate.postForObject(PAYMENT_URL + &quot;/payment/create&quot;, payment, CommonResult.class);    &#125;    @GetMapping(&quot;/consumer/payment/get/&#123;id&#125;&quot;)    public CommonResult getPaymentById(@PathVariable(&quot;id&quot;) Long id)&#123;        return restTemplate.getForObject(PAYMENT_URL+&quot;/payment/get/&quot;+id,CommonResult.class);    &#125;&#125;</code></pre><ol start="5"><li>对RestTemplate配置负载均衡</li></ol><p>&#96;&#96;LoadBalanced <code> </code>作用 在使用 RestTemplate 的时候 如果 RestTemplate 上面有 这个注解，那么 这个 RestTemplate 调用的 远程地址，会走负载均衡器。</p><p>将 <code>LoadBalanced </code> 注解放在RestTemplate 上，将RestTemplate负载均衡</p><p>这一步是必需的，否则程序不知道是服务名称下的哪个结点提供服务</p><pre><code class="java">package com.exercise.springcloud.config;import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.client.RestTemplate;@Configurationpublic class ApplicationContextConfig &#123;    @Bean    @LoadBalanced    public RestTemplate getRestTemplate() &#123;        return new RestTemplate();    &#125;&#125;</code></pre><ol start="6"><li>启动服务，打开Eureka页面</li></ol><p>可以发现有这个微服务名称有两个不同的端口</p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128151338929-1612189062908.png" alt="image-20210128151338929"></p><ol start="7"><li>测试结果</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128153855885-1612189062908.png" alt="image-20210128153855885"></p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128153917776-1612189062908.png" alt="image-20210128153917776"></p><h4 id="Eureka配置微服务的名称和是否显示ip"><a href="#Eureka配置微服务的名称和是否显示ip" class="headerlink" title="Eureka配置微服务的名称和是否显示ip"></a>Eureka配置微服务的名称和是否显示ip</h4><p>这个步骤，不必要。主要后期需要方便查看。可以改成公司名称或者其他名称</p><pre><code class="yaml">eureka:  client:    #是否将自己注册到eureka    register-with-eureka: true    #是否获取eureka注册信息，默认为true，单节点无所谓，集群必须设置true才能够配合ribbon使用负载均衡    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka,http://localhost:7002/eureka#下面的是新加的参数，和上面client位置同级  instance:    instance-id: payment8002    prefer-ip-address: true</code></pre><pre><code class="yaml">eureka:  client:    #是否将自己注册到eureka    register-with-eureka: true    #是否获取eureka注册信息，默认为true，单节点无所谓，集群必须设置true才能够配合ribbon使用负载均衡    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka,http://localhost:7002/eureka  instance:    instance-id: payment8001    prefer-ip-address: true</code></pre><p>结果展示：</p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128155847844-1612189062908.png" alt="名称自定义"></p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128160838684-1612189062908.png" alt="ip显示"></p><h3 id="服务发现Discovery"><a href="#服务发现Discovery" class="headerlink" title="服务发现Discovery"></a>服务发现Discovery</h3><p>方便将微服务的详细信息进行统计。</p><ol><li>添加<code>DiscoveryClient</code>对象</li></ol><pre><code class="java">package com.exercise.springcloud.controller;import com.exercise.springcloud.Service.PaymentService;import com.exercise.springcloud.entities.CommonResult;import com.exercise.springcloud.entities.Payment;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Value;import org.springframework.cloud.client.ServiceInstance;import org.springframework.cloud.client.discovery.DiscoveryClient;import org.springframework.web.bind.annotation.*;import javax.annotation.Resource;import java.util.List;@RestController@Slf4jpublic class PaymentController &#123;    @Resource    private PaymentService paymentService;    @Value(&quot;$&#123;server.port&#125;&quot;)    private String serverPort;    @Resource    private DiscoveryClient discoveryClient;    @PostMapping(&quot;/payment/create&quot;)    public CommonResult create(@RequestBody Payment payment) &#123;        log.debug(&quot;aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaapayment结果:&quot; + payment);        int result = paymentService.create(payment);        log.debug(&quot;插入的结果:&quot; + result);        if (result &gt; 0) &#123;            return new CommonResult(200, &quot;插入数据库成功,serverPort:&quot; + serverPort, result);        &#125; else &#123;            return new CommonResult(444, &quot;插入数据库失败&quot;);        &#125;    &#125;    @GetMapping(&quot;/payment/get/&#123;id&#125;&quot;)    public CommonResult getPaymentById(@PathVariable(&quot;id&quot;) Long id) &#123;        Payment paymentById = paymentService.getPaymentById(id);        log.debug(&quot;查询的:&quot; + paymentById);        if (paymentById != null) &#123;            return new CommonResult(200, &quot;查询成功,serverPort:&quot; + serverPort, paymentById);        &#125; else &#123;            return new CommonResult(444, &quot;查询失败,没有对应记录&quot;);        &#125;    &#125;    //这个方法使用了 Discovery  上面还注入了一个 DiscoveryClient 对象    @GetMapping(&quot;/payment/discovery&quot;)    public Object discovery() &#123;        //获取Eureka中的所有微服务名称        List&lt;String&gt; services = discoveryClient.getServices();        for (String element : services)&#123;            log.info(&quot;*************************************&quot;+element);        &#125;        //获取指定微服务的明细信息        List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(&quot;CLOUD-PAYMENT-SERVICE&quot;);        for (ServiceInstance instance : instances) &#123;            //还可以获取其他信息            log.info(instance.getInstanceId()+&quot;-&quot;+instance.getHost()+&quot;-&quot;+instance.getMetadata()+&quot;-&quot;+instance.getUri());        &#125;        // discoveryClient 对象包含了当前的微服务名称        return this.discoveryClient;    &#125;&#125;</code></pre><ol start="2"><li>启动类加上注解</li></ol><pre><code class="java">@SpringBootApplication@EnableEurekaClient@EnableDiscoveryClientpublic class PaymentMain8001 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(PaymentMain8001.class, args);    &#125;&#125;</code></pre><ol start="3"><li>启动微服务，访问上方的接口查看结果</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128184654730-1612189062908.png" alt="接口返回的DiscoveryClient对象"></p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/eureka/image-20210128184726692-1612189062909.png" alt="输出的值"></p><h2 id="Eureka自我保护"><a href="#Eureka自我保护" class="headerlink" title="Eureka自我保护"></a>Eureka自我保护</h2><h3 id="自我保护模式"><a href="#自我保护模式" class="headerlink" title="自我保护模式"></a>自我保护模式</h3><p>默认情况下，如果EurekaServer在一定时间内没有接受到某个微服务实例的心跳，EurekaServer将会注销该实例（默认90秒）。但是当网络分区故障发生（延时、卡顿、拥挤）时，微服务与EurekaServer之间无法正常通信，以上行为可能变得非常危险–因为微服务本身其实是健康的，此时本不应该注销这个微服务。Eureka通过“自我保护模式”来解决这个问题–当EurekaServer节点在短时间内丢失过多客户端时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。</p><h3 id="自我保护机制"><a href="#自我保护机制" class="headerlink" title="自我保护机制"></a>自我保护机制</h3><p>默认情况下EurekaClient定时向EurekaServer端发送心跳包</p><p>如果Eureka在server端在一定时间内（默认90秒）没有收到EurekaClient发送心跳包，便会直接从服务注册列表中剔除该服务，但是在短时间（90秒中）内丢失了大量的服务实例心跳，这时候EurekaServer会开启自我保护机制，<code>EurekaServer不会将EurekaClient服务及时剔除</code>（该现象可能出现在如果网络不通，但是EurekaClient未出现宕机，此时如果换做别的注册中心如果一定时间内没有收到心跳会将剔除该服务，这样就出现了严重失误，因为客户端还能正常发送心跳，只是网络延迟问题，而保护机制是为了解决此问题而产生的）</p><p>关闭自我保护机制实例（默认开启）</p><h2 id="1、-Eureka服务器端添加配置信息"><a href="#1、-Eureka服务器端添加配置信息" class="headerlink" title="1、 Eureka服务器端添加配置信息"></a>1、 Eureka服务器端添加配置信息</h2><pre><code class="yaml">eureka:  instance:    hostname: eureka7001.com  # eureka 服务器的实例名称  client:  # false 代表不向服务注册中心注册自己，因为它本身就是服务中心    register-with-eureka: false  # false 代表自己就是服务注册中心，自己的作用就是维护服务实例，并不需要去检索服务    fetch-registry: false    service-url:    # 设置与 Eureka Server 交互的地址，查询服务 和 注册服务都依赖这个地址      defaultZone: http://eureka7001.com:7001/eureka/ #Eureka单机    #      defaultZone: http://eureka7002.com:7002/eureka/ # Eureka集群  server:    # 关闭自我保护机制，保证不可用服务及时剔除    enable-self-preservation: false    # 设置剔除间隔毫秒：2秒内微服务没有心跳就剔除，默认是90s的    eviction-interval-timer-in-ms: 2000</code></pre><h2 id="2、-Eureka客户端添加配置信息"><a href="#2、-Eureka客户端添加配置信息" class="headerlink" title="2、 Eureka客户端添加配置信息"></a>2、 Eureka客户端添加配置信息</h2><pre><code class="yaml">eureka:  client:    # 注册进 Eureka 的服务中心    register-with-eureka: true    # 检索 服务中心 的其它服务    fetch-registry: true    service-url:      # 设置与 Eureka Server 交互的地址      defaultZone: http://localhost:7001/eureka/  # 单机版#      defaultZone: http://localhost:7001/eureka/,http://localhost:7002/eureka/  # 集群版  instance:    # 修改在Eureka 注册中心显示的 主机名    instance-id: payment8001    # 显示微服务所在的主机地址    prefer-ip-address: true    # Eureka客户端向服务端发送心跳时间的间隔，单位为秒（默认是30秒）    lease-renewal-interval-in-seconds: 1    # Eureka服务端在收到最后一次心跳后等待时间的一个上限，单位为秒（默认是90秒），超市将剔除服务    lease-expiration-duration-in-seconds: 2</code></pre><p>上面配置完成后，一但微服务2秒没有发送心跳就删除这个微服务的记录</p><h2 id="面试问题"><a href="#面试问题" class="headerlink" title="面试问题"></a>面试问题</h2><h3 id="Eureka远程调用核心？"><a href="#Eureka远程调用核心？" class="headerlink" title="Eureka远程调用核心？"></a>Eureka远程调用核心？</h3><p>高可用，如果只有单节点，如果eureka挂掉，那么整个服务环境将不可用。</p><p>解决办法：搭建Eureka集群，并实现高可用(负载均衡+故障容错)</p>]]></content>
      
      
      <categories>
          
          <category> Spring Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring Cloud </tag>
            
            <tag> Eureka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Zookeeper</title>
      <link href="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/springcloud-zookeeper/"/>
      <url>/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/springcloud-zookeeper/</url>
      
        <content type="html"><![CDATA[<p>SpringCloud-Zookeeper</p><p>因为Eureka闭源，所以使用Zookkeeper作为注册中心</p><h2 id="Zookeeper替换Eureka"><a href="#Zookeeper替换Eureka" class="headerlink" title="Zookeeper替换Eureka"></a>Zookeeper替换Eureka</h2><p>zk当成服务发现和注册的不多。</p><ol><li>更换pom的依赖,将Eureka换成Zookeeper</li></ol><h3 id="zk服务端"><a href="#zk服务端" class="headerlink" title="zk服务端"></a>zk服务端</h3><p>注意事项：</p><blockquote><p> 下方单独引入了lombok，所以排除掉zookeeper自带的log4j</p><p> 因为 zookeeper-discovery 包自带3.5.3 的jar包，而版本不符合，下面单独引入了zk 3.4.5，他俩jar冲突，需要把3.5.3排除掉</p></blockquote><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;parent&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;artifactId&gt;cloud-provider-payment8004&lt;/artifactId&gt;    &lt;properties&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--springboot整合zookeeper客户端--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-zookeeper-discovery&lt;/artifactId&gt;            &lt;!--先排除自带的zookeeper3.5.3--&gt;&lt;!--            &lt;exclusions&gt;--&gt;&lt;!--                &lt;exclusion&gt;--&gt;&lt;!--                    &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;--&gt;&lt;!--                    &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;--&gt;&lt;!--                &lt;/exclusion&gt;--&gt;&lt;!--            &lt;/exclusions&gt;--&gt;        &lt;/dependency&gt;               &lt;!--        &amp;lt;!&amp;ndash;eureka client&amp;ndash;&amp;gt;--&gt;        &lt;!--        &lt;dependency&gt;--&gt;        &lt;!--            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;--&gt;        &lt;!--            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;--&gt;        &lt;!--        &lt;/dependency&gt;--&gt;        &lt;!--添加zookeeper3.4.5版本--&gt;        &lt;!--Zookeeper 替换了 Eureka--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;            &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;            &lt;version&gt;3.4.5&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;log4j&lt;/groupId&gt;                    &lt;artifactId&gt;log4j&lt;/artifactId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;                    &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;!--Springboot整合web组件--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--引入自定义的api通用包，可以使用payment支付Entity--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;            &lt;artifactId&gt;cloud-api-commons&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;excludes&gt;                        &lt;exclude&gt;                            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;                            &lt;artifactId&gt;lombok&lt;/artifactId&gt;                        &lt;/exclude&gt;                    &lt;/excludes&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><ol start="2"><li>修改yml 文件</li></ol><pre><code class="yaml">#端口号配置server:  port: 8004#服务的名称spring:  application:    name: cloud-provider-payment  cloud:    zookeeper:      connect-string: 10.130.210.245:2181,10.130.210.246:2181,10.130.210.247:2181</code></pre><ol start="3"><li>controller测试编写</li></ol><pre><code class="java">package com.exercise.springcloud.controller;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Value;import org.springframework.web.bind.annotation.*;import java.util.UUID;@RestController@Slf4jpublic class PaymentController &#123;    @Value(&quot;$&#123;server.port&#125;&quot;)    private String serverPort;    @GetMapping(&quot;/payment/zk&quot;)    public String create() &#123;        return &quot;springcloud with zookeeper:&quot; + serverPort + &quot;\t&quot; + UUID.randomUUID().toString();    &#125;&#125;</code></pre><ol start="3"><li>启动类修改</li></ol><pre><code class="java">package com.exercise.springcloud;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;@SpringBootApplication//这里一定要使用这个注解，而不能使用@EnableEurekaClient,EnableDiscoveryClient注解可以对所有的服务发现起作用，而 EnableEurekaClient 只能作用在Eureka@EnableDiscoveryClient//@EnableAutoConfiguration(exclude=&#123;DataSourceAutoConfiguration.class&#125;)public class PaymentMain8004 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(PaymentMain8004.class, args);    &#125;&#125;</code></pre><ol start="4"><li>启动后查看结果</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/springcloud-zookeeper/image-20210128195929892-1612189150234.png" alt="访问测试方法的结果"></p><p>在zk中也有了对应的znode数据节点，也可以查看相关的注册信息</p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/springcloud-zookeeper/image-20210128200630746-1612189150234.png" alt="查看zk节点的明细数据"></p><p>zookeeper中的节点是临时节点，一旦服务停止，超过心跳超时时间范围就会删除节点</p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/springcloud-zookeeper/image-20210129141127530-1612189150238.png" alt="image-20210129141127530"></p><h3 id="zk客户端"><a href="#zk客户端" class="headerlink" title="zk客户端"></a>zk客户端</h3><ol><li>修改pom文件</li></ol><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;parent&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;artifactId&gt;cloud-consumerzk-order80&lt;/artifactId&gt;    &lt;properties&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--springboot整合zookeeper客户端--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-zookeeper-discovery&lt;/artifactId&gt;            &lt;!--排除自带的zookeeper--&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;                    &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;!--添加zookeeper3.4.5版本--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;            &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;            &lt;version&gt;3.4.5&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;log4j&lt;/groupId&gt;                    &lt;artifactId&gt;log4j&lt;/artifactId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;                    &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;!--springboot整合web组件--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;excludes&gt;                        &lt;exclude&gt;                            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;                            &lt;artifactId&gt;lombok&lt;/artifactId&gt;                        &lt;/exclude&gt;                    &lt;/excludes&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><ol start="2"><li>yml文件修改</li></ol><pre><code class="yaml">server:  port: 80#服务的名称spring:  application:    name: cloud-orderzk-service  cloud:    zookeeper:      connect-string: 10.130.210.245:2181,10.130.210.246:2181,10.130.210.247:2181</code></pre><ol start="3"><li>config</li></ol><pre><code class="java">package com.exercise.springcloud.config;import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.client.RestTemplate;@Configurationpublic class ApplicationContextConfig &#123;    @Bean    @LoadBalanced    public RestTemplate getRestTemplate() &#123;        return new RestTemplate();    &#125;&#125;</code></pre><ol start="4"><li>启动类</li></ol><pre><code class="java">package com.exercise.springcloud;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class OrderZKMain80 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(OrderZKMain80.class, args);    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> Spring Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zookeeper </tag>
            
            <tag> Spring Cloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Consul</title>
      <link href="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/consul/"/>
      <url>/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/consul/</url>
      
        <content type="html"><![CDATA[<h3 id="Consul是什么"><a href="#Consul是什么" class="headerlink" title="Consul是什么"></a>Consul是什么</h3><p>开源的服务发现和配置管理系统，由HashiCorp公司Go语言开发</p><p>提供了微服务系统的服务治理、配置中心、控制总线等功能。每一个都可以根据需要单独使用，也可以一起构建全方位的服务，Consul提供了一种完整的服务网格解决办法。</p><h3 id="Consul安装"><a href="#Consul安装" class="headerlink" title="Consul安装"></a>Consul安装</h3><ol><li>官方下载包</li><li>进行解压，linux解压后会有一个consul的可执行文件。</li><li>将consul放到 系统目录下，否则无法执行</li></ol><pre><code>mv consul /usr/local/bin/</code></pre><ol start="4"><li>启动consule</li></ol><p>下面命令是开发者模式启动，并且允许所有ip连接。如果没有<code>-client</code>的配置，那么只能本机访问</p><pre><code>consul agent -dev -client 0.0.0.0 -ui</code></pre><p>访问成功页面：</p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/consul/image-20210129153645731.png" alt="访问页面"></p><h2 id="Consul的使用"><a href="#Consul的使用" class="headerlink" title="Consul的使用"></a>Consul的使用</h2><h3 id="服务端配置"><a href="#服务端配置" class="headerlink" title="服务端配置"></a>服务端配置</h3><ol><li>pom文件</li></ol><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;parent&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;artifactId&gt;cloud-providerconsul-payment8006&lt;/artifactId&gt;    &lt;properties&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--springcloud consul-server--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--日常通用jar包--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;excludes&gt;                        &lt;exclude&gt;                            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;                            &lt;artifactId&gt;lombok&lt;/artifactId&gt;                        &lt;/exclude&gt;                    &lt;/excludes&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><ol start="2"><li>yml文件</li></ol><pre><code class="yaml">server:  port: 8006spring:  application:    name: consul-provider-payment    #服务注册中心地址  cloud:    consul:      host: 10.130.210.245      port: 8500      discovery:        #hostname        service-name: $&#123;spring.application.name&#125;</code></pre><ol start="3"><li>启动类</li></ol><pre><code class="java">@SpringBootApplication@EnableDiscoveryClientpublic class PaymentMain8006 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(PaymentMain8006.class, args);    &#125;&#125;</code></pre><ol start="4"><li>controller</li></ol><pre><code class="java">import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Value;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;import java.util.UUID;@RestController@Slf4jpublic class PaymentController &#123;    @Value(&quot;$&#123;server.port&#125;&quot;)    private String serverPort;    @GetMapping(&quot;/payment/consul&quot;)    public String create() &#123;        return &quot;springcloud with consul:&quot; + serverPort + &quot;\t&quot; + UUID.randomUUID().toString();    &#125;&#125;</code></pre><ol start="5"><li>启动后查看consul页面图</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/consul/image-20210129160059319.png" alt="监控页面"></p><h3 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h3><ol><li>pom配置</li></ol><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;cloud-consumerconsul-order80&lt;/artifactId&gt;    &lt;properties&gt;        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--springcloud consul-server--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--springboot整合web组件--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--日常通用jar包配置--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><ol start="2"><li>yml文件配置</li></ol><pre><code class="yaml">server:  port: 80spring:  application:    name: consul-consumer-order    #服务注册中心地址  cloud:    consul:      host: 10.130.210.245      port: 8500      discovery:        #hostname        service-name: $&#123;spring.application.name&#125;        prefer-ip-address: true</code></pre><ol start="3"><li>config配置</li></ol><pre><code class="java">package springcloud.config;import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.client.RestTemplate;@Configurationpublic class ApplicationContextConfig &#123;    @Bean    @LoadBalanced    public RestTemplate getRestTemplate() &#123;        return new RestTemplate();    &#125;&#125;</code></pre><ol start="3"><li>controller配置</li></ol><pre><code class="java">package springcloud.controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.client.RestTemplate;import javax.annotation.Resource;@RestControllerpublic class OrderConsulController &#123;    public static final String INVOKE_URL = &quot;http://consul-provider-payment&quot;;    @Resource    private RestTemplate restTemplate;    @GetMapping(&quot;/consumer/payment/consule&quot;)    public String paymentInfo() &#123;        String result = restTemplate.getForObject(INVOKE_URL + &quot;/payment/consul&quot;, String.class);        return result;    &#125;&#125;</code></pre><ol start="5"><li>main方法配置</li></ol><pre><code class="java">package springcloud;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;@SpringBootApplication@EnableDiscoveryClientpublic class OrderConsulMain80 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(OrderConsulMain80.class, args);    &#125;&#125;</code></pre><ol start="6"><li>启动项目,consul监控页面</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/consul/image-20210129161837078.png" alt="监控页面"></p><p><img src="/java/spring/spring-cloud/fu-wu-zhu-ce-zhong-xin/consul/image-20210129161933194.png" alt="image-20210129161933194"></p><h2 id="Consul、ZK、Eureka的区别"><a href="#Consul、ZK、Eureka的区别" class="headerlink" title="Consul、ZK、Eureka的区别"></a>Consul、ZK、Eureka的区别</h2><table><thead><tr><th>名称</th><th>语言</th><th>CAP</th><th>服务健康检查</th><th>对外暴露接口</th><th>springcloud集成</th></tr></thead><tbody><tr><td>Eureka</td><td>Java</td><td>AP</td><td>可配支持</td><td>HTTP</td><td>已集成</td></tr><tr><td>Consul</td><td>Go</td><td>CP</td><td>支持</td><td>HTTP&#x2F;DNS</td><td>已集成</td></tr><tr><td>Zookeeper</td><td>Java</td><td>CP</td><td>支持</td><td>客户端</td><td>已集成</td></tr></tbody></table><h3 id="CAP原则"><a href="#CAP原则" class="headerlink" title="CAP原则"></a>CAP原则</h3><p>CAP原则又称CAP定理，指的是在一个分布式系统中，<a href="https://baike.baidu.com/item/%E4%B8%80%E8%87%B4%E6%80%A7/9840083">一致性</a>（Consistency）、<a href="https://baike.baidu.com/item/%E5%8F%AF%E7%94%A8%E6%80%A7/109628">可用性</a>（Availability）、<a href="https://baike.baidu.com/item/%E5%88%86%E5%8C%BA%E5%AE%B9%E9%94%99%E6%80%A7/23734073">分区容错性</a>（Partition tolerance）。CAP 原则指的是，这三个<a href="https://baike.baidu.com/item/%E8%A6%81%E7%B4%A0/5261200">要素</a>最多只能同时实现两点，不可能三者兼顾。</p><p>简单理解：</p><p><code>AP</code> 牺牲了一致性实现高可用，可以允许一定范围的数据不一致</p><p><code>CP</code> 保证了一致性，如果不一致就会删除服务，并返回错误信息</p>]]></content>
      
      
      <categories>
          
          <category> Spring Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring Cloud </tag>
            
            <tag> Spring Consul </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring项目构建流程</title>
      <link href="/java/spring/spring-xiang-mu-gou-jian-liu-cheng/"/>
      <url>/java/spring/spring-xiang-mu-gou-jian-liu-cheng/</url>
      
        <content type="html"><![CDATA[<h2 id="Spring-Cloud项目环境搭建流程"><a href="#Spring-Cloud项目环境搭建流程" class="headerlink" title="Spring Cloud项目环境搭建流程"></a>Spring Cloud项目环境搭建流程</h2><h3 id="1、建module"><a href="#1、建module" class="headerlink" title="1、建module"></a>1、建module</h3><h3 id="2、修改pom"><a href="#2、修改pom" class="headerlink" title="2、修改pom"></a>2、修改pom</h3><h3 id="3、写yml"><a href="#3、写yml" class="headerlink" title="3、写yml"></a>3、写yml</h3><p>更改配置，编码字符集，开启注解支持(如果还是不行那就需要安装lombak的插件)，文件过滤，java版本选择，文件过滤后缀设置</p><ol><li>首先建立就一个Spring项目，写入pom文件，cloud和boot的版本要对应上。</li></ol><h3 id="Spring-Boot-x2F-Cloud-版本选择"><a href="#Spring-Boot-x2F-Cloud-版本选择" class="headerlink" title="Spring Boot&#x2F;Cloud 版本选择"></a>Spring Boot&#x2F;Cloud 版本选择</h3><p>Spring Boot 选择2.0以后的版本<br>Java8以上的版本支持，这个项目是SpringBoot 2.3.4</p><p>Spring版本是 <code>A~I开头</code> 的，伦敦地铁站的名称，这个项目使用的是H版本。</p><p><img src="/java/spring/spring-xiang-mu-gou-jian-liu-cheng/image-20210125092647507-1611574568377.png" alt="image-20210125092647507"></p><p>在建立完成后，删除他们src和.mvn目录，resource也不用，只保留pom文件</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;    &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;!--这里只有父类设置成pom，默认是jar，还有war--&gt;    &lt;!--pom 项目里没有java代码，也不执行任何代码，只是为了聚合工程或传递依赖用的。--&gt;    &lt;!--jar 内部调用或者是作服务使用，默认的类型--&gt;    &lt;!--war 部署项目--&gt;    &lt;packaging&gt;pom&lt;/packaging&gt;     &lt;modules&gt;        &lt;module&gt;cloud-provider-payment8001&lt;/module&gt;        &lt;module&gt;eurekaexercise&lt;/module&gt;        &lt;module&gt;cloud-consumer-order80&lt;/module&gt;        &lt;module&gt;cloud-api-commons&lt;/module&gt;    &lt;/modules&gt;    &lt;!--统一管理jar包版本--&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;12&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;12&lt;/maven.compiler.target&gt;        &lt;spring.cloud.version&gt;Hoxton.SR1&lt;/spring.cloud.version&gt;        &lt;spring.boot.version&gt;2.2.2.RELEASE&lt;/spring.boot.version&gt;        &lt;junit.version&gt;4.12&lt;/junit.version&gt;        &lt;lombok.version&gt;1.18.10&lt;/lombok.version&gt;        &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt;        &lt;mysql.version&gt;8.0.18&lt;/mysql.version&gt;        &lt;druid.version&gt;1.1.16&lt;/druid.version&gt;        &lt;mybatis.spring.boot.version&gt;2.1.1&lt;/mybatis.spring.boot.version&gt;    &lt;/properties&gt;    &lt;!--版本管理器，并不会真正引入jar包，子模块引用jar包时不需要指定版本号--&gt;    &lt;dependencyManagement&gt;        &lt;dependencies&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-project-info-reports-plugin&lt;/artifactId&gt;                &lt;version&gt;3.0.0&lt;/version&gt;            &lt;/dependency&gt;            &lt;!--spring boot 2.2.2--&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;                &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt;                &lt;version&gt;$&#123;spring.boot.version&#125;&lt;/version&gt;                &lt;type&gt;pom&lt;/type&gt;                &lt;scope&gt;import&lt;/scope&gt;            &lt;/dependency&gt;            &lt;!--spring cloud Hoxton.SR1--&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;                &lt;version&gt;$&#123;spring.cloud.version&#125;&lt;/version&gt;                &lt;type&gt;pom&lt;/type&gt;                &lt;scope&gt;import&lt;/scope&gt;            &lt;/dependency&gt;            &lt;!--spring cloud 阿里巴巴--&gt;            &lt;dependency&gt;                &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;                &lt;version&gt;2.1.0.RELEASE&lt;/version&gt;                &lt;type&gt;pom&lt;/type&gt;                &lt;scope&gt;import&lt;/scope&gt;            &lt;/dependency&gt;            &lt;!--mysql--&gt;            &lt;dependency&gt;                &lt;groupId&gt;mysql&lt;/groupId&gt;                &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;                &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt;                &lt;scope&gt;runtime&lt;/scope&gt;            &lt;/dependency&gt;            &lt;!-- druid--&gt;            &lt;dependency&gt;                &lt;groupId&gt;com.alibaba&lt;/groupId&gt;                &lt;artifactId&gt;druid&lt;/artifactId&gt;                &lt;version&gt;$&#123;druid.version&#125;&lt;/version&gt;            &lt;/dependency&gt;            &lt;!--mybatis--&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt;                &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;                &lt;version&gt;$&#123;mybatis.spring.boot.version&#125;&lt;/version&gt;            &lt;/dependency&gt;            &lt;!--junit--&gt;            &lt;dependency&gt;                &lt;groupId&gt;junit&lt;/groupId&gt;                &lt;artifactId&gt;junit&lt;/artifactId&gt;                &lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt;            &lt;/dependency&gt;            &lt;!--log4j--&gt;            &lt;dependency&gt;                &lt;groupId&gt;log4j&lt;/groupId&gt;                &lt;artifactId&gt;log4j&lt;/artifactId&gt;                &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt;            &lt;/dependency&gt;        &lt;/dependencies&gt;    &lt;/dependencyManagement&gt;&lt;/project&gt;</code></pre><p>父项目pom文件配置</p><ol><li>packaging设置成pom，来让子项目继承，在打包项目的时候将会见子模块包含到需要的项目中。不要使用jar</li></ol><pre><code>    &lt;packaging&gt;pom&lt;/packaging&gt;</code></pre><p><img src="/java/spring/spring-xiang-mu-gou-jian-liu-cheng/image-20210125101650137.png" alt="image-20210125101650137"></p><ol start="2"><li>删除src文件夹等</li><li>在父项目中配置你的版本号和插件</li></ol><pre><code class="xml">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;    &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;packaging&gt;pom&lt;/packaging&gt;    &lt;modules&gt;        &lt;module&gt;cloud-provider-payment8001&lt;/module&gt;        &lt;module&gt;eurekaexercise&lt;/module&gt;        &lt;module&gt;cloud-consumer-order80&lt;/module&gt;        &lt;module&gt;cloud-api-commons&lt;/module&gt;    &lt;/modules&gt;    &lt;!--统一管理jar包版本--&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;12&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;12&lt;/maven.compiler.target&gt;        &lt;spring.cloud.version&gt;Hoxton.SR1&lt;/spring.cloud.version&gt;        &lt;spring.boot.version&gt;2.2.2.RELEASE&lt;/spring.boot.version&gt;        &lt;junit.version&gt;4.12&lt;/junit.version&gt;        &lt;lombok.version&gt;1.18.10&lt;/lombok.version&gt;        &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt;        &lt;mysql.version&gt;8.0.18&lt;/mysql.version&gt;        &lt;druid.version&gt;1.1.16&lt;/druid.version&gt;        &lt;mybatis.spring.boot.version&gt;2.1.1&lt;/mybatis.spring.boot.version&gt;    &lt;/properties&gt;    &lt;!--maven的版本管理器，让子项目引用依赖时不用在指定版本号，只是声明，并不会引入--&gt;    &lt;dependencyManagement&gt;        &lt;dependencies&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-project-info-reports-plugin&lt;/artifactId&gt;                &lt;version&gt;3.0.0&lt;/version&gt;            &lt;/dependency&gt;            &lt;!--spring boot 2.2.2--&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;                &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt;                &lt;version&gt;$&#123;spring.boot.version&#125;&lt;/version&gt;                &lt;type&gt;pom&lt;/type&gt;                &lt;scope&gt;import&lt;/scope&gt;            &lt;/dependency&gt;            &lt;!--spring cloud Hoxton.SR1--&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;                &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;                &lt;version&gt;$&#123;spring.cloud.version&#125;&lt;/version&gt;                &lt;type&gt;pom&lt;/type&gt;                &lt;scope&gt;import&lt;/scope&gt;            &lt;/dependency&gt;            &lt;!--spring cloud 阿里巴巴--&gt;            &lt;dependency&gt;                &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;                &lt;version&gt;2.1.0.RELEASE&lt;/version&gt;                &lt;type&gt;pom&lt;/type&gt;                &lt;scope&gt;import&lt;/scope&gt;            &lt;/dependency&gt;            &lt;!--mysql--&gt;            &lt;dependency&gt;                &lt;groupId&gt;mysql&lt;/groupId&gt;                &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;                &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt;                &lt;scope&gt;runtime&lt;/scope&gt;            &lt;/dependency&gt;            &lt;!-- druid--&gt;            &lt;dependency&gt;                &lt;groupId&gt;com.alibaba&lt;/groupId&gt;                &lt;artifactId&gt;druid&lt;/artifactId&gt;                &lt;version&gt;$&#123;druid.version&#125;&lt;/version&gt;            &lt;/dependency&gt;            &lt;!--mybatis--&gt;            &lt;dependency&gt;                &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt;                &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt;                &lt;version&gt;$&#123;mybatis.spring.boot.version&#125;&lt;/version&gt;            &lt;/dependency&gt;            &lt;!--junit--&gt;            &lt;dependency&gt;                &lt;groupId&gt;junit&lt;/groupId&gt;                &lt;artifactId&gt;junit&lt;/artifactId&gt;                &lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt;            &lt;/dependency&gt;            &lt;!--log4j--&gt;            &lt;dependency&gt;                &lt;groupId&gt;log4j&lt;/groupId&gt;                &lt;artifactId&gt;log4j&lt;/artifactId&gt;                &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt;            &lt;/dependency&gt;        &lt;/dependencies&gt;    &lt;/dependencyManagement&gt;&lt;/project&gt;</code></pre><p><img src="/java/spring/spring-xiang-mu-gou-jian-liu-cheng/image-20210125093742067.png" alt="image-20210125093742067"></p><h2 id="开启热部署功能"><a href="#开启热部署功能" class="headerlink" title="开启热部署功能"></a>开启热部署功能</h2><h3 id="1、导入devtools的包"><a href="#1、导入devtools的包" class="headerlink" title="1、导入devtools的包"></a>1、导入devtools的包</h3><pre><code>        &lt;!--热部署--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;</code></pre><h3 id="2、进行idea设置"><a href="#2、进行idea设置" class="headerlink" title="2、进行idea设置"></a>2、进行idea设置</h3><p><img src="/java/spring/spring-xiang-mu-gou-jian-liu-cheng/image-20210125142842401.png" alt="image-20210125142842401"></p><p>开启改变项目时重构项目</p><h3 id="3、配置idea"><a href="#3、配置idea" class="headerlink" title="3、配置idea"></a>3、配置idea</h3><p>ctrl+alt+shift+&#x2F;</p><p><img src="/java/spring/spring-xiang-mu-gou-jian-liu-cheng/image-20210125143309330.png" alt="image-20210125143309330"></p><p><img src="/java/spring/spring-xiang-mu-gou-jian-liu-cheng/image-20210125143324213.png" alt="image-20210125143324213"></p>]]></content>
      
      
      <categories>
          
          <category> Spring </category>
          
          <category> Spring Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring Cloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Cloud组件说明</title>
      <link href="/java/spring/spring-cloud/spring-cloud-zu-jian-shuo-ming/"/>
      <url>/java/spring/spring-cloud/spring-cloud-zu-jian-shuo-ming/</url>
      
        <content type="html"><![CDATA[<p>官方文档：<a href="https://spring.io/projects/spring-cloud">https://spring.io/projects/spring-cloud</a></p><h2 id="什么是Spring-Cloud？"><a href="#什么是Spring-Cloud？" class="headerlink" title="什么是Spring Cloud？"></a>什么是Spring Cloud？</h2><p>Spring Cloud为开发人员提供了工具，以快速构建分布式系统中的某些常见模式（例如，配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁，领导选举，分布式会话，群集状态）。分布式系统的协调导致样板式样，并且使用Spring Cloud开发人员可以快速站起来实现这些样板的服务和应用程序。它们可以在任何分布式环境中正常工作，包括开发人员自己的笔记本电脑，裸机数据中心以及Cloud Foundry等托管平台。</p><h2 id="项目服务组件"><a href="#项目服务组件" class="headerlink" title="项目服务组件"></a>项目服务组件</h2><p>Spring Cloud专注于为典型的用例和可扩展性机制（包括其他用例）提供良好的开箱即用体验。</p><ul><li>分布式&#x2F;版本化配置</li><li>服务注册和发现</li><li>路由</li><li>服务到服务的呼叫</li><li>负载均衡</li><li>断路器</li><li>全局锁</li><li>领导选举和集群状态</li><li>分布式消息传递</li></ul><h2 id="Spring-Cloud-的版本"><a href="#Spring-Cloud-的版本" class="headerlink" title="Spring Cloud 的版本"></a>Spring Cloud 的版本</h2><p>Spring Cloud 的版本号并不是我们通常见的数字版本号，而是一些很奇怪的单词。这些单词均为英国伦敦地铁站的站名。同时根据字母表的顺序来对应版本时间顺序，比如：最早 的 Release 版本 Angel，第二个 Release 版本 Brixton（英国地名），然后是 Camden、 Dalston、Edgware、Finchley、Greenwich、Hoxton。</p><p>SpringCloud和SpringBoot的对应版本在官方文档有</p><h2 id="Spring-Cloud-的服务发现框架"><a href="#Spring-Cloud-的服务发现框架" class="headerlink" title="Spring Cloud 的服务发现框架"></a>Spring Cloud 的服务发现框架</h2><p><strong>服务发现</strong>：其实就是一个“中介”，整个过程中有三个角色：**服务提供者(出租房子的)、服务消费者(租客)、服务中介(房屋中介)**。</p><p><strong>服务提供者</strong>： 就是提供一些自己能够执行的一些服务给外界。</p><p><strong>服务消费者</strong>： 就是需要使用一些服务的“用户”。</p><p><strong>服务中介</strong>： 其实就是服务提供者和服务消费者之间的“桥梁”，服务提供者可以把自己注册到服务中介那里，而服务消费者如需要消费一些服务(使用一些功能)就可以在服务中介中寻找注册在服务中介的服务提供者。</p><h3 id="1、服务注册中心"><a href="#1、服务注册中心" class="headerlink" title="1、服务注册中心"></a>1、服务注册中心</h3><h4 id="Eureka"><a href="#Eureka" class="headerlink" title="Eureka"></a>Eureka</h4><p>Eureka官网上已经停止了更新原来的架构，可以替换成zookeeper、Consul。</p><p>主管服务注册的组件。</p><h4 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h4><p>可以在技术不发生大改变的情况下替换掉Eureka</p><h4 id="Consul"><a href="#Consul" class="headerlink" title="Consul"></a>Consul</h4><p>也可以替换掉EureKa，使用go语言编写的，但是不建议使用</p><h4 id="Nacos"><a href="#Nacos" class="headerlink" title="Nacos"></a>Nacos</h4><p>阿里巴巴旗下的产品，替换Eureka的主要组件。</p><h3 id="2、服务负载和调用"><a href="#2、服务负载和调用" class="headerlink" title="2、服务负载和调用"></a>2、服务负载和调用</h3><h4 id="Ribbon"><a href="#Ribbon" class="headerlink" title="Ribbon"></a>Ribbon</h4><p>Ribbon官方进入了维护状态，已经停止了更新。提供了负载均衡的组件</p><h4 id="LoadBalancer"><a href="#LoadBalancer" class="headerlink" title="LoadBalancer"></a>LoadBalancer</h4><p>LoadBalancer是未来替换Ribbon的组件，官方已经声明了，但是还没出来。</p><h3 id="3、服务调用2"><a href="#3、服务调用2" class="headerlink" title="3、服务调用2"></a>3、服务调用2</h3><h4 id="Feign"><a href="#Feign" class="headerlink" title="Feign"></a>Feign</h4><p>官方已经不更新了，而且技术上已经不在使用。</p><h4 id="OpenFeign"><a href="#OpenFeign" class="headerlink" title="OpenFeign"></a>OpenFeign</h4><p>openFeign是Spring自己的组件，替换掉了Feign。</p><h3 id="4、服务降级"><a href="#4、服务降级" class="headerlink" title="4、服务降级"></a>4、服务降级</h3><h4 id="Hystrix"><a href="#Hystrix" class="headerlink" title="Hystrix"></a>Hystrix</h4><p>Spring Cloud自带的服务降级和熔断框架，国内正在大规模使用，但是官网不建议使用，未来会被取代，不建议使用。</p><h4 id="resilience4j"><a href="#resilience4j" class="headerlink" title="resilience4j"></a>resilience4j</h4><p>国外用的多，国内用的很少</p><h4 id="Alibaba-Sentinel"><a href="#Alibaba-Sentinel" class="headerlink" title="Alibaba Sentinel"></a>Alibaba Sentinel</h4><p>阿里旗下的实现<code>熔断和限流</code>的组件，建议使用</p><h3 id="5、服务网关"><a href="#5、服务网关" class="headerlink" title="5、服务网关"></a>5、服务网关</h3><h4 id="Zuul"><a href="#Zuul" class="headerlink" title="Zuul"></a>Zuul</h4><p>网关来进行服务的协调，但是这个组件已经不再使用</p><h4 id="gateway"><a href="#gateway" class="headerlink" title="gateway"></a>gateway</h4><p>Spring Cloud 推荐的网关服务组件，建议使用。着重的接触这个组件</p><h3 id="6、服务配置"><a href="#6、服务配置" class="headerlink" title="6、服务配置"></a>6、服务配置</h3><h4 id="Config"><a href="#Config" class="headerlink" title="Config"></a><a href="https://spring.io/projects/spring-cloud-config">Config</a></h4><p>之前使用的是Config，现在不建议使用了</p><p>Spring Cloud Config为分布式系统中的外部化配置提供服务器和客户端支持。使用Config Server，您可以集中管理所有环境中应用程序的外部属性。客户端和服务器上的概念与SpringEnvironment和PropertySource抽象，因此它们非常适合Spring应用程序，但可以与以任何语言运行的任何应用程序一起使用。当应用程序从开发人员迁移到测试人员并进入生产过程时，您可以管理这些环境之间的配置，并确保应用程序具有迁移时所需的一切。服务器存储后端的默认实现使用git，因此它轻松支持配置环境的标记版本，并且可以通过各种工具来访问这些内容来管理内容。添加替代实现并将其插入Spring配置很容易。</p><h4 id="apollo"><a href="#apollo" class="headerlink" title="apollo"></a>apollo</h4><p>不推荐使用，使用的很少</p><h4 id="Nacos-1"><a href="#Nacos-1" class="headerlink" title="Nacos"></a>Nacos</h4><p>阿里旗下的服务配置组件，后来居上，这里主要使用的组件</p><h3 id="7、服务总线"><a href="#7、服务总线" class="headerlink" title="7、服务总线"></a>7、服务总线</h3><h4 id="Bus"><a href="#Bus" class="headerlink" title="Bus"></a><a href="https://spring.io/projects/spring-cloud-bus">Bus</a></h4><p>Spring Cloud原生的组件，可以使用</p><h4 id="Nacos-2"><a href="#Nacos-2" class="headerlink" title="Nacos"></a>Nacos</h4><p>阿里巴巴旗下的，建议使用。Nacos有服务注册中心，服务配置和服务总线组建的合成</p><h3 id="Spring-Cloud-Netflix"><a href="#Spring-Cloud-Netflix" class="headerlink" title="Spring Cloud Netflix"></a><a href="https://spring.io/projects/spring-cloud-netflix">Spring Cloud Netflix</a></h3><p>与各种Netflix OSS组件（Eureka，Hystrix，Zuul，Archaius等）集成。</p><h3 id="Spring-Cloud-Bus"><a href="#Spring-Cloud-Bus" class="headerlink" title="Spring Cloud Bus"></a><a href="https://spring.io/projects/spring-cloud-bus">Spring Cloud Bus</a></h3>]]></content>
      
      
      <categories>
          
          <category> Spring </category>
          
          <category> Spring Cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring Cloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Alibaba概述</title>
      <link href="/java/spring/spring-cloud/spring-cloud-alibaba/spring-cloud-alibaba/"/>
      <url>/java/spring/spring-cloud/spring-cloud-alibaba/spring-cloud-alibaba/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是Spring-cloud-Alibaba？"><a href="#什么是Spring-cloud-Alibaba？" class="headerlink" title="什么是Spring cloud Alibaba？"></a>什么是Spring cloud Alibaba？</h2><p>Spring Netflix 的项目进入到了维护模式。不会再添加新功能。</p><p>Spring Alibaba 是来代替他们组件的一个框架。</p><h2 id="主要功能："><a href="#主要功能：" class="headerlink" title="主要功能："></a>主要功能：</h2><ul><li><strong>服务限流降级</strong>：默认支持 WebServlet、WebFlux, OpenFeign、RestTemplate、Spring Cloud Gateway, Zuul, Dubbo 和 RocketMQ 限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级 Metrics 监控。</li><li><strong>服务注册与发现</strong>：适配 Spring Cloud 服务注册与发现标准，默认集成了 Ribbon 的支持。</li><li><strong>分布式配置管理</strong>：支持分布式系统中的外部化配置，配置更改时自动刷新。</li><li><strong>消息驱动能力</strong>：基于 Spring Cloud Stream 为微服务应用构建消息驱动能力。</li><li><strong>分布式事务</strong>：使用 @GlobalTransactional 注解， 高效并且对业务零侵入地解决分布式事务问题。</li><li><strong>阿里云对象存储</strong>：阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据。</li><li><strong>分布式任务调度</strong>：提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。同时提供分布式的任务执行模型，如网格任务。网格任务支持海量子任务均匀分配到所有 Worker（schedulerx-client）上执行。</li></ul><h2 id="Spring-cloud-Alibaba的使用"><a href="#Spring-cloud-Alibaba的使用" class="headerlink" title="Spring cloud Alibaba的使用"></a>Spring cloud Alibaba的使用</h2><ol><li>pom文件引入依赖</li></ol><p>这里的Spring boot版本是<code>2.2.2.RELEASE</code></p><pre><code class="xml">            &lt;!--spring cloud 阿里巴巴--&gt;            &lt;dependency&gt;                &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;                &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;                &lt;version&gt;2.1.0.RELEASE&lt;/version&gt;                &lt;type&gt;pom&lt;/type&gt;                &lt;scope&gt;import&lt;/scope&gt;            &lt;/dependency&gt;</code></pre><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-cloud-alibaba/image-20210209105601706.png" alt="与Spring Boot版本对应关系图"></p><h2 id="注册中心各个特性"><a href="#注册中心各个特性" class="headerlink" title="注册中心各个特性"></a>注册中心各个特性</h2><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-cloud-alibaba/image-20210210080806682.png" alt="注册中心特性对比图"></p><h3 id="选择模式使用："><a href="#选择模式使用：" class="headerlink" title="选择模式使用："></a>选择模式使用：</h3><p><strong>重点</strong>：Nacos支持AP和CP特性</p><p>一般来说不需要要存储服务级别的信息且服务实例是通过nacos-client注册，并能保持心跳的上报，选择AP。Spring Cloud和Dubbo都是用的AP模式。伪列服务的可用性减弱了一致性，AP模式下仅支持注册临时实例</p><p>需要在服务级别编辑或者存储配置信息，那么CP是必须，K8S和DNS服务是CP模式。CP模式支持持久化注册，以Raft协议为集群运行模式，该模式下实力必须先注册服务，服务如果不存在，则返回错误</p>]]></content>
      
      
      <categories>
          
          <category> Spring </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring Alibaba </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Sentinel</title>
      <link href="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/"/>
      <url>/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/</url>
      
        <content type="html"><![CDATA[<h2 id="Spring-Sleuth"><a href="#Spring-Sleuth" class="headerlink" title="Spring Sleuth"></a>Spring Sleuth</h2><h2 id="Sleuth-是什么？"><a href="#Sleuth-是什么？" class="headerlink" title="Sleuth 是什么？"></a>Sleuth 是什么？</h2><p>在微服务框架中，一个由客户端发起的请求在后端系统中会经过多个不同的服务节点调用来协同产生最后的请求结果，每一个客户端请求都会形成一条复杂的分布式服务服务调用链路，链路中的任何一环出现高延迟或错误都会引起整个请求最后的失败。</p><p>  微服务跟踪(sleuth)其实是一个工具,它在整个分布式系统中能跟踪一个用户请求的过程(包括数据采集，数据传输，数据存储，数据分析，数据可视化)，捕获这些跟踪数据，就能构建微服务的整个调用链的视图，这是调试和监控微服务的关键工具。</p><h3 id="Sleuth概念图"><a href="#Sleuth概念图" class="headerlink" title="Sleuth概念图"></a>Sleuth概念图</h3><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/640" alt="Sleuth架构图" style="zoom:150%;"><h3 id="Sentinel的组成"><a href="#Sentinel的组成" class="headerlink" title="Sentinel的组成"></a>Sentinel的组成</h3><p>Sentinel由两部分组成</p><p>前台和后台两部分，前台默认端口是8080端口号</p><ul><li>核心库（java客户端） 不依赖任何框架&#x2F;库，运行所有java运行时环境，同时对Dubbo&#x2F;Spring Cloud等框架也有较好的支持。</li><li>控制台（Dashboard）基于Spring Boot 开发，打包后可以直接运行，不需要额外的Tomcat等应用容器</li></ul><h3 id="Sentinel的使用"><a href="#Sentinel的使用" class="headerlink" title="Sentinel的使用"></a>Sentinel的使用</h3><ol><li>下载dashboard的jar包</li></ol><p><a href="https://github.com/alibaba/Sentinel/releases">https://github.com/alibaba/Sentinel/releases</a></p><ol start="2"><li>jar -jar 启动jar</li><li>访问页面</li><li>账号密码都是Sentinel</li></ol><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210212141953193.png" alt="sentinel监控页面"></p><h3 id="项目中的使用"><a href="#项目中的使用" class="headerlink" title="项目中的使用"></a>项目中的使用</h3><ol><li>建立module，修改pom文件</li></ol><p>主要导入依赖：nacos（3个）和sentinel（1个）的依赖</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;cloudalibaba-sentinel-service8401&lt;/artifactId&gt;    &lt;properties&gt;        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--   nacos config     --&gt;        &lt;!--        &lt;dependency&gt;--&gt;        &lt;!--            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;--&gt;        &lt;!--            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;--&gt;        &lt;!--        &lt;/dependency&gt;--&gt;        &lt;!--  SpringCloud alibaba nacos    --&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--    后续做持久化用到  P137课  --&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt;            &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--   sentinel     --&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--   openfeign     --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--  web组件      --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--    引入自定义的api通用包，可以使用Payment支付Entity    --&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.george.springcloud&lt;/groupId&gt;            &lt;artifactId&gt;cloud-api-commons&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><ol start="2"><li>修改yml文件</li></ol><pre><code class="yaml">server:  port: 8401spring:  application:    name: cloudalibaba-sentinel-service  cloud:    nacos:      discovery:        #Nacos注册中心地址        server-addr: localhost:8848    sentinel:      transport:        #配置Sentinel dashboard地址    8080监控8401        dashboard: localhost:8080        #默认8719端口,假如被占用会自动从8719开始依次+1扫描,直至找到未被占用的端口        port: 8719      datasource:        dsl:          nacos:            server-addr: localhost:8848            dataId: cloudalibaba-sentinel-service            groupId: DEFAULT_GROUP            data_type: json            rule-type: flowmanagement:  endpoints:    web:      exposure:        include: &quot;*&quot;</code></pre><ol start="3"><li>编写Controller</li></ol><pre><code class="java">package com.exercise.springcloud.controller;import com.alibaba.csp.sentinel.annotation.SentinelResource;import com.alibaba.csp.sentinel.slots.block.BlockException;import lombok.extern.slf4j.Slf4j;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;import java.util.concurrent.TimeUnit;@RestController@Slf4jpublic class FlowLimitController &#123;    @GetMapping(&quot;/testA&quot;)    public String testA() &#123;        try &#123;            TimeUnit.MILLISECONDS.sleep(800);        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;        return &quot;----------testA&quot;;    &#125;    @GetMapping(&quot;/testB&quot;)    public String testB() &#123;        log.info(&quot;----------testB:[&#123;&#125;]&quot;, Thread.currentThread().getName());        return &quot;----------testB&quot;;    &#125;    @GetMapping(&quot;/testD&quot;)    public String testD() &#123;        try &#123;            TimeUnit.SECONDS.sleep(1);        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;//        log.info(&quot;testD 测试RT&quot;);        log.info(&quot;testD 异常比例&quot;);        int age = 10 / 0;        return &quot;------testD&quot;;    &#125;    @GetMapping(&quot;/testE&quot;)    public String testE() &#123;        log.info(&quot;testE 测试异常数&quot;);        int age = 10 / 0;        return &quot;------testE 测试异常数&quot;;    &#125;    @GetMapping(&quot;/testHotKey&quot;)    @SentinelResource(value = &quot;testHotKey&quot;, blockHandler = &quot;deal_testHotKey&quot;)    public String testHotKey(@RequestParam(value = &quot;p1&quot;, required = false) String p1,                             @RequestParam(value = &quot;p2&quot;, required = false) String p2) &#123;        return &quot;-------testHotKey&quot;;    &#125;    public String deal_testHotKey(String p1, String p2,                                  BlockException exception) &#123;        return &quot;----deal_testHotKey,o(╥﹏╥)o&quot;;    &#125;&#125;</code></pre><ol start="4"><li>启动并查看监控页面</li></ol><p>注意事项：是懒加载模式的，需要先进行访问才能够出现监控结果</p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210212145213776.png" alt="控制台的监控页面"></p><h3 id="流控规则"><a href="#流控规则" class="headerlink" title="流控规则"></a>流控规则</h3><p>流量控制限制规则</p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210212151415939.png" alt="说明图"></p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210212150813328.png" alt="新增流控规则页面"></p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210212150857264.png" alt="QPS含义"></p><p>资源名称</p><p>这里设置阈值是1，代表了每秒只能访问一次，超过就会报错。</p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210212151030607.png" alt="报错界面"></p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210212155001505.png" alt="流量控制"></p><h4 id="warm-Up"><a href="#warm-Up" class="headerlink" title="warm Up"></a>warm Up</h4><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210212155256753.png" alt="warn up描述"></p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210212155504442.png" alt="公式计算"></p><p>就是允许刚开始3  在5秒中不断增加，到达阈值。这里的初始化3是默认值</p><h4 id="排队等待"><a href="#排队等待" class="headerlink" title="排队等待"></a>排队等待</h4><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210212160738204.png" alt="排队说明图"></p><h3 id="降级规则（熔断降级）"><a href="#降级规则（熔断降级）" class="headerlink" title="降级规则（熔断降级）"></a>降级规则（熔断降级）</h3><p>Sentinel 熔断降级会再调用链路中某个资源出现不稳定状态时（例如调用超时或异常比例升高），对这个资源的调用进行限制，让请求快速失败，避免影响到其他的资源而导致级联错误。</p><p>资源被降级后，接下来的降级时间窗口之内，对该资源的调用都自动熔断（默认行为时抛出 DegradeException）</p><h4 id="RT（慢调用比例）"><a href="#RT（慢调用比例）" class="headerlink" title="RT（慢调用比例）"></a>RT（慢调用比例）</h4><p>平均响应时间，超出阈值 且 在时间窗口内通过的请求数&gt;5 两个条件同时满足出发降级</p><p>窗口期过后关闭断路器</p><p>RT最大4900（更大的需要通过-Dcsp.sentinel.statistic.max.rt&#x3D;XXX 才生效）</p><h4 id="异常比例"><a href="#异常比例" class="headerlink" title="异常比例"></a>异常比例</h4><p>QPS&gt;&#x3D; 5 且  异常比例（秒级统计）超过阈值时，触发降级；时间窗口结束后，关闭降级</p><h4 id="异常数"><a href="#异常数" class="headerlink" title="异常数"></a>异常数</h4><p>异常数（分钟统计） 超过阈值时，触发降级；时间窗口结束后，关闭降级</p><h3 id="热点规则"><a href="#热点规则" class="headerlink" title="热点规则"></a>热点规则</h3><p>何为热点？热点即经常访问的数据。很多时候我们希望统计某个热点数据中访问频次最高的 Top K 数据，并对其访问进行限制。比如：</p><ul><li>商品 ID 为参数，统计一段时间内最常购买的商品 ID 并进行限制</li><li>用户 ID 为参数，针对一段时间内频繁访问的用户 ID 进行限制</li></ul><p>热点参数限流会统计传入参数中的热点参数，并根据配置的限流阈值与模式，对包含热点参数的资源调用进行限流。热点参数限流可以看做是一种特殊的流量控制，仅对包含热点参数的资源调用生效。</p><p>注意点：这里只能拦截配置的出错，如qps配置限制，对runtimeexerception</p><h4 id="热点key限流"><a href="#热点key限流" class="headerlink" title="热点key限流"></a>热点key限流</h4><ol><li>controller编写  <code>@SentinelResource</code>  注解使用</li></ol><pre><code class="java">    @GetMapping(&quot;/testHotKey&quot;)    //value 和getmapping一样作为表示，blockhandler 表明他的对应处理方法名称    @SentinelResource(value = &quot;testHotKey&quot;, blockHandler = &quot;deal_testHotKey&quot;)    public String testHotKey(@RequestParam(value = &quot;p1&quot;, required = false) String p1,                             @RequestParam(value = &quot;p2&quot;, required = false) String p2) &#123;        return &quot;-------testHotKey&quot;;    &#125;    public String deal_testHotKey(String p1, String p2,                                  BlockException exception) &#123;        return &quot;----deal_testHotKey,o(╥﹏╥)o&quot;;        //如果不指定方法就会返回异常到前台        // sentinel默认提示都是 Blocked by Sentinel (flow limiting),    &#125;</code></pre><ol start="2"><li><p>yml和pom文件都和上面一样</p></li><li><p>启动项目并指定配置</p></li></ol><p>注意事项：先访问在配置，sentinel  懒加载</p><p>只对指定的索引参数生效</p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210214095239833.png" alt="配置页面"></p><h4 id="指定参数key进行热点限流"><a href="#指定参数key进行热点限流" class="headerlink" title="指定参数key进行热点限流"></a>指定参数key进行热点限流</h4><p>下面的图只有参数0   是5的时候 qps200  其他参数都是1 </p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210214100540946.png" alt="高级选项"></p><h3 id="系统规则"><a href="#系统规则" class="headerlink" title="系统规则"></a>系统规则</h3><h4 id="系统规则添加"><a href="#系统规则添加" class="headerlink" title="系统规则添加"></a>系统规则添加</h4><p>不太使用，做了解</p><p>针对的是整个服务平台，不是某个接口或微服务</p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210214102228674.png" alt="系统规则页面"></p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210214102307160.png" alt="模式说明"></p><h2 id="什么是Zipkin"><a href="#什么是Zipkin" class="headerlink" title="什么是Zipkin"></a>什么是Zipkin</h2><p>Sleuth整合了Zipken</p><h3 id="Zipkin的使用"><a href="#Zipkin的使用" class="headerlink" title="Zipkin的使用"></a>Zipkin的使用</h3><p>Zipkin在Spring Cloud的F版本之后不需要自己来搭建服务端了，只需要下载对应的jar包运行就行。</p><ol><li>下载zipkin然后java -jar 运行</li></ol><pre><code>java -jar zipkin-server-2.12.9-exec.jar</code></pre><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210208193741568.png" alt="运行界面"></p><ol start="2"><li>pom依赖配置</li></ol><p>客户端和服务端都添加</p><pre><code class="xml">        &lt;!-- 包含了 sleuth zipkin 数据链路追踪--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;        &lt;/dependency&gt;</code></pre><ol start="3"><li>服务端yml文件配置</li></ol><p>主要配置： zipkin开始  datasource前面</p><pre><code class="yaml">#端口号配置server:  port: 8001#服务的名称spring:  application:    name: cloud-payment-service  #将数据放到zipkin上打印图标  zipkin:    base-url: http://localhost:9411  sleuth:    sampler:      # 0~1 1 表示全部采集      probability: 1  #jdbc配置  datasource:    url: jdbc:mysql://localhost:3306/exercise?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=UTC    # druid  德鲁伊的数据源    type: com.alibaba.druid.pool.DruidDataSource    # mysql驱动类    driver-class-name: com.mysql.cj.jdbc.Driver    username: root    password: 123456#指定map文件的位置mybatis:  mapper-locations: classpath*:mapper/*.xml  #指定实体包下面类的别名  type-aliases-package: com.exercise.springcloud.entitieseureka:  client:    #是否将自己注册到eureka    register-with-eureka: true    #是否获取eureka注册信息，默认为true，单节点无所谓，集群必须设置true才能够配合ribbon使用负载均衡    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka,http://localhost:7002/eureka  instance:    instance-id: payment8001    prefer-ip-address: true    # Eureka客户端向服务端发送心跳时间的间隔，单位为秒（默认是30秒）#    lease-renewal-interval-in-seconds: 1    # Eureka服务端在收到最后一次心跳后等待时间的一个上限，单位为秒（默认是90秒），超市将剔除服务#    lease-expiration-duration-in-seconds: 2</code></pre><ol start="4"><li>客户端yml配置</li></ol><pre><code class="yaml">server:  port: 80#服务的名称spring:  application:    name: cloud-order-service  #将数据放到zipkin上打印图标  zipkin:    base-url: http://localhost:9411  sleuth:    sampler:      # 0~1 1 表示全部采集      probability: 1eureka:  client:    #是否将自己注册到eureka    register-with-eureka: true    #是否获取eureka注册信息，默认为true，单节点无所谓，集群必须设置true才能够配合ribbon使用负载均衡    fetch-registry: true    service-url:      defaultZone: http://localhost:7001/eureka,http://localhost:7002/eureka</code></pre><ol start="5"><li>访问监控页面</li></ol><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210208182553239.png" alt="zipkin监控页面"></p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210208182703877.png" alt="结果查看"></p><p><img src="/java/spring/spring-cloud/spring-cloud-alibaba/spring-sleuth/image-20210208182744806.png" alt="任务查看"></p><p>可以查看具体请求的请求时间和调用关系，上方可以看到服务有两个，cloud-order-service调用的cloud-payment-service。</p>]]></content>
      
      
      <categories>
          
          <category> Spring </category>
          
          <category> Spring Sentinel </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring Sentinel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Bus</title>
      <link href="/java/spring/spring-cloud/fu-wu-zong-xian/bus/"/>
      <url>/java/spring/spring-cloud/fu-wu-zong-xian/bus/</url>
      
        <content type="html"><![CDATA[<h2 id="Bus是什么？"><a href="#Bus是什么？" class="headerlink" title="Bus是什么？"></a>Bus是什么？</h2><p>Spring cloud bus通过轻量消息代理连接各个分布的节点。这会用在广播状态的变化（例如配置变化）或者其他的消息指令。Spring bus的一个核心思想是通过分布式的启动器对spring boot应用进行扩展，也可以用来建立一个多个应用之间的通信频道。目前唯一实现的方式是用AMQP消息代理作为通道，同样特性的设置（有些取决于通道的设置）在更多通道的文档中。</p><p>这里使用Bus的主要目的是为了实现Config的动态刷新，在Spring cloud中使用Config一般都和Bus一起使用。支持两种消息代理，<strong>RabbitMQ和Kafka</strong> </p><h2 id="什么是消息总线？"><a href="#什么是消息总线？" class="headerlink" title="什么是消息总线？"></a>什么是消息总线？</h2><p><img src="/java/spring/spring-cloud/fu-wu-zong-xian/bus/1593395-20200413193843905-332447609.png" alt="img"></p><h2 id="两种设计思想"><a href="#两种设计思想" class="headerlink" title="两种设计思想"></a>两种设计思想</h2><ol><li>利用消息总线触发一个客户端&#x2F;bus&#x2F;refresh,而刷新所有客户端的配置</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-zong-xian/bus/1593395-20200413195047597-1819837405.png" alt="img"></p><ol start="2"><li>利用消息总线触发一个服务端ConfigServer的&#x2F;bus&#x2F;refresh端点,而刷新所有客户端的配置（更加推荐）</li></ol><p><img src="/java/spring/spring-cloud/fu-wu-zong-xian/bus/1593395-20200413195207019-240234639.png" alt="img"></p><p>消息中间件，整合java事件处理机制和消息中间件的功能，将服务端的数据放入到消息中间件中，客户端通过订阅的方式进行获取数据</p><h2 id="Bus的使用"><a href="#Bus的使用" class="headerlink" title="Bus的使用"></a>Bus的使用</h2><h2 id="全部动态刷新"><a href="#全部动态刷新" class="headerlink" title="全部动态刷新"></a>全部动态刷新</h2><h3 id="服务端配置"><a href="#服务端配置" class="headerlink" title="服务端配置"></a>服务端配置</h3><ol><li>pom文件</li></ol><p>这里使用的是rabbit消息中间件，bus包下面已经引入了，不用专门引入</p><p><img src="/java/spring/spring-cloud/fu-wu-zong-xian/bus/image-20210204104702966.png" alt="image-20210204104702966"></p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;cloud-config-center-3344&lt;/artifactId&gt;    &lt;properties&gt;        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--添加消息总线RbbitMQ支持--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-bus-amqp&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><ol start="2"><li>yml文件配置</li></ol><p>暴露bus节点和 配置rebbitMQ</p><blockquote><p>注意点：bus节点配置：include  <code>bus refresh</code> 是刷新操作</p><p>curl -POST </p></blockquote><pre><code class="yaml">server:  port: 3344spring:  application:    name: cloud-config-center  cloud:    config:      server:        git:          uri: https://gitee.com/zhangwehui/springcloud-config.git   #github仓库上面的git仓库名字          ##搜索目录   写具体的仓库名称          search-paths:            - springcloud-config          skip-ssl-validation: true          username: *************          password: *************      #读取分支      label: master  #rabbit相关配置  rabbitmq:    host: localhost    port: 5672    username: guest    password: guesteureka:  client:    service-url:      defaultZone: http://localhost:7001/eureka #注册进eureka##rabbitmq相关配置，暴露bus刷新配置的端点management:  endpoints:  #暴露bus刷新配置的端点    web:      exposure:        include: &#39;bus-refresh&#39;  #凡是暴露监控、刷新的都要有actuator依赖，bus-refresh就是actuator的刷新操作</code></pre><h3 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h3><p>两个客户端配置</p><ol><li>pom文件配置</li></ol><p>主要配置了 <code>spring-cloud-starter-bus-amqp</code> ，这个依赖服务端和客户端都一样引入</p><pre><code class="java">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;cloud2020&lt;/artifactId&gt;        &lt;groupId&gt;org.exercise.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;cloud-config-client-3355&lt;/artifactId&gt;    &lt;properties&gt;        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--添加消息总线rabbitMQ支持--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-bus-amqp&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;!--不带server了，说明是客户端--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;            &lt;scope&gt;runtime&lt;/scope&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><ol start="2"><li>配置yml</li></ol><p>只有客户端这么配置</p><pre><code class="yaml">server:  port: 3355spring:  application:    name: config-client  cloud:    #Config客户端配置    config:      label: master #分支名称      name: config #配置文件名称      profile: dev #读取后缀名称 上述3个综合：master分支上config-dev.yml的配置文件被读取 http://config-3344.com:3344/master/config-dev.yml      uri: http://localhost:3344    #配置中心地址  # rabbit 相关配置 15672是web管理界面的端口，5672是MQ访问的端口  rabbitmq:    host: localhost    port: 5672    username: guest    password: guest#服务注册到eureka地址eureka:  client:    service-url:      defaultZone: http://localhost:7001/eureka#暴露监控端点management:  endpoints:    web:      exposure:        include: &quot;*&quot;</code></pre><ol start="3"><li>业务类注解</li></ol><pre><code class="java">package com.exercise.springcloud.controller;import org.springframework.beans.factory.annotation.Value;import org.springframework.cloud.context.config.annotation.RefreshScope;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RefreshScopepublic class ConfigClientController &#123;    @Value(&quot;$&#123;server.port&#125;&quot;)    private String serverPort;    @Value((&quot;$&#123;config.info&#125;&quot;))    private String configInfo;    @GetMapping(&quot;/configInfo&quot;)    public String getConfigInfo() &#123;        return &quot;serverPort: &quot; + serverPort + &quot;\t\n\n configInfo: &quot; + configInfo;    &#125;&#125;</code></pre><ol start="4"><li>启动类配置</li></ol><pre><code class="java">@SpringBootApplication@EnableEurekaClientpublic class ConfigClientMain3366 &#123;    public static void main(String[] args) &#123;        SpringApplication.run(ConfigClientMain3366.class,args);    &#125;&#125;</code></pre><ol start="5"><li><p>配置完成后，启动rabbitMQ，然后启动服务。</p></li><li><p>查看结果</p></li></ol><p>在没有修改之前，所有的服务都是最新的3.</p><p>在修改之后，重新刷新，只有服务端是新的。</p><p>这时候需要发送命令</p><pre><code>//只用刷新服务端curl -X POST &quot;http://localhost:3344/actuator/bus-refresh&quot;</code></pre><h2 id="定点通知"><a href="#定点通知" class="headerlink" title="定点通知"></a>定点通知</h2><p>有多个微服务，但是只让其中的1个刷新配置</p><p>配置和上面一样，只是命令不一样</p><p>原来的命令后面加上   客户端微服务名称+端口号</p><pre><code>curl -X POST &quot;http://localhost:3344/actuator/bus-refresh/config-client:3355&quot;</code></pre><p><img src="/java/spring/spring-cloud/fu-wu-zong-xian/bus/image-20210204125301413.png" alt="image-20210204125301413"></p><p><img src="/java/spring/spring-cloud/fu-wu-zong-xian/bus/image-20210204125308836.png" alt="image-20210204125308836"></p><h2 id="Bus执行流程"><a href="#Bus执行流程" class="headerlink" title="Bus执行流程"></a>Bus执行流程</h2><p><img src="/java/spring/spring-cloud/fu-wu-zong-xian/bus/image-20210204125733110.png" alt="image-20210204125733110"></p>]]></content>
      
      
      <categories>
          
          <category> Spring </category>
          
          <category> Spring Bus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
            <tag> Spring Bus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mysql命令总结</title>
      <link href="/mysql/mysql-bu-chang-yong-ming-ling/"/>
      <url>/mysql/mysql-bu-chang-yong-ming-ling/</url>
      
        <content type="html"><![CDATA[<h1 id="Mysql不常用命令总结"><a href="#Mysql不常用命令总结" class="headerlink" title="Mysql不常用命令总结"></a>Mysql不常用命令总结</h1><p>本文总结的不是Mysql8的语法，都是mysql8.0以下版本语法</p><p>数据库操作分类：</p><pre><code>DDL:数据定义语言 用于创建和删除数据库对象等操作 (create drop alter)DML：数据操作语言 用来操作数据库中所包含的数据(insert update delete)DQL：数据查询语言 对数据库中的数据进行查询(select)DCL：数据控制语言 控制数据库组件的存取许可，存取权限等(grant revoke commit rollback)</code></pre><h2 id="mysql连接"><a href="#mysql连接" class="headerlink" title="mysql连接"></a>mysql连接</h2><pre><code> mysql -h110.110.110.110 -u root -p 123; mysql -h 节点 -u 用户 -p 密码</code></pre><h2 id="新增用户"><a href="#新增用户" class="headerlink" title="新增用户"></a>新增用户</h2><p>1、create user </p><pre><code>create user &#39;test&#39;@&#39;%&#39;identified by &#39;123456&#39;;flush privileges;//create user &#39;username&#39;@&#39;登录权限&#39; identified by &#39;password&#39;;create user &#39;用户名&#39;@&#39;指定的ip地址&#39; identified by &#39;密码&#39;;-&gt;用户只能在指定的ip地址登录 create user &#39;用户名&#39;@&#39;%&#39; identified by &#39;密码&#39;-&gt;用户只能在任意的ip地址登录</code></pre><h2 id="给用户授权"><a href="#给用户授权" class="headerlink" title="给用户授权:"></a>给用户授权:</h2><p>grant命令可以用来新增用户和修改用户密码、权限</p><p>下面的写法mysql8.0已经不支持了</p><pre><code>//第一种方式，部分授权的方式grant select,insert,update,delete on *.* to [email=test1@”%]test1@”%[/email]” Identified by “abc”;flush privileges;//第二种方式，授予全部的特权grant all privileges on *.* to &#39;test&#39;@&#39;%&#39; identified by &#39;root&#39; with grant option;flush privileges;//grant privileges select on 数据库.数据库对象 to 用户名@登录主机 identified by &#39;密码&#39;//如果不想用户有密码，可以将identified by &#39;&#39;设置为空来取消密码</code></pre><p>grant 参数说明：</p><pre><code>GRANT privilege [, ...] ON object [, ...] TO &#123; PUBLIC | GROUP group | username &#125;有privilege代表了全部的权限，在不写privilege的时候写指定的权限可能的权限有：SELECT访问声明的表/视图的所有列/字段．INSERT向声明的表中插入所有列字段．UPDATE更新声明的表所有列/字段．DELETE从声明的表中删除所有行．RULE在表/视图上定义规则 （参见 CREATE RULE 语句）．ALL赋予所有权限．object</code></pre><h2 id="回收权限"><a href="#回收权限" class="headerlink" title="回收权限:"></a>回收权限:</h2><pre><code>revoke 权限1,权限2,......权限n  on 数据库.* from  &#39;用户名&#39;@&#39;ip地址&#39;;revoke all on *.* from &#39;用户名&#39;@&#39;ip地址&#39;</code></pre><h2 id="查看用户权限"><a href="#查看用户权限" class="headerlink" title="查看用户权限:"></a>查看用户权限:</h2><pre><code>show grants for &#39;用户名&#39;@&#39;ip地址&#39;</code></pre><h2 id="删除用户"><a href="#删除用户" class="headerlink" title="删除用户"></a>删除用户</h2><p>1、delete </p><pre><code>delete from user where user=&#39;test&#39; and host=&#39;%&#39;;flush privileges;//delete from user where user=&#39;username&#39; and host=&#39;%&#39;</code></pre><p>2、drop user</p><pre><code>DROP USER &#39;test&#39;@&#39;localhost&#39;;flush privileges;//drop user &#39;username&#39;@&#39;登陆权限&#39;</code></pre><h2 id="查看当前登录的用户"><a href="#查看当前登录的用户" class="headerlink" title="查看当前登录的用户"></a>查看当前登录的用户</h2><pre><code>select user();</code></pre><h2 id="修改密码"><a href="#修改密码" class="headerlink" title="修改密码"></a>修改密码</h2><p>1、已过期方式：</p><p>下列命令在Mysql5.6.5中无法识别</p><pre><code>mysqladmin -u root -password ab12//第一次root没有密码，给root用户添加密码为ab12mysqladmin -u root -p ab12 password djg345//现在root有密码 修改密码为djg345//mysqladmin -u用户名 -p旧密码 password 新密码flush privileges;</code></pre><p>2、set password</p><pre><code>set password for &#39;test&#39;@&#39;%&#39; = &#39;123456&#39;//set password for 用户名@localhost = password(&#39;新密码&#39;);  //但是设置密码要求更新的密码是加密后的哈希值flush privileges;</code></pre><p>3、update 修改表</p><pre><code>update user set password=password(&#39;123456&#39;) where user=&#39;test&#39; and host=&#39;%&#39;;  flush privileges;//直接更新mysql数据库的表</code></pre><h2 id="修改表名"><a href="#修改表名" class="headerlink" title="修改表名"></a>修改表名</h2><p>当你执行 RENAME 时，你不能有任何锁定的表或活动的事务。你同样也必须有对原初表的 ALTER 和 DROP 权限，以及对新表的 CREATE 和 INSERT 权限。</p><p>RENAME TABLE 在 MySQL 3.23.23 中被加入。</p><p>1、alter table</p><pre><code>alter table ts01 rename to ts01_new; alter table ts01 rename as ts01_new; //修改表名的语法:alter table rename to/as new_tablename;</code></pre><p>2、rename table</p><pre><code>//rename table tableoldname to tablenewname;</code></pre><h2 id="设置字段默认值"><a href="#设置字段默认值" class="headerlink" title="设置字段默认值"></a>设置字段默认值</h2><pre><code>alter table 表名 add 字段 数据类型 default ‘默认值内容’</code></pre><h2 id="设置字段自增"><a href="#设置字段自增" class="headerlink" title="设置字段自增"></a>设置字段自增</h2><pre><code>alter table 表名 modify 字段 数据类型 auto_increment;</code></pre><h2 id="添加约束"><a href="#添加约束" class="headerlink" title="添加约束"></a>添加约束</h2><pre><code>alter table 表名 add constraint 约束名 约束类型 具体的约束说明ALTER TABLE 子表名称  ADD CONSTRAINT  约束名  Foreign  key(子表字段名称) references  主表名称(主键字段名称)主键 primary key 约束  pk_唯一 unique key 约束 UQ_默认 default key 约束 DF_检查 check key 约束 CK_外键 foreign key 约束 FK_CASCADE：父表delete、update的时候，子表会delete、update掉关联记录；SET NULL：父表delete、update的时候，子表会将关联记录的外键字段所在列设为null，所以注意在设计子表时外键不能设为not null；RESTRICT：如果想要删除父表的记录时，而在子表中有关联该父表的记录，则不允许删除父表中的记录；NO ACTION：同 RESTRICT，也是首先先检查外键；</code></pre><h3 id="添加非空约束"><a href="#添加非空约束" class="headerlink" title="添加非空约束"></a>添加非空约束</h3><blockquote><p>用not null约束的字段不能为null值，必须给定具体的数据</p></blockquote><pre><code>create table t_user(id int(10),name varchar(32) not null);</code></pre><h3 id="添加主键约束"><a href="#添加主键约束" class="headerlink" title="添加主键约束"></a>添加主键约束</h3><blockquote><p>给某个字段添加主键约束之后，该字段不能重复也不能为空，效果和”not null unique”约束相同，但是本质不同。</p></blockquote><blockquote><p>主键约束除了可以做到”not null unique”之外，还会默认添加”索引——index”</p></blockquote><ul><li>主键涉及术语<br>主键约束<br>主键字段<br>主键值</li></ul><pre><code>alter table 表名 add constraint 约束名 Primary key (字段名称);1. alter table 表名 add primary key(主键字段);2. alter table 表名 modify 主键字段 数据类型 primary key;3. alter table 表名 change 主键字段 主键字段 数据类型 primary key;添加复合主键alter table 表名 add primary key(主键字段1,主键字段2);删除主键alter table 表名 drop primary key;//主键 primary key 约束  pk_</code></pre><h3 id="添加唯一约束"><a href="#添加唯一约束" class="headerlink" title="添加唯一约束"></a>添加唯一约束</h3><blockquote><p>unique约束的字段，具有唯一性，不可重复，但可以为null</p></blockquote><pre><code>alter table 表名 add constraint 约束名 UNIQUE (字段名称);//唯一 unique key 约束 UQ_</code></pre><h3 id="添加检查约束"><a href="#添加检查约束" class="headerlink" title="添加检查约束"></a>添加检查约束</h3><pre><code>alter table 表名 add constraint 约束名 check (检查条件);</code></pre><h3 id="添加外键约束"><a href="#添加外键约束" class="headerlink" title="添加外键约束"></a>添加外键约束</h3><ul><li>外键涉及到的术语<br>外键约束<br>外键字段<br>外键值</li></ul><pre><code>alter table 表名 add constraint 约束名 foreign key (子字段名称)references 主表名称 (主键字段名称);</code></pre><h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><p> 事务特性：独立性(隔离性) 持久性 原子性 一致性</p><pre><code>开始事务beginstart transaction;提交事务commit 回滚(撤销)事务rollback;set autocommit = 0/1;0 关闭自动提交1 开启自动提交 关闭自动提交后，从下一条SQL语句开始则开启新事务，需使用COMMIT或ROLLBACK语句结束该事务</code></pre><h2 id="增加表中某一列"><a href="#增加表中某一列" class="headerlink" title="增加表中某一列"></a>增加表中某一列</h2><pre><code>alter table 表名 add 列名 列类型 列参数 [加的列在表的最后面]</code></pre><h2 id="备份数据库"><a href="#备份数据库" class="headerlink" title="备份数据库"></a>备份数据库</h2><p>Mysql中数据备份使用的命令是:****mysqldump*<em><strong>命令将数据库中的数据备份成一个</strong>文本文件</em>*。表的结构和表中的数据将存储在生成的文本文件中。</p><pre><code>#### 数据库中全局锁，存在以下问题：1.做了主从复制，读写分离。在主库上备份，那么备份期间都不能执行更新，业务完全停止。2.在从库上备份，备份期间从库不能同步主库的二进制文件(binlog)，会导致主从延迟。</code></pre><p>备份时开启全局锁，InnoDB引擎，在备份时可以加上参数–single-transaction 参数来完成不加锁的一致性数据备份。</p><pre><code class="sql">mysqldump --single-transaction -h 192.168.111.11 -u root -p 123456 test &gt;test.sql#快照的方式</code></pre><p>mysqldump不要再mysql中执行，要在外部执行</p><img src="/mysql/mysql-bu-chang-yong-ming-ling/image-20210119133604603.png" alt="image-20210119133604603" style="zoom:150%;"><p>1、导出一整个数据库</p><pre><code>mysqldump -u test -p test &gt; test.sql//mysqldump -u 用户名 -p 数据库名 &gt; 导出的文件名</code></pre><p>2、导出一个表</p><pre><code>mysqldump -u test -p testaa testtable &gt; test.sql//mysqldump -u 用户名 -p 数据库名 表名&gt; 导出的文件名</code></pre><p>3、备份多个数据库</p><pre><code>mysqldump -u root -h host -p --databases dbname1, dbname2 &gt; backdb.sql</code></pre><p>4、备份所有数据库</p><pre><code>mysqldump -u root -h host -p --all-databases &gt; backdb.sql</code></pre><h2 id="执行-sql文件"><a href="#执行-sql文件" class="headerlink" title="执行.sql文件"></a>执行.sql文件</h2><p>1、在外部使用</p><p><img src="/mysql/mysql-bu-chang-yong-ming-ling/image-20210119134733857.png" alt="image-20210119134733857"></p><pre><code>mysql -u root -p 123456 &lt; /test.sql如果出错有提示，不出错就什么也没有</code></pre><p>2、在Mysql命令行使用</p><p><img src="/mysql/mysql-bu-chang-yong-ming-ling/image-20210119134547594.png" alt="image-20210119134547594"></p><pre><code>source .sql文件地址</code></pre><h2 id="查看-ibd文件"><a href="#查看-ibd文件" class="headerlink" title="查看.ibd文件"></a>查看.ibd文件</h2><p>&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;itcast&#x2F;</p><pre><code class="mysql">ibd2sdi tablename.ibd</code></pre><h2 id="创建数据库规则"><a href="#创建数据库规则" class="headerlink" title="创建数据库规则"></a>创建数据库规则</h2><p> （Rule）规则 就是数据库中对存储在表的列或用户自定义数据类型中的值的规定和限制。规则是单独存储的独立的数据库对象。规则与其作用的表或用户自定义数据类型是相互独立的，即表或用户自定义对象的删除、修改不会对与之相连的规则产生影响。规则和约束可以同时使用，表的列可以有一个规则及多个CHECK 约束。规则与CHECK 约束很相似，相比之下，使用在ALTERTABLE 或CREATE TABLE 命令中的CHECK 约束是更标准的限制列值的方法，但CHECK 约束不能直接作用于用户自定义数据类型。</p><pre><code>CREATE RULE rule_name AS condition_expression其中condition_expression 子句是规则的定义。condition_expression 子句可以是能用于 WHERE 条件子句中的任何表达式，它可以包含算术运算符、关系运算符和谓词（如IN、LIKE、 BETWEEN 等）。注意：condition_expression子句中的表达式必须以字符“@”开头。</code></pre>]]></content>
      
      
      <categories>
          
          <category> Mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mysql优化</title>
      <link href="/mysql/mysql-de-you-hua/"/>
      <url>/mysql/mysql-de-you-hua/</url>
      
        <content type="html"><![CDATA[<h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><h2 id="SQL优化"><a href="#SQL优化" class="headerlink" title="SQL优化"></a>SQL优化</h2><h3 id="1、插入数据"><a href="#1、插入数据" class="headerlink" title="1、插入数据"></a>1、插入数据</h3><h4 id="insert优化"><a href="#insert优化" class="headerlink" title="insert优化"></a>insert优化</h4><h5 id="批量插入"><a href="#批量插入" class="headerlink" title="批量插入"></a>批量插入</h5><pre><code class="sql">insert into table_name values(value),(value)</code></pre><h5 id="大批量插入数据"><a href="#大批量插入数据" class="headerlink" title="大批量插入数据"></a>大批量插入数据</h5><p>一次性插入大批量数据，使用insert性能低，使用Mysql提供的load指令进行插入。</p><pre><code class="sql">#1.连接服务端时，加上参数 local-infile mysql --local-linfile -u root -p #设置全局参数local_infile为1，开启从本地加载文件导入数据的开关set global local_infile=1;#执行load指令将准备好的数据,加载到表结构中load data local infile &#39;root/sql1.log into table &#39;table_name&#39; fields terminated by &#39;,&#39; lines terminated by &#39;\n&#39;;</code></pre><h5 id="手动提交事务"><a href="#手动提交事务" class="headerlink" title="手动提交事务"></a>手动提交事务</h5><pre><code class="sql">start transaction;insert into table_name values(value),(value);insert into table_name values(value),(value);commit;</code></pre><h5 id="主键顺序插入"><a href="#主键顺序插入" class="headerlink" title="主键顺序插入"></a>主键顺序插入</h5><p>顺序插入数据性能高于乱序插入</p><pre><code>8 1 5 6 46  2 6  584 1 2 3 4 5 6 7 8 9 </code></pre><h3 id="2、主键优化"><a href="#2、主键优化" class="headerlink" title="2、主键优化"></a>2、主键优化</h3><p>数据组织方式</p><p>表数据根据主键顺序组织存放的，这种存储方式表成为<strong>索引组织表</strong></p><p><img src="/mysql/mysql-de-you-hua/image-20220214111245467.png" alt="image-20220214111245467"></p><h4 id="页分裂"><a href="#页分裂" class="headerlink" title="页分裂"></a>页分裂</h4><p>页可以为空，也可以填充一半，也可以填充100%，每个页包含了2-N行数据（如果一行数据多大，会行溢出），根据主键排序。</p><h5 id="主键乱序情况插入"><a href="#主键乱序情况插入" class="headerlink" title="主键乱序情况插入"></a>主键乱序情况插入</h5><p><img src="/mysql/mysql-de-you-hua/image-20220214111840241.png" alt="image-20220214111840241"></p><p>保证主键的顺序性，不会插入到新开的page，会插入到47后面</p><p><img src="/mysql/mysql-de-you-hua/image-20220214111903913.png" alt="image-20220214111903913"></p><p>要插入到47后面，但是没有空间可以插入</p><p><img src="/mysql/mysql-de-you-hua/image-20220214112014657.png" alt="image-20220214112014657"></p><p>会新建立一个page，并把原来需要插入的page进行拆分，变成两个，把后半部分移动到新开的page，并插入到他后面，但是这样的顺序破坏了有序性。</p><p><img src="/mysql/mysql-de-you-hua/image-20220214112231538.png" alt="image-20220214112231538"></p><p>为了保证有序性，把page的指针从新设置</p><p><img src="/mysql/mysql-de-you-hua/image-20220214112441931.png" alt="image-20220214112441931"></p><h4 id="页合并"><a href="#页合并" class="headerlink" title="页合并"></a>页合并</h4><h5 id="主键删除情况"><a href="#主键删除情况" class="headerlink" title="主键删除情况"></a>主键删除情况</h5><p>当删除一行数据时，并没有真正的物理删除，只是记录被标记为删除并且它的空间变得允许被其他记录声明使用</p><p>当页中删除的数据记录达到MERGE_THRESHOLD（合并页的阈值，可以自己设置，默认page50%）innoDB自动寻找靠近的页看看是否可以将两个页合并并优化空间使用</p><p><img src="/mysql/mysql-de-you-hua/image-20220214112950309.png" alt="image-20220214112950309"></p><p><img src="/mysql/mysql-de-you-hua/image-20220214113002845.png" alt="image-20220214113002845"></p><h4 id="主键设计原则"><a href="#主键设计原则" class="headerlink" title="主键设计原则"></a>主键设计原则</h4><ol><li>满足业务需求的情况下，尽量降低主键长度。</li></ol><p>主键长度很长，二级索引表存储着主键值，查询时会占用大量io。</p><ol start="2"><li>插入数据，尽量选择顺序插入，使用AUTO_INCREMENT自增主键，避免页分裂</li><li>尽量不使用用UUID做主键或者其他自然主键，如身份证。不仅长还没有顺序。</li><li>避免对主键的修改。</li></ol><h3 id="3、oreder-by-优化"><a href="#3、oreder-by-优化" class="headerlink" title="3、oreder by 优化"></a>3、oreder by 优化</h3><ol><li><p>建立合适的索引，多字段排序时，遵循最左前缀法则。</p></li><li><p>尽量使用覆盖索引。</p></li><li><p>多字段排序，创建联合索引时的规则。</p></li><li><p>出现filesort大数据量排序时，适当增大排序缓冲区的大小，sort_buffer_size默认大小256K，占满了就到磁盘中排序。</p></li></ol><p>优化时尽量优化到Using index，提高效率。</p><pre><code class="sql">create index index_name on table_name(a,b);#不指定默认都是正序索引explain select id,age,phone from tb_user order by age,phone;#全部走索引，正序，不触发filesortexplain select id,age,phone from tb_user order by age desc,phone desc;#全部倒序走索引，反向扫描，不会触发filesortexplain select id,age,phone from tb_user order by age asc,phone desc;#一个正一个倒，会出现filesort。可以单独创建一个索引：create index index_name on table_name(a asc,b desc);</code></pre><p><img src="/mysql/mysql-de-you-hua/image-20220214121840540.png" alt="image-20220214121840540"></p><h4 id="Using-filesort"><a href="#Using-filesort" class="headerlink" title="Using filesort"></a>Using filesort</h4><p>Using filesort:通过表的索引或者全表扫描，读取满足条件的数据行，在排序缓冲区sort buffer中完成排序操作，suo’you不是通过索引直接返回排序结果的排序都叫FileSort排序。</p><h4 id="Using-index"><a href="#Using-index" class="headerlink" title="Using index"></a>Using index</h4><p>Using index：通过有序的索引顺序扫描直接返回有序数据这种情况即为using index ，不需要额外排序，操作效率高。</p><h3 id="4、group-by-优化"><a href="#4、group-by-优化" class="headerlink" title="4、group by 优化"></a>4、group by 优化</h3><ol><li><p>分组操作时，通过索引来提高效率。</p></li><li><p>分组操作，索引的使用也是满足最左前缀法则的。</p></li></ol><h3 id="5、limit-优化"><a href="#5、limit-优化" class="headerlink" title="5、limit 优化"></a>5、limit 优化</h3><p>查询limit 20000000,10  先排序20000010条数据，在获取最后10条数据，其他数据全部丢弃，查询代价非常大。</p><ol><li>分页查询时，通过覆盖索引+子查询的方式进行优化。</li></ol><pre><code class="sql">explain select * from table_name a,(select id from table order by id limit 20000,10) b where a.id=b.id;#子查询走主键索引#外查询走聚集索引</code></pre><h3 id="6、count-优化"><a href="#6、count-优化" class="headerlink" title="6、count 优化"></a>6、count 优化</h3><p>MyISAM引擎把一个表的总行数存到了磁盘上，因此执行count（*）的时候会直接返回这个数，效率很高。</p><p>InnoDB引擎，执行count(*)时，也需要全量读取数据在累积计数。</p><ol><li>通过redis 自己记录行数。</li></ol><p>count(主键)：遍历整张表，取出所有主键id，返回给服务层。 拿到主键后进行累加。</p><p>count(字段值)：看是否有not null 约束，没有就把值都取出来，进行判断是否为Null，不为Null，计数累加。有约束，取值后直接累加不判断。</p><p>count(1) </p><p>会遍历整张表，不会取值。返回的每一行放一个1进去进行累加。</p><p>count(*)</p><p>InnoDB做了优化，不会取值，直接累加。</p><p>效率排名：count(1)≈count(*)&gt;count(主键id)&gt;count(字段)</p><h3 id="7、update-优化"><a href="#7、update-优化" class="headerlink" title="7、update 优化"></a>7、update 优化</h3><p>开启事务更新数据时，根据建立的索引字段作为条件进行更新。</p><p>主键id作为更新条件时，是row clock （行锁）。行锁是对索引加的锁。</p><p>其他没有索引的字段作为更新条件时，是table clock （表锁）。一旦锁表了并发性能就降低</p>]]></content>
      
      
      <categories>
          
          <category> Mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mysql存储引擎和索引</title>
      <link href="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/"/>
      <url>/mysql/mysql-cun-chu-yin-qing-he-suo-yin/</url>
      
        <content type="html"><![CDATA[<h1 id="Mysql"><a href="#Mysql" class="headerlink" title="Mysql"></a>Mysql</h1><h2 id="体系架构"><a href="#体系架构" class="headerlink" title="体系架构"></a>体系架构</h2><ol><li>连接层</li></ol><p>客户端的连接请求，进行授权，验证，校验权限等</p><ol start="2"><li>服务层</li></ol><p>SQL接口，解析器，优化器，缓存。</p><ol start="3"><li>引擎层</li></ol><p>索引在这一层实现，不同的存储引擎索引结构不同的，可插拔。</p><ol start="4"><li>存储层</li></ol><p>数据库索引和相关的日志文件存储在磁盘当中</p><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213132116941.png" alt="image-20220213132116941"></p><h2 id="存储引擎"><a href="#存储引擎" class="headerlink" title="存储引擎"></a>存储引擎</h2><h3 id="命令操作"><a href="#命令操作" class="headerlink" title="命令操作"></a>命令操作</h3><pre><code class="mysql">show engines;//创建表的时候指定搜索引擎create table xxxx(....) ENGINE=INNODB;</code></pre><h3 id="InnoDB"><a href="#InnoDB" class="headerlink" title="InnoDB"></a>InnoDB</h3><p>高可靠高性能的通用存储引擎，5.5版本以后是Mysql默认的存储引擎。</p><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><p>高可靠：DML遵循ACID，支持<strong>事务</strong>，<strong>行级锁</strong>，提高并发访问性能。</p><p>高性能：支持<strong>外键</strong> FOREIGN KEY 约束，保证数据完整性和正确性。</p><h4 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h4><p>表名.ibd:innoDB引擎每张表都会对应着这样的一个表空间，存储该表的表结构（frm，sdi）、数据和索引。</p><p><strong>sdi</strong>：表结构数据</p><p><strong>参数：</strong>innodb_file_per_table，默认一个表一个文件，这一通过这个参数设置。ibd包含sdi。</p><h4 id="逻辑存储结构"><a href="#逻辑存储结构" class="headerlink" title="逻辑存储结构"></a>逻辑存储结构</h4><h5 id="TableSpece：表空间"><a href="#TableSpece：表空间" class="headerlink" title="TableSpece：表空间"></a><strong>TableSpece：表空间</strong></h5><p>表空间对应一个.ibd文件，一个Mysql实例可以对应多个表空间，用于存储记录、索引等数据。</p><h5 id="Segment：段"><a href="#Segment：段" class="headerlink" title="Segment：段"></a><strong>Segment：段</strong></h5><p>段分为：数据段（Leaf node segment），索引段(Non-leaf node segment)，回滚段(Rollback segment)。InnoDB是索引组织表，数据段就是B+树的叶子节点，索引段即为B+树的非叶子节点。段用来管理多个Extent（区）</p><h5 id="Extent：区"><a href="#Extent：区" class="headerlink" title="Extent：区"></a><strong>Extent：区</strong></h5><p>区，表空间的单元结构，每个区的大小为1M。默认情况下，InnoDB存储引擎页大小为16K，及一个区中一共有64个连续的页。</p><h5 id="Page：页"><a href="#Page：页" class="headerlink" title="Page：页"></a><strong>Page：页</strong></h5><p>InnoDB存储引擎磁盘管理的最小单元，每个页大小默认为16k。为了保证页的连续性，InnoDB存储引擎每次从磁盘中申请4-5个区</p><h5 id="Row：行"><a href="#Row：行" class="headerlink" title="Row：行"></a><strong>Row：行</strong></h5><p>InnoDB存储引擎数据按行进行存放。</p><h6 id="隐藏列"><a href="#隐藏列" class="headerlink" title="隐藏列"></a>隐藏列</h6><p><strong>Trx_id</strong>：每次对某条记录进行改动时，都会把对应的事务id赋值给trx_id隐藏列</p><p><strong>Roll_pointer</strong>：每次对某条引记录进行改动时，都会把旧的版本写入到undo日志中，然后这个隐藏列就相当于一个指针，可以通过它来找到该纪录修改前的信息。</p><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213125740703.png" alt="image-20220213125740703"></p><h4 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h4><p>MySQL5.5版本开始，默认使用InnoDB存储引擎，他擅长事务处理，具有崩溃恢复特性，在日常开发中使用非常广泛。下面是InnoDB架构图</p><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215121459982.png" alt="image-20220215121459982"></p><h5 id="内存架构"><a href="#内存架构" class="headerlink" title="内存架构"></a>内存架构</h5><h6 id="Buffer-Pool"><a href="#Buffer-Pool" class="headerlink" title="Buffer Pool"></a>Buffer Pool</h6><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215123345293.png" alt="image-20220215123345293"></p><h6 id="Change-Buffer"><a href="#Change-Buffer" class="headerlink" title="Change Buffer"></a>Change Buffer</h6><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215131836758.png" alt="image-20220215131836758"></p><h6 id="Adaptive-Hash-Index"><a href="#Adaptive-Hash-Index" class="headerlink" title="Adaptive Hash Index"></a>Adaptive Hash Index</h6><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215132149314.png" alt="image-20220215132149314"></p><h6 id="Log-Buffer"><a href="#Log-Buffer" class="headerlink" title="Log Buffer"></a>Log Buffer</h6><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215132927978.png" alt="image-20220215132927978"></p><h5 id="磁盘架构"><a href="#磁盘架构" class="headerlink" title="磁盘架构"></a>磁盘架构</h5><h6 id="System-Tablespace"><a href="#System-Tablespace" class="headerlink" title="System Tablespace"></a>System Tablespace</h6><h6 id="File-Per-Table-Tablespaces"><a href="#File-Per-Table-Tablespaces" class="headerlink" title="File-Per-Table Tablespaces"></a>File-Per-Table Tablespaces</h6><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215133804910.png" alt="image-20220215133804910"></p><h6 id="Doublewrite-Buffer-Files"><a href="#Doublewrite-Buffer-Files" class="headerlink" title="Doublewrite Buffer Files"></a>Doublewrite Buffer Files</h6><h6 id="Redo-Log"><a href="#Redo-Log" class="headerlink" title="Redo Log"></a>Redo Log</h6><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215134750317.png" alt="image-20220215134750317"></p><h6 id="Undo-TableSpaces"><a href="#Undo-TableSpaces" class="headerlink" title="Undo TableSpaces"></a>Undo TableSpaces</h6><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215134308822.png" alt="image-20220215134308822"></p><h6 id="General-TableSapces"><a href="#General-TableSapces" class="headerlink" title="General TableSapces"></a>General TableSapces</h6><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215134210080.png" alt="image-20220215134210080"></p><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215134141368.png" alt="image-20220215134141368"></p><h6 id="Temporary-TableSpaces"><a href="#Temporary-TableSpaces" class="headerlink" title="Temporary TableSpaces"></a>Temporary TableSpaces</h6><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215134324260.png" alt="image-20220215134324260"></p><h5 id="后台线程"><a href="#后台线程" class="headerlink" title="后台线程"></a>后台线程</h5><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215135311705.png" alt="image-20220215135311705"></p><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220215135235624.png" alt="image-20220215135235624"></p><h4 id="事务原理"><a href="#事务原理" class="headerlink" title="事务原理"></a>事务原理</h4><h4 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h4><h3 id="MyISAM"><a href="#MyISAM" class="headerlink" title="MyISAM"></a>MyISAM</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>MyISAM是MySQL早期的默认存储引擎。</p><h4 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h4><p>不支持事务，不支持外键，支持表锁，不支持行锁，访问速度快。</p><h4 id="文件-1"><a href="#文件-1" class="headerlink" title="文件"></a>文件</h4><p><strong>.MYD</strong>：数据</p><p><strong>.MYI</strong>：索引</p><p><strong>.sdi</strong>：表结构：json格式的文本数据</p><h3 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h3><h4 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h4><p>存储内存中，只能临时表或者当缓存使用</p><h4 id="特点-2"><a href="#特点-2" class="headerlink" title="特点"></a>特点</h4><p>内存存放，hash索引</p><h4 id="文件-2"><a href="#文件-2" class="headerlink" title="文件"></a>文件</h4><p>.sdi：表结构信息</p><h3 id="引擎的特点和区别"><a href="#引擎的特点和区别" class="headerlink" title="引擎的特点和区别"></a>引擎的特点和区别</h3><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213130656498.png" alt="image-20220213130656498"></p><p>innoDB支持事务，MyISAM不支持</p><p>innoDB支持行锁，MyISAM支持行锁</p><p>innoDB支持外键，MyISAM不支持外键</p><h3 id="存储引擎的使用"><a href="#存储引擎的使用" class="headerlink" title="存储引擎的使用"></a>存储引擎的使用</h3><p>根据应用场景选择不同的存储引擎。</p><p>innoDB：事务完整性和并发条件下数据一致性。</p><p>MyISAM：读数据和插入数据为主，很少更新和删除，对事务的完整性、并发性要求不高。这种引擎被nosql数据库替代了，MongoDB数据库。</p><p>MEMORY：将所有数据保存在内存中，访问速度快。表大小有限制，太大的表无法存放内存并保证安全性。被Redis取代了。</p><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><h3 id="1、索引介绍"><a href="#1、索引介绍" class="headerlink" title="1、索引介绍"></a>1、索引介绍</h3><p>索引（index）<strong>帮助Mysql高效的获取数据</strong>的**数据结构(有序)**。在数据之外，数据库还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，在这些数据结构上实现高级查找算法，这种数据结构就是索引。</p><h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p><strong>优点</strong>：</p><ol><li>提高数据检索效率，降低数据库的IO成本</li><li>通过<strong>索引列</strong>对数据进行排序，降低数据排序的成本，降低CPU的消耗</li></ol><p><strong>缺点</strong>：</p><ol><li>索引列暂用额外磁盘空间。</li><li>索引 加快了查询的效率，也同时降低了更新表的效率，DML语句执行效率底下。</li></ol><h3 id="2、索引结构"><a href="#2、索引结构" class="headerlink" title="2、索引结构"></a>2、索引结构</h3><p>Mysql存储引擎和索引在存储引擎层实现，不同存储引擎不同的索引结构：</p><ol><li><p>B+</p><p>常见的索引类型，大部分存储引擎都支持B+树索引</p></li><li><p>Hash索引</p><p>底层数据结构使用Hash表实现的，不支持范围查询，只能精确匹配索引列查询才有效</p></li><li><p>R-Tree（空间索引）</p><p>是MyISAM引擎的一个特殊索引类型，主要用于地理空间数据类型，通常使用较少</p></li><li><p>Full-text</p><p>一种通过建立倒排索引，快速匹配文档的方式。类似lucene,Solr,ES</p></li></ol><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213140106827.png" alt="image-20220213140106827"></p><p> <img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213140204264.png" alt="image-20220213140204264"></p><h4 id="B-树索引"><a href="#B-树索引" class="headerlink" title="B+树索引"></a>B+树索引</h4><p>对经典的B+Tree进行优化。原来B+Tree的基础上，增加了指向相邻叶子结点的链表指针，就形成了带有顺序指针的B+Tree，提高了区间访问的性能</p><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213153919486.png" alt="image-20220213153919486"></p><h4 id="Hash索引"><a href="#Hash索引" class="headerlink" title="Hash索引"></a>Hash索引</h4><p>采用hash算法，将键值转换成新的hash值，映射到对应的槽位上，存储在hash表中。</p><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213154205123.png" alt="image-20220213154205123"></p><h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><ol><li>查询效率高，通常只需要一次检索就可以了，效率通常要高于B+Tree索引。如果hash碰撞了还需要检索链表</li></ol><h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ol><li><p>只能等值匹配，不支持范围查询。</p></li><li><p>无法利用索引完成排序操作</p></li><li><p>将两个或更多的键值映射到相同的hash槽位上，就产生了hash碰撞，可以通过链表来解决。</p></li></ol><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213154439404.png" alt="image-20220213154439404"></p><h3 id="3、索引分类"><a href="#3、索引分类" class="headerlink" title="3、索引分类"></a>3、索引分类</h3><h4 id="索引类型分类："><a href="#索引类型分类：" class="headerlink" title="索引类型分类："></a>索引类型分类：</h4><h5 id="1、主键索引"><a href="#1、主键索引" class="headerlink" title="1、主键索引"></a>1、主键索引</h5><h5 id="2、唯一索引"><a href="#2、唯一索引" class="headerlink" title="2、唯一索引"></a>2、唯一索引</h5><h5 id="3、常规索引"><a href="#3、常规索引" class="headerlink" title="3、常规索引"></a>3、常规索引</h5><h5 id="4、全文索引"><a href="#4、全文索引" class="headerlink" title="4、全文索引"></a>4、全文索引</h5><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213160216277.png" alt="image-20220213160216277"></p><h4 id="索引存储形式分类："><a href="#索引存储形式分类：" class="headerlink" title="索引存储形式分类："></a>索引存储形式分类：</h4><p>innoDB存储引擎中，根据索引的存储形式，又分为以下两种：</p><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213160654994.png" alt="image-20220213160654994"></p><h5 id="1、聚集索引："><a href="#1、聚集索引：" class="headerlink" title="1、聚集索引："></a>1、聚集索引：</h5><p>叶子节点下面挂着的是每一行的数据。</p><h6 id="规则定义："><a href="#规则定义：" class="headerlink" title="规则定义："></a>规则定义：</h6><p>如果有主键，主键索引就是聚集索引</p><p>如果没有主键，将唯一的UNIQUE索引作为聚集索引。</p><p>如果没有主键也没有唯一索引，InnoDB自动生成rowid作为隐藏的聚集索引。</p><h5 id="2、二级索引："><a href="#2、二级索引：" class="headerlink" title="2、二级索引："></a>2、二级索引：</h5><p>叶子节点下面数据是每行数据对应的id。</p><h6 id="规则定义：-1"><a href="#规则定义：-1" class="headerlink" title="规则定义："></a>规则定义：</h6><p>聚集索引之外的就是二级索引。</p><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213161510210.png" alt="image-20220213161510210"></p><h4 id="回表查询"><a href="#回表查询" class="headerlink" title="回表查询"></a>回表查询</h4><p>指的是查询数据的一种流程，查询时先找二级索引找到对应的主键值，再根据主键值到聚集索引中拿到具体的行数据。</p><h3 id="4、索引语法"><a href="#4、索引语法" class="headerlink" title="4、索引语法"></a>4、索引语法</h3><h4 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h4><p>一个索引关联单个字段，被称为单列索引。</p><p>一个索引关联两个或者多个字段，称为<strong>联合索引</strong>或者组合索引。</p><pre><code class="mysql">create [UNIQUE|FULLTEXT] INDEX index_name ON table_name(index_col_name,....)#一个索引可以关联多个字段#常规索引不需要指定关键字</code></pre><h4 id="查看索引"><a href="#查看索引" class="headerlink" title="查看索引"></a>查看索引</h4><pre><code class="mysql">show INDEX From table_name;</code></pre><h4 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h4><pre><code class="mysql">DROP INDEX index_name ON table_name;</code></pre><h3 id="5、SQL性能分析"><a href="#5、SQL性能分析" class="headerlink" title="5、SQL性能分析"></a>5、SQL性能分析</h3><p>主要优化的是select语句，查询时索引是重要的一部分。</p><h4 id="1、频率分析"><a href="#1、频率分析" class="headerlink" title="1、频率分析"></a>1、频率分析</h4><pre><code class="mysql">show [session|global] status;#查看当前数据库的insert，update，delete，select的访问频次。## session查看当前会话信息## global查看全局的信息实例：show global status like &#39;com_____&#39;;##like 中一个下划线代表一个字符。</code></pre><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213173044879.png" alt="image-20220213173044879" style="zoom:67%;"><h4 id="2、时间分析"><a href="#2、时间分析" class="headerlink" title="2、时间分析"></a>2、时间分析</h4><h5 id="慢查询日志"><a href="#慢查询日志" class="headerlink" title="慢查询日志"></a>慢查询日志</h5><p>慢查询日志中记录了执行时间超过了指定参数的Sql语句的日志。</p><p>Mysql的慢查询日志默认没有开启，需要在MySQL的配置文件 &#x2F;etc&#x2F;my.cnf 中配置</p><pre><code class="mysql">show variables like &#39;slow_query_log&#39;#查看慢查询日志开启状态</code></pre><p>vim &#x2F;etc&#x2F;my.cnf</p><pre><code class="shell">slow_query_log = 1;#开启慢查询日志功能long_query_time = 2;#设置慢查询日志的时间，这里设置为2秒</code></pre><p>配置完成后重新启动MySQL服务器，查看  &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;localhost-slow.log。只会记录超过时间的sql。</p><h5 id="profile详情"><a href="#profile详情" class="headerlink" title="profile详情"></a>profile详情</h5><p>查看每一条sql的耗时和各个阶段消耗的时间</p><pre><code class="mysql">select @@have_profiling;#查看MySQL是否支持profile操作</code></pre><p>profiling默认是关闭的，可以通过set在global&#x2F;session级别开启profiling；</p><pre><code class="mysql">set profiling = 1;</code></pre><p>查询profiling</p><pre><code class="mysql">show profiles;#查看每一条SQL的耗时情况show profile for query query_id;#查看指定query_id的SQL语句各个阶段的耗时情况show profile cpu for query query_id;#查看指定query_id的SQL语句CPU的使用情况。</code></pre><h4 id="3、explain执行计划"><a href="#3、explain执行计划" class="headerlink" title="3、explain执行计划"></a>3、explain执行计划</h4><p>EXPLAIN获取MySQL如何执行Select 语句的信息，包括select语句执行过程中如何连接和连接的顺序。</p><pre><code class="mysql">EXPLAIN SELECT 字段列表 FROM 表名 WHERE 条件;#id #表示执行顺序，id相同从上到下，id不同，值越大越先执行。#select_type#select类型，常见类型：SIMPLE(简单表)PRIMARY(主查询，即外层查询)UNION(UNION的第二个或者后面的查询语句)SUBQUERY(SELECT/WHERE之后包含了子查询)等#type#表示连接类型：性能由好到差连接类型为NULL,system，const，eq_ref，ref，range，index，all#不查询任何表就是NULL#根据主键或者唯一索引const#使用非唯一索引就是ref优化时保证尽量不要出现all#possible_key#这张表中可能使用到的索引，一个或者多个#key#实际用到的索引，Null则表示没有用到索引#key_len#表示索引中使用的字节数，该值为索引字段最大可能长度，并非实际使用长度，再不损失精度的前提下，越短越好。#rows#Mysql认为必须要执行查询的行数，在InnoDB引擎表中，是一个估计值，可能并不是准确的。#filtered# 表示返回结果的行数占需读取行的百分比，filtered的值越大越好。#Extra#额外信息字段</code></pre><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213183530729.png" alt="image-20220213183530729" style="zoom:50%;"><p>执行顺序：3<c>、2<sc>、1<subquery2>、1<s></s></subquery2></sc></c></p><h3 id="6、索引使用"><a href="#6、索引使用" class="headerlink" title="6、索引使用"></a>6、索引使用</h3><h4 id="最左前缀法则"><a href="#最左前缀法则" class="headerlink" title="最左前缀法则"></a>最左前缀法则</h4><p>联合索引，要遵循最左前缀法则。最左前缀法则指的是查询从索引的最左列开始，并且不跳过索引中的列。</p><p>如果<strong>跳过某列，索引将部分失效（后面的字段索引失效）。</strong>最左边的列必须存在，<strong>最左边字段不存在则索引全部失效</strong>。</p><p>走不走索引与字段位置无关</p><p>联合索引顺序性：</p><pre><code>（123）col，联合索引定义123，走索引12，走索引1，走索引23，不走索引，全表扫描3，不走索引13，1走索引，3不走索引，遵循顺序原则321，字段全部走索引，字段存在就行，没说必须放最左边，与位置无关。</code></pre><h4 id="范围查询"><a href="#范围查询" class="headerlink" title="范围查询"></a>范围查询</h4><p>联合索引中，出现范围查询(&lt;,&gt;)，范围查询<strong>右侧的列</strong>索引失效。使用范围查询的字段不会失效</p><p>改进：使用&gt;&#x3D;或&lt;&#x3D;，不使用&lt;，&gt;。</p><pre><code>123 col 联合索引定义1，2(&gt;/&lt;)，312走索引，3不走1，2(&gt;=/=&lt;),3 全部走索引</code></pre><h4 id="索引列运算"><a href="#索引列运算" class="headerlink" title="索引列运算"></a>索引列运算</h4><p>不要再索引列上进行运算操作，否则将会失效。</p><h4 id="字符串不加单引号"><a href="#字符串不加单引号" class="headerlink" title="字符串不加单引号"></a>字符串不加单引号</h4><p>字符串类型字段使用时不加单引号，索引将失效。</p><pre><code>#1 type=charwhere 1=&#39;1&#39;   走索引where 1=1     不走索引</code></pre><h4 id="模糊查询"><a href="#模糊查询" class="headerlink" title="模糊查询"></a>模糊查询</h4><p>如果只是尾部模糊查询，索引不会失效。如果是头部模糊匹配，索引失效。</p><pre><code>where 1 like &#39;软件%&#39;;走索引where 1 like &#39;%软件&#39;;不走索引where 1 like &#39;%软件%&#39;;不走索引，只要前面加上模糊查询就不走索引。</code></pre><h4 id="or连接条件"><a href="#or连接条件" class="headerlink" title="or连接条件"></a>or连接条件</h4><p>or分割开的条件，or条件之前的字段有索引，后面的列没有索引，那么不会走索引。</p><pre><code>where a=10 or b=20 #a列创建了索引，b列没有索引，这种情况不会走索引。where a=10 or b=20#a列有索引，b列有索引，这种情况会走索引。</code></pre><h4 id="数据分布影响"><a href="#数据分布影响" class="headerlink" title="数据分布影响"></a>数据分布影响</h4><p>如果Mysql走索引效率比全表扫描慢，那么Mysql会自动走全表扫描。</p><p>取决于你的数据分布情况和条件</p><pre><code class="apl">如果数据中都是有值的，你查询条件是is null ，那么走索引如果数据中都是有值的，你查询条件是is not null ，那么不走索引如果数据中都是空值的，你查询条件是is null ，那么不走索引如果数据中都是空值的，你查询条件是is not null ，那么走索引</code></pre><h4 id="SQL提示"><a href="#SQL提示" class="headerlink" title="SQL提示"></a>SQL提示</h4><p>优化数据库的一个手段，简单来说，在SQL中加入一些人为提示来达到优化的目的。</p><pre><code>a字段是联合索引的最左列，同时还有一个单列索引。在运行时，默认情况：优化器会索引中自动二选一。</code></pre><h5 id="use-index："><a href="#use-index：" class="headerlink" title="use index："></a>use index：</h5><p>告诉数据库你建议用哪个索引，也可能不会用这个索引（不接受你的建议）</p><pre><code class="sql">explainselect * from table_name use index(index_name) where profession=&#39;软件工程&#39;</code></pre><h5 id="ignore-index："><a href="#ignore-index：" class="headerlink" title="ignore index："></a><strong>ignore index：</strong></h5><p>不要用哪个索引。</p><pre><code class="sql">explainselect * from table_name ignore index(index_name) where profession=&#39;软件工程&#39;;</code></pre><h5 id="force-index："><a href="#force-index：" class="headerlink" title="force index："></a>force index：</h5><p>你必须走哪个索引。强制索引</p><pre><code class="sql">explainselect * from table_name force index(index_name) where profession=&#39;软件工程&#39;;</code></pre><h4 id="覆盖索引"><a href="#覆盖索引" class="headerlink" title="覆盖索引"></a>覆盖索引</h4><p>尽量使用覆盖索引，（查询使用了索引，并且需要返回的列，在该索引中已经全部能够找到，<strong>避免回表</strong>），减少select *。</p><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213213940269.png" alt="image-20220213213940269"></p><pre><code>using where ,using index。查询使用了索引，不需要回表查询，因为需要的值在二级索引中都可以拿到。using index condition. 查询使用了索引，因为二级索引中没有name字段值，所以需要回表查询。</code></pre><h4 id="前缀索引"><a href="#前缀索引" class="headerlink" title="前缀索引"></a>前缀索引</h4><p>字符串类型，varchar、text等，如果存储了很长的字符串，就会让索引变得很大，查询时，浪费大量的磁盘IO，影响查询效率。</p><p><strong>可以只将字符串的一部分前缀，建立索引，大大节省存储空间，从而提高查询效率。</strong></p><h5 id="前缀长度"><a href="#前缀长度" class="headerlink" title="前缀长度"></a>前缀长度</h5><p>根据索引的选择性来决定，不重复的索引值（基数）和数据表中的记录总数的比值，索引选择性越高则查询效率越高，唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。</p><pre><code class="sql">select count(distinct substring(col,1,5))/count(*) from table_name;</code></pre><h5 id="创建索引-1"><a href="#创建索引-1" class="headerlink" title="创建索引"></a>创建索引</h5><pre><code class="sql">create index index_name on table _name(column(n));//列名后面加(n)，n代表提取这个字段的前面几个字符。例子：(name(2))name字段前2个字符建立索引。</code></pre><h5 id="查询过程"><a href="#查询过程" class="headerlink" title="查询过程"></a>查询过程</h5><p>先截取字段一部分字符，去查询字段的二级索引，匹配后获取id值进行回表查询，获取数据，进行整个字段匹配，完全匹配就返回这行数据。不完全匹配就继续二级索引中查找下一个配置值，循环这个过程。</p><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213221858545.png" alt="image-20220213221858545"></p><h4 id="单列索引和联合索引"><a href="#单列索引和联合索引" class="headerlink" title="单列索引和联合索引"></a>单列索引和联合索引</h4><p>在业务场景中存在多个查询条件，针对查询字段建立索引，建立联合索引，而非单列索引。主要还是避免回表查询。单列索引很容易就回表查询了。</p><h5 id="联合索引查询过程"><a href="#联合索引查询过程" class="headerlink" title="联合索引查询过程"></a>联合索引查询过程</h5><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213223204760.png" alt="image-20220213223204760"></p><h3 id="7、索引设计原则"><a href="#7、索引设计原则" class="headerlink" title="7、索引设计原则"></a>7、索引设计原则</h3><ol><li><p>针对数据量较大，查询频繁的表建立索引。超过100万再说。</p></li><li><p>针对常作为查询条件，排序，分组操作的字段建立索引。尽量使用联合索引。</p></li><li><p>选择区分度高的列作为索引，建立唯一索引，区分度越高，使用的索引效率越高。</p></li><li><p>字符串类型的字段，字段长度较长，针对他们的特点，可以建立前缀索引</p></li><li><p>尽量使用联合索引，减少单列索引，查询时可以覆盖索引，避免回表操作，节省空间，提高查询效率。</p></li><li><p>控制索引数量，索引越多，维护代价就越大，会影响增删改查的效率。</p></li><li><p>如果索引列不能存储Null值，请在创建表时使用 NOT NULL 来约束他。优化器知道每列是否包含Null值时，他可以更好的确定哪个索引最有效的用于查询。</p></li></ol><h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><h3 id="存储引擎有哪些，有什么区别？"><a href="#存储引擎有哪些，有什么区别？" class="headerlink" title="存储引擎有哪些，有什么区别？"></a>存储引擎有哪些，有什么区别？</h3><p>innoDB支持事务，MyISAM不支持</p><p>innoDB支持行锁，MyISAM支持行锁</p><p>innoDB支持外键，MyISAM不支持外键</p><h3 id="为什么innoDB选择B-Tree索引结构？为什么不选择其他结构？"><a href="#为什么innoDB选择B-Tree索引结构？为什么不选择其他结构？" class="headerlink" title="为什么innoDB选择B+Tree索引结构？为什么不选择其他结构？"></a>为什么innoDB选择B+Tree索引结构？为什么不选择其他结构？</h3><p>相比二叉树，层级更少，搜索效率更高</p><p>相比B树，     一页中存储的键值减少，指针跟着减少，要同样保存大量数据，只能更加树的高度，导致性能降低。</p><p>相比红黑树，</p><p>相比原B+树，增加了叶子节点指针，形成了双向链表，便于范围搜索和排序。</p><h3 id="为什么要用B-树索引而不是用Hash索引？"><a href="#为什么要用B-树索引而不是用Hash索引？" class="headerlink" title="为什么要用B+树索引而不是用Hash索引？"></a>为什么要用B+树索引而不是用Hash索引？</h3><p>对于hash索引来说不支持等值匹配，不支持范围匹配和排序操作的。</p><h3 id="InnoDB主键索引B-Tree高度有多高？"><a href="#InnoDB主键索引B-Tree高度有多高？" class="headerlink" title="InnoDB主键索引B+Tree高度有多高？"></a>InnoDB主键索引B+Tree高度有多高？</h3><p>假设1行数据是1k，一页可以存储16行这样的数据。InnoDB指针占用6个字节的空间，主键为bigint，占用字节数为8。</p><p><img src="/mysql/mysql-cun-chu-yin-qing-he-suo-yin/image-20220213170258820.png" alt="image-20220213170258820"></p>]]></content>
      
      
      <categories>
          
          <category> Mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban的学习笔记</title>
      <link href="/azkaban/azkaban/"/>
      <url>/azkaban/azkaban/</url>
      
        <content type="html"><![CDATA[<h2 id="Azkaban"><a href="#Azkaban" class="headerlink" title="Azkaban"></a>Azkaban</h2><p>azkaban官网使用文档：<a href="https://azkaban.github.io/azkaban/docs/latest/#azkaban-execserver">https://azkaban.github.io/azkaban/docs/latest/#azkaban-execserver</a></p><h3 id="azkaban是什么"><a href="#azkaban是什么" class="headerlink" title="azkaban是什么"></a>azkaban是什么</h3><p>Azkaban是由Linkedin公司推出的一个批量工作流任务调度器，主要用于在一个工作流内以一个特定的顺序运行一组工作和流程，它的配置是通过简单的key:value对的方式，通过配置中的dependencies 来设置依赖关系，这个依赖关系必须是无环的，否则会被视为无效的工作流。Azkaban使用job配置文件建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</p><h2 id="Azkaban架构"><a href="#Azkaban架构" class="headerlink" title="Azkaban架构"></a>Azkaban架构</h2><p>Azkaban由 3 个关键部分组成：</p><ul><li>关系数据库 (MySQL)</li><li>AzkabanWeb服务器</li><li>AzkabanExecutorServer</li></ul><h3 id="关系型数据库（MySQL）"><a href="#关系型数据库（MySQL）" class="headerlink" title="关系型数据库（MySQL）"></a>关系型数据库（MySQL）</h3><p>Azkaban使用数据库存储大部分状态，AzkabanWebServer和AzkabanExecutorServer都需要访问数据库。</p><h4 id="AzkabanWebServer-如何使用数据库？"><a href="#AzkabanWebServer-如何使用数据库？" class="headerlink" title="AzkabanWebServer 如何使用数据库？"></a>AzkabanWebServer 如何使用数据库？</h4><p>Web 服务器使用 db 的原因如下：</p><ul><li><strong>项目管理</strong>- 项目、项目的权限以及上传的文件。</li><li><strong>Executing Flow State</strong> - 跟踪正在执行的流程以及哪个 Executor 正在运行它们。</li><li><strong>以前的流程&#x2F;作业</strong>- 搜索以前执行的作业和流程以及访问它们的日志文件。</li><li><strong>调度程序</strong>- 保持已调度作业的状态。</li><li><strong>SLA</strong> - 保留所有 SLA 规则</li></ul><h4 id="AzkabanExecutorServer使用数据库的原因如下："><a href="#AzkabanExecutorServer使用数据库的原因如下：" class="headerlink" title="AzkabanExecutorServer使用数据库的原因如下："></a>AzkabanExecutorServer使用数据库的原因如下：</h4><ul><li>访问项目：从数据库检索项目文件。</li><li>执行流程&#x2F;作业：检索和更新正在执行的作业流的数据</li><li>日志：将作业和工作流的输出日志存储到数据库中。</li><li>交互依赖关系：如果一个工作流在不同的执行器上运行，它将从数据库中获取状态。</li></ul><h3 id="AzkabanWebServer"><a href="#AzkabanWebServer" class="headerlink" title="AzkabanWebServer"></a>AzkabanWebServer</h3><p>AzkabanWebServer是整个Azkaban工作流系统的主要管理者，它负责project管理、用户登录认证、定时执行工作流、跟踪工作流执行进度等一系列任务。同时，它还提供Web服务操作的接口，利用该接口，用户可以使用curl或其他ajax的方式，来执行azkaban的相关操作。操作包括：用户登录、创建project、上传workflow、执行workflow、查询workflow的执行进度、杀掉workflow等一系列操作，且这些操作的返回结果均是json的格式。并且Azkaban使用方便，Azkaban使用以.job为后缀名的键值属性文件来定义工作流中的各个任务，以及使用dependencies属性来定义作业间的依赖关系链。这些作业文件和关联的代码最终以*.zip的方式通过Azkaban UI上传到Web服务器上。</p><h3 id="AzkabanExecutorServer"><a href="#AzkabanExecutorServer" class="headerlink" title="AzkabanExecutorServer"></a>AzkabanExecutorServer</h3><p>以前版本的Azkaban在单个服务中具有AzkabanWebServer和AzkabanExecutorServer功能，目前Azkaban已将AzkabanExecutorServer分离成独立的服务器，拆分AzkabanExecutorServer的原因有如下几点：</p><p>某个任务流失败后，可以更方便的将其重新执行<br>便于Azkaban升级，AzkabanExecutorServer主要负责具体的工作流的提交、执行，可以启动多个执行服务器，它们通过mysql数据库来协调任务的执行。</p><h2 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h2><ol><li><a href="https://azkaban.github.io/azkaban/docs/latest/#database-setup">设置数据库</a></li><li><a href="https://azkaban.github.io/azkaban/docs/latest/#webserver-setup">下载并安装 Web 服务器</a></li><li><a href="https://azkaban.github.io/azkaban/docs/latest/#executor-setup">下载并安装执行服务器</a></li><li><a href="https://azkaban.github.io/azkaban/docs/latest/#plugin-setup">安装 Azkaban 插件</a></li></ol><p>配置时区</p><pre><code>1、先生成时区配置文件Asia/Shanghai，用交互式命令 tzselect 即可2、拷贝该时区文件，覆盖系统本地时区配置cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  检验是否生效： dateTue Mar 24 17:25:45 CST 2020</code></pre><h2 id="Azkaban架构的三种运行模式"><a href="#Azkaban架构的三种运行模式" class="headerlink" title="Azkaban架构的三种运行模式"></a>Azkaban架构的三种运行模式</h2><p>在版本3.0中，Azkaban提供了以下三种模式：</p><p>solo server mode：最简单的模式，数据库内置的H2数据库，AzkabanWebServer和AzkabanExecutorServer都在一个进程中运行，任务量不大项目可以采用此模式。<br>two server mode：数据库为MySQL，管理服务器和执行服务器在不同进程，这种模式下，AzkabanWebServer和AzkabanExecutorServer互不影响。<br>multiple executor mode：该模式下，AzkabanWebServer和AzkabanExecutorServer运行在不同主机上，且AzkabanExecutorServer可以有多个。</p><h2 id="azkaban使用"><a href="#azkaban使用" class="headerlink" title="azkaban使用"></a>azkaban使用</h2><p>projects：最重要的部分，创建一个工程，所有flows将在工程中运行。</p><p> scheduling: 显示定时任务</p><p> executing: 显示当前运行的任务</p><p> history: 显示历史运行任务</p><h3 id="1-创建project"><a href="#1-创建project" class="headerlink" title="1. 创建project"></a>1. 创建project</h3><p><img src="/azkaban/azkaban/image-20220601154120623.png" alt="image-20220601154120623"></p><h3 id="2-定义job"><a href="#2-定义job" class="headerlink" title="2.定义job"></a>2.定义job</h3><p>官方文档：<a href="https://azkaban.github.io/azkaban/docs/latest/#job-configuration">https://azkaban.github.io/azkaban/docs/latest/#job-configuration</a></p><p>编写具体执行任务和任务依赖</p><h4 id="Job编写"><a href="#Job编写" class="headerlink" title=".Job编写"></a>.Job编写</h4><p>编写完成后，打包成zip文件</p><h4 id="start-job"><a href="#start-job" class="headerlink" title="start.job"></a>start.job</h4><pre><code class="shell">type=commandcommand=echo &quot;start&quot;</code></pre><h4 id="end-job"><a href="#end-job" class="headerlink" title="end.job"></a>end.job</h4><p>作业的依赖项总是在作业本身可以运行之前运行：<code>end</code>依赖于<code>start</code></p><pre><code class="shell">type=commanddependencies=start//等start执行完毕在执行command=echo &quot;end&quot;</code></pre><h4 id="sh-job"><a href="#sh-job" class="headerlink" title="sh.job"></a>sh.job</h4><pre><code class="shell">type=commandcommand= sh test.sh</code></pre><h3 id="嵌入式流"><a href="#嵌入式流" class="headerlink" title="嵌入式流"></a>嵌入式流</h3><p>一个流也可以作为<strong>嵌入流</strong>中的一个节点包含在其他流中。要创建嵌入式流，只需创建一个<code>.job</code>文件，<code>type=flow</code>并将<code>flow.name</code>其设置为流的名称。例如：</p><pre><code class="shell"># baz.job type=flow flow.name=end</code></pre><table><thead><tr><th align="left">范围</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">retries</td><td align="left">对失败的作业将自动尝试的重试次数</td></tr><tr><td align="left">retry.backoff</td><td align="left">每次重试之间的毫秒时间</td></tr><tr><td align="left">working.dir</td><td align="left">覆盖执行的工作目录。默认情况下，这是包含正在运行的作业文件的目录。</td></tr><tr><td align="left">env.<em>property</em></td><td align="left">使用命名<em>属性设置环境变量</em></td></tr><tr><td align="left">failure.emails</td><td align="left">在失败期间要通知的电子邮件的逗号分隔列表。*</td></tr><tr><td align="left">success.emails</td><td align="left">成功期间要通知的电子邮件的逗号分隔列表。*</td></tr><tr><td align="left">notify.emails</td><td align="left">在成功或失败期间要通知的电子邮件的逗号分隔列表。*</td></tr></tbody></table><h4 id="parameter-sh"><a href="#parameter-sh" class="headerlink" title="parameter.sh"></a>parameter.sh</h4><pre><code class="shell">#!/bin/bash##############################################                                           ##  @author hunter@doitedu                   ##  @date   2020-10-10                       ##  @desc   id绑定评分计算任务启动脚本       ##                                           ##############################################export SPARK_HOME=/opt/apps/spark-2.4.4$&#123;SPARK_HOME&#125;/bin/spark-submit \--master yarn \--deploy-mode cluster \--class cn.doitedu.spark.WordCount \--name wordcount \--driver-memory 1024M \--executor-memory 2G \--queue default \--num-executors 1 azkaban_demos-1.0.jar $1 $2</code></pre><h3 id="3-upload任务压缩包至项目"><a href="#3-upload任务压缩包至项目" class="headerlink" title="3.upload任务压缩包至项目"></a>3.upload任务压缩包至项目</h3><p><img src="/azkaban/azkaban/image-20220601154431369.png" alt="image-20220601154431369"></p><h3 id="4-查看任务，执行任务"><a href="#4-查看任务，执行任务" class="headerlink" title="4.查看任务，执行任务"></a>4.查看任务，执行任务</h3><p>点击Flows中 command任务，可以进入到任务的具体界面，Execute Flow 可以执行任务</p><p><img src="/azkaban/azkaban/image-20220601154524574.png" alt="image-20220601154524574"></p><h3 id="失败选项"><a href="#失败选项" class="headerlink" title="失败选项"></a>失败选项</h3><p>当流程中的作业失败时，您可以控制流程的其余部分如何成功。</p><ul><li><strong>Finish Current Running</strong>将完成当前正在运行的作业，但不会启动新作业。一旦一切完成，流程将进入<code>FAILED FINISHING</code>状态并设置为 FAILED。</li><li><strong>Cancel All</strong>将立即终止所有正在运行的作业并将执行流程的状态设置为 FAILED。</li><li>只要满足其依赖关系，<strong>Finish All Possible将继续在流程中执行作业。</strong>一旦一切完成，流程将进入<code>FAILED FINISHING</code>状态并设置为 FAILED。</li></ul><h3 id="并发选项"><a href="#并发选项" class="headerlink" title="并发选项"></a>并发选项</h3><p>如果在流程并发执行时调用流程执行，则可以设置几个选项。</p><ul><li><p><strong>如果流程已在运行，则跳过执行</strong>选项将不会运行该流程。</p></li><li><p><strong>Run Concurrently</strong>选项将运行流程，无论其是否正在运行。执行被赋予不同的工作目录。</p></li><li><p>管道</p><p>以新执行不会超出并发执行的方式运行流程。</p><ul><li>级别 1：阻塞执行<strong>作业 A</strong>，直到前一个流程的<strong>作业 A</strong>完成。</li><li>级别 2：阻止执行<strong>作业 A</strong>，直到前一个流的<strong>作业 A</strong>的子级完成。如果您需要在已经执行的流程后面几个步骤运行流程，这很有用。</li></ul></li></ul><h3 id="项目权限"><a href="#项目权限" class="headerlink" title="项目权限"></a>项目权限</h3><p>创建项目时，创建者会自动获得项目的管理员状态。这允许创建者查看、上传、更改作业、运行流程、删除和向项目添加用户权限。管理员可以删除其他管理员，但不能删除自己。这可以防止项目成为无管理员，除非管理员被具有管理员角色的用户删除。</p><p>可以从项目页面访问权限页面。在权限页面上，管理员可以将其他用户、组或代理用户添加到项目中。</p><p><img src="/azkaban/azkaban/image-20220601160929238.png" alt="image-20220601160929238"></p><table><thead><tr><th align="left">Permission</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">ADMIN</td><td align="left">允许用户对这个项目做任何事情，以及添加权限和删除项目</td></tr><tr><td align="left">READ</td><td align="left">用户可以查看作业、流程、执行日志</td></tr><tr><td align="left">WRITE</td><td align="left">可以上传项目文件，也可以修改作业文件。</td></tr><tr><td align="left">EXECUTE</td><td align="left">允许用户执行、暂停、取消作业。</td></tr><tr><td align="left">SCHEDULE</td><td align="left">允许用户从计划中添加、修改和删除流。</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH5.16卸载</title>
      <link href="/cdh/cdh5.16-xie-zai/"/>
      <url>/cdh/cdh5.16-xie-zai/</url>
      
        <content type="html"><![CDATA[<p>集群卸载，因为一些搞不定的问题，所以卸载重装一遍</p><p>本文参考<br><a href="https://blog.csdn.net/weixin_35852328/article/details/81774627">https://blog.csdn.net/weixin_35852328&#x2F;article&#x2F;details&#x2F;81774627</a></p><h2 id="1、停掉你的所有服务，包括cm"><a href="#1、停掉你的所有服务，包括cm" class="headerlink" title="1、停掉你的所有服务，包括cm"></a>1、停掉你的所有服务，包括cm</h2><p><img src="https://img-blog.csdnimg.cn/2020110609233197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20201106092615880.png#pic_center" alt="在这里插入图片描述"></p><h2 id="2、停止完毕服务后，把之前安装的parcel的包-停用-删除"><a href="#2、停止完毕服务后，把之前安装的parcel的包-停用-删除" class="headerlink" title="2、停止完毕服务后，把之前安装的parcel的包 停用-删除"></a>2、停止完毕服务后，把之前安装的parcel的包 停用-删除</h2><p><img src="https://img-blog.csdnimg.cn/20201106092948211.png#pic_center" alt="在这里插入图片描述"><br>下面的图是开头博主的图<br><img src="https://img-blog.csdnimg.cn/20201106093025338.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="3、删除你的cluster"><a href="#3、删除你的cluster" class="headerlink" title="3、删除你的cluster"></a>3、删除你的cluster</h2><p><img src="https://img-blog.csdnimg.cn/20201106093201361.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="4、停止你的进程"><a href="#4、停止你的进程" class="headerlink" title="4、停止你的进程"></a>4、停止你的进程</h2><pre><code>/opt/cm-5.16.2/etc/init.d/cloudera-scm-server stop/opt/cm-5.16.2/etc/init.d/cloudera-scm-agent stop</code></pre><h2 id="5、查看又没有相关的进程"><a href="#5、查看又没有相关的进程" class="headerlink" title="5、查看又没有相关的进程"></a>5、查看又没有相关的进程</h2><pre><code>ps -ef |grep clouderakill 掉查出来的进程信息ps -ef | grep supervisordkill 掉查出来的进程信息</code></pre><h2 id="6、开始删除你的配置文件"><a href="#6、开始删除你的配置文件" class="headerlink" title="6、开始删除你的配置文件"></a>6、开始删除你的配置文件</h2><pre><code>rm -rf /var/lib/flume-ng /var/lib/hadoop* /var/lib/hue  /var/lib/navigator /var/lib/oozie /var/lib/solr /var/lib/sqoop*  /var/lib/zookeeper /var/lib/kudu /var/lib/kafka/ /var/lib/impala/ /var/lib/cloudera-scm-*rm -rf /etc/cloudera*umount /opt/cm-5.16.2/run/cloudera-scm-agent/process  rm -rf /var/run/hdfs-socketsrm -rf /usr/lib/huerm -rf /usr/bin/hadoop* /usr/bin/zookeeper* /usr/bin/hbase* /usr/bin/hive* /usr/bin/hdfs /usr/bin/mapred /usr/bin/yarn /usr/bin/sqoop* /usr/bin/oozie /usr/bin/impala /usr/bin/spark*rm -rf /etc/alternatives/hadoop* /etc/alternatives/flume-ng* /etc/alternatives/hbase* /etc/alternatives/hdfs /etc/alternatives/hive* /etc/alternatives/hue* /etc/alternatives/impala* /etc/alternatives/mahout* /etc/alternatives/mapred /etc/alternatives/oozie /etc/alternatives/pig* /etc/alternatives/solr* /etc/alternatives/spark* /etc/alternatives/sqoop* /etc/alternatives/yarn /etc/alternatives/zookeeper*rm -rf /var/cloudera-scm-serverrm -rf /var/lib/cloudera* /var/lib/yarn*rm -rf /dfsrm -rf /tmp/.scm_prepare_node.lockrm -rf /opt/cloudera/rm -rf /opt/cm-5.16.2</code></pre><p>删除和卸载他们的相关数据和日志以及源文件存放的位置</p><pre><code>//下面是CDH的目录的说明/var/lib/cloudera-scm-server-db/data : 内嵌数据库目录。/etc/cloudera-scm-agent/ : agent的配置目录。/etc/cloudera-scm-server/ : server的配置目录。/var/run下面Hadoop生成的文件/var/log/cloudera-scm-installer : 安装日志目录。/var/log/* : 相关日志文件（相关服务的及CM的）。/usr/share/cmf/ : 程序安装目录。/usr/lib64/cmf/ : Agent程序代码。/usr/bin/postgres : 内嵌数据库程序。/opt/cloudera/parcels/ : Hadoop相关服务安装目录。/opt/cloudera/parcel-repo/ : 下载的服务软件包数据，数据格式为parcels。/opt/cloudera/parcel-cache/ : 下载的服务软件包缓存数据。/etc/hadoop/* : 客户端配置文件目录。/dfs:hadoop格式化的目录</code></pre><h2 id="7、-Mysql"><a href="#7、-Mysql" class="headerlink" title="7、 Mysql"></a>7、 Mysql</h2><p>mysql我的话没有遇到问题，也没有重新安装<br>如果只要要重新安装在mysql没有问题的情况下不需要动mysql，只需要把mysql的中的cm数据库和其他amon,oozie,hue和hive数据库删除就行</p><p>下面是删除数据库的过程</p><pre><code>service mysql stopps -ef |grep mysqlkill pidrpm -qa |grep -i mysqlyum remove  **********find / -name mysqlrm -rf rm -rf /etc/my.cnfrm -rf /var/log/mysqld.log</code></pre><p>上面是说明的步骤和执行哪些，下面是整体需要执行的语句</p><pre><code>rm -rf /var/lib/flume-ng /var/lib/hadoop* /var/lib/hue  /var/lib/navigator /var/lib/oozie /var/lib/solr /var/lib/sqoop*  /var/lib/zookeeper /var/lib/kudu /var/lib/kafka/ /var/lib/impala/ /var/lib/cloudera-scm-*rm -rf /etc/cloudera*umount /opt/cm-5.16.2/run/cloudera-scm-agent/process  rm -rf /var/run/hdfs-socketsrm -rf /usr/lib/huerm -rf /usr/bin/hadoop* /usr/bin/zookeeper* /usr/bin/hbase* /usr/bin/hive* /usr/bin/hdfs /usr/bin/mapred /usr/bin/yarn /usr/bin/sqoop* /usr/bin/oozie /usr/bin/impala /usr/bin/spark*rm -rf /etc/alternatives/hadoop* /etc/alternatives/flume-ng* /etc/alternatives/hbase* /etc/alternatives/hdfs /etc/alternatives/hive* /etc/alternatives/hue* /etc/alternatives/impala* /etc/alternatives/mahout* /etc/alternatives/mapred /etc/alternatives/oozie /etc/alternatives/pig* /etc/alternatives/solr* /etc/alternatives/spark* /etc/alternatives/sqoop* /etc/alternatives/yarn /etc/alternatives/zookeeper*rm -rf /var/cloudera-scm-serverrm -rf /var/lib/cloudera* /var/lib/yarn*rm -rf /dfs</code></pre>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kibana报：Driver class &#39;org.gjt.mm.mysql.Driver&#39; could not be found</title>
      <link href="/elk/kettle-an-zhuang-wan-bao-cuo-driver-class-org.gjt.mm.mysql.driver-could-not-be-found/"/>
      <url>/elk/kettle-an-zhuang-wan-bao-cuo-driver-class-org.gjt.mm.mysql.driver-could-not-be-found/</url>
      
        <content type="html"><![CDATA[<p>今天安装Kettle使用，下载解压到本地后双击<code>Spoon.bat</code> 打开应用<br>首先测试连接Mysql，但是报错，这时候知道他没有需要的Connection包，到Mysql官网下载后，这里我的Mysql是8.0.22的，下载的也是connect也是8.0.22的包<br>下载解压这个包之后<br><img src="https://img-blog.csdnimg.cn/20201124162623149.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>把这个包放入到  <code> pdi-ce-8.2.0.0-342\data-integration\lib</code>    目录下<br>再次启动<br>还是报错，感觉没什么变化，还是依旧</p><p>仔细查看报错内容，发现要  ‘org.gjt.mm.mysql.Driver’  这个驱动</p><pre><code>Driver class &#39;org.gjt.mm.mysql.Driver&#39; could not be found, make sure the &#39;MySQL&#39; driver (jar file) is installed.org.gjt.mm.mysql.Driver</code></pre><p>这个包在新版本中是没有的，而在老版本中有<br>Mysql官网下载5.1.49的connect包，解压后<br><img src="https://img-blog.csdnimg.cn/20201124162956846.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>可以看到两个connect的包，把这两个都放入到 <code> pdi-ce-8.2.0.0-342\data-integration\lib</code>  目录下，重新启动kettle，再次尝试连接MySQL服务，成功！</p>]]></content>
      
      
      <categories>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kibana </tag>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kibana的安装</title>
      <link href="/elk/kibana7.10.0-an-zhuang/"/>
      <url>/elk/kibana7.10.0-an-zhuang/</url>
      
        <content type="html"><![CDATA[<p><strong>Kibana，数据可视化平台</strong></p><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>官网地址: <a href="https://www.elastic.co/cn/downloads/kibana">https://www.elastic.co/cn/downloads/kibana</a></p><h2 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h2><pre><code class="shell">tar -zxvf kibana-7.10.0-linux-x86_64.tar.gz</code></pre><h2 id="配置-kibana-home"><a href="#配置-kibana-home" class="headerlink" title="配置  kibana_home"></a>配置  kibana_home</h2><p>config&#x2F;kibana.yml<br><img src="https://img-blog.csdnimg.cn/2020113014380215.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="启动kibana"><a href="#启动kibana" class="headerlink" title="启动kibana"></a>启动kibana</h2><p>完成上面配置后，进行启动</p><pre><code>bin/kibana后台启动：bin/kibana &amp;启动后等待一段时间这里报错，不影响使用，他在重复入值的时报的错</code></pre><p><img src="https://img-blog.csdnimg.cn/20201130144217995.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
      
      
      <categories>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kibana </tag>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kylin学习笔记</title>
      <link href="/kylin/kylin/"/>
      <url>/kylin/kylin/</url>
      
        <content type="html"><![CDATA[<h1 id="Kylin"><a href="#Kylin" class="headerlink" title="Kylin"></a>Kylin</h1><p>这里没有详细学习和使用，官方文档中文很友好，就没有做笔记</p><p>官方文档地址：(<a href="http://kylin.apache.org/cn/docs/">http://kylin.apache.org/cn/docs/</a>)</p><h2 id="Kylin是什么？"><a href="#Kylin是什么？" class="headerlink" title="Kylin是什么？"></a>Kylin是什么？</h2><p>Apache Kylin™是一个开源的、分布式的分析型数据仓库，提供 Hadoop 之上的 SQL 查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc.开发并贡献至开源社区。</p><h2 id="原理及核心概念"><a href="#原理及核心概念" class="headerlink" title="原理及核心概念"></a><strong>原理及核心概念</strong></h2><p><strong>工作原理</strong> ：kylin先对数据模型做cube预计算，并将计算结果存储下来，即空间换时间的原理</p><p><strong>事实表和维度表</strong>：事实表是指存储有事实记录的表，事实表的记录在不断地动态增长，所以它的体积通常远大于其他表。维度表主要用来保存维度的属性值</p><p><strong>核心概念：</strong> 维度和度量，维度通常记录一个属性，例如时间，地点，标签等。度量是居于数据所计算出来的统计值，例如 某商品销售总额，人群库中男性总人数</p><h4 id="Cube-CubeId-和-Cube-Segment"><a href="#Cube-CubeId-和-Cube-Segment" class="headerlink" title="Cube CubeId  和 Cube Segment"></a>Cube CubeId  和 Cube Segment</h4><p>cube(或 data cube) 即数据立方体，是一种常用于数据分析与索引的技术，它可以对原始数据建立多维索引，通过cube对数据进行分析，可以大大加快数据的查询效率。</p><p>cubeid在kylin中特指在某一种维度组合下所计算的数据。</p><p>cube Segment 是指针对数据源中的某一个片段，计算出来的cube数据。通常数据仓库中的数据数量会随着时间的增长而增长，而cube segment 也是按时间顺序来构建的。</p><h4 id="DIMENSION-amp-MEASURE的种类"><a href="#DIMENSION-amp-MEASURE的种类" class="headerlink" title="DIMENSION &amp; MEASURE的种类"></a>DIMENSION &amp; MEASURE的种类</h4><ul><li>Mandotary：强制维度，所有cuboid必须包含的维度。</li><li>Hierarchy：层次关系维度，维度之间具有层次关系性，只需要保留一定层次关系的cuboid即可。</li><li>Derived：衍生维度，在lookup 表中，有一些维度可以通过它的主键衍生得到，所以这些维度将不参加cuboid的构建。</li><li>Count Distinct(HyperLogLog) ：直接进行count distinct是很难去计算的，一个近似的算法HyperLogLog可以保持错误率在一个很低的范围内。</li><li>Count Distinct(Precise)：将基于RoaringBitMap进行计算，目前只支持int和BigInt。</li></ul><h4 id="Cube-Action种类"><a href="#Cube-Action种类" class="headerlink" title="Cube Action种类"></a>Cube Action种类</h4><ul><li>BUILD：给定一个分区列指定的时间间隔，对Cube进行Build，创建一个新的cube Segment。</li><li>REFRESH：这个操作，将在一些分期周期内对cube Segment进行重新build。</li><li>MERGE：这个操作将合并多个cube segments。这个操作可以在构建cube时，设置为自动完成。</li><li>PURGE：清理一个Cube实例下的segment，但是不会删除HBase表中的Tables。</li></ul><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="旧版架构图"><a href="#旧版架构图" class="headerlink" title="旧版架构图"></a><strong>旧版架构图</strong></h3><p><img src="/kylin/kylin/plugin_arch_overview.png" alt="Plugin Architecture Overview"></p><h3 id="新版架构图"><a href="#新版架构图" class="headerlink" title="新版架构图"></a><strong>新版架构图</strong></h3><p><img src="/kylin/kylin/kylin_diagram-16435477750873.png" alt="img"></p><h2 id="安装配置启动"><a href="#安装配置启动" class="headerlink" title="安装配置启动"></a>安装配置启动</h2><p><strong>1.下载解压</strong></p><p><strong>2、修改&#x2F;etc&#x2F;profile文件</strong></p><p>source &#x2F;etc&#x2F;profile 刷新配置文件</p><pre><code class="shell">export JAVA_HOME=/root/Downloads/jdk1.8.0_161export PATH=$PATH:$JAVA_HOME/binexport HADOOP_HOME=/root/Downloads/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport ZOOKEEPER_HOME=/root/Downloads/zookeeper-3.4.5export PATH=$PATH:$ZOOKEEPER_HOME/binexport HIVE_HOME=/root/Downloads/apache-hive-1.2.0-binexport PATH=$PATH:$HIVE_HOME/binexport FLUME_HOME=/root/Downloads/apache-flume-1.6.0-binexport PATH=$PATH:$FLUME_HOME/binexport HBASE_HOME=/root/Downloads/hbase-1.2.6export PATH=$PATH:$HBASE_HOME/binexport KYLIN_HOME=/opt/apps/kylin-3.1.0/export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin:$HIVE_HOME/bin</code></pre><p><strong>3.修改 hbase-site.xml，将 zookeeper 地址中端口号去掉</strong></p><pre><code class="xml">&lt;property&gt;&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;&lt;value&gt;doitedu01,doitedu02,doitedu03&lt;/value&gt;&lt;/property&gt;</code></pre><p><strong>4.检查运行环境</strong></p><pre><code class="shell">$&#123;KYLIN_HOME&#125;/bin/check-env.sh</code></pre><p><img src="/kylin/kylin/image-20220130212906638.png" alt="image-20220130212906638"></p><p><strong>5.启动kylin</strong></p><pre><code class="shell">保证 HDFS 工作正常保证 HIVE METASTORE 工作正常保证 HBASE 工作正常[root@doitedu03 kylin-3.1.0]# bin/kylin.sh start停止 bin/kylin.sh stop</code></pre><p><img src="/kylin/kylin/image-20220130212927117.png" alt="image-20220130212927117"></p><p><strong>6.访问 WebUi</strong> </p><pre><code>账号：ADMIN密码：KYLIN</code></pre><p><img src="/kylin/kylin/image-20220130213047291.png" alt="image-20220130213047291"></p><h2 id="WebUI使用"><a href="#WebUI使用" class="headerlink" title="WebUI使用"></a>WebUI使用</h2><p>简单的看kylin官方文档，这里对过程中的一些设置属性进行说明</p><p><a href="https://kylin.apache.org/cn/docs/tutorial/create_cube.html">https://kylin.apache.org/cn/docs/tutorial/create_cube.html</a></p><h3 id="1-创建工程项目"><a href="#1-创建工程项目" class="headerlink" title="1.创建工程项目"></a>1.创建工程项目</h3><p>添加表的元数据信息后</p><p>创建models（只支持星型模型和雪花模型），告诉它什么是事实表什么是维度表，字段之间的关联</p><p>计算完成结果放入hbase，通过 <strong>insight</strong> 输入sql查询</p><p><img src="/kylin/kylin/image-20220130213955704.png" alt="image-20220130213955704"></p><h3 id="2-同步hive表元数据"><a href="#2-同步hive表元数据" class="headerlink" title="2.同步hive表元数据"></a>2.同步hive表元数据</h3><h3 id="3-创建Data-Model"><a href="#3-创建Data-Model" class="headerlink" title="3.创建Data Model"></a>3.创建Data Model</h3><p>创建 cube 前，需定义一个数据模型。数据模型定义了一个星型（star schema）或雪花（snowflake schema）模型。一个模型可以被多个 cube 使用。</p><p><strong>新建 Data Model</strong>：new model-创建数据模型（星型，雪花。定义事实表，维度表，关联类型和主外键）-选择维度列-选择事实表度量(指标列)-设置参数</p><h3 id="4-创建Cube"><a href="#4-创建Cube" class="headerlink" title="4.创建Cube"></a>4.创建Cube</h3><p><strong>1.选择data model ，输入 cube 名字</strong></p><p><strong>2.选择维度：Lookup 表的列有2个选项：“Normal” 和 “Derived”（默认）。</strong></p><p>同一个维度表中，有选择derived时，不会真正参与构建，参与构建的是这个表对应事实表的外键</p><p>简单得来说，在维度表得字段选用了derived后，在进行预计算时，维度表得字段并不会真正参与，参与得是对应这个维度表中事实表得外键，查询时，用外键进行替换，最后进行聚合得到查询结果</p><p>最后尽量不要使用衍生维度</p><p>如果两者都存在，则有normal选normal，有derived 选择事实表主键</p><p><img src="/kylin/kylin/7%20cube-dimension-batch.png" alt="img"></p><p>3.<strong>度量</strong></p><p>根据你的度量来确定你的指标和统计的方式</p><p>4.更新配置 <strong>Refresh Setting</strong> 碎片化管理</p><p>这里的配置是增量数据的配置</p><pre><code>Auto Merge Thresholds: 自动合并小的 segments 到中等甚至更大的 segment。如果不想自动合并，删除默认2个选项。每个segment在hbase中就是一张独立的表//自动合并一定时间内的数据 //例子：设置5，1012341-5t1-5t,6781-5t,6-10t,...Volatile Range: 默认为0，会自动合并所有可能的 cube segments，或者用 ‘Auto Merge’ 将不会合并最新的 [Volatile Range] 天的 cube segments。//设置最近多少天的数据不会被合并Retention Threshold: 只会保存 cube 过去几天的 segment，旧的 segment 将会自动从头部删除；0表示不启用这个功能。//删除历史数据，这里是设置保存之前多少天的数据Partition Start Date: cube 的开始日期.//不写也可以，这里是start date 数据获取的日期</code></pre><p><img src="/kylin/kylin/image-20220201121844235.png" alt="image-20220201121844235"></p><h4 id="自动合并机制"><a href="#自动合并机制" class="headerlink" title="自动合并机制"></a>自动合并机制</h4><p>阈值可以设置为多个层级，比如： 7 28 60；</p><p>首先，会先查看所有 segment 中是否有若干个连续的 segment，范围加起来正好 60 天，如果满足，则 将之合并； 然后，看是否有若干个连续的 segment，范围加起来正好 28 天，如果满足，则将之合并； 然后，再看是否有若干个连续的 segment，范围加起来正好 7 天，如果满足，则将之合并； 否则，不会创建合并任务</p><p><strong>5.高级设置 Advanced Setting</strong> </p><p><strong>Aggregation Groups 聚合组</strong></p><p>聚合组可以有多个，组包含不同维度，组内的维度进行组合，不会跨组进行聚合。</p><p>将不同的维度根据主题划分为不同的聚合组</p><pre><code>主题 1——聚合组 1：设备类型，操作系统，系统版本，网络类型主题 2——聚合组 2：广告平台，广告系列，广告位置主题 3——聚合组 3：频道，栏目，页面主题 4——聚合组 4：国家，省，市，区</code></pre><p><strong>Mandatory（固定） Dimension</strong></p><p>强制维度：所有cuboid必须包含的维度，不会计算不包含强制维度的cuboid。</p><p><strong>Hierarchy Dimensions</strong></p><p>层级维度：层级维度，例如 “国家” -&gt; “省” -&gt; “市” 是一个层级；不符合此层级关系的 cuboid 可以被跳过计算，例如 [“省”], [“市”]. 定义层级维度时，将父级别维度放在子维度的左边。</p><p><strong>Joint Dimensions</strong></p><p>联合维度：联合维度，有些维度往往一起出现(一对一)，或者它们的基数非常接近（有1:1映射关系）。例如 “user_id” 和 “email”。把多个维度定义为组合关系后，所有不符合此关系的 cuboids 会被跳过计算。</p><h3 id="Cube的核心操作"><a href="#Cube的核心操作" class="headerlink" title="Cube的核心操作"></a>Cube的核心操作</h3><ol><li>SLICE (切片)</li></ol><p>将某一个（或多个）维度上的值锁定，只观察当这个维度取这个值时的情形，相当于将一个立方体做 了一个切片</p><ol start="2"><li>DICE (切块）</li></ol><p>将某一个（或多个）维度上的值固定在一个区间内，观察这个cube的情形，相当于将一个立方体做 了一个切块</p><ol start="3"><li>ROLL UP (上卷)</li></ol><p>沿着某一个（或多个）维度进行聚合，观察聚合后其他维度上的汇总数据，相当于将一个立方体沿着 某个维度压缩（聚合）在一起。</p><ol start="4"><li>DRILL DOWN (下钻）</li></ol><p>沿着某一个（或多个）维度在更细粒度层面上进行展开，观察展开后其他维度上的对应数据，相当于 将一个立方体沿着某个维度拉伸，拉伸的结果就是粒度变细，比如时间维度从季度拉伸到月</p><ol start="5"><li>PIVOT (旋转)</li></ol><p>将维度的位置互换。在二维表格中就是行变列，列变行。</p><h2 id="什么是增量构建"><a href="#什么是增量构建" class="headerlink" title="什么是增量构建"></a>什么是增量构建</h2><p>Hive 中的源表，是按时间（天）分区的，里面的数据也是按分区逐渐增加的 那么，我们在 kylin 中对这个表进行 cube 预计算的时候，有两种方案：  </p><p><strong>全量构建</strong>，针对整个表，进行 cube 预计算； </p><p>​<strong>优点</strong>：时间维度上的各种粒度都可以被预计算； </p><p>​<strong>缺点</strong>：每次都要对整个表进行 cube 计算，运算量大，数据量大； </p><p><strong>增量构建</strong>，一次构建一个分区的数据； </p><p>​<strong>缺点</strong>：因为一次只计算了一天，所有当上层要查询 count(distinct guid) group by month 这种更粗粒 度的时间维度下的指标时，必须在天 cube 结果数据之上进行在线聚合； </p><p>​<strong>优点</strong>：每次构建的量较少；</p><h3 id="碎片化管理"><a href="#碎片化管理" class="headerlink" title="碎片化管理"></a>碎片化管理</h3><p>因为采用增量构建，就会导致这个 cube 中会有大量的 “天 segment”，而每一个 segment 结果数据 都会在 hbase</p><p>大量的 segment 会对 kylin、zookeeper、hbase 等系统都带来额外的压力； 所以，需要定期将这些同一个 cube 的大量 segment 进行合并存储</p><p>具体的合并手段，有两种：</p><p><a href="#4.%E5%88%9B%E5%BB%BACube">自动合并</a></p><p>手动合并：自己控制合并哪些 segment</p><h2 id="kylin-cube表在hbase的表结构"><a href="#kylin-cube表在hbase的表结构" class="headerlink" title="kylin cube表在hbase的表结构"></a>kylin cube表在hbase的表结构</h2><p>将一个长字符串转换为更短的字符串，这样减少存储在hbase数据量以及提高查询效率</p><p>00000001+0 ：表示仅仅查询city&#x3D;0，根据字典转换后 即是查询city&#x3D;beijing，根据原始表可知1、2、4行符合记录加起来80（如果是00000011+xx 表示查询两个纬度year 和city）</p><p>根据上面所描述的，hbase rowkey &#x3D; cube id + 具体的维度值，要查询各个条件下的count值，最终只需要组装成相应的rowkey，直接 到hbase查询得到结果！<br>————————————————<br>原文链接：<a href="https://blog.csdn.net/lidaxueh_heart/article/details/87737010">https://blog.csdn.net/lidaxueh_heart/article/details/87737010</a></p><p><img src="/kylin/kylin/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpZGF4dWVoX2hlYXJ0,size_16,color_FFFFFF,t_70.png" alt="img"></p><h2 id="Bitmap"><a href="#Bitmap" class="headerlink" title="Bitmap"></a>Bitmap</h2><p>新版本（V2.3）kylin采用bitmap来精确统计，我们知道bit 存储的0或者1，假设我们需要统计总人数，可以用bit[] 来表示所有人是否存在的分布情况，bit数组的索引值表示用户id，对应的值表示是否存在（1表示存在，0表示不存在）即如果bit[15]&#x3D;1表示用户id&#x3D;15的用户存在。 </p><p>使用bitmap最大的优点是：用bit存储节省了很大的空间开销，其次执行 sql中的and 或者 or 等操作直接等价于bit[]对应的 &amp;  | 等操作 。</p><p>我们需要数值类型，如果我们精确统计列本来是int类型可以直接用，但是如果是非数值类型（string）我们就需要将string 类型映射为int值 ，该过程俗称构建全局字典。</p><p>构建全局字典还没有看</p><p><a href="https://cwiki.apache.org/confluence/display/KYLIN/Global+Dictionary+on+Spark">https://cwiki.apache.org/confluence/display/KYLIN/Global+Dictionary+on+Spark</a></p><h2 id="Kylin的剪枝优化"><a href="#Kylin的剪枝优化" class="headerlink" title="Kylin的剪枝优化"></a>Kylin的剪枝优化</h2><h3 id="使用衍生维度（derived-dimension）"><a href="#使用衍生维度（derived-dimension）" class="headerlink" title="使用衍生维度（derived dimension）"></a>使用衍生维度（derived dimension）</h3><pre><code>在kylin中属于构建优化，在选择维度表时有normal和derived，derived就是使用衍生维度。加快了计算，减少了查询得速度为了减少计算得cuboid同一个维度表中，有选择derived时，不会真正参与构建，参与构建的是这个表对应事实表的外键如果两者都存在，则有normal选normal，有derived 选择事实表主键简单得来说，在维度表得字段选用了derived后，在进行预计算时，维度表得字段并不会真正参与，参与得是对应这个维度表中事实表得外键，查询时，用外键进行替换，最后进行聚合得到查询结果最后尽量不要使用衍生维度</code></pre><h3 id="碎片化管理（Refresh-Setting）"><a href="#碎片化管理（Refresh-Setting）" class="headerlink" title="碎片化管理（Refresh Setting）"></a>碎片化管理（Refresh Setting）</h3><p><a href="#4.%E5%88%9B%E5%BB%BACube">自动合并</a></p><h3 id="使用聚合组（Aggregation-group）"><a href="#使用聚合组（Aggregation-group）" class="headerlink" title="使用聚合组（Aggregation group）"></a>使用聚合组（Aggregation group）</h3><p>将不同的维度划分为不同的主题。大量的减少组合数（cuboid）</p><h3 id="使用必选维度（Mandatory-Dimensions）"><a href="#使用必选维度（Mandatory-Dimensions）" class="headerlink" title="使用必选维度（Mandatory Dimensions）"></a>使用必选维度（Mandatory Dimensions）</h3><p>因为每个要预计算的 cuboid 中必须包含设置的“必选维度”，也会成倍降低 cuboid 组合数</p><h3 id="使用层级维度（Hierarchy-Dimensions）"><a href="#使用层级维度（Hierarchy-Dimensions）" class="headerlink" title="使用层级维度（Hierarchy Dimensions）"></a>使用层级维度（Hierarchy Dimensions）</h3><p>设置了层级维度，</p><p>比如：省，市，区 Cuboid 就只会有：（省） （省市） （省市区） 成倍降低了 cuboid 个数</p><h3 id="使用联合维度-（Joint-Dimensions）"><a href="#使用联合维度-（Joint-Dimensions）" class="headerlink" title="使用联合维度 （Joint Dimensions）"></a><strong>使用联合维度</strong> （Joint Dimensions）</h3><p>一般针对 “一对一” 的维度字段，</p><p>比如 device_type_id,device_type_name Lanmu_id ,lanmu_name 那么，在预计算 cuboid 时，“device_type_id,device_type_name”必须同时出现</p><h2 id="Cube其他优化"><a href="#Cube其他优化" class="headerlink" title="Cube其他优化"></a>Cube其他优化</h2>]]></content>
      
      
      <categories>
          
          <category> Kylin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kylin </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mysql分库分表</title>
      <link href="/mysql/mysql-fen-ku-fen-biao/"/>
      <url>/mysql/mysql-fen-ku-fen-biao/</url>
      
        <content type="html"><![CDATA[<h1 id="为什么要分库分表？"><a href="#为什么要分库分表？" class="headerlink" title="为什么要分库分表？"></a>为什么要分库分表？</h1><p><strong>原文链接</strong>：<a href="https://juejin.im/post/6844903992909103117">原文链接</a> </p><p>我没有用过分库分表，靠着这篇博客了解的</p><p>关系型数据库本身比较容易成为系统瓶颈，单机存储容量、连接数、处理能力都有限。当单表的数据量达到1000W或100G以后，由于查询维度较多，即使添加从库、优化索引，做很多操作时性能仍下降严重。此时就要考虑对其进行切分了，切分的目的就在于减少数据库的负担，缩短查询时间。</p><p>而切分数据库和表的方式就两种，垂直切分和水平切分</p><h1 id="数据库瓶颈产生原因和解决办法"><a href="#数据库瓶颈产生原因和解决办法" class="headerlink" title="数据库瓶颈产生原因和解决办法"></a>数据库瓶颈产生原因和解决办法</h1><p>不管是IO瓶颈还是CPU瓶颈，最终都会导致数据库的活跃连接数增加，进而逼近甚至达到数据库可承载的活跃连接数的阈值。在业务service来看， 就是可用数据库连接少甚至无连接可用，接下来就可以想象了（并发量、吞吐量、崩溃）。</p><h2 id="IO瓶颈"><a href="#IO瓶颈" class="headerlink" title="IO瓶颈"></a>IO瓶颈</h2><p>第一种：</p><p>磁盘读IO瓶颈，热点数据太多，数据库缓存放不下，每次查询会产生大量的IO，降低查询速度-&gt;分库和垂直分表</p><p>第二种：</p><p>网络IO瓶颈，请求的数据太多，网络带宽不够 -&gt;分库</p><h2 id="CPU瓶颈"><a href="#CPU瓶颈" class="headerlink" title="CPU瓶颈"></a>CPU瓶颈</h2><p>第一种：</p><p>SQl问题：如SQL中包含join,group by, order by，非索引字段条件查询等，增加CPU运算的操作-&gt;SQL优化，建立合适的索引，在业务Service层进行业务计算。</p><p>第二种：</p><p>单表数据量太大，查询时扫描的行太多，SQl效率低，增加CPU运算的操作。-&gt;水平分表。</p><h1 id="分库分表"><a href="#分库分表" class="headerlink" title="分库分表"></a>分库分表</h1><h2 id="水平分库"><a href="#水平分库" class="headerlink" title="水平分库"></a>水平分库</h2><p><img src="https://img-blog.csdnimg.cn/20201111123353102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>一张表的数据通过不同的分发策略发送到不同的数据库中</p><ul><li>两个数据库中的表结构完全一致</li><li>他们的数据完全不同</li><li>两个库中的所有数据的并集是全部的数据</li></ul><p>适用场景：<br>库多了，并发就上去了，解决了你的并发量问题</p><h2 id="垂直分库"><a href="#垂直分库" class="headerlink" title="垂直分库"></a>垂直分库</h2><p><img src="https://img-blog.csdnimg.cn/20201111124105338.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>根据表中业务的不同来进行划分不同字段到不同的库中</p><ul><li>每个库中表结构都不同</li><li>每个库中数据和字段不一样</li><li>所有库的并集是全量数据</li></ul><p>适用场景：</p><p>划分成多个库，提高了并发度，并且需要按照业务模块划分到不同表中</p><p>表中业务模块都定义好的情况下，才能够使用，否则后期使用是灾难</p><h2 id="水平分表"><a href="#水平分表" class="headerlink" title="水平分表"></a>水平分表</h2><p><img src="https://img-blog.csdnimg.cn/20201111124945960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>将一张表的数据按照某种策略拆分到多个表中</p><ul><li>每个表的字段结构都相同</li><li>每个表的数据不一样</li><li>所有表的并集是全部的数据</li></ul><p>适用场景：</p><p>系统并发量没有到瓶颈，就是单表数据过多，导致查询缓慢，加重了CPU的负担，这种情况可以使用水平分表</p><h2 id="垂直分表"><a href="#垂直分表" class="headerlink" title="垂直分表"></a>垂直分表</h2><p><img src="https://img-blog.csdnimg.cn/2020111112551481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>将一张表的字段按照字段的活跃性拆分到不同的表中</p><ul><li>每个表的字段结构不一样</li><li>每个表的数据没有相同的，但是会有一个交集，通常是一个主键，用于关联数据</li><li>所有表的并集是全部的数据</li></ul><p>适用场景：</p><p>并发量没有上来，表的记录并不多，但是字段多，并且热点数据和非热点数据在一起，单行数据所需的存储空间较大，以至于数据库缓存的数据行减少，查询时回去读磁盘数据产生大量随机读IO，产生IO瓶颈。<br>总结就是，字段太多，查询时产生大量io，产生io瓶颈，导致查询慢，可以使用这种方式</p><h2 id="分库分表工具"><a href="#分库分表工具" class="headerlink" title="分库分表工具"></a>分库分表工具</h2><ol><li>List item</li><li>sharding-jdbc（当当）</li><li>TSharding（蘑菇街）</li><li>Atlas（奇虎360）</li><li>Cobar（阿里巴巴）</li><li>MyCAT（基于Cobar）</li><li>Oceanus（58同城）</li><li>Vitess（谷歌） 各种工具的利弊自查</li></ol><p>原文链接：<a href="https://juejin.im/post/6844903992909103117">https://juejin.im/post/6844903992909103117</a></p><h1 id="分库分表带来的问题"><a href="#分库分表带来的问题" class="headerlink" title="分库分表带来的问题"></a>分库分表带来的问题</h1><p>分库分表能有效缓解单机和单表带来的性能瓶颈和压力，突破网络IO、硬件资源、连接数的瓶颈，同时也带来一些问题，下面将描述这些问题和解决思路。</p><h2 id="事务一致性问题"><a href="#事务一致性问题" class="headerlink" title="事务一致性问题"></a>事务一致性问题</h2><h3 id="分布式事务"><a href="#分布式事务" class="headerlink" title="分布式事务"></a>分布式事务</h3><p>当更新内容同时存在于不同库找那个，不可避免会带来跨库事务问题。跨分片事务也是分布式事务，没有简单的方案，一般可使用“XA协议”和“两阶段提交”处理。<br>分布式事务能最大限度保证了数据库操作的原子性。但在提交事务时需要协调多个节点，推后了提交事务的时间点，延长了事务的执行时间，导致事务在访问共享资源时发生冲突或死锁的概率增高。随着数据库节点的增多，这种趋势会越来越严重，从而成为系统在数据库层面上水平扩展的枷锁。</p><h3 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a>最终一致性</h3><p>对于那些性能要求很高，但对一致性要求不高的系统，往往不苛求系统的实时一致性，只要在允许的时间段内达到最终一致性即可，可采用事务补偿的方式。与事务在执行中发生错误立刻回滚的方式不同，事务补偿是一种事后检查补救的措施，一些常见的实现方法有：对数据进行对账检查，基于日志进行对比，定期同标准数据来源进行同步等。</p><h2 id="跨节点关联查询join问题"><a href="#跨节点关联查询join问题" class="headerlink" title="跨节点关联查询join问题"></a>跨节点关联查询join问题</h2><p>切分之前，系统中很多列表和详情表的数据可以通过join来完成，但是切分之后，数据可能分布在不同的节点上，此时join带来的问题就比较麻烦了，考虑到性能，尽量避免使用Join查询。解决的一些方法：</p><h3 id="全局表"><a href="#全局表" class="headerlink" title="全局表"></a>全局表</h3><p>全局表，也可看做“数据字典表”，就是系统中所有模块都可能依赖的一些表，为了避免库join查询，可以将这类表在每个数据库中都保存一份。这些数据通常很少修改，所以不必担心一致性的问题。</p><h3 id="字段冗余"><a href="#字段冗余" class="headerlink" title="字段冗余"></a>字段冗余</h3><p>一种典型的反范式设计，利用空间换时间，为了性能而避免join查询。例如，订单表在保存userId的时候，也将userName也冗余的保存一份，这样查询订单详情顺表就可以查到用户名userName，就不用查询买家user表了。但这种方法适用场景也有限，比较适用依赖字段比较少的情况，而冗余字段的一致性也较难保证。</p><h3 id="数据组装"><a href="#数据组装" class="headerlink" title="数据组装"></a>数据组装</h3><p>在系统service业务层面，分两次查询，第一次查询的结果集找出关联的数据id，然后根据id发起器二次请求得到关联数据，最后将获得的结果进行字段组装。这是比较常用的方法。</p><h3 id="ER分片"><a href="#ER分片" class="headerlink" title="ER分片"></a>ER分片</h3><p>关系型数据库中，如果已经确定了表之间的关联关系（如订单表和订单详情表），并且将那些存在关联关系的表记录存放在同一个分片上，那么就能较好地避免跨分片join的问题，可以在一个分片内进行join。在1:1或1：n的情况下，通常按照主表的ID进行主键切分。</p><h2 id="跨节点分页、排序、函数问题"><a href="#跨节点分页、排序、函数问题" class="headerlink" title="跨节点分页、排序、函数问题"></a>跨节点分页、排序、函数问题</h2><p>跨节点多库进行查询时，会出现limit分页、order by 排序等问题。分页需要按照指定字段进行排序，当排序字段就是分页字段时，通过分片规则就比较容易定位到指定的分片；当排序字段非分片字段时，就变得比较复杂.需要先在不同的分片节点中将数据进行排序并返回，然后将不同分片返回的结果集进行汇总和再次排序，最终返回给用户如下图：<br><img src="/mysql/mysql-fen-ku-fen-biao/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center.png" alt="在这里插入图片描述"></p><p>上图只是取第一页的数据，对性能影响还不是很大。但是如果取得页数很大，情况就变得复杂的多，因为各分片节点中的数据可能是随机的，为了排序的准确性，需要将所有节点的前N页数据都排序好做合并，最后再进行整体排序，这样的操作很耗费CPU和内存资源，所以页数越大，系统性能就会越差。<br>在使用Max、Min、Sum、Count之类的函数进行计算的时候，也需要先在每个分片上执行相应的函数，然后将各个分片的结果集进行汇总再次计算。</p><h2 id="全局主键避重问题"><a href="#全局主键避重问题" class="headerlink" title="全局主键避重问题"></a>全局主键避重问题</h2><p>在分库分表环境中，由于表中数据同时存在不同数据库中，主键值平时使用的自增长将无用武之地，某个分区数据库自生成ID无法保证全局唯一。因此需要单独设计全局主键，避免跨库主键重复问题。这里有一些策略：</p><h3 id="UUID"><a href="#UUID" class="headerlink" title="UUID"></a>UUID</h3><p>UUID标准形式是32个16进制数字，分为5段，形式是8-4-4-4-12的36个字符。UUID是最简单的方案，本地生成，性能高，没有网络耗时，但是缺点明显，占用存储空间多，另外作为主键建立索引和基于索引进行查询都存在性能问题，尤其是InnoDb引擎下，UUID的无序性会导致索引位置频繁变动，导致分页。</p><h3 id="结合数据库维护主键ID表"><a href="#结合数据库维护主键ID表" class="headerlink" title="结合数据库维护主键ID表"></a>结合数据库维护主键ID表</h3><p>在数据库中建立sequence表：</p><pre><code>CREATE TABLE `sequence` (    `id` bigint(20) unsigned NOT NULL auto_increment,    `stub` char(1) NOT NULL default &#39;&#39;,    PRIMARY KEY  (`id`),    UNIQUE KEY `stub` (`stub`)  ) ENGINE=MyISAM;</code></pre><p>stub字段设置为唯一索引，同一stub值在sequence表中只有一条记录，可以同时为多张表生辰全局ID。使用MyISAM引擎而不是InnoDb，已获得更高的性能。MyISAM使用的是表锁，对表的读写是串行的，所以不用担心并发时两次读取同一个ID。当需要全局唯一的ID时，执行：</p><pre><code>REPLACE INTO sequence (stub) VALUES (&#39;a&#39;);  SELECT 1561439;  SELECT LAST_INSERT_ID();</code></pre><p>此方案较为简单，但缺点较为明显：存在单点问题，强依赖DB，当DB异常时，整个系统不可用。配置主从可以增加<br>可用性。另外性能瓶颈限制在单台Mysql的读写性能。</p><p>另有一种主键生成策略，类似sequence表方案，更好的解决了单点和性能瓶颈问题。这一方案的整体思想是：建立2个以上的全局ID生成的服务器，每个服务器上只部署一个数据库，每个库有一张sequence表用于记录当前全局ID。表中增长的步长是库的数量，起始值依次错开，这样就能将ID的生成散列到各个数据库上<br><img src="/mysql/mysql-fen-ku-fen-biao/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center-16448205707311.png" alt="在这里插入图片描述"></p><p>这种方案将生成ID的压力均匀分布在两台机器上，同时提供了系统容错，第一台出现了错误，可以自动切换到第二台获取ID。但有几个缺点：系统添加机器，水平扩展较复杂；每次获取ID都要读取一次DB，DB的压力还是很大，只能通过堆机器来提升性能。</p><h2 id="Snowflake分布式自增ID算法"><a href="#Snowflake分布式自增ID算法" class="headerlink" title="Snowflake分布式自增ID算法"></a>Snowflake分布式自增ID算法</h2><p>Twitter的snowfalke算法解决了分布式系统生成全局ID的需求，生成64位Long型数字，组成部分：</p><p>第一位未使用</p><p>接下来的41位是毫秒级时间，41位的长度可以表示69年的时间</p><p>5位datacenterId,5位workerId。10位长度最多支持部署1024个节点</p><p>最后12位是毫秒内计数，12位的计数顺序号支持每个节点每毫秒产生4096个ID序列。</p><h2 id="数据迁移、扩容问题"><a href="#数据迁移、扩容问题" class="headerlink" title="数据迁移、扩容问题"></a>数据迁移、扩容问题</h2><p>当业务高速发展、面临性能和存储瓶颈时，才会考虑分片设计，此时就不可避免的需要考虑历史数据的迁移问题。一般做法是先读出历史数据，然后按照指定的分片规则再将数据写入到各分片节点中。此外还需要根据当前的数据量个QPS，以及业务发展速度，进行容量规划，推算出大概需要多少分片（一般建议单个分片的单表数据量不超过1000W）</p><h2 id="什么时候考虑分库分表"><a href="#什么时候考虑分库分表" class="headerlink" title="什么时候考虑分库分表"></a>什么时候考虑分库分表</h2><p>能不分就不分<br>并不是所有表都需要切分，主要还是看数据的增长速度。切分后在某种程度上提升了业务的复杂程度。不到万不得已不要轻易使用分库分表这个“大招”，避免“过度设计”和“过早优化”。分库分表之前，先尽力做力所能及的优化：升级硬件、升级网络、读写分离、索引优化等。当数据量达到单表瓶颈后，在考虑分库分表。</p><p>数据量过大，正常运维影响业务访问<br>这里的运维是指：</p><p>对数据库备份，如果单表太大，备份时需要大量的磁盘IO和网络IO</p><p>对一个很大的表做DDL，MYSQL会锁住整个表，这个时间会很长，这段时间业务不能访问此表，影响很大。</p><p>大表经常访问和更新，就更有可能出现锁等待。</p><p>随着业务发展，需要对某些字段垂直拆分<br>这里就不举例了。在实际业务中都可能会碰到，有些不经常访问或者更新频率低的字段应该从大表中分离出去。</p><p>数据量快速增长<br>随着业务的快速发展，单表中的数据量会持续增长，当性能接近瓶颈时，就需要考虑水平切分，做分库分表了。</p>]]></content>
      
      
      <categories>
          
          <category> Mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mysql </tag>
            
            <tag> 分库分表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx学习笔记</title>
      <link href="/nginx/nginx/"/>
      <url>/nginx/nginx/</url>
      
        <content type="html"><![CDATA[<h3 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h3><h4 id="什么是nginx"><a href="#什么是nginx" class="headerlink" title="什么是nginx?"></a>什么是nginx?</h4><p>Nginx (engine x) 是一个高性能的HTTP和反向代理web服务器，同时也提供了IMAP&#x2F;POP3&#x2F;SMTP服务。</p><p>其特点是占有内存少，并发能力强  </p><p><strong>HTTP</strong>：说明nginx可以提供http服务</p><p>​web服务器：提供资源服务<br>​             nginx做web服务器只能支持<strong>静态资源</strong></p><p><strong>正向代理和反向代理:</strong><br>                  正向代理 代理的是客户端<br>                  反向代理 代理的是服务器端</p><p><strong>反向代理</strong>：反向代理服务器位于用户与目标服务器之间，但是对于用户而言，反向代理服务器就相当于目标服务器，即用户直接访问反向代理服务器就可以获得目标服务器的资源。同时，用户不需要知道目标服务器的地址，也无须在用户端作任何设定。反向代理服务器通常可用来作为Web加速，即使用反向代理作为Web服务器的前置机来降低网络和服务器的负载，提高访问效率。</p><p><strong>IMAP&#x2F;POP3&#x2F;SMTP服务</strong>:nginx可以作为邮件服务器使用</p><h4 id="怎么安装Nginx？"><a href="#怎么安装Nginx？" class="headerlink" title="怎么安装Nginx？"></a>怎么安装Nginx？</h4><p> <a href="https://www.cnblogs.com/james-roger/p/7804612.html">https://www.cnblogs.com/james-roger/p/7804612.html</a> </p><p>1、安装prce（重定向支持）和openssl（https支持，如果不需要https可以不安装）</p><pre><code>yum -y install pcre*yum -y install openssl*</code></pre><p>2、下载Nginx.jar,进行上传，解压</p><p>3、进入目录编译安装，</p><pre><code>cd nginx-1.7.8./configure --prefix=/usr/local/nginx(此目录必须先建好)</code></pre><p>4、不报错执行</p><pre><code>makemake install</code></pre><p>5、开启Nginx</p><p>sbin&#x2F;nginx</p><p>进入到欢迎页面，即成功</p><h4 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h4><h5 id="反向代理负载均衡配置文件"><a href="#反向代理负载均衡配置文件" class="headerlink" title="反向代理负载均衡配置文件"></a>反向代理负载均衡配置文件</h5><p>修改nginx.conf配置文件</p><p>这里使用Nginx来负载均衡2个tomcat的访问请求</p><pre><code class="conf">#upstream 修改哪几台进行负载均衡upstream localhost&#123;    #1是请求比例3分之一请求到8081端口上    server localhost:8081 weight=1;    server localhost:8082 weight=2;&#125;#nginx服务默认监听80端口  80端口是http访问的默认端口server &#123;        listen       80;        #server_name  localhost;        server_name flume01;        #charset koi8-r;        #access_log  logs/host.access.log  main;        #在location内部要配置此时nginx作用是代理服务器        #location进行了修改，使用正则表达式，满足正则表达式则可以触发收集日志操作，写入到logs中的日志文件中        location / &#123;               proxy_connect_timeout   3;               proxy_send_timeout      30;               proxy_read_timeout      30;              #代理的地址               proxy_pass http://localhost;            #root   html;  指明请求资源保存的位置 默认是安装之后的html目录 可以改            #index  index.html index.htm;  指明哪个页面是欢迎页面        &#125;</code></pre><h5 id="收集日志配置文件"><a href="#收集日志配置文件" class="headerlink" title="收集日志配置文件"></a>收集日志配置文件</h5><p>修改nginx.conf配置文件</p><pre><code>#采集的名称lf  log_format  配置日志写入的内容和日志的格式log_format lf &#39;$remote_addr^A$msec^A$http_host^A$request_uri&#39;;#ip地址#时间戳#访问域名和ip地址#客户端uriserver &#123;        listen       80;        #server_name  localhost;        server_name hadoop01;        location ~ .*(bwImg)\.(gif)$        &#123;           default_type image/gif;           access_log /export/server/nginx/logs/access_log.log lf;           root /export/server/nginx/source;        &#125;</code></pre><h3 id="Nginx常用使用方式"><a href="#Nginx常用使用方式" class="headerlink" title="Nginx常用使用方式"></a>Nginx常用使用方式</h3><h4 id="做静态资源服务器"><a href="#做静态资源服务器" class="headerlink" title="做静态资源服务器"></a>做静态资源服务器</h4><p>在实际项目中通常都是把动态资源  静态资源分开部署</p><p>1、编辑配置文件，在http {下面添加以下内容：</p><blockquote><p>vi &#x2F;etc&#x2F;nginx&#x2F;nginx.conf</p></blockquote><pre><code>autoindex on; #开启nginx目录浏览功能autoindex_exact_size off; #文件大小从KB开始显示autoindex_localtime on; #显示文件修改时间为服务器本地时间</code></pre><p><img src="/nginx/nginx/image-20210122141501484.png" alt="配置图"></p><p>2、重新启动nginx</p><pre><code>nginx -s stop ＃停止nginxnginx     #重新启动ngnix</code></pre><p>3、上传资源到Nginx目录下</p><blockquote><p>&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html</p></blockquote><p><img src="/nginx/nginx/image-20210122141927043.png" alt="image-20210122141927043"></p><p>4、尝试外部访问资源</p><blockquote><p>访问成功</p></blockquote><p><img src="/nginx/nginx/image-20210122142224262.png" alt="image-20210122142224262"></p><h4 id="做负载均衡服务器"><a href="#做负载均衡服务器" class="headerlink" title="做负载均衡服务器"></a>做负载均衡服务器</h4><p>例子：使用nginx  给2个tomcat服务端 做负载均衡</p><p>2台tomcat是在1台机器安装了2个tomcat实例</p><p>如何在1台机器可以安装多个tomcat实例?<br>修改2个tomcat实例的端口号来区分不同的tomcat实例<br>               修改接收关闭信息的端口号<br>               修改接收http请求的端口号<br>               修改ajp协议的端口号</p><pre><code></code></pre><h4 id="做日志服务器"><a href="#做日志服务器" class="headerlink" title="做日志服务器"></a>做日志服务器</h4><p>在当前的项目中模拟点击网站上某张图片  记录行为数据 写入nginx日志文件</p><p>nginx.conf文件中的log_format  配置日志写入的内容和日志的格式</p><pre><code>log_format lf &#39;$remote_addr^A$msec^A$http_host^A$request_uri&#39;;//日志写入数据的分隔符是&quot;^A&quot;            //日志写入数据的内容是             //客户端的ip地址            //写入日志的时间戳            //客户端输入的访问域名或者ip地址            //客户端请求资源的uri日志样例：192.168.108.1^A1576549355.042^A192.168.108.128^A/bwImg.gif</code></pre><pre><code>location进行了修改:        使用正则表达式指定了为哪些请求提供服务        .*(bwImg)\.(gif)$       http://192.168.108.128/bwImg.gif?xx=aa&amp;yy=bb        access_log 配置的是日志文件的位置和名称        root 配置访问资源图片的位置日志样例：192.168.108.1^A1576549355.042^A192.168.108.128^A/bwImg.gif</code></pre>]]></content>
      
      
      <categories>
          
          <category> Nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase的学习笔记</title>
      <link href="/cdh/hbase/hbase/"/>
      <url>/cdh/hbase/hbase/</url>
      
        <content type="html"><![CDATA[<h2 id="Hbase"><a href="#Hbase" class="headerlink" title="Hbase"></a>Hbase</h2><p>谷歌三篇论文中的bigtable衍生而来</p><h2 id="什么是Hbase？"><a href="#什么是Hbase？" class="headerlink" title="什么是Hbase？"></a>什么是Hbase？</h2><p><strong>hbase是一个高可靠，高性能，面向列，可伸缩的分布式存储系统，非关系型数据库，典型的nosql，低延迟，存储容量多</strong>只能够进行简单的增删改查(没有条件)，不支持事务、join、group by等，是对mysql的优化，单位是TB级别的，他的读写是实时的，因为进入memstore后，会返回结果</p><p>单元格在没有值的时候，不会显示null</p><p>hbase是一个面向列的数据库，表中由行排序，一个表中有多个列族，每一个列族由任意数量的列。后序列的值连续存储到磁盘上。hbase根据列族来存储数据的，列族在创建表的时候就必须指定。</p><ul><li><p>表是行的集合。</p></li><li><p>行是列族的集合。</p></li><li><p>列族是列的集合。</p></li><li><p>列是键值对的集合。</p></li></ul><p>是一主多从结构的，Master&#x2F;Slave架构，将数据存储到hdfs中</p><h2 id="什么是NoSQL"><a href="#什么是NoSQL" class="headerlink" title="什么是NoSQL"></a>什么是NoSQL</h2><p>NoSQL(NoSQL &#x3D; Not Only SQL )，不仅仅是SQL，具体来说就是跟关系型数据库有些类似（查询低延迟），但同时能够存储的数据类型却更加灵活。</p><p>NoSQL数据库的产生就是为了解决大规模数据集合多重数据种类带来的挑战，尤其是大数据应用难题。</p><h3 id="常见NoSQL数据库"><a href="#常见NoSQL数据库" class="headerlink" title="常见NoSQL数据库"></a>常见NoSQL数据库</h3><p>非关系型数据库——列存储（HBase）<br>非关系型数据库——文档型存储（MongoDb）<br>非关系型数据库——内存式存储（redis）  KV  memorycache<br>非关系型数据库——图形模型（Graph）</p><h2 id="Hbase全分布式的安装"><a href="#Hbase全分布式的安装" class="headerlink" title="Hbase全分布式的安装"></a>Hbase全分布式的安装</h2><p>(保证有一个独立的zookeeper集群)</p><p>1、在上传hbase的tar包后进行解压</p><p>2、修改&#x2F;etc&#x2F;profile文件</p><p>source &#x2F;etc&#x2F;profile 刷新配置文件</p><pre><code>export JAVA_HOME=/root/Downloads/jdk1.8.0_161export PATH=$PATH:$JAVA_HOME/binexport HADOOP_HOME=/root/Downloads/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport ZOOKEEPER_HOME=/root/Downloads/zookeeper-3.4.5export PATH=$PATH:$ZOOKEEPER_HOME/binexport HIVE_HOME=/root/Downloads/apache-hive-1.2.0-binexport PATH=$PATH:$HIVE_HOME/binexport FLUME_HOME=/root/Downloads/apache-flume-1.6.0-binexport PATH=$PATH:$FLUME_HOME/binexport HBASE_HOME=/root/Downloads/hbase-1.2.6export PATH=$PATH:$HBASE_HOME/bin</code></pre><p>3、修改conf下的hbase-env.sh</p><pre><code>#不使用hbase本身的zookeeperexport HBASE_MANAGES_ZK=falseexport JAVA_HOME=/root/Downloads/jdk1.8.0_161export HADOOP_HOME=/root/Downloads/hadoop-2.6.5</code></pre><p>4、修改conf下的hbase-site.xml</p><pre><code class="xml">&lt;!-- 配置HBase使用分布式方式--&gt;&lt;property&gt;    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!--hbase中的数据在HDFS上的位置（此处假设HDFS已经做了HA）--&gt;这个目录不需要自己创建，HBase会帮助我们进行配置。如果你自己创建了这个目录，HBase会尝试将其做一个迁移，这可能不是你想要的结果。&lt;property&gt;    &lt;name&gt;root.dir&lt;/name&gt;    &lt;value&gt;hdfs://linux01:9000/hbase&lt;/value&gt;&lt;/property&gt;&lt;!--hbase的zookeeper的进程(节点)--&gt;&lt;property&gt;    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;    &lt;value&gt;linux01,linux02,linux03&lt;/value&gt;&lt;/property&gt;&lt;!--hbase中保存zookeeper数据的文件夹地址--&gt;&lt;property&gt;    &lt;name&gt;hbase.zookeeper.property.data.dir&lt;/name&gt;    &lt;value&gt;/home/bigdata/hbase/var/zookeeper&lt;/value&gt;&lt;/property&gt;</code></pre><p>5、修改conf下的regionservers文件</p><p>配置hbase regin的机器名称，跟hadoop中的slaves文件类似</p><pre><code>linux01linux02linux03</code></pre><p>6、把hbase的包scp到其他机器上</p><pre><code>scp -r hbase-1.2.6 root@linux02:/root/Downloads/scp -r hbase-1.2.6 root@linux03:/root/Downloads/</code></pre><p>7、启动hbase</p><p>hbase依赖hadoop和zookeeper，所以在启动Hbase时，保证两者已经启动</p><pre><code>start-all.sh  启动集群zkServer.sh start  启动zookeeper  三台都得启动start-hbase.sh  启动hbase服务</code></pre><p>8、jps查看进程</p><pre><code>QuorumPeerMain是zookeeper的进程已启动HquorumPeerHbase内部的zookeeper进程已启动HRegionServer HMaster Main</code></pre><p>9、进入HBase</p><pre><code>hbase shell 进入Hbase的客户端quit退出Hbase客户端sh stop-hbase.sh 关闭Hbase服务</code></pre><p>10、浏览器中查看hbase的监控页面</p><pre><code>linux01:16010可以查看    master节点运行情况    每个regionserver的运行情况    已经创建数据表的状态    默认配置项 (hbase configuration可以找到hbase的所有配置项，都可以放到hbase-site.xml中进行配置)start-hbase.sh启动的机器就是HMaster所在的的位置只能通过HMaster访问</code></pre><h2 id="HBase命令"><a href="#HBase命令" class="headerlink" title="HBase命令"></a>HBase命令</h2><h4 id="查看命名空间"><a href="#查看命名空间" class="headerlink" title="查看命名空间"></a>查看命名空间</h4><pre><code>list_namespace  列出所有的命名空间//在HBase中有两个默认的命名空间,在创建表时不指定命名空间时会默认放在defaultNAMESPACEdefault</code></pre><h4 id="命名空间查看表"><a href="#命名空间查看表" class="headerlink" title="命名空间查看表"></a>命名空间查看表</h4><pre><code>list_namespace_tables &#39;命名空间名称&#39;</code></pre><h4 id="创建命名空间"><a href="#创建命名空间" class="headerlink" title="创建命名空间"></a>创建命名空间</h4><pre><code>create_namespace &#39;命名空间名称&#39;例：create_namespace &#39;n1&#39;</code></pre><h4 id="删除命名空间"><a href="#删除命名空间" class="headerlink" title="删除命名空间"></a>删除命名空间</h4><pre><code>drop_namespace &#39;命名空间名称&#39;</code></pre><h4 id="查看表"><a href="#查看表" class="headerlink" title="查看表"></a>查看表</h4><pre><code>list</code></pre><h4 id="查询服务器的状态"><a href="#查询服务器的状态" class="headerlink" title="查询服务器的状态"></a>查询服务器的状态</h4><pre><code>status</code></pre><h4 id="查看HBase版本"><a href="#查看HBase版本" class="headerlink" title="查看HBase版本"></a>查看HBase版本</h4><pre><code>version</code></pre><h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><pre><code>语法：create &#39;表名称&#39;,&#39;列族&#39;例：create &#39;n1:student&#39;,&#39;name&#39;,&#39;info&#39;,&#39;address&#39;//创建数据表时 列族设计固定即可(列族最多5个  习惯使用不要超过3个  列族的个数越少越好)//&#39;n1:&#39;可以指定表放在哪个命名空间中//在HBase中有两个默认的命名空间,在创建表时不指定命名空间时会默认放在defaultNAMESPACEdefault</code></pre><h4 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h4><pre><code>drop &#39;表名&#39;在删除表的时候需要把表禁用disable &#39;表名&#39; 禁用表enable &#39;表名&#39; 开启表</code></pre><h4 id="判断表是否enable开启"><a href="#判断表是否enable开启" class="headerlink" title="判断表是否enable开启"></a>判断表是否enable开启</h4><pre><code>is_enabled &#39;表名&#39;</code></pre><h4 id="判断表是否disable禁用"><a href="#判断表是否disable禁用" class="headerlink" title="判断表是否disable禁用"></a>判断表是否disable禁用</h4><pre><code>is_disabled &#39;表名&#39;</code></pre><h4 id="插入记录-amp-amp-修改数据"><a href="#插入记录-amp-amp-修改数据" class="headerlink" title="插入记录&amp;&amp;修改数据"></a>插入记录&amp;&amp;修改数据</h4><pre><code>put &#39;表名&#39;,&#39;row_key行名&#39;,&#39;列名&#39;,&#39;值&#39;例：put &#39;n1:member2&#39;,&#39;zhangsan&#39;,&#39;info:age&#39;,&#39;99&#39;//必须指定rowkey的值。//在hbase没有修改，没有update，不是修改原值，二十在添加一个新版本的值</code></pre><h4 id="增加列族的版本"><a href="#增加列族的版本" class="headerlink" title="增加列族的版本"></a>增加列族的版本</h4><pre><code>//将列族f1的版本扩大3。alter &#39;n1:t1&#39;, NAME =&gt; &#39;f1&#39;, VERSIONS =&gt; 3//修改值三次后put &#39;n1:t1&#39;,&#39;r1&#39;,&#39;f1:c1&#39;,&#39;one01&#39;put &#39;n1:t1&#39;,&#39;r1&#39;,&#39;f1:c1&#39;,&#39;one02&#39;put &#39;n1:t1&#39;,&#39;r1&#39;,&#39;f1:c1&#39;,&#39;one03&#39;get &#39;n1:t1&#39;, &#39;r1&#39;, &#123;COLUMN =&gt; &#39;f1:c1&#39;, VERSIONS =&gt; 3&#125;会同时显示 &#39;f1:c1&#39; 3个版本的值get &#39;n1:t1&#39;,&#39;r1&#39;,&#39;f1:c1&#39;只显示 &#39;f1:c1&#39; 最高版本即最新的值</code></pre><h4 id="全表查询"><a href="#全表查询" class="headerlink" title="全表查询"></a>全表查询</h4><pre><code>scan &#39;表名&#39;scan &#39;表名&#39;,&#39;列名称&#39;scan &#39;表名&#39;,&#123;COLUMNS=&gt;&#39;列族&#39;&#125;scan &#39;表名&#39;,&#123;COLUMNS=&gt;[&#39;列&#39;,&#39;列&#39;]&#125;//显示表中列族所有的版本号的值scan &#39;n1:t1&#39;, &#123;RAW =&gt; true, VERSIONS =&gt; 3&#125;scan &#39;member2&#39;,&#123;COLUMNS=&gt;[&#39;info:age&#39;,&#39;info:birthday&#39;]&#125;//scan只适合全表扫描，不适合多条件查询，不要轻易使用scan//aip中可以使用scan对象结合hbase，api中提供filter过滤器配合查询</code></pre><h4 id="单行查询"><a href="#单行查询" class="headerlink" title="单行查询"></a>单行查询</h4><pre><code>get &#39;表名&#39;,&#39;行名&#39;,&#39;列名&#39;例：get &#39;n1:t1&#39;,&#39;r1&#39; //查询 &#39;n1:t1&#39; 数据表中  &#39;r1&#39; 行的数据例：get &#39;n1:t1&#39;,&#39;r1&#39;,&#39;f1:c1&#39;//查询 &#39;n1:t1&#39; 数据表中  &#39;r1&#39;行中  &#39;f1:c1&#39; 列的值</code></pre><h4 id="删除数据"><a href="#删除数据" class="headerlink" title="删除数据"></a>删除数据</h4><pre><code>delete &#39;表名&#39;,&#39;行名&#39;,&#39;列名&#39;//删除一行中的一部分delete &#39;n1:t1&#39;,&#39;r2&#39;,&#39;f1:c1&#39;//一次性删除指定行的所有数据deleteall &#39;n1:t1&#39;,&#39;r2&#39;//hbase表中没有实际删除数据，而只是给数据打上一个删除标记</code></pre><h4 id="清空表数据"><a href="#清空表数据" class="headerlink" title="清空表数据"></a>清空表数据</h4><pre><code>truncate &#39;表名&#39;这是对表中数据进行操作是将表删除后再重新创建</code></pre><h4 id="查看行数"><a href="#查看行数" class="headerlink" title="查看行数"></a>查看行数</h4><pre><code>count &#39;表名&#39;count &#39;member2&#39;</code></pre><h4 id="判断表是否存在"><a href="#判断表是否存在" class="headerlink" title="判断表是否存在"></a>判断表是否存在</h4><pre><code>语法：exists &#39;表名&#39;实例：exists &#39;member&#39;</code></pre><h4 id="表的字段信息"><a href="#表的字段信息" class="headerlink" title="表的字段信息"></a>表的字段信息</h4><pre><code>语法：describe &#39;表名&#39;实例：describe &#39;user&#39;</code></pre><h4 id="删除一个列族"><a href="#删除一个列族" class="headerlink" title="删除一个列族"></a>删除一个列族</h4><pre><code>语法：alter &#39;表名&#39;,NAME=&gt;&#39;列名&#39;,METHOD=&gt;&#39;delete&#39;实例：alter &#39;member&#39;,NAME=&gt;&#39;member_id&#39;,METHOD=&gt;&#39;delete&#39;</code></pre><h4 id="timestamp时间戳"><a href="#timestamp时间戳" class="headerlink" title="timestamp时间戳"></a>timestamp时间戳</h4><pre><code>get &#39;表名&#39;,&#39;行名&#39;,&#123;COLUMN=&gt;&#39;列名&#39;,TIMESTAMP=&gt;时间戳&#125;可以根据时间戳来获取不同的版本get &#39;表名&#39;,&#39;行名&#39;,&#123;COLUMN=&gt;[],TIMESTAMP=&gt;时间戳&#125;</code></pre><h4 id="flush刷新"><a href="#flush刷新" class="headerlink" title="flush刷新"></a>flush刷新</h4><p><strong>flush是把每个store中的memstore内存中的数据手动刷新到硬盘保存</strong><br>每次执行一次对表或者对区域flush操作，都会多一份物理数据文件<br>flush刷新可以是定时或者定量在每次memstore写满时需要刷出数据，可以使用配置文件定memstore内存<br><strong>flush 之后 更新的过程数据  和 删除打标记数据还是存在</strong></p><h4 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h4><pre><code>major_compact &#39;n1:t1&#39;//清除无用数据</code></pre><p>执行之后修改的过程版本还在，但是打删除标记的无用数据直接被清除。</p><p>对于多个flush产生物理文件进行合并，合并成一个文件</p><h4 id="切割"><a href="#切割" class="headerlink" title="切割"></a>切割</h4><pre><code></code></pre><h4 id="预切割"><a href="#预切割" class="headerlink" title="预切割"></a>预切割</h4><pre><code></code></pre><h2 id="HBase的存储"><a href="#HBase的存储" class="headerlink" title="HBase的存储"></a>HBase的存储</h2><p>hbase的数据存储在hdfs上，在hbase以Hregion进行存储，region可以理解成hdfs上的数据指针</p><p>region是hbase上数据分配的基本单位，每个hregion都会存储在一个regionserver上，regionserver有多个hregion</p><h3 id="两个系统表"><a href="#两个系统表" class="headerlink" title="两个系统表"></a>两个系统表</h3><p>HBase中存有两张特殊的表，-ROOT-和.META.。<br><strong>.META.<strong>：记录了用户表的Region的起始rowkey和结束rowkey，及region的范围。一个region会在.meta表中对应一条数据。.meta表也可理解成hbase中的一个表，仍以region为单位进行存储，当数据量过多，同样</strong>会进行分裂</strong>，会存在多个.meta.表，会花费大量的时间检索meta；在此阶段：用户检索信息：先检索.meta.获取真正的数据存储在哪个region上，然后进行访问。</p><p> -**ROOT-**：存储.meta 的索引表，记录了.META.表的Region信息，-ROOT-只有一个region, <strong>root表不进行分裂</strong>。Zookeeper中记录了-ROOT-表的 location</p><p><strong>寻址过程</strong>：-root表的region位置存在zookeeper中，所以在进行寻址时，先去zookeeper的-root中检索.meta的存储位置，再去相应的meta的region中查询原始数据的region的存储位置，最后去相应的region直接检索要查询的原始数据</p><h3 id="默认的两个命名空间"><a href="#默认的两个命名空间" class="headerlink" title="默认的两个命名空间"></a>默认的两个命名空间</h3><p><strong>default</strong>：如果在自定义数据表时，不指明保存在那个命名空间，默认放在default下</p><p><strong>hbase</strong>：</p><h3 id="HBase存储格式"><a href="#HBase存储格式" class="headerlink" title="HBase存储格式"></a>HBase存储格式</h3><p>hbase数据库中的数据存储格式都是byte[]字节数组，可以存储任何格式的数据</p><h3 id="Hbase的文件分割过程"><a href="#Hbase的文件分割过程" class="headerlink" title="Hbase的文件分割过程"></a>Hbase的文件分割过程</h3><p>所有的行都按照row key 的字典排序，table行的方向分隔成多个hregion，</p><p>HMaster来管理HRegionServer，</p><p>HRegionserer来对数据进行计算</p><p>HRegionserver对HRegion和Hlog进行管理</p><p>HRegion是存储数据的地方，里面有Store</p><p>store是最小的单位</p><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><p><strong>结构化</strong>：数据结构字段含义确定，清晰，典型的如数据库中的表结构</p><p><strong>半结构化</strong>：具有一定结构的，但语义不够明确，典型的如html网页有些字段是确定的(title),有些是不确定的（table）</p><p><strong>非结构化</strong>：杂乱无章的数据，很难按照一个感念去抽取，无规律性</p><p>mysql——–写模式：写数据时进行校验</p><p>hive —–读模式的数据仓库：读数据的时候进行校验</p><p>hbase—–无严格模式的数据库</p><p>hbase中的数据最终存储在hdfs上</p><p><img src="/cdh/hbase/hbase/image-20191127114211875.png" alt="image-20191127114211875"></p><h4 id="逻辑模型"><a href="#逻辑模型" class="headerlink" title="逻辑模型"></a>逻辑模型</h4><p>HBase使用坐标来识别单元里的数据</p><p>行键+列族+列+时间戳  来取一个值</p><h4 id="存储模型"><a href="#存储模型" class="headerlink" title="存储模型"></a>存储模型</h4><p>存储模型：列式存储</p><p>(还有SQL模式和行式模式)</p><h4 id="物理模型"><a href="#物理模型" class="headerlink" title="物理模型"></a>物理模型</h4><p>物理模型：面向列族</p><p>HBase按照列族分组，每个列族在硬盘上有自己的HFile集合。物理上的隔离允许列族底层HFile层面上分别进行管理。进一步考虑到合并，每个列族的HFile都是单独管理的</p><p>HBase的记录按照键值对存储在HFile里，<strong>HFile自身是二进制文件</strong>，不是直接可读的。一行中一个列族的的数据不一定放在同一个HFile里。一行中的列族数据需要物理存放在一起。</p><p>1.每个列族使用自己的HFile，意味着，当执行读操作时HBase不需要读出一行中的所有数据，只读取用得到的列族数据。</p><p>2.面向列代表着指定单元时，HBase不需要读占位符记录。</p><p>这两个物理细节有利于稀疏数据集合的高效存储和快速读取</p><p><img src="/cdh/hbase/hbase/image-20220130184641486.png" alt="image-20220130184641486"></p><p><strong>hbase&#x2F;data&#x2F;命名空间&#x2F;表名&#x2F;region&#x2F;列族&#x2F;hfile</strong></p><h2 id="HBase基本概念"><a href="#HBase基本概念" class="headerlink" title="HBase基本概念"></a>HBase基本概念</h2><h3 id="RowKey（行键）"><a href="#RowKey（行键）" class="headerlink" title="RowKey（行键）"></a>RowKey（行键）</h3><p>是Byte array，是表中每条记录的“主键”，方便快速查找，Rowkey的设计非常重要；是一行数据的标志。</p><p>hbase中会对行键进行字典顺序排序</p><h4 id="Rowkey的设计原则"><a href="#Rowkey的设计原则" class="headerlink" title="Rowkey的设计原则"></a>Rowkey的设计原则</h4><p><strong>长度原则</strong>：</p><p>rowkey是一个二进制码流，可以是任意字符串，最大长度64kb，实际应用中一般为10-100bytes，以byte[]形式保 存，一般设计成定长。</p><p>数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光 rowkey就要占用100*1000w&#x3D;10亿个字节，将近1G数据，这样会极大影响HFile的存储效率； MemStore将缓存部 分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索 效率。 目前操作系统都是64位系统，内存8字节对齐，控制在16个字节，8字节的整数倍利用了操作系统的最佳特 性。</p><p>最大长度64kb，越短越好，不要超过16字节</p><p><strong>散列原则</strong></p><p>如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。如果没有 散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。</p><p><strong>唯一原则</strong></p><p>必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排 序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。</p><p>主查询条件，一定是放入 rowkey，其他次要条件也依次拼入 rowkey 有一些不那么常用的查询条件，我们可以建二级索引表，来提升查询速度</p><p>测试数据rowkey：start_ts+sessinid</p><h3 id="Row（行）"><a href="#Row（行）" class="headerlink" title="Row（行）"></a><strong>Row（行）</strong></h3><p>由行键和一个或多个列的值组成</p><h3 id="ColumnFamily（列族）"><a href="#ColumnFamily（列族）" class="headerlink" title="ColumnFamily（列族）"></a>ColumnFamily（列族）</h3><p> 列族，是列的集合，拥有一个名称(string)，包含一个或者多个相关列 。</p><p>列中的数据都 是以二进制形式存在，没有数据类型。在物理存储结构上，每个表中的每个列族单独以一个文件夹存储</p><p>需要在表创建时就定义好，不能修改太频繁，数量不能太多，最多五个，一般最多三个，不要太多，会因为列族过多而影响效率。列族中的列数量没有限制。</p><h3 id="Column（列）"><a href="#Column（列）" class="headerlink" title="Column（列）"></a>Column（列）</h3><p>属于某一个columnfamily，familyName:columnName，每条记录可动态添加</p><p>hbase的基本单位，HBase列包含一个列族和列限定符，列属于一个列族，列族属于一个行。列中的内容不需要指定类型，这也是关系型数据库的不同之处。</p><h3 id="Qualifier（列限定符）"><a href="#Qualifier（列限定符）" class="headerlink" title="Qualifier（列限定符）"></a>Qualifier（列限定符）</h3><p>列族有多个列，列限定符就是某个列。列：sex:19，sex就是Qualifier</p><h3 id="Cell（单元格）"><a href="#Cell（单元格）" class="headerlink" title="Cell（单元格）"></a>Cell（单元格）</h3><p>行键、列族、列和时间戳的结合，通过这三个值的坐标来确定一个单元格。单元格的数据是没有类型的，全部以字节码形式存储。没有值就是空(null)，不占用底层物理存储。</p><h3 id="Timestamp（时间戳）"><a href="#Timestamp（时间戳）" class="headerlink" title="Timestamp（时间戳）"></a>Timestamp（时间戳）</h3><p>是列的一个属性，列的单元格值都有时间戳，系统指定，可以被使用，可根据版本(VERSIONS)或时间戳来区分不同版本的值，不同版本的值按照降序排列，访问时优先读取最新的值，新值比老值更容易读取，如果不指定，返回最新版本的值</p><h3 id="Region（区域）"><a href="#Region（区域）" class="headerlink" title="Region（区域）"></a>Region（区域）</h3><p>HBase自动把表水平（row）划分成的多个区域，划分的区域随着数据的增大而增多</p><h2 id="Hbase的结构组件"><a href="#Hbase的结构组件" class="headerlink" title="Hbase的结构组件"></a>Hbase的结构组件</h2><p><img src="/cdh/hbase/hbase/1217276-20180502141711373-31653278.png" alt="img"></p><h3 id="1、client"><a href="#1、client" class="headerlink" title="1、client"></a>1、client</h3><p>使用HBase RPC机制与HMaster和HRegionServer进行通信</p><p>Client与HMaster进行通信进行管理类操作</p><p>Client与HRegionServer进行数据读写类操作</p><h3 id="2、HMaster"><a href="#2、HMaster" class="headerlink" title="2、HMaster"></a>2、HMaster</h3><p>主要就是前三点</p><p>1、为Regionserver分配region</p><p>2、负责Regionserver的负载均衡</p><p>3、发现失效的regionserver重新分配其上的region（故障转移）</p><p>4、GFS上的垃圾回收。</p><p>5、处理用户对表的增删改查操作 </p><p>可以启动多个HMaster，需要zookeeper，通过zookeeper保证总有一个HMaster运行。相当于管理者，类似namenode，其他的HMaster处于热备份的状态。</p><p>一般情况下会启动两个HMaster，非Active的HMaster会定期的和Active HMaster通信以获取其最新状态，从而保证它是实时更新的，因而如果启动了多个HMaster反而增加了Active HMaster的负担。</p><h3 id="3、Zookeeper"><a href="#3、Zookeeper" class="headerlink" title="3、Zookeeper"></a>3、Zookeeper</h3><p>zookeeper为HBase集群提供协调服务，保证只有一个HMaster，避免单点问题，存储着所有region的寻址入口，包括-ROOT-表地址、HMaster地址。管理着HMaster和HRegionServer的状态，并在宕机时通知给HMaster，Zookeeper集群使用了一致性协议（paxos协议）保证了每个结点状态的一致性</p><h3 id="3、Hregionserver"><a href="#3、Hregionserver" class="headerlink" title="3、Hregionserver"></a>3、Hregionserver</h3><p>hmaster的管理单位，是从节点，对文件或数据进行处理及计算，client直接连接HRegionServer，并通信获取Hbase中的数据。（从HMaster中获取元数据，找到RowKey所在的HRegion&#x2F;HregionServer后）</p><p>主要负责响应客户io请求，向HDFS文件系统中读写数据。</p><ol><li>维护Master分配给它的region，处理对这些region的IO请求（管理自己节点上的Region）</li><li>负责切分在运行过程中变得过大的region</li></ol><h3 id="4、Hlog"><a href="#4、Hlog" class="headerlink" title="4、Hlog"></a>4、Hlog</h3><p>操作日志文件，恢复元数据，类似fsimage和edits</p><p>放置hbase中进行操作的时候没有真正的成功而丢失数据</p><p>每个regionserver都有且只有一个hlog，是一个实现wal的类，是一个<strong>预写式日志</strong></p><h3 id="5、Hregion"><a href="#5、Hregion" class="headerlink" title="5、Hregion"></a>5、Hregion</h3><p>Region是hbase中分布式存储和负载均衡的最小单位。最小单元就表示不同的Hregion可以分布在不同的HRegion server上。但一个Hregion是不会拆分到多个server上的。</p><p><strong>HBase表按照按照row_key水平进行切割成多个HRegion，Region是按照大小分隔的， 随着region不断增大，当增大到一个阀值的时候，region就会分成两个region。</strong> 当table中的行不断增多，就会有越来越多的Hregion。</p><p><img src="/cdh/hbase/hbase/28115110_d9yr-16435354354142.jpg" alt="本页无标题"></p><p><img src="/cdh/hbase/hbase/28115110_cWA1.jpg" alt="本页无标题"></p><p><strong>每个Hregion都记录了他的startkey和endkey第一个HRegion的StartKey为空，最后一个HRegion的EndKey为空），rowkey是排序的，client可以通过HMaster定位每个rowkey在那个HRegion。HRegion由HMaster分配到相应的HRegionServer，HRegionServer来负责HRegion的启动和管理</strong></p><h3 id="6、Store"><a href="#6、Store" class="headerlink" title="6、Store"></a>6、Store</h3><p>store是跟列族数量相关的，表有多少个列族就有多少个store</p><p>表中的<strong>每一个列族切一个store</strong></p><h3 id="7、MemStore"><a href="#7、MemStore" class="headerlink" title="7、MemStore"></a>7、MemStore</h3><p>是一个（写）缓存，在存入数据时，在到hdfs前先插入到memstore中，memstore的文件是存在内存中的，返回插入成功，相当于缓存。<strong>每个列族有一个MemStore</strong></p><h3 id="8、StoreFile"><a href="#8、StoreFile" class="headerlink" title="8、StoreFile"></a>8、StoreFile</h3><p>memstore中达到阀值后，进行溢写文件到storeFile,进行溢写文件后缓存文件将不存在，他是Hfile的封装。当溢写文件过多时，会触发<strong>compaction</strong>操作，将多个文件合并成一个storefile。单个文件过大也不利于文件的检索，这时会触发分裂操作，将HRegion分成两个来管理.</p><h3 id="9、Hfile"><a href="#9、Hfile" class="headerlink" title="9、Hfile"></a>9、Hfile</h3><p>Hfile由很多个数据块(block)组成，并且有一个固定的结尾块 其中的数据块是由一个header和多个key-value的键值对组成 。 在HDFS中的数据默认会有3份。因此这里不需要HFile 本身的可靠性。 </p><p><strong>HFile自身是二进制文件</strong></p><p>一个列族可以有多个HFile，但是一个HFile不能有多个列族的数据，storeFile存储到hdfs的单位叫hfile，HFile数据格式中的KeyValue数据格式，value是二进制形式存在的</p><p>Hfile内部结构：</p><p><a href="https://blog.csdn.net/zhanglh046/article/details/78510291">https://blog.csdn.net/zhanglh046/article/details/78510291</a></p><p>四个部分：</p><p><strong>scanned block section</strong>       （真正数据存储的位置 里面包含data block和blond block 可以开启布隆过滤器）Hbase真正的数据存储在Data Block中，用户查询是会被扫描到</p><p><strong>non-scanned block section</strong>   顺序扫描HFile，这个section的数据不会被读取，主要包括元数据数据块等</p><p><strong>load-on-open section</strong>  存储元数据信息</p><p><strong>trailer</strong>记录Hfile的基本信息，各个部分的偏移量和寻址信息</p><p><strong>namespace</strong>：数据表的结合</p><h2 id="HBase的写流程和更新流程"><a href="#HBase的写流程和更新流程" class="headerlink" title="HBase的写流程和更新流程"></a>HBase的写流程和更新流程</h2><p>客户端发送请求，根据rowkey和表数据从zookeeper的meta表中查询region的startrwokey和endrowkey。选择对应的HRegionServer，将请求发送给相应的HRegion，在HRegionServer将put操作写入wal（预写式日志）日志文件中（磁盘中）</p><p>HRegionServer根据put中的tablename和rowkey找到对应的HRegion，然后找到Store，写入到Store中的MemStore，并返回写成功，并通知客户端。</p><p>每一次put&#x2F;delete请求都是先写入到MemStore中，当MemStore中到达阀值后，flush成一个新的storeFile</p><p><img src="/cdh/hbase/hbase/1608104-20190222103800677-2022161869.png" alt="img"></p><p>执行写入时会写到两个地方，预写式日志(HLog)和MemStore。HBase默认在写入动作首先记录到这两个地方（这个操作是事务性的一起成功和失败）。 当系统出现意外时，Memstore中的数据丢失，此时使用HLog来恢复checkpoint之后的数据。 </p><p>MemStore是内存里写入缓存区，在永久写入磁盘之前在这里积累。当MemStore填满后，刷到磁盘，形成一个HFile，将内存中的数据删除，同时删除HLog中的历史数据；每个Store有一个MemStore。</p><h2 id="HBase的读流程"><a href="#HBase的读流程" class="headerlink" title="HBase的读流程"></a>HBase的读流程</h2><p>client先从缓存中定位region，如果没有缓存，就先访问zookeeper(zookeeper保存着meta的)，找到meta表，返回对应的regionserver的地址。</p><p>连接regionserver，client向regionserver发送数据读取的请求，先从memstore中找如果没有，再到blockcache找，再到磁盘查找storefile。（如果从storefile中找到了，就先写入到blockcache，在返回给客户端）</p><h2 id="HBase的API"><a href="#HBase的API" class="headerlink" title="HBase的API"></a>HBase的API</h2><pre><code class="java">//创建连接Configuration conf= HBaseConfiguration.create();conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;linux01:2181&quot;);</code></pre><h3 id="在HBase中创建表"><a href="#在HBase中创建表" class="headerlink" title="在HBase中创建表"></a>在HBase中创建表</h3><pre><code class="java">//连接HbaseConfiguration conf = HBaseConfiguration.create();conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;linux01:2181,linux02:2181,linux03:2181&quot;);//操作单位是HBaseAdminHBaseAdmin admin = new HBaseAdmin(conf);//表名HTableDescriptor tabledesc = new HTableDescriptor(&quot;biaoming&quot;);//列族名称HColumnDescriptor columns = new HColumnDescriptor(&quot;info&quot;);//列族名称HColumnDescriptor columns2 = new HColumnDescriptor(&quot;address&quot;);//分别把列添加进表中tabledesc.addFamily(columns);tabledesc.addFamily(columns2);//如果表在hbase中已存在，禁用表，然后删除if (admin.tableExists(&quot;biaoming&quot;)) &#123;admin.disableTable(&quot;biaoming&quot;);admin.deleteTable(&quot;biaoming&quot;);&#125;//在这里创建表admin.createTable(tabledesc);System.out.println(&quot;建表完成&quot;);</code></pre><h3 id="往HBase表中插入数据"><a href="#往HBase表中插入数据" class="headerlink" title="往HBase表中插入数据"></a>往HBase表中插入数据</h3><pre><code class="java">public static void insertdata() throws Exception&#123;    //新建表操作单位    HTable htable = new HTable(conf, &quot;biaoming&quot;);    //row_key  行    Put put = new Put(&quot;gaoda&quot;.getBytes());    //column  创建列族 列 和 cell的值 都是byte数组形式    put.addColumn(&quot;info&quot;.getBytes(), &quot;bianxing&quot;.getBytes(), &quot;chenggong&quot;.getBytes());    put.addColumn(&quot;info&quot;.getBytes(), &quot;bianxing&quot;.getBytes(), &quot;shibai&quot;.getBytes());    put.addColumn(&quot;info&quot;.getBytes(), &quot;bianxing&quot;.getBytes(), &quot;meinengliang&quot;.getBytes());    put.addColumn(&quot;info&quot;.getBytes(), &quot;bianxing&quot;.getBytes(), null);    //正式插入表    htable.put(put);    System.out.println(&quot;插入数据成功&quot;);&#125;</code></pre><h3 id="List来批量插入数据"><a href="#List来批量插入数据" class="headerlink" title="List来批量插入数据"></a>List来批量插入数据</h3><p>最好使用批量数据插入Hbase，减少开销，不同的hbase的时候批次数量建议提前测试，大概3000-5000最优</p><p>Hbase插入之所以尽量选择批量插入的原因是：</p><p>Hbase底层使用了<strong>LSM树</strong>架构，牺牲了部分的读性能，大幅提高了写性能。</p><p>LSM树的设计思想非常朴素：<br>将对数据的修改增量保持在内存中，达到指定的大小限制后将这些修改操作批量写入磁盘，<br>不过读取的时候稍微麻烦，需要合并磁盘中历史数据和内存中最近修改操作，<br>所以写入性能大大提升，读取时可能需要先看是否命中内存，<br>否则需要访问较多的磁盘文件。</p><pre><code class="java">//这里用的是list来插入数据//普通的插入数据很繁琐，list来插入制定规则的    public static void insertlistdata() throws Exception&#123;        HTable htable = new HTable(conf, &quot;biaoming&quot;);        ArrayList&lt;Put&gt; list = new ArrayList&lt;&gt;();        for (int i = 0; i &lt; 10000; i++) &#123;            Put put = new Put((&quot;gaoda&quot;+i).getBytes());            put.addColumn(&quot;info&quot;.getBytes(), &quot;bianxing&quot;.getBytes(), (i+&quot;&quot;).getBytes());            list.add(put);        &#125;        htable.put(list);        System.out.println(&quot;list数据插入成功&quot;);    &#125;</code></pre><h3 id="Hbase扫描数据"><a href="#Hbase扫描数据" class="headerlink" title="Hbase扫描数据"></a>Hbase扫描数据</h3><pre><code class="scala">HTable table = new HTable(conf,&quot;biaoming&quot;);Scan sc = new Scan(&quot;Nurse00000&quot;.getBytes(),&quot;Nurse0000100&quot;.getBytes());for(Result r : scanner) &#123;System.out.println(&quot;row&quot;+new String(r.getRow()));System.out.println(new String(r.getValue(&quot;info&quot;.getBytes(), &quot;name&quot;.getBytes())));&#125;</code></pre><h3 id="删除数据-1"><a href="#删除数据-1" class="headerlink" title="删除数据"></a>删除数据</h3><pre><code class="scala">HTable table = new HTable(conf,&quot;biaoming&quot;);Delete delete = new Delete(&quot;Nurse00001&quot;.getBytes());table.delete(delete);</code></pre><h3 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h3><h4 id="行键过滤器"><a href="#行键过滤器" class="headerlink" title="行键过滤器"></a>行键过滤器</h4><pre><code class="scala">Filter filter = new RowFilter(CompareFilter.CompareOp.LESS,new BinaryComparator(&quot;Nurse0000100&quot;.getBytes()));</code></pre><h4 id="前缀过滤器"><a href="#前缀过滤器" class="headerlink" title="前缀过滤器"></a>前缀过滤器</h4><pre><code class="scala">Filter filter = new PrefixFilter(&quot;Nurse00009999&quot;.getBytes());</code></pre><h4 id="时间过滤器"><a href="#时间过滤器" class="headerlink" title="时间过滤器"></a>时间过滤器</h4><pre><code class="scala">List&lt;Long&gt; list = new ArrayList&lt;Long&gt;();list.add(1558940210551L);Filter filter = new TimestampsFilter(list);</code></pre><h4 id="字段过滤器"><a href="#字段过滤器" class="headerlink" title="字段过滤器"></a>字段过滤器</h4><p>只查含有这个列的行</p><pre><code class="scala">Filter filter = new QualifierFilter(CompareFilter.CompareOp.EQUAL,new RegexStringComparator(&quot;age&quot;));</code></pre><h3 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h3><ul><li>LESS 匹配小于设定值的值 </li><li>LESS_OR_EQUAL 匹配小于或等于设定值的值 </li><li>EQUAL 匹配等于设定值的值 </li><li>NOT_EQUAL 匹配与设定值不相等的值 </li><li>GREATER_OR_EQUAL 匹配大于或等于设定值的值</li><li>GREATER 匹配大于设定值的值 NO_OP 排除一切值</li></ul><h3 id="比较器有哪些？"><a href="#比较器有哪些？" class="headerlink" title="比较器有哪些？"></a>比较器有哪些？</h3><ul><li>BinaryComparator 使用Bytes.compareTo()比较当前的阈值 </li><li>BinaryPrefixComparator 与上面的相似，使用Bytes.compareTo()进行匹配，但是是从左端开始前缀匹配 </li><li>NullComparator 不做匹配，只判断当前值不是null </li><li>BitComparator 通过BitWiseOp类提供的按位与（AND）、或（OR）、异或（XOR）操作执行位级比较。 </li><li>RegexStringComparator 根据一个正则表达式，在实例化这个比较器的时候去匹配表中的数据。 </li><li>SubStringComparator 把阈值和表中数据String实例，同时通过contains()操作匹配字符串</li></ul><h2 id="什么是热点？"><a href="#什么是热点？" class="headerlink" title="什么是热点？"></a>什么是热点？</h2><p>HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。热点发生在大量的client直接访问集群的一 个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。设计良好的数据访问模式以使集群被充分，均衡的利用。 为了避免写热点，设计rowkey使得不同 行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。</p><h2 id="如何避免热点问题？"><a href="#如何避免热点问题？" class="headerlink" title="如何避免热点问题？"></a>如何避免热点问题？</h2><p>（1）<strong>加盐</strong><br>是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。给多少个前缀？ 这个数量应该和我们想要分散数据到不同的region的数量一致（类似hive里面的分桶）。<br>（ 自己理解： 即region数量是一个范围，我们给rowkey分配一个随机数，前缀（随机数）的范围是region的数量）<br>加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。</p><p>（2）<strong>哈希</strong><br>哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。</p><p>（3）<strong>反转</strong><br>第三种防止热点的方法是反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。反转rowkey的例子：以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，从而避免诸如139、158之类的固定号码开头导 致的热点问题。</p><p>（4）<strong>时间戳反转</strong><br>一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用Long.Max_Value – timestamp追加到key的末尾，例如[key][reverse_timestamp] ,[key] 的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据。</p><p>（5）尽量减少行和列的大小<br>在HBase中，value永远和它的key一起传输的。当具体的值在系统间传输时，它的rowkey，列名，时间戳也会一起传输。如果你的rowkey和列名很大，HBase storefiles中的索引（有助于随机访问）会占据HBase分配的大量内存，因为具体的值和它的key很大。可以增加block大小使得storefiles索引再更大的时间间隔增加，或者修改表的模式以减小rowkey和列名的大小。压缩也有助于更大的索引。</p><p>（6）其他办法<br>列族名的长度尽可能小，最好是只有一个字符。冗长的属性名虽然可读性好，但是更短的属性名存储在HBase中会更好。也可以在建表时预估数据规模，预留region数量，例如create ‘myspace:mytable’, SPLITS &#x3D;&gt; [01,02,03,,…99]<br>————————————————<br>版权声明：本文为CSDN博主「大数据私房菜」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/zuochang_liu/article/details/105274316">https://blog.csdn.net/zuochang_liu/article/details/105274316</a></p><h2 id="布隆过滤器在Hbase的应用"><a href="#布隆过滤器在Hbase的应用" class="headerlink" title="布隆过滤器在Hbase的应用"></a>布隆过滤器在Hbase的应用</h2><h2 id="BulkLoader"><a href="#BulkLoader" class="headerlink" title="BulkLoader"></a>BulkLoader</h2><h3 id="什么是BulkLoader？"><a href="#什么是BulkLoader？" class="headerlink" title="什么是BulkLoader？"></a>什么是BulkLoader？</h3><p><strong>hive 数据导入hbase</strong></p><p><strong>核心要点</strong>：像明细数据这种场景，数据量很大，要从 hive 导入 hbase，需要考虑效率的问题 如果直接使用 hbase 客户端 api 进行数据插入，效率太低优选<strong>bulkloader</strong> 工具进行数据导入 （核心原理：利用 hbase 之外的外部运算引擎比如 mr、spark，将源数据直接加工成 hbase 的底层文件格式：HFILE，然后通知到 hbase)</p><h3 id="使用方法1"><a href="#使用方法1" class="headerlink" title="使用方法1"></a>使用方法1</h3><p>hbase自带一个工具包：Hbase&#x2F;lib&#x2F;&#x2F;hbase-mapreduce-2.0.6.jar。</p><p>弊端：只支持TSV文本类型，是mr程序比较慢且不灵活。</p><p>1.数据格式转Hfile</p><pre><code class="shell"># 执行 hbase 自带的 importtsv 程序（mapreduce 程序），将原始文件转成 hfile# 设置 HADOOP_CLASSPATH 变量来引入 Hbase 库 jar 包export HADOOP_CLASSPATH=`$&#123;HBASE_HOME&#125;/bin/hbase classpath` # 提交转 HFILE 的 mapreduce 程序hadoop jar /opt/apps/hbase-2.0.6/lib/hbase-mapreduce-2.0.6.jar \importtsv -Dimporttsv.columns=HBASE_ROW_KEY,f1:area,f1:floor,f2:price \&#39;-Dimporttsv.separator=,&#39; \-Dimporttsv.bulk.output=hdfs://doitedu01:8020/bulkload/output1 bulk_test1 \hdfs://doitedu01:8020/bulkload/test1/</code></pre><pre><code class="shell">完整参数：-Dimporttsv.bulk.output=/path/for/output 输出目录-Dimporttsv.skip.bad.lines=false         是否跳过脏数据行&#39;-Dimporttsv.separator=|&#39;  指定分隔符-Dimporttsv.timestamp=currentTimeAsLong  是否指定时间戳-Dimporttsv.mapper.class=my.Mapper  替换默认的 Mapper 类</code></pre><p>2.通知 hbase 并加载数据</p><pre><code class="shell">bin/hbase org.apache.hadoop.hbase.tool.LoadIncrementalHFiles hdfs://doitedu01:8020/bulkload/output1 bulk_test1#bin/hbase org.apache.hadoop.hbase.tool.LoadIncrementalHFiles &lt;hdfs://hfilefile/output&gt; &lt;tablename&gt;</code></pre><h3 id="使用方法2"><a href="#使用方法2" class="headerlink" title="使用方法2"></a>使用方法2</h3><p>自己写代码（spark、mr）生成 HFile 并导入</p><p>核心要点 1： 要输出 HFile，必须要有一种对应的 OutputFormat 实现：HFileOutputFormat2 </p><p>核心要点 2： HBASE 中的数据组织结构</p><pre><code>r1 ,f1: (k1-&gt;v1,k2-&gt;v2), f2:(ka-&gt;vx,kb-&gt;vx)</code></pre><p>物理视图： </p><pre><code>r1,f1:k1,v1 r1,f1:k1,v2 r1,f2:kx,vx r1,f2:ky,vy</code></pre><p>1.导入依赖</p><pre><code class="xml">&lt;dependency&gt;&lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;&lt;artifactId&gt;hbase-mapreduce&lt;/artifactId&gt;&lt;version&gt;2.0.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;&lt;artifactId&gt;hbase-client&lt;/artifactId&gt;&lt;version&gt;2.0.4&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>2.编写代码</p><pre><code class="java">import org.apache.hadoop.fs.&#123;FileSystem, Path&#125;import org.apache.hadoop.hbase.client.&#123;ConnectionFactory, Table&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2import org.apache.hadoop.hbase.tool.LoadIncrementalHFilesimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.&#123;HBaseConfiguration, KeyValue, TableName&#125;import org.apache.hadoop.mapreduce.Jobimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.sql.SparkSessionimport scala.collection.mutable.ListBuffer/** * 将hive中的流量会话聚合表，通过bulkloader导入hbase * * bulkloader原理： * 在hbase体系之外，通过自己的手段（比如mapreduce程序，spark程序）将自己的数据整理成hbase的底层文件格式：HFile * 然后通过客户端，请求Hbase的Master进行相关表的元数据创建修改，并将上一步骤生成好的HFile加载到HBase存储目录中对应表的表目录下 * * 事先需要把hbase的表创建好 * create &#39;session_agr&#39;,&#39;f&#39; * */object ImportSessionAgr2Hbase &#123;  def main(args: Array[String]): Unit = &#123;    Logger.getLogger(&quot;org&quot;).setLevel(Level.WARN)    val spark = SparkSession.builder()      .appName(&quot;hive表 流量会话聚合表 导入  HBASE&quot;)      .master(&quot;local&quot;)      .enableHiveSupport()      .getOrCreate()    // 用spark去加载hive表    val agrTable = spark.read.table(&quot;dws17.app_trf_agr_session&quot;).where(&quot;dt=&#39;2020-10-07&#39;&quot;)    // guid|session_id|start_ts|end_ts|first_page_id|last_page_id|pv_cnt|isnew|hour_itv|country|province|city|region|device_type    // 整理成hbase的KV结构    val argTableFlat: RDD[(String, String, String, String)] = agrTable.rdd      .flatMap(row =&gt; &#123;        val guid = row.getAs[String](&quot;guid&quot;) + &quot;&quot;        val session_id = row.getAs[String](&quot;session_id&quot;) + &quot;&quot;        val start_ts = row.getAs[Long](&quot;start_ts&quot;) + &quot;&quot;        val end_ts = row.getAs[Long](&quot;end_ts&quot;) + &quot;&quot;        val first_page_id = row.getAs[String](&quot;first_page_id&quot;) + &quot;&quot;        val last_page_id = row.getAs[String](&quot;last_page_id&quot;) + &quot;&quot;        val pv_cnt = row.getAs[Int](&quot;pv_cnt&quot;) + &quot;&quot;        val isnew = row.getAs[Int](&quot;isnew&quot;) + &quot;&quot;        val hour_itv = row.getAs[Int](&quot;hour_itv&quot;) + &quot;&quot;        val country = row.getAs[String](&quot;country&quot;) + &quot;&quot;        val province = row.getAs[String](&quot;province&quot;) + &quot;&quot;        val city = row.getAs[String](&quot;city&quot;) + &quot;&quot;        val region = row.getAs[String](&quot;region&quot;) + &quot;&quot;        val device_type = row.getAs[String](&quot;device_type&quot;) + &quot;&quot;        val lst = new ListBuffer[(String, String, String, String)]        // rowkey, family , qualifier ,value        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;guid&quot;, guid))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;session_id&quot;, session_id))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;start_ts&quot;, start_ts))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;end_ts&quot;, end_ts))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;first_page_id&quot;, first_page_id))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;last_page_id&quot;, last_page_id))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;pv_cnt&quot;, pv_cnt))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;isnew&quot;, isnew))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;hour_itv&quot;, hour_itv))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;country&quot;, country))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;province&quot;, province))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;city&quot;, city))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;region&quot;, region))        lst += ((start_ts + &quot;|&quot; + session_id, &quot;f&quot;, &quot;device_type&quot;, device_type))        lst      &#125;)      .filter(tp =&gt; !tp._1.contains(&quot;null&quot;))    /**     * 会话表数据转换类型     */    // 对数据排序:先按rowkey，再按family，再按qualifier    val sorted = argTableFlat.sortBy(tp =&gt; (tp._1, tp._2, tp._3))    // 将初始类型，转成HFile所需要的(immutableBytesWritable,KeyValue)    val kvData = sorted.map(tp =&gt; &#123;      val rowKey = new ImmutableBytesWritable(Bytes.toBytes(tp._1))      // rowKey,family,qualifier,value      val kv = new KeyValue(Bytes.toBytes(tp._1), Bytes.toBytes(tp._2), Bytes.toBytes(tp._3), Bytes.toBytes(tp._4))      (rowKey, kv)    &#125;)    /**     * 生成入口页字段的二级索引表数据     */    val firstPageIndex = argTableFlat      .filter(tp=&gt;tp._3.equals(&quot;first_page_id&quot;))      .map(tp=&gt;&#123;        // 入口页,对应会话表中的rowkey        (tp._4,tp._1)      &#125;)      .groupByKey()      .mapValues(iter=&gt;iter.mkString(&quot;\001&quot;))      .map(tp=&gt;&#123;        (tp._1,&quot;f&quot;,&quot;q&quot;,tp._2)      &#125;)      .sortBy(tp=&gt;(tp._1,tp._2,tp._3))      .map(tp=&gt;&#123;        val rowKey = new ImmutableBytesWritable(Bytes.toBytes(tp._1))        // rowKey,family,qualifier,value        val kv = new KeyValue(Bytes.toBytes(tp._1), Bytes.toBytes(tp._2), Bytes.toBytes(tp._3), Bytes.toBytes(tp._4))        (rowKey, kv)      &#125;)    // 以下是bulkLoader的模板代码    val conf = HBaseConfiguration.create()    conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://doitedu01:8020&quot;)    conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;doitedu01,doitedu02,doitedu03&quot;)    val job = Job.getInstance(conf)    val tableName = TableName.valueOf(&quot;session_agr&quot;)    val conn = ConnectionFactory.createConnection(conf)    val table: Table = conn.getTable(tableName)    val locator = conn.getRegionLocator(tableName)    // 配置HFileOutputFormat2    HFileOutputFormat2.configureIncrementalLoad(job, table, locator)    // 将准备好的数据，调用saveAsNewApiHadoopFile来保存    val outPath = &quot;/bulkload/session_agr/2020-10-07&quot;;    val fs = FileSystem.get(conf)    if(fs.exists(new Path(outPath))) fs.delete(new Path(outPath),true)    kvData.saveAsNewAPIHadoopFile(outPath, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat2], job.getConfiguration)    // 通知hbase master，并加载生成好的hfile文件    val loader = new LoadIncrementalHFiles(job.getConfiguration)    loader.doBulkLoad(new Path(outPath), conn.getAdmin, table, locator)    conn.close()  &#125;&#125;</code></pre><h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><h4 id="HBase了解多少？"><a href="#HBase了解多少？" class="headerlink" title="HBase了解多少？"></a>HBase了解多少？</h4><p>HBase是一个高可靠，高性能，面向列，可伸缩的分布式存储系统，非关系型数据库，典型的nosql，低延迟查询，存储容量多。存储在hdfs中，以byte字节形式存储。行会自动排序进行存储。</p><p>hbase的结构也说一下。hmast，zk，regionserver，hlog，region，memstore，storefile，。。</p><h4 id="Hbase的rowkey如何进行设计？"><a href="#Hbase的rowkey如何进行设计？" class="headerlink" title="Hbase的rowkey如何进行设计？"></a>Hbase的rowkey如何进行设计？</h4><p>一条数据的唯一标识就是rowkey，rowkey决定数据分布在哪个region，防止数据热点问题。</p><p>设计时不超过16个字节的，并且满足<strong>长度、散列、唯一</strong> 原则 ，尽可能避免热点问题。</p><p>查询主列当rowkey，条件查询es</p><p>两位随机数+date+eventid(hash码)</p><p><strong>实例</strong>：loap 用户会话明细：</p><p>查询hbase会话聚合表，时间当rowkey，查会话访问的所有pv，根据sessionid 当rowkey</p><p><img src="/cdh/hbase/hbase/image-20220130155107566.png" alt="image-20220130155107566"></p><h4 id="HBase的优化"><a href="#HBase的优化" class="headerlink" title="HBase的优化"></a>HBase的优化</h4><ol><li>高可用的Hmaster</li></ol><p>在HBase中Hmaster负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Hmaster挂掉了，那么整个HBase集群将陷入不健康的状态，此时的工作状态并不会维持太久。所以需要配置hbase的高可用</p><ol start="2"><li>region的预分区</li></ol><p>每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。在Hbase插入数据时，只会有一个Region，不够了才会进行创建，可以先调用Hbase的api预先创建Hregion来使用，写入的时候按照分区情况做负载均衡。</p><ol start="3"><li>rowkey的设计:</li></ol><p>rowkey最大长度64kb，越短越好，最好不超过16字节。按照字典顺序会进行排序,前期设计rowkey会让hbase的使用变简单</p><p>遵循 长度、散列、唯一原则 ，尽可能避免热点问题</p><ol start="4"><li>内存优化</li></ol><p>设置HBASE的内存一般16~48G内存就可以</p><ol start="5"><li>读缓存</li></ol><p>读缓存设置，查数据的时候回先从metastore查询数据，没有命中，然后查询blockcache，没有命中再去查询磁盘，提升查询的效率。</p><ol start="6"><li>批量插入</li></ol><p>插入数据尽量使用批量插入，利用LSM树的特点。先载入到缓存，在同意刷入到磁盘中</p><ol start="7"><li>不创建过多的列族</li></ol><p>现在Hbase不能很好的处理超过2-3个列族，并且官方也不推荐创建过多。当前项目中一个就够用</p><ol start="8"><li>布隆过滤器</li></ol><p><a href="https://mp.weixin.qq.com/s?__biz=MzI2MDQzOTk3MQ==&amp;mid=2247485551&amp;idx=1&amp;sn=46419f4119b09fd7b35e69ef7a44f77a&amp;chksm=ea68e2a3dd1f6bb5bb515e8f68df588886af284dd6e24ba7143c341931e10e46ef5e9583f56d&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzI2MDQzOTk3MQ==&amp;mid=2247485551&amp;idx=1&amp;sn=46419f4119b09fd7b35e69ef7a44f77a&amp;chksm=ea68e2a3dd1f6bb5bb515e8f68df588886af284dd6e24ba7143c341931e10e46ef5e9583f56d&amp;scene=21#wechat_redirect</a></p><p>当我们随机读get数据时，如果采用hbase的块索引机制，hbase会加载很多块文件。</p><p>采用布隆过滤器后，它能够准确判断该HFile的所有数据块中是否含有我们查询的数据，从而大大减少不必要的块加载，增加吞吐，降低内存消耗，提高性能</p><p>在读取数据时，hbase会首先在布隆过滤器中查询，根据布隆过滤器的结果，再在MemStore中查询，最后再在对应的HFile中查询。</p><h4 id="Hbase宕机怎么办？"><a href="#Hbase宕机怎么办？" class="headerlink" title="Hbase宕机怎么办？"></a>Hbase宕机怎么办？</h4><p>hregionserver宕机，将hregion交给其他regionserver来管理，并拆分hlog到其他hregionserver加载，然后修改meta表</p><p>hmaster宕机，hbase可以启动多个hmaster，有其他hmaster来进行管理。</p><h4 id="Hbase二级索引？"><a href="#Hbase二级索引？" class="headerlink" title="Hbase二级索引？"></a>Hbase二级索引？</h4><p><a href="https://mp.weixin.qq.com/s?__biz=MzI2MDQzOTk3MQ==&amp;mid=2247485566&amp;idx=1&amp;sn=069730018236fabf184aedd09d29f677&amp;chksm=ea68e2b2dd1f6ba42c25d89cc81975381bba60bcdd4ebef0af77053fb257dc93d6fb8dbed523&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzI2MDQzOTk3MQ==&amp;mid=2247485566&amp;idx=1&amp;sn=069730018236fabf184aedd09d29f677&amp;chksm=ea68e2b2dd1f6ba42c25d89cc81975381bba60bcdd4ebef0af77053fb257dc93d6fb8dbed523&amp;scene=21#wechat_redirect</a></p><p>用es做二级索引，将索引列和查询的字段放入到es中，多条件查询先查询es，获取到</p><p>rowkey在查询hbase，提高查询效率。</p><p><img src="/cdh/hbase/hbase/640" alt="图片"></p><h4 id="Hbase的表是怎么设计的？有多少张表？"><a href="#Hbase的表是怎么设计的？有多少张表？" class="headerlink" title="Hbase的表是怎么设计的？有多少张表？"></a>Hbase的表是怎么设计的？有多少张表？</h4><h4 id="hbase的region-怎么分裂的？条件有哪几个？"><a href="#hbase的region-怎么分裂的？条件有哪几个？" class="headerlink" title="hbase的region 怎么分裂的？条件有哪几个？"></a>hbase的region 怎么分裂的？条件有哪几个？</h4><p>表的大小随着数据的不断写入会一直增大，hbase最初只有1个region来存储数据。hregion越来越大，到了一定阈值自动分裂成两个hregion，过程不断进行，hregion会越来越多。</p><p>当memsotre刷出的文件块达到四块，hmaster会将数据块加载到本地进行合并。</p><p>当合并大小超过256M，就会进行拆分，拆分后的region给给其他hregionserver进行管理，然后修改meta表的元数据。</p><h4 id="按照什么规则merge？"><a href="#按照什么规则merge？" class="headerlink" title="按照什么规则merge？"></a>按照什么规则merge？</h4><p>合并相邻的region。</p><h4 id="每次merge-产生什么结果？"><a href="#每次merge-产生什么结果？" class="headerlink" title="每次merge 产生什么结果？"></a>每次merge 产生什么结果？</h4><p>将多个region合并成一个region。</p><h4 id="手动merge过吗-？什么情况下要merge-region？"><a href="#手动merge过吗-？什么情况下要merge-region？" class="headerlink" title="手动merge过吗 ？什么情况下要merge region？"></a>手动merge过吗 ？什么情况下要merge region？</h4><pre><code class="sql">-- 手动合并region命令merge_region &#39;encoden_regionName&#39;,&#39;encoden_regionName&#39;Examples:hbase(main):002:0&gt; merge_region &#39;c6a66e3d6534bdc036593b278f387c1c&#39;,&#39;9ef976c7103ba0fc68a5f803ab898fbc&#39;0 row(s) in 0.2410 secondshbase(main):003:0&gt;</code></pre><p>当一个table数据量较小，而region数量多，并且region体积都很小，就需要merge region。</p><p>因为前期预分区数量大于数据量，所以出现了这种情况。</p><h3 id="如何排查问题"><a href="#如何排查问题" class="headerlink" title="如何排查问题"></a>如何排查问题</h3><p>出现瓶颈后排查问题，瓶颈只有三个（IO、网络、内存）</p>]]></content>
      
      
      <categories>
          
          <category> Hbase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习笔记</title>
      <link href="/cdh/hive/hive/"/>
      <url>/cdh/hive/hive/</url>
      
        <content type="html"><![CDATA[<h3 id="HIVE是什么"><a href="#HIVE是什么" class="headerlink" title="HIVE是什么"></a>HIVE是什么</h3><p><strong>hive是hadoop的一个数据仓库工具</strong>，将结构化数据文件映射成一张数据库表，并提供sql查询功能，把sql语句转化成mr任务执行</p><p>不建议存储，更用于查询和分析结果。它比数据库高一个级别，面向主题，包含数据库的许多表。可以通过etl(数据抽取，转换，加载)来完成合并数据</p><h4 id="什么是数据仓库"><a href="#什么是数据仓库" class="headerlink" title="什么是数据仓库"></a>什么是数据仓库</h4><p> 很大的数据存储集合 ,数据仓库是一个支持管理决策的数据集合，面向主题，集成的，不易丢失的并且是时变的</p><p>数据仓库是所有操作环境和外部数据源的快照集合，不需要非常精确，因为它必须在特定的时间基础上从操作环境中提取出来</p><p>hive支持索引</p><p>hive支持行级事务</p><p>hive支持视图</p><p>hive不支持存储过程</p><p>hive不支持非等值连接</p><h4 id="hive的组成结构"><a href="#hive的组成结构" class="headerlink" title="hive的组成结构"></a>hive的组成结构</h4><pre><code class="xml">&lt;!--用户接口--&gt;webui（浏览器访问hive）ui用户接口的意思Client CLI(hive shell 命令行)console uiJDBC/ODBC(java访问hive)thriftserver(当在项目中要求使用java直连hive  一定要打开hive的thriftserver服务)&lt;!--元数据存储(Metastore)--&gt;默认存储在 derby(嵌入式数据库)线上使用时一般换为MySQL&lt;!--驱动器(Driver)--&gt;1、解析器(SQL Parser):将SQL字符转换成抽象语法树AST,这一步一般使用都是第三方工具库完成,比如antlr,对AST进行语法分析,比如表是否存在,字段是否存在,SQL语句是否有误2、编译器(Physical Plan):将AST编译生成逻辑执行计划3、优化器(Query Optimizer):对逻辑执行计划进行优化4、执行器(Execution):把逻辑执行计划转换成可以运行的物理计划,对于Hive来说,就是MR/Spark&lt;!--hadoop--&gt;用MapReduce进行计算，用hdfs来进行存储</code></pre><h4 id="Hive是如何将SQL转化为MapReduce任务的？"><a href="#Hive是如何将SQL转化为MapReduce任务的？" class="headerlink" title="Hive是如何将SQL转化为MapReduce任务的？"></a>Hive是如何将SQL转化为MapReduce任务的？</h4><p>hiveSQL转换成MapReduce的执行计划包括如下几个步骤：<br> HiveSQL -&gt;AST(抽象语法树) -&gt; QB(查询块) -&gt;OperatorTree（操作树）-&gt;优化后的操作树-&gt;mapreduce任务树-&gt;优化后的mapreduce任务树<br>链接：<a href="https://www.jianshu.com/p/660fd157c5eb">https://www.jianshu.com/p/660fd157c5eb</a></p><p>1、Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree</p><p>2、遍历AST Tree，抽象出查询的基本组成单元QueryBlock</p><p>3、遍历QueryBlock，翻译为执行操作树OperatorTree</p><p>4、逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量</p><p>5、遍历OperatorTree，翻译为MapReduce任务</p><p>6、物理层优化器进行MapReduce任务的变换，生成最终的执行计划</p><h4 id="什么是hive的元数据"><a href="#什么是hive的元数据" class="headerlink" title="什么是hive的元数据"></a>什么是hive的元数据</h4><p>是对真实数据的描述信息</p><p>真实数据的位置信息、时间戳、分片信息等</p><p>表名,表所属数据库(默认是default) ,表的拥有者,列&#x2F;分区字段,表的类型(是否是外部表),表的数据所在目录等</p><h4 id="HIVE在hdfs的存储位置"><a href="#HIVE在hdfs的存储位置" class="headerlink" title="HIVE在hdfs的存储位置"></a>HIVE在hdfs的存储位置</h4><p>&#x2F;<strong>hive的默认存储位置是&#x2F;user&#x2F;hive&#x2F;warehouse</strong>&#x2F;</p><h4 id="hive的语法"><a href="#hive的语法" class="headerlink" title="hive的语法"></a>hive的语法</h4><p>Hql语法和数据库表的名字都不区分大小写</p><p>Mysql中的表名区分大小写</p><h4 id="hive的三种模式"><a href="#hive的三种模式" class="headerlink" title="hive的三种模式"></a>hive的三种模式</h4><p>单用户模式多用户模式   远程服务器模式 </p><h4 id="hive的读时模式"><a href="#hive的读时模式" class="headerlink" title="hive的读时模式"></a>hive的读时模式</h4><p>HIVI处理的数据是大数据，在保存数据时不对数据进行效验，而是在读数据时效验，不符合格式的数据设置为null</p><p>传统的数据库如mysql，oracle是写时模式，不符合格式的数据写不进去</p><h3 id="安装Hive"><a href="#安装Hive" class="headerlink" title="安装Hive"></a>安装Hive</h3><p> <a href="https://blog.csdn.net/ntuxiaolei/article/details/81777335">https://blog.csdn.net/ntuxiaolei/article/details/81777335</a> （Centos7 yum安装mysql（完整版））</p><p>单机版一般只在测试环境使用</p><p>&#x2F;&#x2F;centos6.5安装步骤</p><p>（先安装mysql）解压hive的包</p><p>在apache-hive-1.2.0-bin&#x2F;conf&#x2F;下新建一个hive-site.xml文件</p><p>在文件中写内容</p><pre><code class="xml">&lt;configuration&gt;&lt;!--下面这个是如果hive在2.0以上需要配置的--&gt;&lt;property&gt;  &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;  &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;!--下面必须配置--&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;    &lt;!--jdbc连接mysql，连结地址（mysql安装的地址），元数据放置的的数据库（在mysql中必须创建数据库）和数据库字符编码 --&gt;&lt;value&gt;jdbc:mysql://192.168.237.12:3306/hive?characterEncoding=UTF-8&amp;amp;useSSL=false&lt;/value&gt;&lt;/property&gt;&lt;property&gt;     &lt;!--mysql驱动程序--&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;!--账号--&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt;     &lt;!--密码--&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;&lt;value&gt;123456&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre><p>更改&#x2F;etc&#x2F;profile文件 </p><p><img src="/cdh/hive/hive/1567687924055.png" alt="1567687924055"></p><pre><code>export JAVA_HOME=/root/Downloads/jdk1.8.0_161export PATH=$PATH:$JAVA_HOME/binexport HADOOP_HOME=/root/Downloads/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport ZOOKEEPER_HOME=/root/Downloads/zookeeper-3.4.5export PATH=$PATH:$ZOOKEEPER_HOME/binexport HIVE_HOME=/root/Downloads/apache-hive-1.2.0-binexport PATH=$PATH:$HIVE_HOME/bin</code></pre><p>&#x2F;<strong>在hive包下面的lib目录下导入mysql-connector-java的控制包</strong>&#x2F;</p><p>在hadoop安装目录中core-site.xml中添加配置文件</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;</code></pre><p>在全部完成后(包括mysql)在hive的bin目录中执行命令初始化数据库</p><pre><code>./schematool -initSchema -dbType mysql</code></pre><p>hive安装完成在mysql安装完后可以启动 </p><pre><code>//单机交互,使用的本地metastorehive</code></pre><h4 id="启动metastroe"><a href="#启动metastroe" class="headerlink" title="启动metastroe"></a>启动metastroe</h4><pre><code class="properties">//使用远程的metastore//启动metastore服务，默认9083端口$HIVE_HOME/bin/hive --service metastore//在hive-site.xml中设置metastore地址&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;    &lt;property&gt;       &lt;name&gt;hive.metastore.uris&lt;/name&gt;       &lt;value&gt;thrift://metastore_server_ip:9083&lt;/value&gt;&lt;!-- 此处是服务器1的ip --&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>启动元数据服务后，hiveserver或者spark，impala等程序都会先访问hive metastore服务，这个metastore去操作mysql。封装的思想，降低了开发难度</p><p><img src="/cdh/hive/hive/image-20220516210334970.png" alt="元数据服务"></p><h4 id="启动单机版hive"><a href="#启动单机版hive" class="headerlink" title="启动单机版hive"></a>启动单机版hive</h4><pre><code>//直接启动即可,单机版hive交互程序,使用的是hiveServer1//Hive CLI使用Apache Thrift协议连接到远程Hiveserver1实例。要连接到服务器，必须指定主机名。端口号是可选的，默认端口10000。hive提示符显示当前库：set hive.cli.print.current.db=true;显示查询结果时显示字段名称：set hive.cli.print.header=true;</code></pre><h4 id="启动hive服务端"><a href="#启动hive服务端" class="headerlink" title="启动hive服务端"></a>启动hive服务端</h4><p>Hive 内置了 HiveServer 和 HiveServer2 <strong>服务</strong>，两者都允许客户端使用多种编程语言进行连接，但是 <strong>HiveServer 不能处理多个客户端的并发请求</strong>，所以产生了 HiveServer2。Server将你的sql翻译成mr任务去运行。beeline是一个命令行的客户端。</p><pre><code class="properties">//Beeline使用JDBC连接到远程HiveServer2实例。连接参数包括JDBC URL。HiveServer2 是 Hive 开发维护的重点(Hive0.15 后就不再支持 hiveserver)，所以 Hive CLI 已经不推荐使用了，官方更加推荐使用 Beeline。hiveserver2//上面启动成功后使用beeline去连接// Beeline主要是开发来与新服务器进行交互。beeline -u jdbc:hive2:/localhost:10000 -n root -p 123456 </code></pre><pre><code>//启动hiveWebInterface，通过网页访问hive，默认端口号9999hive --service hwi</code></pre><h3 id="Hcatalog"><a href="#Hcatalog" class="headerlink" title="Hcatalog"></a>Hcatalog</h3><pre><code>//使用HCatalog访问hive//HCatalog是基于Apache Hadoop之上的数据表和存储管理服务，支持跨数据处理工具，如Pig，Mapreduce，Streaming，Hive。使用HCatalog，则hive的元数据也可以为其他基于Hadoop的工具所使用。无论用户用哪个数据处理工具，通过HCatalog，都可以操作同一个数据。//可以通过以下命令启动HCatalog$HIVE_HOME/hcatalog/sbin/hcat_server.sh start//可以通过以下命令启动HCatalog的cli界面$HIVE_HOME/hcatalog/bin/hcat//HCatalog的WebHCat 也提供一套REST API接口访问hive数据可以通过以下命令启动WebHCat$HIVE_HOME/hcatalog/sbin/webhcat_server.sh start</code></pre><p>HCatalog  API接口官网地址：<a href="https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference">https://cwiki.apache.org/confluence/display/Hive/WebHCat+Reference</a></p><h3 id="启动元数据服务"><a href="#启动元数据服务" class="headerlink" title="启动元数据服务"></a>启动元数据服务</h3><p>1.查看mysql是否能连上，如果不能连上，先解决mysql的问题</p><p>2.在hive-site.xml中配置</p><pre><code class="properties">&lt;property&gt;      &lt;name&gt;hive.metastore.uris&lt;/name&gt;      &lt;value&gt;thrift://xxxxxxxx&lt;/value&gt;  &lt;/property&gt;</code></pre><p>3.启动命令</p><pre><code>hive --service metastore &amp;</code></pre><p>4.jps查看进程，默认端口是9083</p><p>有一个RunJar的进程在运行</p><h3 id="安装MySQL"><a href="#安装MySQL" class="headerlink" title="安装MySQL"></a>安装MySQL</h3><pre><code>centos7安装yum数据源的mysqlhttps://blog.csdn.net/ntuxiaolei/article/details/81777335</code></pre><p>centos6.5安装mysql数据库是为了在数据库中存储元数据，在没有mysql时，存放在derby中，随着更换目录会丢失文件，安装完成mysql后，将元数据存储到数据库中，不会丢失，可以获取全部的数据</p><p>查询系统自带数据库：</p><pre><code>rpm -qa | grep mysql</code></pre><p>需要删除系统自带数据库</p><pre><code>rpm -e --nodeps mysql-libs-5.1.71-1.el6.x86_64</code></pre><p>上传MySQL的client和server的包，为了让节点运行资源平衡上传到其他节点上，在上传到节点之后安装mysql</p><pre><code>rpm -ivh MySQL-server-5.5.47-1.linux2.6.x86_64.rpmrpm -ivh MySQL-client-5.5.47-1.linux2.6.x86_64.rpm</code></pre><p>在上面mysql安装完毕后进行</p><pre><code>/usr/bin/mysql_secure_installation//进行初始化设置service mysql start//开启mysql服务</code></pre><p>设置root密码</p><p><img src="/cdh/hive/hive/1567694893302.png" alt="1567694893302"></p><p>进入到mysql中进行的操作 远程连接和给予权限</p><pre><code class="hive">mysql -u root -p密码//登陆mysql来进入mysql ——&gt;mysql&gt;查看数据库show databases 显示所有的数据库use mysql   更改显示的数据库show tables 显示当前数据库所有的表show tables from database;  查看指定数据库的所有表select * from user 查看表结构 存储着元数据//修改mysql数据库下面的user表//先删除除了localhost的所有本地连接delete from user where host!=&#39;localhost&#39;;//update user set host=&#39;%&#39;;//授权‘%’所有的grant all privileges on *.* to root@&#39;%&#39; identified by &#39;123456&#39;;//在授权完毕后要刷新权限flush privileges;//权限开放完成，其他节点安装mysql-client-rpm后可登录mysql服务端  //在其他节点登录登陆mysql时mysql -uroot -p123456 -h 192......;//根据hive-site.xml中的配置文件来进行新建数据库,因为是存储元数据，数据库的编码是latin1create database hive charset=&#39;latin1&#39;;//先启动hfds和yarn//在hive尝试启动hivehive</code></pre><h3 id="MySQL数据库存放的文件数据说明"><a href="#MySQL数据库存放的文件数据说明" class="headerlink" title="MySQL数据库存放的文件数据说明"></a>MySQL数据库存放的文件数据说明</h3><pre><code>//在存放元数据的hive数据库中    DBS存放hive中所有数据库的基本信息    SDS存储hive中文件存储的基本信息    TABL存放hive表，视图，索引表的基本信息</code></pre><p><img src="/cdh/hive/hive/1567756068267.png" alt="1567756068267"></p><h4 id="建立表的范式"><a href="#建立表的范式" class="headerlink" title="建立表的范式"></a>建立表的范式</h4><p>目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、（前3种重要）<br>巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF，又称完美范式）。</p><p>第一范式是原子性，每一个表的字段属性不能再分隔</p><p>第二范式是主键</p><p>第三范式是主外键</p><h4 id="表对象的关系"><a href="#表对象的关系" class="headerlink" title="表对象的关系"></a>表对象的关系</h4><p><strong>一对一</strong></p><p>可以把任何一个表的主键放入到另一个表中做外键</p><p><strong>一对多</strong></p><p>把一的主键放入到多的表中做外键</p><p><strong>多对多</strong></p><p>需要单独提出一个数据关系表放置两张表中的主键做外键</p><h3 id="hive的数据类型"><a href="#hive的数据类型" class="headerlink" title="hive的数据类型"></a>hive的数据类型</h3><pre><code>//基本数据类型tinyint1byte有符号整型smallint2byte有符号整型int 4byte有符号整型bigint8byte有符号整型boolean布尔类型float单精度浮点型double双精度浮点型string字符串//复合数据类型timestamp时间戳array数组Array(1,2)map一组无序的键值对map(&#39;a&#39;,1,&#39;b&#39;)struct一组命名的字段Struct(&#39;1&#39;,1,1,3)</code></pre><h3 id="内部表和外部表"><a href="#内部表和外部表" class="headerlink" title="内部表和外部表"></a>内部表和外部表</h3><p>内部表：元数据和数据表的数据都被hive管理的表叫做内部表</p><p>外部表：元数据被hive管理，对数据表没有管理权限</p><p><strong>区别</strong>：外部表在删除数据表时只删除元数据，数据表不删除，内部表在删除时元数据和数据表都会删除</p><p>外部表是因为某个需求需要使用hive统计某些数据</p><h4 id="创建外部表"><a href="#创建外部表" class="headerlink" title="创建外部表"></a>创建外部表</h4><pre><code>create [EXTERNAL] table ...//创建外部表  一般创建基准表时使用//外部表load hdfs文件 删除表还是会删除hdfs源文件的//外部表load local文件 删除表不会删除源文件</code></pre><h3 id="HIVE语法"><a href="#HIVE语法" class="headerlink" title="HIVE语法"></a>HIVE语法</h3><h5 id="在hive中创建表"><a href="#在hive中创建表" class="headerlink" title="在hive中创建表"></a>在hive中创建表</h5><pre><code>create table tablename (id int,name String,age tinyint);因为表默认的分隔符是^A，并不识别空格，所以在加载进来文件时会出现null，有两种解决方法1.在创建表时，自定义分隔符create table tablename (字段名 类型) row format delimited fields terminated by &quot; &quot;;2.在要加载的文件中加入默认的字段分隔符^Actrl+v   ctrl+a</code></pre><h5 id="hive的location"><a href="#hive的location" class="headerlink" title="hive的location"></a>hive的location</h5><p>location可以指定文件存储的路径，这样就不会放在默认的路径下面了</p><h5 id="hive的stored"><a href="#hive的stored" class="headerlink" title="hive的stored"></a>hive的stored</h5><p>可以定义hive文件存储格式</p><p>1、<strong>TEXTFILE(默认格式)</strong></p><p>建表时不指定默认为这个格式，导入数据时会直接把数据文件拷贝到hdfs上不进行处理；</p><p>2、SEQUENCEFILE</p><p>二进制文件,以&lt;key,value&gt;的形式序列化到文件中；</p><p>3、RCFILE</p><p>存储方式：数据按行分块 每块按照列存储；</p><p>4、ORCFILE(0.11以后出现)</p><p>存储方式：数据按行分块 每块按照列存储；</p><p>5、PARQUET</p><p>类似于orc，相对于orc文件格式，hadoop生态系统中大部分工程都支持parquet文件。</p><p>————————————————<br>版权声明：本文为CSDN博主「小飞猪666」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/yangshaojun1992/article/details/85124287">https://blog.csdn.net/yangshaojun1992/article/details/85124287</a></p><h5 id="Hive的不等于"><a href="#Hive的不等于" class="headerlink" title="Hive的不等于"></a>Hive的不等于</h5><pre><code>&lt;&gt;再hive中&#39;&lt;&gt;&#39;代表不等于，hive的不等于会自动过滤null数据，不会查询出来的，如果要查询出包含null的数据需要where （white_level&lt;&gt;&#39;3&#39; or  white_level is null） 或者 where (white_level!=&#39;3&#39; or white_level is null )  来保留null 的情况。</code></pre><h5 id="HIVE中导入数据的方式-4种"><a href="#HIVE中导入数据的方式-4种" class="headerlink" title="HIVE中导入数据的方式(4种)"></a>HIVE中导入数据的方式(4种)</h5><pre><code>//1、载入本地数据load data local inpath &#39;/本地路径&#39; into table tablename;//2、载入hdfs上的数据load data inpath &#39;/hdfs路径&#39; into table tablename;//上面两种导入方式会把原始数据变成hive的数据,除非外部表//3、追加数据insert into tablename1 select * from tablename2;// 覆盖数据insert overwrite tablename1 select * from table2;//4、在创建表的时候直接加载数据create table .... location hdfs的某个路径</code></pre><p><img src="/cdh/hive/hive/1567758509495.png" alt="1567758509495"></p><h5 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h5><pre><code>drop table tablename;</code></pre><h5 id="增加字段"><a href="#增加字段" class="headerlink" title="增加字段"></a>增加字段</h5><pre><code>alter table tablename add columns (字段 类型,字段 类型)</code></pre><h5 id="查看表的分区信息"><a href="#查看表的分区信息" class="headerlink" title="查看表的分区信息"></a>查看表的分区信息</h5><pre><code>show partitions tablenames;//查看表的分区信息</code></pre><h5 id="删除分区"><a href="#删除分区" class="headerlink" title="删除分区"></a><strong>删除分区</strong></h5><pre><code>alter table tablename drop partition(dt=&#39;2020-10-07&#39;);</code></pre><h5 id="查询数据库信息"><a href="#查询数据库信息" class="headerlink" title="查询数据库信息"></a>查询数据库信息</h5><pre><code>DESCRIBE DATABASE [EXTENDED] 数据库名称；注意：添加EXTENDED关键字可以查看更多信息</code></pre><h5 id="查看表的字段信息"><a href="#查看表的字段信息" class="headerlink" title="查看表的字段信息"></a>查看表的字段信息</h5><pre><code>desc formatted tablename;//查看所有信息</code></pre><h5 id="hive中操作hdfs"><a href="#hive中操作hdfs" class="headerlink" title="hive中操作hdfs"></a>hive中操作hdfs</h5><pre><code>hive&gt; dfs -mkdir -p /logdata/app/2020-10-07//在hive中递归创建了文件夹</code></pre><h5 id="更改表的名称"><a href="#更改表的名称" class="headerlink" title="更改表的名称"></a>更改表的名称</h5><pre><code>ALTER TABLE primevaltablename RENAME TO newtablename;//把 primevaltablename 修改为 newtablename。</code></pre><h3 id="HIVE的SQL和MySQL的区别（9个）"><a href="#HIVE的SQL和MySQL的区别（9个）" class="headerlink" title="HIVE的SQL和MySQL的区别（9个）"></a>HIVE的SQL和MySQL的区别（9个）</h3><h4 id="一定要注意-在hive中-子查询的结果集一定要给表名"><a href="#一定要注意-在hive中-子查询的结果集一定要给表名" class="headerlink" title="一定要注意 在hive中 子查询的结果集一定要给表名"></a>一定要注意 在hive中 子查询的结果集一定要给表名</h4><h5 id="1、聚合函数通常都和group-by联用"><a href="#1、聚合函数通常都和group-by联用" class="headerlink" title="1、聚合函数通常都和group by联用"></a>1、聚合函数通常都和group by联用</h5><p>因为只能显示group by字段，使用聚合函数可以显示其他要使用的字段</p><h5 id="2、hive可以在聚合函数中使用distinct关键字做去重处理"><a href="#2、hive可以在聚合函数中使用distinct关键字做去重处理" class="headerlink" title="2、hive可以在聚合函数中使用distinct关键字做去重处理"></a>2、hive可以在聚合函数中使用distinct关键字做去重处理</h5><h5 id="3、limit-num-num只是从结果中正数取num条数据"><a href="#3、limit-num-num只是从结果中正数取num条数据" class="headerlink" title="3、limit num  num只是从结果中正数取num条数据"></a>3、limit num  num只是从结果中正数取num条数据</h5><h5 id="4、order-by-和-sort-by区别"><a href="#4、order-by-和-sort-by区别" class="headerlink" title="4、order by 和 sort by区别"></a>4、order by 和 sort by区别</h5><h6 id="order-by-排序"><a href="#order-by-排序" class="headerlink" title="order by 排序"></a>order by 排序</h6><p>​指明了hive底层mr中reduct只是用一个  保持全局有序</p><h6 id="sort-by-排序"><a href="#sort-by-排序" class="headerlink" title="sort by  排序"></a>sort by  排序</h6><p>​指明了hive底层使用了多个reducer，只能保持reducer中的数据有序，保持局部有序</p><h5 id="5、distribute-by-和-sort-by"><a href="#5、distribute-by-和-sort-by" class="headerlink" title="5、distribute by 和 sort by"></a>5、distribute by 和 sort by</h5><p><strong>distribute by 通常和 sort by 一起使用，也可以单独使用</strong></p><h6 id="distribute-by"><a href="#distribute-by" class="headerlink" title="distribute by"></a>distribute by</h6><p>hive执行sql maptask阶段是按照什么列数据进行分区采用hash进行分区</p><p>采集hash算法，在map端将查询的结果中hash值相同的结果分发到对应的reduce文件中。</p><p>sort by为每一个reducer产生一个排序文件</p><pre><code>select * from t_emp distribute by deptno sort by salary desc ;//他们划分区域的字段和排序的字段可以不同但是distribute必须写在sort by前边</code></pre><h6 id="sort-by"><a href="#sort-by" class="headerlink" title="sort by"></a>sort by</h6><p>控制reduct中数据的排序</p><p>select … from table distribute by column01 sort by column02;</p><h6 id="Distribute-by和sort-by的使用场景"><a href="#Distribute-by和sort-by的使用场景" class="headerlink" title="Distribute by和sort by的使用场景"></a>Distribute by和sort by的使用场景</h6><p>1.Map输出的文件大小不均。</p><p>2.Reduce输出文件大小不均。</p><p>3.小文件过多。</p><p>4.文件超大。</p><h5 id="6、cluster-by"><a href="#6、cluster-by" class="headerlink" title="6、cluster by"></a>6、cluster by</h5><p>cluster by 是distribute by 和 sort by 的结合 </p><p>如果distribute by 和 sort by 后面的列名称是相同的  那么就可以直接使用cluster by 列名称</p><p>有排序和分区功能,cluster by指定的排序只能降序，不能指定desc和asc</p><p>只能按照一个字段进行分发 并且只能按照这个字段进行排序</p><pre><code>select mid, money, name from store cluster by mid等价于select mid, money, name from store distribute by mid sort by mid </code></pre><h5 id="7、rlike"><a href="#7、rlike" class="headerlink" title="7、rlike"></a>7、rlike</h5><p>rlike可以把某列的值和某个正则表达式进行匹配比较</p><p>where name like ‘%李’   匹配以’李’结尾的</p><p>where name rlike ‘[李]’  匹配包含’李’的</p><h5 id="8、join"><a href="#8、join" class="headerlink" title="8、join"></a>8、join</h5><h6 id="left-semi-join"><a href="#left-semi-join" class="headerlink" title="left semi join"></a>left semi join</h6><p>​在hive低版本(in、exists)关键字不支持，可以使用left semi join 代替  </p><p>map join 在关联时，使用mr中的mapjoin需要配置的参数</p><pre><code>map join  小表和大表  小表放入到内存中set hive.auto.convert.join=true; 开启map端joinset hive.mapjoin.smalltable.filesize=2500000;  设定小表的大小set hive.auto.convert.join.noconditionaltask=true;  开启多个小表的mapjoin的合并set hive.auto.convert.join.noconditionaltask.size=??; 小于这个设定值的所有小表文件大小  可以使用多个mapjoin合并</code></pre><p>避免了笛卡尔积（两个表字段的乘积）的出现，提高了运行效率，前面的表是基准表，不会有重复的记录</p><p>因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join 则会一直遍历。这就导致右表有重复值得情况下 left semi join 只产生一条，join 会产生多条，也会导致 left semi join 的性能更高</p><pre><code>select * from t_emp where deptno in (select deptno from t_dept)等价select * from t_emp left semi join t_dept on t_emp.deptno=t_dept.deptno;</code></pre><h5 id="9、hive的非等值连接"><a href="#9、hive的非等值连接" class="headerlink" title="9、hive的非等值连接"></a>9、hive的非等值连接</h5><p><strong>hive中不能使用非等值连接</strong></p><p>通常都是把mysql中的非等值连接修改成别的连接方式或者使用hive函数进行处理</p><h3 id="Hive的4个by的区别"><a href="#Hive的4个by的区别" class="headerlink" title="Hive的4个by的区别"></a>Hive的4个by的区别</h3><p><a href="https://blog.csdn.net/u010003835/article/details/80938339">https://blog.csdn.net/u010003835/article/details/80938339</a></p><h5 id="SORT-BY"><a href="#SORT-BY" class="headerlink" title="SORT BY"></a>SORT BY</h5><p> SORT BY 是一个部分排序方案， 其只会在每个reducer 中对数据进行排序，</p><p>也就是执行一个局部排序过程。</p><p>注意：</p><p>**使用sort by 你可以指定执行的reduce 个数 （set mapred.reduce.tasks&#x3D;）</p><p><strong>对输出的数据再执行归并排序，即可以得到全部结果。</strong></p><h5 id="ORDER-BY"><a href="#ORDER-BY" class="headerlink" title="ORDER BY"></a>ORDER BY</h5><p>ORDER BY 可以指定多个字段，可以按照某个字段进行 升序ASC , 或者 降序DESC.</p><p>  Hive 中 ORDER BY 和其他SQL 方言并没区别，会对查询结果进行一个全局排序。</p><p>其缺点：</p><ol><li>由于是全局排序，所以<strong>所有的数据会通过一个Reducer 进行处理</strong>，<strong>当数据结果较大的时候，</strong></li></ol><p><strong>一个Reducer 进行处理十分影响性能。</strong></p><p>注意事项：</p><p> <strong>当开启MR 严格模式的时候ORDER BY 必须要设置 LIMIT 子句 ，否则会报错</strong></p><p>开启严格模式：</p><p> set hive.mapred.mode&#x3D;strict;</p><h5 id="DISTRIBUTE-BY"><a href="#DISTRIBUTE-BY" class="headerlink" title="DISTRIBUTE BY"></a>DISTRIBUTE BY</h5><p>DISTRIBUTE BY 控制map 中的输出在 reducer 中是如何进行划分的。结合sort by一起使用</p><p>使用DISTRIBUTE BY 可以保证相同KEY的记录被划分到一个Reduce 中。</p><h5 id="CLUSTER-BY"><a href="#CLUSTER-BY" class="headerlink" title="CLUSTER BY"></a>CLUSTER BY</h5><p>如果对某一列既想采用SORT BY 也想采用 DISTRIBUTE BY ,</p><p>那么可以使用CLUSTER BY 进行排序。</p><p>注意：</p><p><strong>排序只能是升序排序（默认排序规则），不能指定排序规则为asc 或者desc。</strong>*</p><h3 id="Hive常用函数"><a href="#Hive常用函数" class="headerlink" title="Hive常用函数"></a>Hive常用函数</h3><p><a href="https://blog.csdn.net/qq_26442553/article/details/79465417">https://blog.csdn.net/qq_26442553/article/details/79465417</a></p><h4 id="if"><a href="#if" class="headerlink" title="if"></a>if</h4><p> if(boolean testCondition, T valueTrue, T valueFalseOrNull)</p><p><strong>如果testCondition 为true就返回valueTrue,否则返回valueFalseOrNull ，（valueTrue，valueFalseOrNull为泛型）</strong></p><pre><code>if( 100 is not null , &#39;a=100&#39;,&#39;a=99&#39; )  结果：a=100if( null,100,00 ) 结果：00</code></pre><h4 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h4><p>非空查找函数</p><pre><code class="php">COALESCE(T v1, T v2, …)说明:  返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL    select COALESCE(null,&#39;100&#39;,&#39;50′) from dual;100</code></pre><h4 id="case"><a href="#case" class="headerlink" title="case"></a>case</h4><p>条件判断函数</p><pre><code class="ruby">CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END说明：如果 a 等于 b ，那么返回 c ；如果 a 等于 d ，那么返回 e ；否则返回 f举例：hive&gt; Select case 100 when 50 then &#39;tom&#39; when 100 then &#39;mary&#39; else &#39;tim&#39; end from dual;mary</code></pre><h4 id="get-json-object"><a href="#get-json-object" class="headerlink" title="get_json_object()"></a>get_json_object()</h4><p>将数据转换成json对象并获取指定的字段值</p><p>get_json_object函数第一个参数填写json对象变量，第二个参数使用$表示json变量标识，然后用 . 或 [] 读取对象或数组；</p><pre><code>selectget_json_object(line,&#39;$.mid&#39;) mid_id,get_json_object(line,&#39;$.uid&#39;) user_id,**fromt1</code></pre><h4 id="json-tuple"><a href="#json-tuple" class="headerlink" title="json_tuple"></a>json_tuple</h4><p>当使用json_tuple对象时，可以显著提高效率，一次获取多个对象并且可以被组合使用，必须和 lateral view 连用 写法如下：</p><pre><code>select a.* from test lateral view json_tuple(‘$&#123;hivevar:msg&#125;’,’server’,’host’) a as f1,f2;</code></pre><h4 id="sub-str"><a href="#sub-str" class="headerlink" title="sub_str()"></a>sub_str()</h4><pre><code>substr（string A，int start）返回字符串A从下标start位置到结尾的字符串substr（string A，int start，int end）substr() 第一个参数表示待截取的字段名称，第二个参数表示截取的起始位置，第三个参数表示截取的长度。</code></pre><h4 id="group-by-分组"><a href="#group-by-分组" class="headerlink" title="group by 分组"></a>group by 分组</h4><p>group by 可以进行自动的去重</p><p><strong>能用group by 进行去重不要使用distinct，groupby比distinct效率高很多</strong></p><p>group by 会造成数据倾斜，资源不一致，运行时间不一致很容易造成宕机</p><p>有四点</p><p>打散 聚合 设置计算机的处理能力 设置阀值（超过阀值会自动优化）</p><pre><code>//1.打散set hive.groupby.skewindata=true;//打散 map端地数据要尽量打散//2.聚合set hive.map.aggr=true;//设置map端地聚合//3.设置计算机的处理能力 set hive.groupby.mapaggr.checkinterval=100000;//计算机本身的处理能力不超过100000条数据的话 能够进行自行优化，不会产生数据倾斜问题4.set hive.map.aggr.hash.min.reduction=0.5;//设置阀值，超过阀值自动优化//上面的都是自动优化 </code></pre><h4 id="having"><a href="#having" class="headerlink" title="having"></a>having</h4><p><strong>分组之后过滤用having，分组之前用where</strong></p><p>having 过滤 跟group by 一起使用，使用聚合函数在没有group by 不能使用 ，</p><p>可以选择分组之后结果的范围</p><p>跟where都是限定返回的数据集，但是where子句中不能使用聚合函数，在having中可以使用聚合函数</p><pre><code>例：select * from t_emp group by deptno having avg(salary) &gt; 5000;</code></pre><h4 id="collect-list"><a href="#collect-list" class="headerlink" title="collect_list"></a>collect_list</h4><p>可以将同一分组不同行的数据合成一个集合，不会去重，可以取出group by 限制的字段，在使用concat进行拆分</p><h4 id="collect-set"><a href="#collect-set" class="headerlink" title="collect_set()"></a>collect_set()</h4><p>可以将同一分组不同行的数据合成一个集合，会将结果去重，可以取出group by 限制的字段</p><pre><code>select course,avg(score),collect_set(area) from stud group by course;//course_c1_c2//chinese79.0[&quot;sh&quot;,&quot;bj&quot;]//math93.5[&quot;bj&quot;]</code></pre><h4 id="concat-1-2"><a href="#concat-1-2" class="headerlink" title="concat($1,*,$2)"></a>concat($1,*,$2)</h4><p>concat**网页<a href="https://www.cnblogs.com/wqbin/p/10266783.html">https://www.cnblogs.com/wqbin/p/10266783.html</a></p><p>CONCAT（）函数用于将多个字符串连接成一个字符串。</p><p>返回结果为连接参数产生的字符串。如<strong>有任何一个参数为NULL ，则返回值为 NULL。</strong>可以有一个或多个参数。</p><pre><code>+----+--------+| id | name   |+----+--------+|  1 | BioCyc |+----+--------+   SELECT CONCAT(id, ‘，’, name) AS con FROM info LIMIT 1;+----------+| con      |+----------+| 1,BioCyc |+----------+</code></pre><h4 id="concat-ws-1-2-…"><a href="#concat-ws-1-2-…" class="headerlink" title="concat_ws(*,$1,$2,…)"></a>concat_ws(*,$1,$2,…)</h4><p>第一个参数是其它参数的分隔符。分隔符的位置放在要连接的两个字符串之间。分隔符可以是一个字符串，也可以是其它参数。如果分隔符为 NULL，则结果为 NULL。函数会忽略任何分隔符参数后的 NULL 值。但是CONCAT_WS()不会忽略任何空字符串。 (然而会忽略所有的 NULL）。</p><pre><code>CONCAT_WS(separator,str1,str2,…)concat_ws(string SEP, array&lt;string&gt;)SELECT CONCAT_WS(&#39;_&#39;,id,name) AS con_ws FROM info LIMIT 1;+----------+| con_ws   |+----------+| 1_BioCyc |+----------+SELECT CONCAT_WS(&#39;,&#39;,&#39;First name&#39;,NULL,&#39;Last Name&#39;);+----------------------------------------------+| CONCAT_WS(&#39;,&#39;,&#39;First name&#39;,NULL,&#39;Last Name&#39;) |+----------------------------------------------+| First name,Last Name                         |+----------------------------------------------+</code></pre><h4 id="regexp-replace-‘-‘-’-‘-’-‘"><a href="#regexp-replace-‘-‘-’-‘-’-‘" class="headerlink" title="regexp_replace(‘ ‘,’ ‘,’ ‘)"></a>regexp_replace(‘ ‘,’ ‘,’ ‘)</h4><pre><code>语法: regexp_replace(string A, string B, string C)操作类型: strings返回值: string说明: 将字符串A中的符合java正则表达式B的部分替换为C。hive&gt; select regexp_replace(&#39;h234ney&#39;, &#39;\\d+&#39;, &#39;o&#39;);OKhoney</code></pre><h4 id="regexp-extract-‘’-’’-int"><a href="#regexp-extract-‘’-’’-int" class="headerlink" title="regexp_extract(‘’,’’,int)"></a>regexp_extract(‘’,’’,int)</h4><pre><code>语法: regexp_extract(string subject, string pattern, int index)操作类型: strings返回值: string说明: 将字符串subject中的符合java正则表达式pattern规则的部分进行拆分，返回index指定的字符hive&gt; select regexp_extract(&#39;foothebar&#39;, &#39;foo(.*?)(bar)&#39;, 2);OKbar</code></pre><h4 id="str-to-map"><a href="#str-to-map" class="headerlink" title="str_to_map()"></a>str_to_map()</h4><pre><code class="sql">语法:STR_TO_MAP(VARCHAR text, VARCHAR listDelimiter, VARCHAR keyValueDelimiter)操作类型:varchar返回值:map说明:使用listDelimiter将text分隔成K-V对，然后使用keyValueDelimiter分隔每个K-V对，组装成MAP返回。默认listDelimiter为（ ，），keyValueDelimiter为（=）。str_to_map(&#39;1001=2020-06-14,1002=2020-06-14&#39;,  &#39;,&#39;  ,  &#39;=&#39;)输出&#123;&quot;1001&quot;:&quot;2020-06-14&quot;,&quot;1002&quot;:&quot;2020-06-14&quot;&#125;</code></pre><h4 id="with-as"><a href="#with-as" class="headerlink" title="with as"></a>with as</h4><p><a href="https://www.cnblogs.com/jeasonit/p/11600083.html">https://www.cnblogs.com/jeasonit/p/11600083.html</a></p><p>with as 也叫做子查询部分，首先定义一个sql片段，该sql片段会被整个sql语句所用到，为了让sql语句的可读性更高些，作为提供数据的部分，也常常用在union等集合操作中。</p><p>with as就类似于一个视图或临时表，可以用来存储一部分的sql语句作为别名，不同的是with as 属于一次性的，而且必须要和其他sql一起使用才可以！</p><p>提高代码的可读性</p><pre><code class="hsql">with 表名 asselect ....,表明2 as </code></pre><h3 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a>日期函数</h3><h4 id="date-format"><a href="#date-format" class="headerlink" title="date_format"></a>date_format</h4><p>时间格式化函数</p><pre><code>//只保留年和月select date_format(&#39;2020-05-15&#39;,&#39;yyyy-MM&#39;);//2020-05</code></pre><h4 id="date-add"><a href="#date-add" class="headerlink" title="date_add"></a>date_add</h4><p>加减日期</p><pre><code>//当前日期的前一星期日期select date_add(&#39;2020-05-15&#39;,-7);//2020-05-08//后一星期日期select date_add(&#39;2020-05-15&#39;,+7);//2020-05-22</code></pre><h4 id="next-day"><a href="#next-day" class="headerlink" title="next_day"></a>next_day</h4><p>获取当前时间下一周的某天</p><pre><code>获取下一周的星期一select next_day(&#39;2020-05-15&#39;,&#39;MO&#39;);//2020-05-18//星期一到星期日MOTUWETHFRSASU//获取当前周的周一select date_add(next_day(&#39;2020-05-15&#39;,&#39;MO&#39;),-7);</code></pre><h4 id="last-day"><a href="#last-day" class="headerlink" title="last_day()"></a>last_day()</h4><p><strong>返回这个月的最后一天的日期，忽略时分秒部分（HH:mm:ss）</strong> last_day(string date)</p><pre><code>返回值：string2 hive&gt; select  last_day(&#39;2017-02-17 08:34:23&#39;);3 OK4 2017-02-285 Time taken: 0.082 seconds, Fetched: 1 row(s)</code></pre><h4 id="date-sub"><a href="#date-sub" class="headerlink" title="date_sub()"></a>date_sub()</h4><pre><code>date_sub (string startdate,int days)</code></pre><p>返回值: string </p><p>说明: 返回开始日期startdate减少days天后的日期。 </p><pre><code>select date_sub(&#39;2016-12-29&#39;,10);//2016-12-19</code></pre><h4 id="months-between"><a href="#months-between" class="headerlink" title="months_between"></a>months_between</h4><p>用于计算date1和date2之间有几个月。    如果date1在日历中比date2晚，那么MONTHS_BETWEEN()就返回一个正数。<br>如果date1在日历中比date2早，那么MONTHS_BETWEEN()就返回一个负数。<br>如果date1和date2日期一样，那么MONTHS_BETWEEN()就返回一个0。</p><pre><code>months_between(date1,date2)</code></pre><h4 id="from-unixtime"><a href="#from-unixtime" class="headerlink" title="from_unixtime()"></a>from_unixtime()</h4><p>将时间戳变成timestamp格式返回</p><pre><code class="shell">from_unixtime(1606216525,&#39;yyyy-MM-dd&#39;)//结果10  指10点整</code></pre><h3 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h3><p><a href="https://www.cnblogs.com/logon/p/3748020.html">https://www.cnblogs.com/logon/p/3748020.html</a></p><h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><p>默认inner join，</p><p>inner join 产生的结果是两个表的<strong>交集</strong>（匹配上的才会展示）</p><p>如果右表中有多条重复的字段，只要匹配上就显示</p><h4 id="left-semi-join-1"><a href="#left-semi-join-1" class="headerlink" title="left semi join"></a>left semi join</h4><p><a href="https://mp.weixin.qq.com/s?__biz=MzI2MDQzOTk3MQ==&amp;mid=2247484361&amp;idx=1&amp;sn=bed9fe760c35f93cd63d580d27a4b9f9&amp;chksm=ea68e905dd1f6013b563787bce4544baae0fa66ffe83bbc1dedc43ad2e0b1635943a75af625f&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzI2MDQzOTk3MQ==&amp;mid=2247484361&amp;idx=1&amp;sn=bed9fe760c35f93cd63d580d27a4b9f9&amp;chksm=ea68e905dd1f6013b563787bce4544baae0fa66ffe83bbc1dedc43ad2e0b1635943a75af625f&amp;scene=21#wechat_redirect</a></p><p>1）Semi Join，也叫半连接</p><p>2）left semi join 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。</p><p>3）<strong>遇到右表重复记录，左表会跳过</strong>（多条数据只会有一条），而 join on 则会一直遍历。</p><p>4）left semi join 中<strong>最后 select 的结果只许出现左表</strong>，因为右表只有 join key 参与关联计算了，而 join on 默认是整个关系模型都参与计算了。</p><p>left semi join 如果右表中有多条重复的字段，只会显示一次</p><h4 id="left-outer-join"><a href="#left-outer-join" class="headerlink" title="left outer join"></a>left outer join</h4><p>left outer join 显示左表的全部，右表</p><h4 id="full-join"><a href="#full-join" class="headerlink" title="full join"></a>full join</h4><p>full join 是全连接，会显示两个表的全部内容，没有匹配到的值显示Null</p><h4 id="union"><a href="#union" class="headerlink" title="union"></a>union</h4><p>UNION 操作符用于合并两个或多个 SELECT 语句的结果集。</p><p>注意，UNION 内部的 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每条 SELECT 语句中的列的顺序必须相同。UNION 只选取记录，而UNION ALL会列出所有记录。</p><pre><code>select * from teblename1 union tablename2 //相同的字段只会显示一遍select * from tablename1 union all tablename2 //相同的字段也会显示select_statement UNION [ALL | DISTINCT] select_statement UNION [ALL | DISTINCT] select_statement …//查询语句 UNION [ALL | DISTINCT] 查询语句 ...</code></pre><h4 id="union-all"><a href="#union-all" class="headerlink" title="union all"></a>union all</h4><p>union all会显示全部的内容，不会进行去重，效率比union高。</p><h5 id="union和union-all的区别"><a href="#union和union-all的区别" class="headerlink" title="union和union all的区别"></a>union和union all的区别</h5><p>union会去重，但会耗费一定资源，经常使用union all，因为效率高</p><h5 id="union-all和full-join的区别"><a href="#union-all和full-join的区别" class="headerlink" title="union all和full join的区别"></a>union all和full join的区别</h5><p>union all关联的两个查询必须拥有相同数量的列。列也必须拥有相似的数据类型</p><p>full join 没有这个限制</p><h5 id="union和join的区别"><a href="#union和join的区别" class="headerlink" title="union和join的区别"></a>union和join的区别</h5><p>join适合有相同字段的表进行join（<strong>可以on</strong>）</p><p>union适合没有相同字段的表</p><h4 id="Map-join"><a href="#Map-join" class="headerlink" title="Map join"></a>Map join</h4><p>将小文件加载到内存</p><p>它通常会用在如下的一些情景：在二个要连接的表中，有一个很大，有一个很小，这个小表可以存放在内存中而不影响性能。<br>这样我们就把小表文件复制到每一个Map任务的本地，再让Map把文件读到内存中待用。</p><pre><code>set hive.auto.convert.join=true;//set hive.mapjoin.smalltable.filesize=25000000; //小于25M例：select /*+MAPJOIN(t_dept)*/* from t_emp e join t_dept d on e.deptno=d.deptno;</code></pre><h4 id="特殊-is-Null"><a href="#特殊-is-Null" class="headerlink" title="特殊:is Null"></a>特殊:is Null</h4><p>可以使用is null来那出特殊的一部分</p><p><img src="/cdh/hive/hive/241947220904425.jpg" alt="img"></p><p>DDL：对于表结构的处理</p><pre><code>alter table tablname rename to new tablename;//更改表的名称alter table tablename change column old column new column coltype;//表中字段的修改</code></pre><p>DML：对于表中获取的数据的修改</p><pre><code>later table tablename add columns (col coltype);//增加表中的字段alter table tablename replace columns (col,是最后的结果展示)//减少表中的字段 age name id ——&gt;&gt;age id </code></pre><h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><h5 id="什么是分区？为什么要分区？"><a href="#什么是分区？为什么要分区？" class="headerlink" title="什么是分区？为什么要分区？"></a>什么是分区？为什么要分区？</h5><p>hive中查询数据表，默认都是全表扫描，如果数据量很大，查询的效率会很低(hive底层是mr)</p><p>为了提升查询效率，通常把经常使用的字段  作为分区字段</p><p><strong>普通表和分区表的区别</strong>在于：一个Hive表在HDFS上是有一个对应的目录来存储数据，普通表的数据直接存储在这个目录下，而分区表数据存储时，是再划分子目录来存储的。一个分区一个子目录。主要作用是来优化查询性能。</p><p>来减少数量，加快查询效率</p><h5 id="原始表什么字段适合分区？"><a href="#原始表什么字段适合分区？" class="headerlink" title="原始表什么字段适合分区？"></a>原始表什么字段适合分区？</h5><p>  这个字段经常被放到where后面做查询条件<br>  这个字段的字段值要符合某个规律</p><h5 id="如何创建分区"><a href="#如何创建分区" class="headerlink" title="如何创建分区"></a>如何创建分区</h5><p>hive中通常把数据表分为原始表，分区表，临时表，结果表。</p><p><strong>原始表</strong>：一般都是把采集到的日志数据文件导入一个表中，这个表中的数据都是日志原始数据</p><p><strong>分区表</strong>：为了提高查询效率，根据某一列进行分区</p><p><strong>临时表</strong>：有原始表到结果表可能会保存临时的统计结果，通常是内部表</p><p><strong>结果表</strong>：存放最终的统计结果，通常也是内部表</p><h5 id="单分区"><a href="#单分区" class="headerlink" title="单分区"></a>单分区</h5><pre><code>//创建基础表create teble tepyb(id int,temp int,hour int,day int)row format  delimited fields terminated by &quot; &quot;;//创建分区表，分区的伪列不要写在前面的建表语句create table tep(id int,temp int,hour int) partitioned by(day int) row format delimited fields terminated by &#39;\t&#39; lines terminated by &#39;\n&#39; //行之间的分隔符stored as textfile;////往基础表中加载数据load data local inpath &#39;/tt.txt&#39; into table tep ;//指定分区的列按照什么字段值来分alter table tablename add partition (day=2);alter table tablename add partition (day=3);//删除分区的分区条件alter table tablename drop partition (分区条件)alter table part drop partition (mouth=&#39;8&#39;,day=14);//增加分区alter table tablename add partition (分区条件)//给每个分区添加数据//建立分区就是在hdfs数据表的目录下再创建目录  //目录名称规则是分区列名称=分区值insert into tep partition (day) select id temp hour day from tpp_bb;//在完成后可以通过linux01:50070下面的/user/hive/warehouse来监控分区分桶后的文件夹//分区是按照字段值把数据放到不同的目录中//  创建好分区表之后  如果再需要按照day进行查询   就不再使用原始表tepyb       而是直接使用分区表tep进行按照国家查询select * from tep where day =2;</code></pre><h5 id="多分区"><a href="#多分区" class="headerlink" title="多分区"></a>多分区</h5><p>加了一层分区 更快更便捷</p><pre><code>//创建分区表create table tep(id int,temp int,hour int) partitioned by(mouth string,day string)row format delimited fields terminated by &#39;\t&#39;lines terminated by &#39;\n&#39;stored as textfile;//创建基准表create table tep2(id int,temp int,hour int,mouth string,day string) row format delimited fields terminated by &#39;\t&#39;lines terminated by &#39;\n&#39;stored as filetext;//给分区添加数据insert into table tablenamepartition (mouth=&#39;7&#39;,day=&#39;10&#39;)select mouth,day from tep2 where mouth=&#39;7&#39; day=&#39;10&#39;; </code></pre><h5 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h5><p>动态分区列必须在SELECT语句的列中最后指定，并且与它们在partition()子句中出现的顺序相同。</p><pre><code>//对应的伪列进行全部的分区//开启动态分区set hive.exec.dynamic.partition=true;//指定动态分区采用的非严格模式set hive.exec.dynamic.partition.mode=nonstrict;//指定动态分区总数SET hive.exec.max.dynamic.partitions=100000;//设定每个节点的最大分区数量SET hive.exec.max.dynamic.partitions.pernode=100000;//创建基准表导入数据....//创建分区表create table tempon.t_access_sum_d (ucount int) partitioned by (uyear string,umonth string)row format delimited fields terminated by &#39;\t&#39;lines terminated by &#39;\n&#39;;stored as textfile;//导入数据到分区表中insert overwrite tempon.t_access_sum_d partition (uyear,umonth) select sum(ucount),year(umonth),month(umonth) from tempon.t_access2 group by year(umonth),month(umonth)</code></pre><pre><code>//查看表的分区show partitions tablename;//增加表的分区条件alter table tablename add partition (分区字段)alter table part add partition (mouth=&#39;8&#39;,day=14);//删除表的分区alter table tablename drop partition (分区字段)alter table part drop partition (mouth=&#39;8&#39;,day=14);</code></pre><h5 id="混合分区"><a href="#混合分区" class="headerlink" title="混合分区"></a>混合分区</h5><pre><code>//混合了动态分区和静态分区。且静态分区应该放到动态分区的前面。//插入数据时，第一级分区写死，第二级分区使用动态分区。这样就有效控制了由于分区过多，导致文件名过多，影响hdfs性能的问题。//！！注意静态分区字段要在动态前面insert overwrite  table tempon.t_access_sum_dpartition (uyear=&#39;2015&#39;,umonth)select sum(ucount),month(umonth) from tempon.t_access2 group by year(umonth),month(umonth) having year(umonth) = &#39;2015&#39;;</code></pre><h3 id="分桶"><a href="#分桶" class="headerlink" title="分桶"></a>分桶</h3><h5 id="什么是分桶？为什么要分桶？"><a href="#什么是分桶？为什么要分桶？" class="headerlink" title="什么是分桶？为什么要分桶？"></a>什么是分桶？为什么要分桶？</h5><p>桶是比区更细粒度的划分，</p><p>Hive采用对列值哈希，按照hash值除以桶的个数求余的方式决定该条记录存放在哪个桶当中。</p><p>分桶的作用：加快查询的效率，减少 join的笛卡尔积</p><h5 id="如何创建分桶"><a href="#如何创建分桶" class="headerlink" title="如何创建分桶"></a>如何创建分桶</h5><p>通过数据量的大小来定有多少个桶</p><h5 id="表中哪些列适合分桶"><a href="#表中哪些列适合分桶" class="headerlink" title="表中哪些列适合分桶"></a>表中哪些列适合分桶</h5><p>经常被用在where之后出现</p><p>这个字段的字段值没有任何规律</p><h5 id="分桶开始"><a href="#分桶开始" class="headerlink" title="分桶开始"></a>分桶开始</h5><pre><code>//开启强制执行分桶set hive.enforce.bucketing=true;//创建分桶表create table tmw(id int,name string) clustered by (id) sorted by &#39;id asc&#39; into 3 buckets;//创建基准表create table tmwyb(id int,name string) row format delimited fields terminated by &#39;\t&#39; lines terminated by &#39;\n&#39;stored as textfile;//load给基准表加载数据load data local inpath &#39;/buck.txt&#39; into table twmyb;//分桶表插入数据insert into tmw select * from tmwyb;//原始表的数据会被放入桶表中  hdfs上按照桶的数量拆分成文件</code></pre><h5 id="分区和分桶一起"><a href="#分区和分桶一起" class="headerlink" title="分区和分桶一起"></a>分区和分桶一起</h5><pre><code>set hive.enforce.bucketing=true;//开启分桶set hive.optimize.bucketmapjoin=true;设置：是否是严格 是否开启分桶set mapreduce.job.reduces=4;//设置reduce的数量create table tmw(id int,name string) partitioned by(sex string) clustered by (id) into 3 buckets;//创建分区分桶表  into num 来指定多少个桶create table tmw2(id int,name string,sex string) row format delimited fields terminated by &quot;\t&quot; lines terminated by &#39;\n&#39; stored as textfile;//创建基础表load data local inpath &#39;/&#39; into table tmw2;//加载数据insert into tmw partition(sex) select id,name,sex from tmw2;//分区表中进行插入数据</code></pre><h4 id="分区和分桶的区别"><a href="#分区和分桶的区别" class="headerlink" title="分区和分桶的区别"></a>分区和分桶的区别</h4><p>分区是按照分区字段值把数据放入不同的分区目录中  ，分桶是把原始数据按照hash值除以桶的数量取余数放入对应的桶文件中<br>      分区可以使用静态分区和动态分区<br>              静态分区是手动指定分区值<br>      动态分区是hive自动按照分区字段的值(按照相同的值进行分区)进行分区<br>      分桶是按照分桶列的值hash 随机进行分区</p><p>分区是非随机的 有规律的   分桶是随机的</p><h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h4><p>分区分桶越多，在查询时越快捷和方便，但是每分区和分桶一次就会产生大量的元数据，会增大namenode的压力，索引是为了减少namenode的压力，创建好的索引文件来存放位置信息，索引文件是有序的，存放在本地的mysql</p><p>索引文件可以有多个，（多种表达形式）</p><pre><code>//创建索引文件create index index_name on table table_name(id)//创建索引文件 索引名称 表名称 as&quot;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&quot;//工具类，生成文件，排序，记录相关位置with deferred rebuild;//先不建立索引;在这里没有建立索引，等数据过来之后建立//真正创建索引文件alter index index_name on table_name rebuild;</code></pre><p>索引文件的名称</p><p><img src="/cdh/hive/hive/1568202672470.png" alt="1568202672470"></p><p>表及文件，可以使用sql语句来查看文件</p><p>id——位置——偏移量</p><p><img src="/cdh/hive/hive/1568202741562.png" alt="1568202741562"></p><h4 id="hive中的复杂数据类型"><a href="#hive中的复杂数据类型" class="headerlink" title="hive中的复杂数据类型"></a>hive中的复杂数据类型</h4><h5 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h5><pre><code>create table ma(id int,name string,array array)row format delimited fields terminated by &quot; &quot;;collection items terminated by &quot;,&quot;;//在创建表时除了指定表自身的分隔符还要制定array数据的分隔符数据例：1 zhangsan a,b,c,d</code></pre><h5 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h5><pre><code>create table ma(id int,name string,favi map&lt;string,string&gt;)row format delimited fields terminated by &quot; &quot;collection items terminated by &quot;,&quot;map keys terminated by &quot;:&quot;;//表自身的分隔符，map kv对的分隔符，kv之间的分隔符数据例：1 zhangsan a:b,c:d</code></pre><h5 id="struct"><a href="#struct" class="headerlink" title="struct"></a>struct</h5><pre><code>create table tt(id int,name string,address struct&lt;province:string,city:string,xian:string&gt;)row format delimited fields terminated by &quot; &quot; collection items terminated by &quot;,&quot;;//struct类似于数组，但是key是在建表时在struct中指定好的数据例：1 zhangsan a,b,c,d</code></pre><p><img src="/cdh/hive/hive/1568203440357.png" alt="1568203440357"></p><h4 id="炸裂函数"><a href="#炸裂函数" class="headerlink" title="炸裂函数"></a>炸裂函数</h4><h4 id="HIVE函数-八种"><a href="#HIVE函数-八种" class="headerlink" title="HIVE函数(八种)"></a>HIVE函数(八种)</h4><p> <a href="https://www.cnblogs.com/MOBIN/p/5618747.html">https://www.cnblogs.com/MOBIN/p/5618747.html</a></p><h5 id="数字函数"><a href="#数字函数" class="headerlink" title="数字函数"></a>数字函数</h5><pre><code>round(DOUBLE a)  返回对a四舍五入的bigint值round(Double a,int d)  返回double型d的保留n位小数的double值floor(Double)  向下取整，返回bigintceil(double a)  给其不greatest(T v1, T v2,...) 求最大值least(T v1, T v2,...)  求最小值</code></pre><h5 id="集合函数"><a href="#集合函数" class="headerlink" title="集合函数"></a>集合函数</h5><pre><code>size(Map&lt;k.v&gt;)  求map的长度size(Array&lt;T&gt;)  求数组的长度map_keys(Map&lt;k.v&gt;)返回map中的所有keymap_values(Map&lt;k.v&gt;)返回map中所有的valuearray_contains(Array&lt;T&gt;,value)  该数组array&lt;T&gt;包含value返回true，否则返回flasesort_array(Array&lt;T&gt;)  按照自然顺序对数组进行排序并返回</code></pre><h5 id="类型转换函数"><a href="#类型转换函数" class="headerlink" title="类型转换函数"></a>类型转换函数</h5><pre><code>binary(string|binary) 将输入的值转换成二进制cast(expr as &lt;type&gt;)将expr转换成type类型 </code></pre><h5 id="日期函数-1"><a href="#日期函数-1" class="headerlink" title="日期函数"></a>日期函数</h5><pre><code>from_unixtime(bigint unixtime[,string format])可以将时间戳格式化成format格式，format可为“yyyy-MM-dd hh:mm:ss”,“yyyy-MM-dd hh”,“yyyy-MM-dd hh:mm”等等）如from_unixtime(1250111000,&quot;yyyy-MM-dd&quot;) 得到2009-03-12unix_timestamp(String date) 将&#39;yyyy-MM-dd HH:mm:ss&#39;格式时间字符转化成bigint时间戳to_date(string timestamp)   返回时间字符串的日期部分(string类型)year(string date)   返回时间字符串的年份部分mouth(string date)返回时间字符串的月份部分day(string date)返回时间字符串的天hour(string date)返回时间字符串的小时minute(string date)返回时间字符串的分钟second(string date)返回时间字符串的秒current_date      返回当前时间date类型datediff(string enddate,string startdate)    返回结束日期减去开始日期的天数。          date_add(string startdate, int days)         返回开始日期startdate增加days天后的日期。date_sub (string startdate,int days)         返回开始日期startdate减少days天后的日期。add_months(string startdate,int days)        返回开始日期startdate减少months月后的日期。    months_betwween</code></pre><h5 id="条件函数"><a href="#条件函数" class="headerlink" title="条件函数"></a>条件函数</h5><pre><code class="scala">if(boolean testcondition,T valueTrue,T valueFalseOrNull)   //如果testCondition为ture返回valueTrue，false返回valueFalseOrNullnvl(T value,T defalut_value)//如果value值为null返回defalut_value，否则返回value//一般case语句使用以下两种形式CASE a WHEN b THEN c [WHEN d THEN e] * [ELSE f] ENDCASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END//When a = b, returns c; when a = d, returns e; else returns f.//如果a=b就返回c,a=d就返回e，否则返回f  //如CASE 4 WHEN 5  THEN 5 WHEN 4 THEN 4 ELSE 3 END 将返回4isnull(a)如果a为null就返回true，否则返回falseisnotnull(a)如果a时非null返回tuue，否则返回false//非空查找函数coalesce(s1,s2,s3,s4,s5,s6)//说明:  返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL</code></pre><h5 id="字符函数"><a href="#字符函数" class="headerlink" title="字符函数"></a>字符函数</h5><pre><code>length(string A)  返回字符串的长度reverse(string A)反转字符串rtrim(string A)去掉字符串后面出现空格</code></pre><h5 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h5><pre><code>count(*)   总计sum(col)指定列的和avg(col)指定列的平均值min(col)指定列的最小值max(col)指定列的最大值</code></pre><h5 id="表生成函数"><a href="#表生成函数" class="headerlink" title="表生成函数"></a>表生成函数</h5><pre><code>不常用 没有写，在需要时查看连接 https://www.cnblogs.com/MOBIN/p/5618747.html</code></pre><h5 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h5><pre><code>instr(string str, string substr)str中包含substr，则返回1，否则返回0，任意参数为空则返回null</code></pre><h4 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h4><pre><code>//数字函数pomd()//取模，求余数rount()//四舍五入floor()//向下取整coli()//先上取整//集合函数size(array/map)//长度大小  select size(score) from tt;map_keysselect explode(map_keys(字段名)) from tablename;//显示所有的keymap_valuesselect explode(map_values(字段名)) from tablename;//显示所有的value值sort_array对查询的结果进行排序</code></pre><h4 id="函数黄金组合"><a href="#函数黄金组合" class="headerlink" title="函数黄金组合"></a>函数黄金组合</h4><h5 id="sum-if"><a href="#sum-if" class="headerlink" title="sum(if)"></a>sum(if)</h5><h4 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h4><h5 id="自定义函数的类型"><a href="#自定义函数的类型" class="headerlink" title="自定义函数的类型"></a>自定义函数的类型</h5><p>UDF：进一出一  eg:trim(string A)  给定一个字符串  trim去除两边的空格  掌握<br>UDAF：进多出一  eg:sum()  对一列值求和   熟悉<br>UDTF：进一出多  了解</p><h5 id="UDF的编写过程"><a href="#UDF的编写过程" class="headerlink" title="UDF的编写过程"></a>UDF的编写过程</h5><ol><li><p>新建java项目添加<strong>hive-exec-2.1.0.jar</strong>和<strong>hadoop-common-2.7.3.jar</strong></p></li><li><p>编写自定义类继承<strong>UDF</strong>类</p></li><li><p>编写方法evaluate （方法的参数就是自定义函数的进一数据的类型）</p></li><li><p>把编写的类打成jar包</p></li><li><p>hive中注册jar包</p><pre><code>hive&gt; add jar /home/hadoop/xxx.jar;</code></pre></li><li><pre><code>CREATE TEMPORARY FUNCTION xxx AS &#39;描述名&#39;;</code></pre></li></ol><h5 id="UDTF的编写过程"><a href="#UDTF的编写过程" class="headerlink" title="UDTF的编写过程"></a>UDTF的编写过程</h5><p>继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF,实现initialize, process, close三个方法。</p><p>UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，名称和类型）。</p><p>初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward()调用产生一行；如果产生多列可以将多个列的值放在一个数组中，然后将该数组传入到forward()函数。</p><p>最后close()方法调用，对需要清理的方法进行清理。</p><p>把编写的类打成jar包</p><p>hive中注册jar包</p><h5 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h5><p>concat(string1,string,…)　　&#x2F;&#x2F;连接括号内字符串，数量不限。</p><p>concat_ws(separator,string1,string2,…)　　</p><p>concat_ws(string SEP, array<string>)</string></p><p>&#x2F;&#x2F;连接括号内字符串，数量不限，连接符为separator。</p><p>collect_set(col)　　&#x2F;&#x2F;此函数只接受基本类型，主要是将字段的值进行去重汇总，产生array类型字段。</p><h5 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h5><p>explode()</p><p>就是把array，map这种数据集合分散开来成为单个数据</p><pre><code>select explode(字段名称) from 表名;//填写字段名称，字段的类型是array或map数据类型的//但是select不能加explode以外的任何显示内容lateral view：侧视图配合explode（或者其他的UDTF），一个语句生成把单行数据拆解成多行后的数据结果集。　　//LATERAL VIEW explode(split(goods_id,&#39;,&#39;))goods相当于一个虚拟表</code></pre><p><img src="/cdh/hive/hive/1568204725217.png" alt="1568204725217"></p><h6 id="！爆炸函数的局限性"><a href="#！爆炸函数的局限性" class="headerlink" title="！爆炸函数的局限性"></a>！爆炸函数的局限性</h6><p>在使用爆炸函数的时候，select后面只能跟爆炸函数，其他的不能跟</p><p><img src="/cdh/hive/hive/1568205862236.png" alt="1568205862236"></p><h6 id="lateral-view"><a href="#lateral-view" class="headerlink" title="lateral view"></a>lateral view</h6><p>斜写视图，为了解决爆炸函数的局限性，lateral view是Hive中提供给UDTF的结合，它可以解决UDTF不能添加额外的select列的问题。</p><p>使用测写视图的方式，在求出爆炸函数结果的同时，求出除了爆炸函数字段之外的字段。需要添加爆炸之后的表名，和爆炸之后的字段名，这个字段名可以放置于select之后，查询爆炸之后的字段值。</p><p>lateral view其实就是用来和想类似explode这种UDTF函数联用的，lateral view会将UDTF生成的结果放到一个虚拟表中，然后这个虚拟表会和输入行进行join来达到连接UDTF外的select字段的目的。</p><pre><code>lateral view udtf(expression) tableAlias as columnAlias (,columnAlias)*</code></pre><p>lateral view在UDTF前使用，表示连接UDTF所分裂的字段。</p><p>UDTF(expression)：使用的UDTF函数，例如explode()。</p><p>tableAlias：表示UDTF函数转换的虚拟表的名称。</p><p>columnAlias：表示虚拟表的虚拟字段名称，如果分裂之后有一个列，则写一个即可；如果分裂之后有多个列，按照列的顺序在括号中声明所有虚拟列名，以逗号隔开。</p><pre><code>select name,category_infofrom movie_infolateral view explode(category) tmp_tbl as category_info;//解决了爆炸函数的局限性，在需要爆炸之后的字段，把字段放置到爆炸之后，然后在前面调用，这里category_info就是select * from sdk_logtable lateral view flat_analizer(ops) temp_sdk as event_name,event_json;//flat_analizer是自定义UDTF//temp_sdk相当于一个临时表//event_name,event_json udtf后临时表中的字段名称</code></pre><h4 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h4><p> <a href="https://www.cnblogs.com/skyEva/p/5730531.html">https://www.cnblogs.com/skyEva/p/5730531.html</a> </p><h5 id="Ntile"><a href="#Ntile" class="headerlink" title="Ntile"></a>Ntile</h5><p>Ntile是hive很强大的一个分析函数</p><p>它把<strong>有序的数据集合平均分配到指定数量个桶</strong>中，将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。 </p><pre><code class="sql">-- 把用户和消费表，按消费下降顺序平均分成2份drop table if exists test_by_payment_ntile;create table test_by_payment_ntile asselect       nick,       payment ,      NTILE(2) OVER(ORDER BY payment desc) AS rn from test_nick_payment;-- 分别对每一份计算平均值，就可以得到消费靠前50%和后50%的平均消费select    &#39;avg_payment&#39; as inf,   t1.avg_payment_up_50 as avg_payment_up_50,   t2.avg_payment_down_50 as avg_payment_down_50from (select         avg(payment) as avg_payment_up_50   from test_by_payment_ntile   where rn=1)t1   join(select           avg(payment) as avg_payment_down_50  from test_by_payment_ntile  where rn=2)t2on (t1.dp_id=t2.dp_id);</code></pre><h5 id="Rank-over"><a href="#Rank-over" class="headerlink" title="Rank() over()"></a>Rank() over()</h5><p>相同的值会输出相同的序号，不间断(1、2、3、3、5)</p><h5 id="Dense-Rank-over"><a href="#Dense-Rank-over" class="headerlink" title="Dense_Rank() over()"></a>Dense_Rank() over()</h5><p>相同的值会输出相同的序号，间断(1、2、3、3、4)</p><h5 id="Row-Number-over"><a href="#Row-Number-over" class="headerlink" title="Row_Number() over()"></a>Row_Number() over()</h5><p>在使用 row_number() over()函数时候，over()里头的分组以及排序的执行晚于 where group by  order by 的执行。</p><p>相同的值会输出不同的序号，唯一不可重复（1、2、3、4、5）</p><pre><code>row_number  给行加上编号row_number() over (partition by 字段a order by 字段b desc) rank//partition by分区//order by 排序 （！可以和分区的字段不同！）//rank 是排序的别名</code></pre><p><strong>例子：</strong></p><pre><code>select    class1,   score,   rank() over(partition by class1 order by score desc) rk1,   dense_rank() over(partition by class1 order by score desc) rk2,   row_number() over(partition by class1 order by score desc) rk3from zyy_test1;</code></pre><pre><code>//可以用于先分组之后对分组后的内容进行排序select t1.name,t2.salary-t1.salary from(select row_number() over(partition by name order by salary desc)rn,* from carry1) t1,(select row_number() over(partition by name order by salary desc)rn,* from carry1) t2where t1.rn=t2.rn+1 and t1.name=t2.name;</code></pre><p><strong>运行结果：</strong></p><img src="https://images2015.cnblogs.com/blog/548357/201608/548357-20160812172352046-1842997041.png" alt="img" style="zoom:100%;"> <h5 id="Lag-over"><a href="#Lag-over" class="headerlink" title="Lag() over()"></a>Lag() over()</h5><p>LAG(col,n,DEFAULT) 用于统计窗口内往上第n行值</p><h5 id="Lead-over"><a href="#Lead-over" class="headerlink" title="Lead() over()"></a>Lead() over()</h5><p>LEAD(col,n,DEFAULT) 用于统计窗口内往下第n行值, 与LAG相反</p><h5 id="First-value-over"><a href="#First-value-over" class="headerlink" title="First_value() over()"></a>First_value() over()</h5><p>first_value:  取分组内排序后，截止到当前行，第一个值</p><h5 id="Last-value-over"><a href="#Last-value-over" class="headerlink" title="Last_value() over()"></a>Last_value() over()</h5><p>last_value:  取分组内排序后，截止到当前行，最后一个值</p><h5 id="count-over"><a href="#count-over" class="headerlink" title="count() over()"></a>count() over()</h5><h2 id="Cube"><a href="#Cube" class="headerlink" title="Cube"></a>Cube</h2><p>指标（事实）分别按照不同维度组合进行聚合</p><p>立方体其本身只有三维，多维模型不仅限于三维模型，可以组合更多的维度 </p><p>为什么叫数据立方体? 一方面是出于更方便地解释和描述，同时也是给思维成像和想象的空间； </p><p>另一方面是为了与传统关系型数据库的二维表区别开来</p><h3 id="Cube的核心操作"><a href="#Cube的核心操作" class="headerlink" title="Cube的核心操作"></a>Cube的核心操作</h3><ol><li>SLICE (切片)</li></ol><p>将某一个（或多个）维度上的值锁定，只观察当这个维度取这个值时的情形，相当于将一个立方体做 了一个切片</p><ol start="2"><li>DICE (切块）</li></ol><p>将某一个（或多个）维度上的值固定在一个区间内，观察这个cube的情形，相当于将一个立方体做 了一个切块</p><ol start="3"><li>ROLL UP (上卷)</li></ol><p>沿着某一个（或多个）维度进行聚合，观察聚合后其他维度上的汇总数据，相当于将一个立方体沿着 某个维度压缩（聚合）在一起。</p><ol start="4"><li>DRILL DOWN (下钻）</li></ol><p>沿着某一个（或多个）维度在更细粒度层面上进行展开，观察展开后其他维度上的对应数据，相当于 将一个立方体沿着某个维度拉伸，拉伸的结果就是粒度变细，比如时间维度从季度拉伸到月</p><ol start="5"><li>PIVOT (旋转)</li></ol><p>将维度的位置互换。在二维表格中就是行变列，列变行。</p><h3 id="Hive的cube函数"><a href="#Hive的cube函数" class="headerlink" title="Hive的cube函数"></a>Hive的cube函数</h3><p><a href="https://blog.csdn.net/weixin_40873462/article/details/102902687">https://blog.csdn.net/weixin_40873462/article/details/102902687</a></p><h4 id="1-with-cube"><a href="#1-with-cube" class="headerlink" title="1. with cube"></a>1. with cube</h4><p>会生成所有的组合可能</p><pre><code>a b c 结果：aa ba cbb ca b cc </code></pre><h4 id="2-with-rollup"><a href="#2-with-rollup" class="headerlink" title="2. with rollup()"></a>2. with rollup()</h4><p>会按照顺序生成值，</p><pre><code>a b c 结果：aa ba b c</code></pre><h4 id="3-grouping-sets"><a href="#3-grouping-sets" class="headerlink" title="3. grouping sets()"></a>3. grouping sets()</h4><p>可以自定义组合的排序。</p><pre><code>SELECT f1,         f2,         f3,         sum(cnt),         GROUPING__ID,         rpad(reverse(bin(cast(GROUPING__ID AS bigint))),3,&#39;0&#39;)  FROM test  GROUP BY f1,           f2,           f3  GROUPING SETS((f1),(f1,f2))  a b c 结果：aa bba b</code></pre><h3 id="SerDe（序列化插件）"><a href="#SerDe（序列化插件）" class="headerlink" title="SerDe（序列化插件）"></a>SerDe（序列化插件）</h3><p>下载和参考文章：<a href="https://blog.csdn.net/weixin_43215250/article/details/93783266">https://blog.csdn.net/weixin_43215250/article/details/93783266</a></p><p>SerDe是Serialize&#x2F;Deserilize的简称，目的是用于序列化和反序列化。</p><p>hive创建表时， 通过自定义的SerDe或使用Hive内置的SerDe类型指定数据的序列化和反序列化方式。</p><p>github上有别人写好的代码或打包好的jar可以下载直接使用</p><p>把jar包放入到lib目录下，并重启hiveserver2，命令行添加jar包。</p><pre><code class="sql">hive &gt; add jar MySerDe.jarCREATE EXTERNAL TABLE IF NOT EXISTS teacher (       id BIGINT,       name STRING,      age INT)ROW FORMAT SERDE &#39;com.coder4.hive.MySerDe&#39;STORED AS TEXTFILELOCATION &#39;/usr/hive/text/&#39;</code></pre><p>SERDE 指定你的类名</p><p>这里使用的是json-serde.jar ,可以直接查询json数据</p><pre><code class="sql">create external table test_json_data (  id string,  list array&lt;struct&lt;col:string&gt;&gt; ) common &quot;测试Json表&quot;row format serde &#39;org.openx.data.jsonserde.JsonSerDe&#39;stored as textfilelocation &#39;/user/root/json_data&#39;;</code></pre>]]></content>
      
      
      <categories>
          
          <category> Hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zookeeper</title>
      <link href="/cdh/zookeeper/zookeeper/"/>
      <url>/cdh/zookeeper/zookeeper/</url>
      
        <content type="html"><![CDATA[<h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><p>注意：Zookeeper节点个数只能是基数！</p><h3 id="ZOOKEEPER（-分布式协调服务）"><a href="#ZOOKEEPER（-分布式协调服务）" class="headerlink" title="ZOOKEEPER（ 分布式协调服务）"></a>ZOOKEEPER（ 分布式协调服务）</h3><p>Zookeeper 分布式服务框架是 Apache Hadoop 的一个子项目，主要是用来解决分布式应用中经常遇到的一些数据管理问题</p><p>如：集群管理、统一命名服务、分布式配置管理、分布式消息队列jmq active amq、分布式锁、分布式通知协调等。</p><p>它是树形结构，每个节点称作一个ZNode，每个ZNode通过其路径唯一标识在每个ZNode可存储少量数据（默认是1M，可以通过配置修改，不建议在ZNode上存储大量的数据），在ZNode上存储了Acl信息，每个ZNode的Acl是独立的，字节点不会继承父节点</p><p>ZNode根据本身的特性，分为两类</p><p>常规性的ZNode，用户需要显示的创建、删除</p><p>临时性的ZNode，用户创建之后，可以删除，也可以在创建它的session结束后，有zookeeper server自动删除 </p><p>两组心跳</p><p>client 和 server 保持着心跳 </p><p>leader 和 follower 保持着心跳 </p><pre><code>czxid  是创建事务id，创建操作 会被记录ctime创建时间mzxid修改的事务的id mtime 修改的时间pZxid当前znode所在文件夹的事务id（子节点个数）ephemeralOwner临时节点的拥有者zookeeper也有类似的fsimage和edits分别是Edits （wal）  fsImage（snapshot）</code></pre><h3 id="zookeeper读写模式"><a href="#zookeeper读写模式" class="headerlink" title="zookeeper读写模式"></a>zookeeper读写模式</h3><p>在zookeeper集群中，读可以从任意一个zookeeper server读，保证了较好的读性能的关键，向forwarder到leader，然后leader来通过通过zookeeper中的原子广播协议，将请求广播给所有的foollower，leader收到一半以上的写成功的ack后，就认为写成功了</p><h3 id="zookeeper分布式搭建"><a href="#zookeeper分布式搭建" class="headerlink" title="zookeeper分布式搭建"></a>zookeeper分布式搭建</h3><p>1.解压zookeepertar包<br>2.配置文件 &#x2F;etc&#x2F;profile<br>3.conf 文件夹中修改zoo.cfg文件</p><pre><code>tickTime=2000initLimit=10#用来配置Zookeeper接受客户端初始化连接时最长能忍受多少个心跳时间间隔数，超过10个心跳时间长度后没有收到客户端的返回信息，表明这个客户端连接失败syncLimit=5#（同步的几倍心跳时间）配置leader和follower之间发送消息，请求和应答时间长度，最长不能超过多少个tickTime的时间长度,这里是5倍的心跳时间clientPort=2181#端口号server.1=192.168.237.1:2888:3888 server.2=192.168.237.2:2888:3888server.3=192.168.237.3:2888:38882888  zk节点通信端口号3888  选取机制的端口号fifo  先进先出 #server.A=B:C:D表达了a是一个数据这是几号服务器b是服务器的ip地址c是服务器和集群中leader服务器交换信息的端口d是执行选举时的服务器相互通信的端口</code></pre><p><img src="/cdh/zookeeper/zookeeper/1571141126560.png" alt="1571141126560"></p><p>4.创建文件夹logs和data(文件夹必须存在)</p><p>5.myid 文件</p><p>data文件中创建myid文件并 三个节点写不同的值 1，2，3</p><p>传到三台机器上</p><p>6.启动zookeeper</p><pre><code class="xml">//启动zookeeper进程 QuorumPeerMain进程zkServer.sh start //查看zookeeper的启动状态 zkServer.sh status</code></pre><p>6.进入到zookeeper客户端 </p><p>zkCli.sh</p><h3 id="zookeeper命令"><a href="#zookeeper命令" class="headerlink" title="zookeeper命令"></a>zookeeper命令</h3><pre><code>//查看目录下的文件ls ///创建字节点 create 目录名 挂载数据//查看目录下的文件get /目录名 connect ip:2181  //切换server服务端cZxid = 0x200000002ctime = Mon Oct 21 01:46:04 PDT 2019mZxid = 0x200000002mtime = Mon Oct 21 01:46:04 PDT 2019pZxid = 0x200000002cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 2numChildren = 0czxid  是创建事务id，创建操作 会被记录ctime创建时间mzxid修改的事务的id,自动增加，以编号大的为准，全局有序mtime 修改的时间pZxid当前znode所在文件夹的事务id（子节点个数）ephemeralOwner临时节点的拥有者//修改目录下的数据 set 目录名 数据 delete  path  //删除节点rmr  path   强制删除节点-e  临时节点，会话失效后自动删除zookeeper的状态有四种not_connected //没有链接connecting    //连接中connected   //连接上closed      //连接关闭</code></pre><h3 id="ZNode的基本概念"><a href="#ZNode的基本概念" class="headerlink" title="ZNode的基本概念"></a>ZNode的基本概念</h3><p>Zookeeper 分布式服务框架是 Apache Hadoop 的一个子项目，主要是用来解决分布式应用中经常遇到的一些数据管理问题</p><p>1 分布式的：大量的节点 。实现集群来管理集群</p><p>2 数据管理： 管理数据的 管理集群的变化 —数据</p><p>ZooKeeper数据模型的结构与Linux文件系统很类似(根目录下的分配)，整体上可以看作是一棵树，每个节点称做一个ZNode。每个ZNode都可以通过其路径唯一标识，在每个ZNode上可存储少量数据(默认是1M, 可以通过配置修改, 通常不建议在ZNode上存储大量的数据)。zookeeper挂载的数据存放在内存中 备份 （磁盘），放重要文件，读取和写入速度快。</p><p>ZNode根据其本身的特性，可以分为下面两类：</p><p>常规型ZNode, 用户需要显式的创建、删除</p><p>临时型ZNode, 用户创建它之后，可以正常的删除，也可以在创建它的Session结束后，由ZooKeeper Server自动删除</p><pre><code>znode -s -e</code></pre><h3 id="Zookeeper这种数据结构有如下这些特点："><a href="#Zookeeper这种数据结构有如下这些特点：" class="headerlink" title="Zookeeper这种数据结构有如下这些特点："></a>Zookeeper这种数据结构有如下这些特点：</h3><p>1）类似于Linux的节点文件夹都被称作为znode，这个znode是被它所在的路径唯一标识，如Server1这个znode的标识为&#x2F;NameService&#x2F;Server1。</p><p>2）znode可以有子节点目录，并且每个znode可以存储数据，注意临时类型的目录节点不能有子节点目录。ZNode有一个特性，如果创建的时候指定-s的话，该ZNode的名字后面会自动Append一个不断增加的SequenceNo</p><p>3）znode可以是临时节点，可以是持久节点。如果创建的是临时节点，一旦创建这个EPHEMERALznode的客户端与服务器失去联系，这个znode也将自动删除，Zookeeper的客户端和服务器进行连接，每个客户端和服务器通过心跳来保持连接，这个连接状态称为session，如果znode是临时节点，这个session失效，znode也就删除了。</p><p>4）znode的目录名可以自动编号，如App1已经存在，再创建的话，将会自动命名为App2。-s的情况下</p><p>5）znode可以被监控，包括这个目录节点中存储的数据的修改，子节点目录的变化等，一旦变化可以通知设置监控的客户端，这个是Zookeeper的核心特性，Zookeeper的很多功能都是基于这个特性实现的。Watcher ZooKeeper支持一种Watch操作，Client可以在某个ZNode上设置一个Watcher，来Watch该ZNode上的变化。如果该ZNode上有相应的变化，就会触发这个Watcher，把相应的事件通知给设置Watcher的Client。需要注意的是，ZooKeeper中的Watcher是一次性的，即触发一次就会被取消，如果想继续Watch的话，需要客户端重新设置Watcher。</p><p>6）ZXID：每次对Zookeeper的状态的改变都会产生一个zxid，zxid是全局有序的，如果zxid1小于zxid2，则zxid1在zxid2之前发生。</p><p>7)Session: Client与ZooKeeper之间的通信，需要创建一个Session，这个Session会有一个超时时间，所以在Session没超时之前，Client与ZooKeeper Server的连接可以在各个ZooKeeper Server之间透明地移动  connect + ip:端口号）ZooKeeper Client会每t&#x2F;3 ms发一次心跳给Server，如果Client 2t&#x2F;3 ms没收到来自Server的心跳回应，就会换到一个新的ZooKeeper Server上。这里t是用户配置的Session的超时时间。</p><p><img src="/cdh/zookeeper/zookeeper/clip_image002.jpg" alt="img"> <img src="/cdh/zookeeper/zookeeper/clip_image004.jpg" alt="img"></p><p>(client不论连接到哪个Server，展示给它都是同一个视图，这是zookeeper最重要的性能。)</p><h3 id="读写模式"><a href="#读写模式" class="headerlink" title="读写模式"></a>读写模式</h3><p>在ZooKeeper集群中，读可以从任意一个ZooKeeper Server读，这一点是保证ZooKeeper比较好的读性能的关键；写的请求会先Forwarder到Leader，然后由Leader来通过ZooKeeper中的原子广播协议，将请求广播给所有的Follower，Leader收到一半以上的写成功的Ack（acknowlage）后，就认为该写成功了，就会将该写进行持久化，并告诉客户端写成功了。</p><h3 id="Zookeeper具体写入过程："><a href="#Zookeeper具体写入过程：" class="headerlink" title="Zookeeper具体写入过程："></a>Zookeeper具体写入过程：</h3><p>election投票功能</p><p>正常情况下：集群中，形成读写分离的状态，读可以从任何一台机器进行读取，</p><p>Zk里面在写入数据的时候有独特的算法：</p><p>F1（zxid 0） f2  f3</p><p>Client – f1 f2 f3</p><p>client提交请求，set &#x2F;zhangsan 3  zxid</p><p>F1 (leader)提议修改数据 - f2  - f3</p><p>F1 f2 f3 如果要进行文件修改，那么就会对应一个zxid这个zxid1是最新的，</p><p>F1提议修改数据，那么f1提议的数据一定要是大于0，f1 f2 f3只能接受最新的提议  zxid—-&gt;最新的</p><p>F2 f3 f1 三个人都要判决，如果投票大于半数以上，那么这个提议就生效，三台机器将数据修改</p><p>集群中机器比较多：冲突</p><p>F1 zhangsan 3 zxid 1 f2 zhangan 4 zxid 1</p><p>争夺，f1 f2同时到达，那么就应该进行投票，如果其中一个提议投票大于半数，那么将要执行提议，另一个提议因为他的zxid和刚才执行完毕的zxid一样，这个提议属于过期提议 （以用的那个提议的zxid当做是最新的zxid），紧接着可以发送第二次的提议</p><p>因为当前的提议人数较多：</p><p>出现leader的角色，因为leader的出现，使得于这个提议的功能只能有leader执行，client —leader那么你的这次请求会直接被提议</p><p>Client—follower—leader，因为leader的存在，使得所有的提议都交给leader，那么leader就会形成一个队列，所以所有的修改都是全局有序的</p><h3 id="WAL和Snapshot"><a href="#WAL和Snapshot" class="headerlink" title="WAL和Snapshot"></a>WAL和Snapshot</h3><p>Edits （wal）</p><p>fsImage（snapshot）</p><p>Edits+fsimage</p><p>快照  &#x3D; secondaryNamenode的日志合并</p><p>Edits日志文件保存的是全部的操作信息</p><p>和大多数分布式系统一样，ZooKeeper也有WAL(Write-Ahead-Log)edits，对于每一个更新操作，ZooKeeper都会先写WAL, 然后再对内存中的数据做更新，然后向Client通知更新结果。另外，ZooKeeper还会定期将内存中的目录树进行Snapshot fsimage，落地到磁盘上，这个跟HDFS中的FSImage是比较类似的。这么做的主要目的，一当然是数据的持久化，二是加快重启之后的恢复速度，如果全部通过Replay WAL的形式恢复的话，会比较慢。</p><h3 id="FIFO"><a href="#FIFO" class="headerlink" title="FIFO"></a>FIFO</h3><p>对于每一个ZooKeeper客户端而言，所有的操作都是遵循FIFO顺序的，这一特性是由下面两个基本特性来保证的：一是ZooKeeper Client与Server之间的网络通信是基于TCP，TCP保证了Client&#x2F;Server之间传输命令的顺序；二是ZooKeeper Server执行客户端请求也是严格按照FIFO顺序的。</p><h3 id="ZooKeeper-Session"><a href="#ZooKeeper-Session" class="headerlink" title="ZooKeeper Session"></a><strong>ZooKeeper Session</strong></h3><p>Client和Zookeeper集群建立连接，整个session状态变化如图所示：</p><p><img src="/cdh/zookeeper/zookeeper/clip_image005.jpg" alt="img"></p><p>如果Client因为Timeout和Zookeeper Server失去连接，client处在CONNECTING状态，会自动尝试再去连接Server，如果在session有效期内再次成功连接到某个Server，则回到CONNECTED状态。</p><p>注意：如果因为网络状态不好，client和Server失去联系，client会停留在当前状态，会尝试主动再次连接Zookeeper Server。client不能宣称自己的session expired(会话超时)，session expired是由Zookeeper Server来决定的，client可以选择自己主动关闭session。 </p><h3 id="ZooKeeper-Watch"><a href="#ZooKeeper-Watch" class="headerlink" title="ZooKeeper Watch"></a><strong>ZooKeeper Watch</strong></h3><p>Zookeeper watch是一种监听通知机制。Zookeeper所有的读操作getData(), getChildren()和 exists()都可以设置监视(watch)，监视事件可以理解为一次性的触发器 One-time trigger</p><p>当设置监视的数据发生改变时，该监视事件会被发送到客户端，例如，如果客户端调用了getData(“&#x2F;znode1”, true) 并且稍后 &#x2F;znode1 节点上的数据发生了改变或者被删除了，客户端将会获取到 &#x2F;znode1 发生变化的监视事件，而如果 &#x2F;znode1 再一次发生了变化，除非客户端再次对&#x2F;znode1 设置监视，否则客户端不会收到事件通知。</p><h3 id="ZooKeeper的工作原理"><a href="#ZooKeeper的工作原理" class="headerlink" title="ZooKeeper的工作原理"></a><strong>ZooKeeper的工作原理</strong></h3><p>在zookeeper的集群中，各个节点共有下面3种角色和4种状态：</p><ul><li>角色：leader,follower,observer</li><li>Follower 同步数据 和leader心跳，投票，投票选举leader，自身竞选leader 可以作为备份</li><li>Observer：同步数据 和leader心跳，投票     但是没有自身竞选功能</li><li>100 1leader 5follower 94observer</li><li>状态：leading,following,observing,looking</li></ul><p>Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（Recovery选主）和广播模式（Broadcast同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式（safemode），当领导者被选举出来，且大多数Server完成了和leader的状态同步以后(内存信息加载)，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。</p><p>LOOKING：当前Server不知道leader是谁，正在搜寻。恢复模式</p><p>LEADING：当前Server即为选举出来的leader。选举完毕有leader</p><p>FOLLOWING：leader已经选举出来，当前Server与之同步。</p><p>OBSERVING：observer的行为在大多数情况下与follower完全一致，但是他们不参加本身选举 而仅仅接受选举和投票的结果。</p><h3 id="Leader-Election"><a href="#Leader-Election" class="headerlink" title="Leader Election"></a><strong>Leader Election</strong></h3><p>当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的Server都恢复到一个正确的状态。Zk的选举算法</p><p>1.选举线程由当前Server发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的Server；</p><p>2.选举线程首先向所有Server发起一次询问（包括自己）；</p><p>3.选举线程收到回复后，验证是否是自己发起的询问（验证zxid是否一致），然后获取对方的id（myid），并存储到当前询问对象列表中，最后获取对方提议的leader相关信息（id,zxid），并将这些信息存储到当次选举的投票记录表中；</p><p>4.收到所有Server回复以后，就计算出zxid最大的那个Server，并将这个Server相关信息设置成下一次要投票的Server；</p><p>5.线程将当前zxid最大的Server设置为当前Server要推荐的Leader，如果此时获胜的Server获得n&#x2F;2 + 1的Server票数，设置当前推荐的leader为获胜的Server，将根据获胜的Server相关信息设置自己的状态，否则，继续这个过程，直到leader被选举出来。</p><p>通过流程分析我们可以得出：要使Leader获得多数Server的支持，则Server总数必须是奇数2n+1，且存活的Server的数目不得少于n+1. </p><p>奇数：1.因为奇数台机器，永远都不会两个leader拥有同样的follower</p><h3 id="Leader工作流程"><a href="#Leader工作流程" class="headerlink" title="Leader工作流程"></a><strong>Leader工作流程</strong></h3><p>Leader主要有三个功能：</p><p>1.恢复数据；</p><p>2.维持与Learner(follower与observer的统称 )的心跳，接收Learner请求并判断Learner的请求消息类型；都需要由leader进行提议 （提议加判断）</p><p>3.Learner的消息类型主要有PING消息、REQUEST消息（提议的消息）、acknowlage（确认消息）消息、REVALIDATE （延长session的连接时间）消息，根据不同的消息类型，进行不同的处理。</p><p>PING消息是指Learner的心跳信息；REQUEST消息是Follower发送的提议信息，包括写请求及同步请求；</p><p>ACK消息是Follower的对提议的回复确认消息，超过半数的Follower通过，则commit该提议；REVALIDATE消息是用来延长SESSION有效时间。</p><h3 id="Follower工作流程"><a href="#Follower工作流程" class="headerlink" title="Follower工作流程"></a><strong>Follower工作流程</strong></h3><p>Follower主要有四个功能：</p><ol><li>向Leader发送请求（PING消息、REQUEST消息、ACK消息、REVALIDATE消息）；</li><li>接收Leader消息并进行处理；</li><li>接收Client的请求，如果为写请求，发送给Leader进行投票；</li><li>返回Client结果（返回给client的写入结果）</li></ol>]]></content>
      
      
      <categories>
          
          <category> Zookeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringMvc学习</title>
      <link href="/java/spring/springmvc-jian-dan-shi-yong-kuang-jia-san/"/>
      <url>/java/spring/springmvc-jian-dan-shi-yong-kuang-jia-san/</url>
      
        <content type="html"><![CDATA[<h2 id="SpringMvc是什么？"><a href="#SpringMvc是什么？" class="headerlink" title="SpringMvc是什么？"></a>SpringMvc是什么？</h2><p>SpringMvc是一个表现层框架</p><p>后面框架整合的时候是先通过springmvc，spring</p><p>底层是Servlet，处理请求的机制是一个核心控制器</p><p>springmvc是一个基于servlet的mvc框架</p><h2 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h2><ol><li>request请求发送过来</li><li>请求查找handler</li><li>由处理器映射器handlermappering 进行处理这个请求，<br>能够知道发送过来的请求让controller中哪个方法来帮你执行<br>通过请求路径来匹配<br>采用适配器的模式，</li><li>请求适配器执行 handleradapter 去执行hadnler(handler也叫做Controller)</li><li>去执行，返回返回结果</li><li>controller(handler)执行并返回 ModleAndView 对象给 DispatcherServlet</li><li>DispatcherServlet 让viewResolver 来进行视图解析</li><li>viewResolver 解析完毕后返回一个View对象</li><li>视图渲染，将结果交给request域，然后相应客户端请求</li></ol><h2 id="Springmvc组件"><a href="#Springmvc组件" class="headerlink" title="Springmvc组件"></a>Springmvc组件</h2><p>在SpringMvc各个组件：<br><strong>处理器映射器</strong>，<br><strong>处理器适配器</strong>，<br><strong>视图解析器配</strong><br>成为Springmvc的三大组件</p><pre><code class="xml">&lt;mvc:annotation-driven&gt;注解在xml中配置可以 开启springmvc的框架注解支持</code></pre><h2 id="SpringMvc环境配置"><a href="#SpringMvc环境配置" class="headerlink" title="SpringMvc环境配置"></a>SpringMvc环境配置</h2><p><strong>springmvc.xml环境配置</strong>：</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot;       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xsi:schemaLocation=&quot;        http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://www.springframework.org/schema/mvc        http://www.springframework.org/schema/mvc/spring-mvc.xsd        http://www.springframework.org/schema/context        http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt;    &lt;!-- 开启注解扫描 --&gt;    &lt;context:component-scan base-package=&quot;cn.itcast&quot;/&gt;    &lt;!-- 视图解析器对象 --&gt;    &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt;        &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt;        &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt;    &lt;/bean&gt;    &lt;!--配置自定义类型转换器--&gt;    &lt;bean id=&quot;conversionService&quot; class=&quot;org.springframework.context.support.ConversionServiceFactoryBean&quot;&gt;        &lt;property name=&quot;converters&quot;&gt;            &lt;set&gt;                &lt;bean class=&quot;cn.itcast.utils.StringToDateConverter&quot;/&gt;            &lt;/set&gt;        &lt;/property&gt;    &lt;/bean&gt;    &lt;!-- 开启SpringMVC框架注解的支持 --&gt;    &lt;mvc:annotation-driven conversion-service=&quot;conversionService&quot;/&gt;&lt;/beans&gt;</code></pre><p><strong>web.xml配置</strong>：</p><pre><code class="xml">&lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt;&lt;web-app&gt;  &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt;  &lt;!--配置Spring的监听器，默认只加载WEB-INF目录下的applicationContext.xml配置文件--&gt;  &lt;listener&gt;    &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;  &lt;/listener&gt;      &lt;!--设置配置文件的路径--&gt;  &lt;context-param&gt;    &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;    &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt;  &lt;/context-param&gt;      &lt;context-param&gt;    &lt;param-name/&gt;    &lt;param-value/&gt;  &lt;/context-param&gt;  &lt;!--配置前端控制器--&gt;  &lt;servlet&gt;    &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;    &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;    &lt;!--加载springmvc.xml配置文件--&gt;    &lt;init-param&gt;      &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;      &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt;    &lt;/init-param&gt;    &lt;!--启动服务器，创建该servlet--&gt;    &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;  &lt;/servlet&gt;        &lt;!--引用前端控制器，并配置他的作用范围--&gt;  &lt;servlet-mapping&gt;    &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;    &lt;url-pattern&gt;/&lt;/url-pattern&gt;  &lt;/servlet-mapping&gt;  &lt;!--解决中文乱码的过滤器--&gt;  &lt;filter&gt;    &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;    &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;    &lt;init-param&gt;      &lt;param-name&gt;encoding&lt;/param-name&gt;      &lt;param-value&gt;UTF-8&lt;/param-value&gt;    &lt;/init-param&gt;  &lt;/filter&gt;  &lt;filter-mapping&gt;    &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;    &lt;url-pattern&gt;/*&lt;/url-pattern&gt;  &lt;/filter-mapping&gt;  &lt;/web-app&gt;</code></pre><h3 id="请求参数的绑定"><a href="#请求参数的绑定" class="headerlink" title="请求参数的绑定"></a>请求参数的绑定</h3><p>springmvc可以利用反射的机制进行参数的绑定 </p><p>将你请求中的参数，自定映射到方法@RequestMapper的方法字段中</p><pre><code class="java">    /**     * 请求参数绑定入门     * @return     */    @RequestMapping(&quot;/testParam&quot;)    public String testParam(String username,String password)&#123;        System.out.println(&quot;执行了...&quot;);        System.out.println(&quot;用户名：&quot;+username);        System.out.println(&quot;密码：&quot;+password);        return &quot;success&quot;;    &#125;    &lt;a href=&quot;anno/testRequestParam?name=哈哈&quot;&gt;RequestParam&lt;/a&gt;</code></pre><h3 id="请求对象的绑定"><a href="#请求对象的绑定" class="headerlink" title="请求对象的绑定"></a>请求对象的绑定</h3><p>注意！ 名称必须一致，它是通过调用set方法来进行赋值的。</p><pre><code class="java">public class Account implements Serializable&#123;    private String username;    private String password;    private Double money;        get set.....&#125;/**     * 请求参数绑定把数据封装到JavaBean的类中     * @return     */    @RequestMapping(&quot;/saveAccount&quot;)    public String saveAccount(Account account)&#123;        System.out.println(&quot;执行了...&quot;);        System.out.println(account);        return &quot;success&quot;;    &#125;</code></pre><pre><code class="html">    &lt;!--把数据封装Account类中--&gt;    &lt;form action=&quot;param/saveAccount&quot; method=&quot;post&quot;&gt;        姓名：&lt;input type=&quot;text&quot; name=&quot;username&quot; /&gt;&lt;br/&gt;        密码：&lt;input type=&quot;text&quot; name=&quot;password&quot; /&gt;&lt;br/&gt;        金额：&lt;input type=&quot;text&quot; name=&quot;money&quot; /&gt;&lt;br/&gt;        &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt;    &lt;/form&gt;</code></pre><h3 id="请求对象中引用对象的绑定"><a href="#请求对象中引用对象的绑定" class="headerlink" title="请求对象中引用对象的绑定"></a>请求对象中引用对象的绑定</h3><pre><code class="java">//java beanpublic class Account implements Serializable&#123;    private String username;    private String password;    private Double money;       private User user;&#125;public class User implements Serializable&#123;    private String uname;    private Integer age;&#125;</code></pre><pre><code class="java">    /**     * 请求参数绑定把数据封装到JavaBean的类中     * @return     */    @RequestMapping(&quot;/saveAccount&quot;)    public String saveAccount(Account account)&#123;        System.out.println(&quot;执行了...&quot;);        System.out.println(account);        return &quot;success&quot;;    &#125;</code></pre><pre><code class="html">  jsp页面  &lt;!--把数据封装Account类中--&gt;    &lt;form action=&quot;param/saveAccount&quot; method=&quot;post&quot;&gt;        姓名：&lt;input type=&quot;text&quot; name=&quot;username&quot; /&gt;&lt;br/&gt;        密码：&lt;input type=&quot;text&quot; name=&quot;password&quot; /&gt;&lt;br/&gt;        金额：&lt;input type=&quot;text&quot; name=&quot;money&quot; /&gt;&lt;br/&gt;        用户姓名：&lt;input type=&quot;text&quot; name=&quot;user.uname&quot; /&gt;&lt;br/&gt;        用户年龄：&lt;input type=&quot;text&quot; name=&quot;user.age&quot; /&gt;&lt;br/&gt;        &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt;    &lt;/form&gt;</code></pre><h3 id="集合类型的绑定"><a href="#集合类型的绑定" class="headerlink" title="集合类型的绑定"></a>集合类型的绑定</h3><pre><code class="java">//java beanpublic class Account implements Serializable&#123;    private String username;    private String password;    private Double money;   // private User user;    private List&lt;User&gt; list;    private Map&lt;String,User&gt; map;&#125;public class User implements Serializable&#123;    private String uname;    private Integer age;&#125;</code></pre><pre><code class="java">    /**     * 请求参数绑定把数据封装到JavaBean的类中     * @return     */    @RequestMapping(&quot;/saveAccount&quot;)    public String saveAccount(Account account)&#123;        System.out.println(&quot;执行了...&quot;);        System.out.println(account);        return &quot;success&quot;;    &#125;</code></pre><pre><code class="html">&lt;!--jsp 页面--&gt;    &lt;form action=&quot;param/saveAccount&quot; method=&quot;post&quot;&gt;        姓名：&lt;input type=&quot;text&quot; name=&quot;username&quot; /&gt;&lt;br/&gt;        密码：&lt;input type=&quot;text&quot; name=&quot;password&quot; /&gt;&lt;br/&gt;        金额：&lt;input type=&quot;text&quot; name=&quot;money&quot; /&gt;&lt;br/&gt;        &lt;!--指定list的下标位置--&gt;        用户姓名：&lt;input type=&quot;text&quot; name=&quot;list[0].uname&quot; /&gt;&lt;br/&gt;        用户年龄：&lt;input type=&quot;text&quot; name=&quot;list[0].age&quot; /&gt;&lt;br/&gt;        &lt;!--指定map的key是one，v是--&gt;        用户姓名：&lt;input type=&quot;text&quot; name=&quot;map[&#39;one&#39;].uname&quot; /&gt;&lt;br/&gt;        用户年龄：&lt;input type=&quot;text&quot; name=&quot;map[&#39;one&#39;].age&quot; /&gt;&lt;br/&gt;        &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt;    &lt;/form&gt;</code></pre><h2 id="SpringMvc过滤器-解决post请求乱码问题"><a href="#SpringMvc过滤器-解决post请求乱码问题" class="headerlink" title="SpringMvc过滤器,解决post请求乱码问题"></a>SpringMvc过滤器,解决post请求乱码问题</h2><p>前端多滤器，设置他们的字符编码</p><p>post方法会乱码，get方式不会乱码</p><p>在web.xml中配置全局过滤器，配置时过滤器位置放在servlet位置前面</p><pre><code class="xml">&lt;web-app&gt;  &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt;      &lt;!--配置解决中文乱码的过滤器--&gt;  &lt;filter&gt;    &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;    &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;    &lt;init-param&gt;      &lt;param-name&gt;encoding&lt;/param-name&gt;      &lt;param-value&gt;UTF-8&lt;/param-value&gt;    &lt;/init-param&gt;  &lt;/filter&gt;  &lt;filter-mapping&gt;    &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;    &lt;url-pattern&gt;/*&lt;/url-pattern&gt;  &lt;/filter-mapping&gt;      &lt;!--配置前端控制器--&gt;  &lt;servlet&gt;    &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;    &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;    &lt;init-param&gt;      &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;      &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt;    &lt;/init-param&gt;    &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;  &lt;/servlet&gt;  &lt;servlet-mapping&gt;    &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;    &lt;url-pattern&gt;/&lt;/url-pattern&gt;  &lt;/servlet-mapping&gt;&lt;/web-app&gt;</code></pre><h2 id="类型转换器"><a href="#类型转换器" class="headerlink" title="类型转换器"></a>类型转换器</h2><p>Springmvc会对一些简单属性进行转换 如 input输入age是22 自动转换成Integer，<strong>但是复杂类型需要自己来实现转换</strong></p><pre><code class="java">//java beanpublic class User implements Serializable&#123;    private String uname;    private Integer age;    private Date date;&#125;</code></pre><pre><code class="java">    /**     * 使用了自定义类型转换器     * @param user     * @return     */    @RequestMapping(&quot;/saveUser&quot;)    public String saveUser(User user)&#123;        System.out.println(&quot;执行了...&quot;);        System.out.println(user);        return &quot;success&quot;;    &#125;</code></pre><pre><code class="html">&lt;!--jsp 页面--&gt;    &lt;form action=&quot;param/saveUser&quot; method=&quot;post&quot;&gt;        用户姓名：&lt;input type=&quot;text&quot; name=&quot;uname&quot; /&gt;&lt;br/&gt;        用户年龄：&lt;input type=&quot;text&quot; name=&quot;age&quot; /&gt;&lt;br/&gt;        用户生日：&lt;input type=&quot;text&quot; name=&quot;date&quot; /&gt;&lt;br/&gt;        &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt;    &lt;/form&gt;</code></pre><h3 id="自定义类型转换器编写"><a href="#自定义类型转换器编写" class="headerlink" title="自定义类型转换器编写"></a><strong>自定义类型转换器编写</strong></h3><ol><li><p>需要实现converter接口，定义泛型&lt;String,Date&gt;</p><p> 左边类型是原始类型，右边类型是转换类型</p></li></ol><pre><code class="java">/** * 把字符串转换日期 */public class StringToDateConverter implements Converter&lt;String,Date&gt;&#123;    /**     * String source    传入进来字符串     * @param source         * @return     */    public Date convert(String source) &#123;        // 判断        if(source == null)&#123;            throw new RuntimeException(&quot;请您传入数据&quot;);        &#125;        DateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);        try &#123;            // 把字符串转换日期            return df.parse(source);        &#125; catch (Exception e) &#123;            throw new RuntimeException(&quot;数据类型转换出现错误&quot;);        &#125;    &#125;&#125;</code></pre><ol start="2"><li>在springmvc.xml中注册类型转换器</li></ol><pre><code class="xml">    &lt;!--配置自定义类型转换器--&gt;    &lt;bean id=&quot;conversionService&quot; class=&quot;org.springframework.context.support.ConversionServiceFactoryBean&quot;&gt;        &lt;property name=&quot;converters&quot;&gt;            &lt;set&gt;                &lt;bean class=&quot;cn.itcast.utils.StringToDateConverter&quot;/&gt;            &lt;/set&gt;        &lt;/property&gt;    &lt;/bean&gt;    &lt;!-- 开启SpringMVC框架注解的支持 --&gt;    &lt;mvc:annotation-driven conversion-service=&quot;conversionService&quot;/&gt;</code></pre><h2 id="调用Servlet原生API"><a href="#调用Servlet原生API" class="headerlink" title="调用Servlet原生API"></a>调用Servlet原生API</h2><p>在Springmvc中获取到原来servlet的内置对象request，response，session和servletcontext</p><pre><code class="java">/** * 原生的API * @return */@RequestMapping(&quot;/testServlet&quot;)public String testServlet(HttpServletRequest request, HttpServletResponse response)&#123;    System.out.println(&quot;执行了...&quot;);    System.out.println(request);    HttpSession session = request.getSession();    System.out.println(session);    ServletContext servletContext = session.getServletContext();    System.out.println(servletContext);    System.out.println(response);    return &quot;success&quot;;&#125;</code></pre><pre><code class="html">&lt;a href=&quot;param/testServlet&quot;&gt;Servlet原生的API&lt;/a&gt;</code></pre><h2 id="SpringMvc常用注解"><a href="#SpringMvc常用注解" class="headerlink" title="SpringMvc常用注解"></a>SpringMvc常用注解</h2><h3 id="Controller"><a href="#Controller" class="headerlink" title="@Controller"></a>@Controller</h3><p> @Controller 用于标记在一个类上，使用它标记的类就是一个SpringMVC Controller 对象。分发处理器将会扫描使用了该注解的类的方法，并检测该方法是否使用了@RequestMapping 注解。</p><p>@Controller 只是定义了一个控制器类，而使用@RequestMapping 注解的方法才是真正处理请求的处理器。</p><h3 id="RequestMapping"><a href="#RequestMapping" class="headerlink" title="@RequestMapping"></a>@RequestMapping</h3><p>时间里请求URL和处理请求方法之间的关系</p><p><strong>属性：</strong></p><p>value，path这两个作用是一样的，都是指定访问请求路径</p><p>method 指定他的请求方式，指定Post就只能Post请求</p><pre><code class="java">@RequestMapping(value=&quot;/username&quot;,params =&#123;username&#125;,method=RequestMethod.PUT)</code></pre><p>params指定他的参数，调用时必须传入这个名称的参数才能调用</p><pre><code class="java">@RequestMapping(value=&quot;/username&quot;,params =&#123;username&#125;)指定了调用时必须有username这个字段@RequestMapping(value=&quot;/username&quot;,params =&#123;username=heihei&#125;)指定了调用时必须有username这个字段，并且字段的值只能是heihei</code></pre><p><strong>headers</strong>指定限制请求消息头的条件</p><pre><code class="java">@RequestMapper(value=&quot;/username&quot;,params =&#123;username=heihei&#125;,headers=&#123;&quot;Accept&quot;&#125;)</code></pre><h3 id="RequestParam"><a href="#RequestParam" class="headerlink" title="@RequestParam"></a><strong>@RequestParam</strong></h3><p>把请求指定名称的参数给控制器中的形参赋值，用于参数名称和你方法参数名称不匹配的情况。</p><p><strong>属性</strong>：</p><p>value请求参数中的名称</p><p>required请求参数中是否必须提供此参数，默认true，表示必须提供，不提供就报错</p><p>例子：</p><pre><code class="java">@Controller@RequestMapping(&quot;/anno&quot;)public class AnnoController &#123;    @RequestMapping(&quot;/testRequestParam&quot;)    public String testRequestParam(@RequestParam(name=&quot;name&quot;) String username)&#123;        System.out.println(&quot;执行了...&quot;);        System.out.println(username);        return &quot;success&quot;;    &#125;&#125;//在这里默认使用了required，默认是true，前端必须只能传name字段的值，字段name变成uname都会报错</code></pre><pre><code class="jsp">//jsp    &lt;a href=&quot;anno/testRequestParam?name=哈哈&quot;&gt;RequestParam&lt;/a&gt;</code></pre><h3 id="RequestBody"><a href="#RequestBody" class="headerlink" title="@RequestBody"></a>@RequestBody</h3><p>用于获取请求体内容，直接使用得到 key&#x3D;value&amp;key&#x3D;value 结构的数据，可以直接<strong>名称调用</strong>，也可以直接<strong>封装到对象中</strong></p><p>get方法不适用（get请求都封装到了地址栏上，没有请求体）</p><p><strong>属性：</strong></p><p>required是否必须有请求体，默认是true。true的时候get方法会报错，取值为false，get请求得到的是null</p><p>例子：</p><pre><code class="html">    &lt;form action=&quot;anno/testRequestBody&quot; method=&quot;post&quot;&gt;        用户姓名：&lt;input type=&quot;text&quot; name=&quot;username&quot; /&gt;&lt;br/&gt;        用户年龄：&lt;input type=&quot;text&quot; name=&quot;age&quot; /&gt;&lt;br/&gt;        &lt;input type=&quot;submit&quot; value=&quot;提交&quot; /&gt;    &lt;/form&gt;</code></pre><pre><code class="java">    /**     * 获取到请求体的内容     * @return     */    @RequestMapping(&quot;/testRequestBody&quot;)    public String testRequestBody(@RequestBody String body)&#123;        System.out.println(&quot;执行了...&quot;);        System.out.println(body);        return &quot;success&quot;;    &#125;</code></pre><p>图片</p><h3 id="ResponseBody"><a href="#ResponseBody" class="headerlink" title="@ResponseBody"></a>@ResponseBody</h3><p>和RequestBody作用相反，用来响应客户端请求，返回指定类型</p><h3 id="PathVariable"><a href="#PathVariable" class="headerlink" title="@PathVariable"></a><strong>@PathVariable</strong></h3><p>拥有绑定url中的占位符，例如url中 &#x2F;delete&#x2F;{id} 这个{id} 就是url占位符</p><p>url支持占位符是spring3.0之后加入的，是SpringMvc支持rest分隔url的一个重要标志</p><p><strong>Restful编程风格</strong></p><p>请求地址一样，但是能够根据请求方式的不同(get,put,post)来执行不同的方法</p><p><strong>属性：</strong></p><p>value用于指定url占位符的名称</p><p>required是否必须提供占位符</p><p><strong>例子：</strong></p><pre><code class="html">    &lt;a href=&quot;anno/testPathVariable/10&quot;&gt;testPathVariable&lt;/a&gt;</code></pre><pre><code class="java">    /**     * PathVariable注解     * @return     */    @RequestMapping(value=&quot;/testPathVariable/&#123;sid&#125;&quot;)    public String testPathVariable(@PathVariable(name=&quot;sid&quot;) String id)&#123;        System.out.println(&quot;执行了...&quot;);        System.out.println(id);        return &quot;success&quot;;    &#125;</code></pre><h3 id="HiddentHttpMethodFilter"><a href="#HiddentHttpMethodFilter" class="headerlink" title="@HiddentHttpMethodFilter"></a>@HiddentHttpMethodFilter</h3><p>了解即可，是为了模拟不同的发送请求方式</p><p>浏览器的form表单只支持get和post请求，而Delelte，put等method并不支持，Spring3.0添加了一个过滤器，可以将浏览器请求去改为指定的请求方式，发送给我们的控制器方法，使得支持get，post，put与delete请求</p><p>使用方法:</p><ol><li>在web.xml中配置过滤器</li><li>请求法师必须使用post请求</li><li>按照要求提供_method请求参数，该参数的取值就是我们需要的请求方式</li></ol><h3 id="RequestHeader"><a href="#RequestHeader" class="headerlink" title="@RequestHeader"></a>@RequestHeader</h3><p>开发中不怎么用</p><p>用于 获取请求消息头的值</p><p><strong>属性：</strong></p><p>value提供消息头名称</p><p>required是否必须有此消息头</p><p><strong>例子：</strong></p><pre><code class="jsp">    &lt;a href=&quot;anno/testRequestHeader&quot;&gt;RequestHeader&lt;/a&gt;</code></pre><pre><code class="java">    /**     * 获取请求头的值     * @param header     * @return     */    @RequestMapping(value=&quot;/testRequestHeader&quot;)    public String testRequestHeader(@RequestHeader(value=&quot;Accept&quot;) String header, HttpServletRequest request,HttpServletResponse response) throws IOException &#123;        System.out.println(&quot;执行了...&quot;);        System.out.println(header);        // return &quot;success&quot;;        // response.sendRedirect(request.getContextPath()+&quot;/anno/testCookieValue&quot;);        return &quot;redirect:/param.jsp&quot;;    &#125;</code></pre><p>图片</p><h3 id="Cookie"><a href="#Cookie" class="headerlink" title="@Cookie"></a>@Cookie</h3><p>把指定cookie名称传入控制器方法参数</p><p><strong>属性：</strong></p><p>value指定cookie名称</p><p>required 是否必须由此cookie</p><p><strong>例子：</strong></p><pre><code class="jsp">    &lt;a href=&quot;anno/testCookieValue&quot;&gt;CookieValue&lt;/a&gt;</code></pre><pre><code class="java">    /**     * 获取Cookie的值     * @return     */    @RequestMapping(value=&quot;/testCookieValue&quot;)    public String testCookieValue(@CookieValue(value=&quot;JSESSIONID&quot;) String cookieValue)&#123;        System.out.println(&quot;执行了...&quot;);        System.out.println(cookieValue);        return &quot;success&quot;;    &#125;</code></pre><h3 id="MouelAttribute"><a href="#MouelAttribute" class="headerlink" title="@MouelAttribute"></a>@MouelAttribute</h3><p>该注解SpringMVC4.3版本之后加入的，可以用于修饰方法和参数</p><p>出现在方法上，在控制前方法执行之前，先执行。可以修饰没有返回值的方法，也可以修饰具有返回值的方法</p><p><strong>属性：</strong></p><p>Value用于获取数据的key，key可以是pojo的属性名称，也可以是map结构的key</p><p><strong>应用场景：</strong></p><p>但表单提交不是完整的实体类数据时，保证没有提交数据的字段使用数据库对象原来的数据</p><p><strong>例子：</strong></p><p>没有返回值的类型 </p><pre><code class="java">    /** 没有返回值的类型   参数中使用@ModelAttribute 指定key来获取值     * ModelAttribute注解     * @return     */    @RequestMapping(value=&quot;/testModelAttribute&quot;)    public String testModelAttribute(@ModelAttribute(&quot;abc&quot;) User user)&#123;        System.out.println(&quot;testModelAttribute执行了...&quot;);        System.out.println(user);        return &quot;success&quot;;    &#125;///////没有返回值的类型，值需要添加一个Map集合    @ModelAttribute    public void showUser(String uname, Map&lt;String,User&gt; map)&#123;        System.out.println(&quot;showUser执行了...&quot;);        // 通过用户查询数据库（模拟）        User user = new User();        user.setUname(uname);        user.setAge(20);        user.setDate(new Date());        map.put(&quot;abc&quot;,user);    &#125;</code></pre><p>有返回值的类型 </p><pre><code class="java">    /**有返回值的类型        * ModelAttribute注解     * @return     */    @RequestMapping(value=&quot;/testModelAttribute&quot;)    public String testModelAttribute(User user)&#123;        System.out.println(&quot;testModelAttribute执行了...&quot;);        System.out.println(user);        return &quot;success&quot;;    &#125;   /**  有返回值的类型     * 该方法会先执行     */    @ModelAttribute    public User showUser(String uname)&#123;        System.out.println(&quot;showUser执行了...&quot;);        // 通过用户查询数据库（模拟）        User user = new User();        user.setUname(uname);        user.setAge(20);        user.setDate(new Date());        return user;    &#125;</code></pre><h3 id="SessoinAttribute"><a href="#SessoinAttribute" class="headerlink" title="@SessoinAttribute"></a>@SessoinAttribute</h3><p>只能作用在类上,没听太懂，视频182</p><p>多次执行控制器方法将的参数共享</p><p><strong>属性：</strong></p><p>value属性的名称</p><p>type 属性的类型</p><p><strong>例子：</strong></p><pre><code class="java">import org.springframework.ui.Model;import org.springframework.ui.ModelMap;@Controller@RequestMapping(&quot;/anno&quot;)@SessionAttributes(value=&#123;&quot;msg&quot;&#125;)   // 把 msg=美美 存入到session域对中public class AnnoController &#123;    /**     * SessionAttributes的注解     * @return     */    @RequestMapping(value=&quot;/testSessionAttributes&quot;)    public String testSessionAttributes(Model model)&#123;        System.out.println(&quot;testSessionAttributes...&quot;);        // 底层会存储到request域对象中        model.addAttribute(&quot;msg&quot;,&quot;美美&quot;);        return &quot;success&quot;;    &#125;    /**     * 获取值     * @param modelMap     * @return     */    @RequestMapping(value=&quot;/getSessionAttributes&quot;)    public String getSessionAttributes(ModelMap modelMap)&#123;        System.out.println(&quot;getSessionAttributes...&quot;);        String msg = (String) modelMap.get(&quot;msg&quot;);        System.out.println(msg);        return &quot;success&quot;;    &#125;    /**     * 清除     * @param status     * @return     */    @RequestMapping(value=&quot;/delSessionAttributes&quot;)    public String delSessionAttributes(SessionStatus status)&#123;        System.out.println(&quot;getSessionAttributes...&quot;);        status.setComplete();        return &quot;success&quot;;    &#125;    &#125;</code></pre><pre><code class="html">&lt;a href=&quot;anno/testSessionAttributes&quot;&gt;testSessionAttributes&lt;/a&gt;success.jsp：&lt;body&gt;    &lt;h3&gt;入门成功&lt;/h3&gt;    $&#123; msg &#125;    $&#123;sessionScope&#125;&lt;/body&gt;</code></pre><h2 id="SpringMvc响应方式"><a href="#SpringMvc响应方式" class="headerlink" title="SpringMvc响应方式"></a>SpringMvc响应方式</h2><h3 id="根据返回值分类"><a href="#根据返回值分类" class="headerlink" title="根据返回值分类"></a>根据返回值分类</h3><h4 id="1、字符串"><a href="#1、字符串" class="headerlink" title="1、字符串"></a>1、字符串</h4><p>字符串类型的返回值 前端控制器会按照返回值跳转到对应的jsp页面</p><h4 id="2、void"><a href="#2、void" class="headerlink" title="2、void"></a>2、void</h4><p>SpringMvc提供的转发和重定向</p><p>没有返回值的时候，它默认是按照RequestMapping的value去寻找.jsp页面</p><p>在跳转的时候通过request 进行转发，response进行重定向</p><pre><code class="java">    /**     * 是void     * 请求转发一次请求，不用编写项目的名称     * 重定向是两次请求，需要编写项目名称     */    @RequestMapping(&quot;/testVoid&quot;)    public void testVoid(HttpServletRequest request, HttpServletResponse response) throws Exception &#123;        System.out.println(&quot;testVoid方法执行了...&quot;);         // 编写请求转发的程序         request.getRequestDispatcher(&quot;/WEB-INF/pages/success.jsp&quot;).forward(request,response);         // 重定向   是重新发了个请求  web-inf是直接进不去的        //request.getContextPath()  获取项目名称        response.sendRedirect(request.getContextPath()+&quot;/index.jsp&quot;);        // 设置中文乱码        response.setCharacterEncoding(&quot;UTF-8&quot;);        response.setContentType(&quot;text/html;charset=UTF-8&quot;);        // 直接会进行响应浏览器        response.getWriter().print(&quot;你好&quot;);        return;    &#125;    /**     * 使用关键字的方式进行转发或者重定向     * @return     */    @RequestMapping(&quot;/testForwardOrRedirect&quot;)    public String testForwardOrRedirect()&#123;        System.out.println(&quot;testForwardOrRedirect方法执行了...&quot;);        // 请求的转发        // return &quot;forward:/WEB-INF/pages/success.jsp&quot;;        // 重定向        return &quot;redirect:/index.jsp&quot;;    &#125;</code></pre><h2 id="前端控制器配置拦截资源"><a href="#前端控制器配置拦截资源" class="headerlink" title="前端控制器配置拦截资源"></a>前端控制器配置拦截资源</h2><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot;       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xsi:schemaLocation=&quot;        http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://www.springframework.org/schema/mvc        http://www.springframework.org/schema/mvc/spring-mvc.xsd        http://www.springframework.org/schema/context        http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt;    &lt;!-- 开启注解扫描 --&gt;    &lt;context:component-scan base-package=&quot;cn.itcast&quot;/&gt;    &lt;!-- 视图解析器对象 --&gt;    &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt;        &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt;        &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt;    &lt;/bean&gt;    &lt;!--前端控制器，哪些静态资源不拦截--&gt;    &lt;mvc:resources location=&quot;/css/&quot; mapping=&quot;/css/**&quot;/&gt;    &lt;mvc:resources location=&quot;/images/&quot; mapping=&quot;/images/**&quot;/&gt;    &lt;mvc:resources location=&quot;/js/&quot; mapping=&quot;/js/**&quot;/&gt;    &lt;!-- 开启SpringMVC框架注解的支持 --&gt;    &lt;mvc:annotation-driven /&gt;&lt;/beans&gt;</code></pre><h2 id="ModelAndView对象"><a href="#ModelAndView对象" class="headerlink" title="ModelAndView对象"></a>ModelAndView对象</h2><p>这个对象底层用的modelMap对象，是request范围的</p><pre><code class="java">    /**     * 返回ModelAndView     * @return     */    @RequestMapping(&quot;/testModelAndView&quot;)    public ModelAndView testModelAndView()&#123;        // 创建ModelAndView对象        ModelAndView mv = new ModelAndView();        System.out.println(&quot;testModelAndView方法执行了...&quot;);        // 模拟从数据库中查询出User对象        User user = new User();        user.setUsername(&quot;小凤&quot;);        user.setPassword(&quot;456&quot;);        user.setAge(30);        // 把user对象存储到mv对象中，也会把user对象存入到request对象        mv.addObject(&quot;user&quot;,user);        // 跳转到哪个页面        mv.setViewName(&quot;success&quot;);        return mv;    &#125;</code></pre><pre><code class="xml">&lt;%--  Created by IntelliJ IDEA.  User: Administrator  Date: 2018/5/1  Time: 1:18  To change this template use File | Settings | File Templates.--%&gt;&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; isELIgnored=&quot;false&quot; %&gt;&lt;html&gt;&lt;head&gt;    &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    &lt;h3&gt;执行成功&lt;/h3&gt;    $&#123;user.username&#125;    $&#123;user.password&#125;&lt;/body&gt;&lt;/html&gt;</code></pre><h2 id="ajax请求响应json数据"><a href="#ajax请求响应json数据" class="headerlink" title="ajax请求响应json数据"></a>ajax请求响应json数据</h2><pre><code class="js">    &lt;script&gt;        // 页面加载，绑定单击事件        $(function()&#123;            $(&quot;#btn&quot;).click(function()&#123;                // alert(&quot;hello btn&quot;);                // 发送ajax请求                $.ajax(&#123;                    // 编写json格式，设置属性和值                    url:&quot;user/testAjax&quot;,                    contentType:&quot;application/json;charset=UTF-8&quot;,                    data:&#39;&#123;&quot;username&quot;:&quot;hehe&quot;,&quot;password&quot;:&quot;123&quot;,&quot;age&quot;:30&#125;&#39;,                    dataType:&quot;json&quot;,                    type:&quot;post&quot;,                    success:function(data)&#123;                        // data服务器端响应的json的数据，进行解析                        alert(data);                        alert(data.username);                        alert(data.password);                        alert(data.age);                    &#125;                &#125;);            &#125;);        &#125;);    &lt;/script&gt;</code></pre><pre><code class="java">    /**     * 模拟异步请求响应     */    @RequestMapping(&quot;/testAjax&quot;)    public @ResponseBody User testAjax(@RequestBody String body)&#123;        System.out.println(&quot;testAjax方法执行了...&quot;);        System.out.println(body);    &#125;//控制台会输出 ：&#123;&quot;username&quot;:&quot;hehe&quot;,&quot;password&quot;:&quot;123&quot;,&quot;age&quot;:30&#125;</code></pre><h2 id="Json字符串封装到-Java-Bean中"><a href="#Json字符串封装到-Java-Bean中" class="headerlink" title="Json字符串封装到 Java Bean中"></a>Json字符串封装到 Java Bean中</h2><p>Json字符串转换成对象、对象转换成Json字符串  都是spring给我们转换，我们只需要导入 <strong>jackson</strong> 的包</p><pre><code class="xml">   &lt;dependency&gt;      &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;      &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;      &lt;version&gt;2.9.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;      &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;      &lt;version&gt;2.9.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;      &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;      &lt;version&gt;2.9.0&lt;/version&gt;    &lt;/dependency&gt;</code></pre><h2 id="使用-ResponseBody-和-RequestBody-注解来进行转换"><a href="#使用-ResponseBody-和-RequestBody-注解来进行转换" class="headerlink" title="使用 ResponseBody 和 RequestBody 注解来进行转换"></a>使用 ResponseBody 和 RequestBody 注解来进行转换</h2><pre><code class="java">//java bean    public class User implements Serializable&#123;    private String username;    private String password;    private Integer age;    &#125;    /**     * 模拟异步请求响应     * responseBody 相应的  将bean转换成 json字符串     * responseBody返回的类型跟你ajax设置的datetype相关     * requestBody 接受的     */    @RequestMapping(&quot;/testAjax&quot;)    public @ResponseBody User testAjax(@RequestBody User user)&#123;        System.out.println(&quot;testAjax方法执行了...&quot;);        // 客户端发送ajax的请求，传的是json字符串，后端把json字符串封装到user对象中        System.out.println(user);        // 做响应，模拟查询数据库        user.setUsername(&quot;haha&quot;);        user.setAge(40);        // 做响应        return user;    &#125;//控制台会输出 ：&#123;&quot;username&quot;:&quot;hehe&quot;,&quot;password&quot;:&quot;123&quot;,&quot;age&quot;:30&#125;</code></pre><h2 id="SpringMvc异常处理"><a href="#SpringMvc异常处理" class="headerlink" title="SpringMvc异常处理"></a>SpringMvc异常处理</h2><ol><li><p>编写自定义异常类（做提示信息的）</p></li><li><p>编写异常处理器</p></li><li><p>配置异常处理器（跳转到提示页面）</p></li></ol><pre><code class="java">/** * 自定义异常类 */public class SysException extends Exception&#123;    // 存储提示信息的    private String message;    public String getMessage() &#123;        return message;    &#125;    public void setMessage(String message) &#123;        this.message = message;    &#125;    public SysException(String message) &#123;        this.message = message;    &#125;&#125;</code></pre><pre><code class="java">/** * 异常处理器    需要实现handlerExceptionResolver接口，处理异常 */public class SysExceptionResolver implements HandlerExceptionResolver&#123;    /**     * 处理异常业务逻辑     * @param request     * @param response     * @param handler     * @param ex     * @return     */    public ModelAndView resolveException(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) &#123;        // 获取到异常对象        SysException e = null;        if(ex instanceof SysException)&#123;            e = (SysException)ex;        &#125;else&#123;            e = new SysException(&quot;系统正在维护....&quot;);        &#125;        // 创建ModelAndView对象        ModelAndView mv = new ModelAndView();        //request范围的值kv入参        mv.addObject(&quot;errorMsg&quot;,e.getMessage());        //使用modelandview  进行页面的跳转        mv.setViewName(&quot;error&quot;);        return mv;    &#125;&#125;</code></pre><pre><code class="java">&lt;!--发生error跳转的页面--&gt;&lt;%--  Created by IntelliJ IDEA.  User: Administrator  Date: 2018/5/5  Time: 22:28  To change this template use File | Settings | File Templates.--%&gt;&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; isELIgnored=&quot;false&quot; %&gt;&lt;html&gt;&lt;head&gt;    &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;    $&#123;errorMsg&#125;&lt;/body&gt;&lt;/html&gt;</code></pre><pre><code class="xml">//在xml文件中配置这个  异常处理器&lt;bean id=&quot;sysExceptionResolver&quot; class=&quot;cn.itcast.exception.SysExceptionResolver&quot;/&gt;</code></pre><h2 id="SpringMvc拦截器配置"><a href="#SpringMvc拦截器配置" class="headerlink" title="SpringMvc拦截器配置"></a>SpringMvc拦截器配置</h2><p>编写拦截器步骤：</p><ol><li><p>编写拦截器类，实现MandlerIntercepter接口</p></li><li><p>配置拦截器</p></li></ol><pre><code class="java">/** * 自定义拦截器 */public class MyInterceptor1 implements HandlerInterceptor&#123;    /**     * 预处理，controller方法执行前     * return true 放行，执行下一个拦截器，如果没有，执行controller中的方法     * return false不放行  就不会执行controller中的方法     * @param request     * @param response     * @param handler     * @return     * @throws Exception     */    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123;        System.out.println(&quot;MyInterceptor1执行了...前1111&quot;);        // request.getRequestDispatcher(&quot;/WEB-INF/pages/error.jsp&quot;).forward(request,response);        return true;    &#125;    /**     * 后处理方法，controller方法执行后，success.jsp执行之前     * @param request     * @param response     * @param handler     * @param modelAndView     * @throws Exception     */    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123;        System.out.println(&quot;MyInterceptor1执行了...后1111&quot;);        // request.getRequestDispatcher(&quot;/WEB-INF/pages/error.jsp&quot;).forward(request,response);    &#125;    /**     * success.jsp页面执行后，该方法会执行  可用于释放资源     * @param request     * @param response     * @param handler     * @param ex     * @throws Exception     */    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123;        System.out.println(&quot;MyInterceptor1执行了...最后1111&quot;);    &#125;&#125;</code></pre><pre><code class="java">在SpringMvc.xml中配置这个拦截器 &lt;!--配置拦截器--&gt;    &lt;mvc:interceptors&gt;        &lt;!--配置拦截器--&gt;        &lt;mvc:interceptor&gt;            &lt;!--要拦截的具体的方法--&gt;            &lt;mvc:mapping path=&quot;/user/*&quot;/&gt;            &lt;!--不要拦截的方法            &lt;mvc:exclude-mapping path=&quot;&quot;/&gt;            --&gt;            &lt;!--配置拦截器对象--&gt;            &lt;bean class=&quot;cn.itcast.controller.cn.itcast.interceptor.MyInterceptor1&quot; /&gt;        &lt;/mvc:interceptor&gt;        &lt;!--配置第二个拦截器--&gt;        &lt;mvc:interceptor&gt;            &lt;!--要拦截的具体的方法--&gt;            &lt;mvc:mapping path=&quot;/**&quot;/&gt;            &lt;!--不要拦截的方法            &lt;mvc:exclude-mapping path=&quot;&quot;/&gt;            --&gt;            &lt;!--配置拦截器对象--&gt;            &lt;bean class=&quot;cn.itcast.controller.cn.itcast.interceptor.MyInterceptor2&quot; /&gt;        &lt;/mvc:interceptor&gt;    &lt;/mvc:interceptors&gt;</code></pre>]]></content>
      
      
      <categories>
          
          <category> Java框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SrpingMvc </tag>
            
            <tag> Java框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSM</title>
      <link href="/java/spring/spring-ssm-zheng-he-kuang-jia-si/"/>
      <url>/java/spring/spring-ssm-zheng-he-kuang-jia-si/</url>
      
        <content type="html"><![CDATA[<h1 id="SSM开发环境搭建"><a href="#SSM开发环境搭建" class="headerlink" title="SSM开发环境搭建"></a>SSM开发环境搭建</h1><p>SSM是Spring为中心，来整合其他框架</p><h2 id="搭建Spring"><a href="#搭建Spring" class="headerlink" title="搭建Spring"></a>搭建Spring</h2><p>Mybatis会给你生成实现类，你只要写接口就好了.</p><p>编写dao层和service层的接口</p><pre><code class="java">@Repositorypublic interface AccountDao &#123;    // 查询所有账户    @Select(&quot;select * from account&quot;)    public List&lt;Account&gt; findAll();    // 保存帐户信息    @Insert(&quot;insert into account (name,money) values (#&#123;name&#125;,#&#123;money&#125;)&quot;)    public void saveAccount(Account account);&#125;public interface AccountService &#123;    // 查询所有账户    public List&lt;Account&gt; findAll();    // 保存帐户信息    public void saveAccount(Account account);&#125;</code></pre><h2 id="Spring整合SpringMvc"><a href="#Spring整合SpringMvc" class="headerlink" title="Spring整合SpringMvc"></a>Spring整合SpringMvc</h2><ol><li>配置Spring配置文件  <strong>applicatiocontext.xml</strong></li></ol><p>创建 applicationcontext.xml 配置文件，导入他们的头部文件</p><p>因为service和dao层由Spring来管理，而Controller由SpringMvc来，所以在扫描的时候不需要让他去扫描</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;       xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans    http://www.springframework.org/schema/beans/spring-beans.xsd    http://www.springframework.org/schema/context    http://www.springframework.org/schema/context/spring-context.xsd    http://www.springframework.org/schema/aop    http://www.springframework.org/schema/aop/spring-aop.xsd    http://www.springframework.org/schema/tx    http://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt;    &lt;!--开启注解的扫描，希望处理service和dao，controller不需要Spring框架去处理--&gt;    &lt;context:component-scan base-package=&quot;cn.itcast&quot; &gt;        &lt;!--配置哪些注解不扫描  写了controller的全路径--&gt;        &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot; /&gt;    &lt;/context:component-scan&gt;        &lt;!--Spring整合MyBatis框架--&gt;    &lt;!--配置连接池--&gt;    &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt;        &lt;property name=&quot;driverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;        &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql:///ssm&quot;/&gt;        &lt;property name=&quot;user&quot; value=&quot;root&quot;/&gt;        &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt;    &lt;/bean&gt;    &lt;!--配置SqlSessionFactory工厂--&gt;    &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt;        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;    &lt;/bean&gt;    &lt;!--配置AccountDao接口所在包--&gt;    &lt;bean id=&quot;mapperScanner&quot; class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt;        &lt;property name=&quot;basePackage&quot; value=&quot;cn.itcast.dao&quot;/&gt;    &lt;/bean&gt;        &lt;!--配置Spring框架声明式事务管理--&gt;    &lt;!--配置事务管理器--&gt;    &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt;        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;    &lt;/bean&gt;    &lt;!--配置事务通知--&gt;    &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt;        &lt;tx:attributes&gt;            &lt;tx:method name=&quot;find*&quot; read-only=&quot;true&quot;/&gt;            &lt;tx:method name=&quot;*&quot; isolation=&quot;DEFAULT&quot;/&gt;        &lt;/tx:attributes&gt;    &lt;/tx:advice&gt;    &lt;!--配置AOP增强--&gt;    &lt;aop:config&gt;        &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut=&quot;execution(* cn.itcast.service.impl.*ServiceImpl.*(..))&quot;/&gt;    &lt;/aop:config&gt;&lt;/beans&gt;</code></pre><ol start="2"><li>配置SpringMvc的两个文件：</li></ol><p><strong>springmvc.xml</strong>:</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xsi:schemaLocation=&quot;        http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://www.springframework.org/schema/mvc        http://www.springframework.org/schema/mvc/spring-mvc.xsd        http://www.springframework.org/schema/context        http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt;    &lt;!--开启注解扫描，只扫描Controller注解--&gt;    &lt;context:component-scan base-package=&quot;cn.itcast&quot;&gt;        &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot; /&gt;    &lt;/context:component-scan&gt;    &lt;!--配置的视图解析器对象--&gt;    &lt;bean id=&quot;internalResourceViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt;        &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;/&gt;        &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt;    &lt;/bean&gt;    &lt;!--过滤静态资源--&gt;    &lt;mvc:resources location=&quot;/css/&quot; mapping=&quot;/css/**&quot; /&gt;    &lt;mvc:resources location=&quot;/images/&quot; mapping=&quot;/images/**&quot; /&gt;    &lt;mvc:resources location=&quot;/js/&quot; mapping=&quot;/js/**&quot; /&gt;    &lt;!--开启SpringMVC注解的支持--&gt;    &lt;mvc:annotation-driven/&gt;&lt;/beans&gt;</code></pre><p><strong>web.xml</strong>:</p><pre><code class="xml">&lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt;&lt;web-app&gt;  &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt;  &lt;!--配置Spring的监听器，默认只加载WEB-INF目录下的applicationContext.xml配置文件--&gt;  &lt;listener&gt;    &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;  &lt;/listener&gt;  &lt;!--设置配置文件的路径--&gt;  &lt;context-param&gt;    &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;    &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt;  &lt;/context-param&gt;  &lt;context-param&gt;    &lt;param-name/&gt;    &lt;param-value/&gt;  &lt;/context-param&gt;  &lt;!--配置前端控制器--&gt;  &lt;servlet&gt;    &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;    &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;    &lt;!--加载springmvc.xml配置文件--&gt;    &lt;init-param&gt;      &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;      &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt;    &lt;/init-param&gt;    &lt;!--启动服务器，创建该servlet--&gt;    &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;  &lt;/servlet&gt;  &lt;servlet-mapping&gt;    &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;    &lt;url-pattern&gt;/&lt;/url-pattern&gt;  &lt;/servlet-mapping&gt;  &lt;!--解决中文乱码的过滤器--&gt;  &lt;filter&gt;    &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;    &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;    &lt;init-param&gt;      &lt;param-name&gt;encoding&lt;/param-name&gt;      &lt;param-value&gt;UTF-8&lt;/param-value&gt;    &lt;/init-param&gt;  &lt;/filter&gt;  &lt;filter-mapping&gt;    &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;    &lt;url-pattern&gt;/*&lt;/url-pattern&gt;  &lt;/filter-mapping&gt;  &lt;/web-app&gt;</code></pre><h2 id="Spring整合Mybatis"><a href="#Spring整合Mybatis" class="headerlink" title="Spring整合Mybatis"></a>Spring整合Mybatis</h2><p>更改Spring配置文件applicationcontext.xml，加入以下内容，将sessionfactory、dao、代理对象 都存入到ioc容器中</p><pre><code class="xml">    &lt;!--Spring整合MyBatis框架--&gt;    &lt;!--配置连接池--&gt;    &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt;        &lt;property name=&quot;driverClass&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;        &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql:///ssm&quot;/&gt;        &lt;property name=&quot;user&quot; value=&quot;root&quot;/&gt;        &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt;    &lt;/bean&gt;    &lt;!--配置SqlSessionFactory工厂--&gt;    &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt;        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;    &lt;/bean&gt;    &lt;!--配置AccountDao接口所在包--&gt;    &lt;bean id=&quot;mapperScanner&quot; class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt;        &lt;property name=&quot;basePackage&quot; value=&quot;cn.itcast.dao&quot;/&gt;    &lt;/bean&gt;</code></pre><h3 id="事务管理器"><a href="#事务管理器" class="headerlink" title="事务管理器"></a>事务管理器</h3><p>在applicationcontext.xml添加以下内容</p><h1 id="元注解说明"><a href="#元注解说明" class="headerlink" title="元注解说明"></a>元注解说明</h1><p>@Target()<br>说明了这个注解可以作用的范围，<br>method就是可以作用在方法上，type就是类和接口</p>]]></content>
      
      
      <categories>
          
          <category> Java框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java框架 </tag>
            
            <tag> SSM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mybatis-框架一</title>
      <link href="/java/spring/mybatis-de-jian-dan-shi-yong-kuang-jia-yi/"/>
      <url>/java/spring/mybatis-de-jian-dan-shi-yong-kuang-jia-yi/</url>
      
        <content type="html"><![CDATA[<p>最近要写接口，赶时间，看了下黑马视频，原理没有涉及到，后续再好好刷一次，具体掌握如何使用</p><blockquote><p>参考文章:  <a href="https://blog.csdn.net/hejingyuan6/article/details/36203505#">https://blog.csdn.net/hejingyuan6/article/details/36203505#</a></p></blockquote><h1 id="Mybatis"><a href="#Mybatis" class="headerlink" title="Mybatis"></a>Mybatis</h1><h2 id="什么是Mybatis？"><a href="#什么是Mybatis？" class="headerlink" title="什么是Mybatis？"></a>什么是Mybatis？</h2><p>持久层框架，底层是用java编写的</p><p>它封装了jdbc操作的很多细节，只用关注sql本身，无需关注注册驱动，创建链接等繁杂过程，使用了ORM(对象关系映射)思想实现了结果集的封装</p><h2 id="技术解决方案"><a href="#技术解决方案" class="headerlink" title="技术解决方案"></a>技术解决方案</h2><p>JDBC：</p><p>connection</p><p>PreparedStatement</p><p>ResultSet</p><p>Spring的JdbcTemplate</p><p>Spring对JDBC的简单封装</p><p>Apache的DBUtils</p><p>他和Spring的JDBCTemplate很像，也是对JDBC的简单封装</p><h2 id="Mybatis环境搭建"><a href="#Mybatis环境搭建" class="headerlink" title="Mybatis环境搭建"></a>Mybatis环境搭建</h2><ul><li><p>创建maven工程导入坐标</p></li><li><p>创建实体类和dao层接口</p></li><li><p>创建mybatis的主配置文件</p></li><li><p>创建映射配置文件</p></li></ul><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ul><li><p>创建.xml文件是为了保持和之前<strong>文件目录结构</strong>保持一致。持久层操作接口名称和映射文件一一对应</p></li><li><p>mybatis映射文件位置必须和dao层接口的包结果相同</p></li><li><p>映射配置文件的mapper标签namespace属性的取值必须是dao接口的全限定类名</p></li><li><p>映射配置文件操作配置，select id 属性值必须是dao接口的方法名</p></li></ul><h3 id="搭建的不同方式"><a href="#搭建的不同方式" class="headerlink" title="搭建的不同方式"></a>搭建的不同方式</h3><p>有两种方式，一种是XML文件的方式，一种是注解的方式，还可以实现你的接口来实现功能</p><h4 id="XML文件的方式配置"><a href="#XML文件的方式配置" class="headerlink" title="XML文件的方式配置"></a>XML文件的方式配置</h4><p>使用XML方式，需要在主配置文件中指定mapper的<strong>resource</strong><br>不需要使用注解，需要你生成一个XML文件来存放映射关系</p><p>使用XML方式，需要在主配置文件中指定mapper的<strong>resource</strong><br><img src="https://img-blog.csdnimg.cn/20201125202927769.png#pic_center" alt="在这里插入图片描述"><br>不需要使用注解，需要你生成一个XML文件来存放映射关系</p><h5 id="主配置文件："><a href="#主配置文件：" class="headerlink" title="主配置文件："></a>主配置文件：</h5><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE configuration        PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;        &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt;    &lt;!-- 配置properties--&gt;    &lt;properties resource=&quot;jdbcConfig.properties&quot;&gt;&lt;/properties&gt;    &lt;!--使用typeAliases配置别名，它只能配置domain中类的别名 --&gt;    &lt;typeAliases&gt;        &lt;package name=&quot;com.itheima.domain&quot;&gt;&lt;/package&gt;    &lt;/typeAliases&gt;    &lt;!--配置环境--&gt;    &lt;environments default=&quot;mysql&quot;&gt;        &lt;!-- 配置mysql的环境--&gt;        &lt;environment id=&quot;mysql&quot;&gt;            &lt;!-- 配置事务 --&gt;            &lt;transactionManager type=&quot;JDBC&quot;&gt;&lt;/transactionManager&gt;            &lt;!--配置连接池--&gt;            &lt;dataSource type=&quot;POOLED&quot;&gt;                &lt;property name=&quot;driver&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;&gt;&lt;/property&gt;                &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;&gt;&lt;/property&gt;                &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;&gt;&lt;/property&gt;                &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;/property&gt;            &lt;/dataSource&gt;        &lt;/environment&gt;    &lt;/environments&gt;    &lt;!-- 配置映射文件的位置 --&gt;    &lt;mappers&gt;        &lt;package name=&quot;com.itheima.dao&quot;&gt;&lt;/package&gt;    &lt;/mappers&gt;</code></pre><h5 id="映射文件"><a href="#映射文件" class="headerlink" title="映射文件"></a>映射文件</h5><pre><code class="xml">&lt;!DOCTYPE mapper        PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;        &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;!--指定在哪个类中--&gt;&lt;mapper namespace=&quot;com.mybatisexercise.dao.UserDao&quot;&gt;    &lt;!--指定类中的方法和返回值类型--&gt;    &lt;select id=&quot;findAll&quot; resultType=&quot;com.mybatisexercise.domain.User&quot;&gt;        select * from user;    &lt;/select&gt;&lt;/mapper&gt;</code></pre><h4 id="注解方式"><a href="#注解方式" class="headerlink" title="注解方式"></a>注解方式</h4><p>使用注解的方式，需要在主配置文件中指定mapper的<strong>class</strong></p><p>然后在你的接口上使用注解</p><h3 id="读取配置文件两种方式"><a href="#读取配置文件两种方式" class="headerlink" title="读取配置文件两种方式"></a>读取配置文件两种方式</h3><h4 id="使用类加载器"><a href="#使用类加载器" class="headerlink" title="使用类加载器"></a>使用类加载器</h4><p>它只能读取类路径的配置文件</p><p>使用servletContext对象的个体Real Path()</p><h2 id="工厂模式的优势"><a href="#工厂模式的优势" class="headerlink" title="工厂模式的优势"></a>工厂模式的优势</h2><p>降低类之间的依赖关系，生产SqlSession就使用了工厂模式</p><h2 id="代理模式的优势"><a href="#代理模式的优势" class="headerlink" title="代理模式的优势"></a>代理模式的优势</h2><p>代理是一种常用的设计模式，其目的就是为其他对象提供一个代理以控制对某个对象的访问。代理类负责为委托类预处理消息，过滤消息并转发消息，以及进行消息被委托类执行后的后续处理。<br>更通俗的说，代理解决的问题当两个类需要通信时，引入第三方代理类，将两个类的关系解耦，让我们只了解代理类即可，而且代理的出现还可以让我们完成与另一个类之间的关系的统一管理，但是切记，代理类和委托类要实现相同的接口，因为代理真正调用的还是委托类的方法。</p><h3 id="1、静态代理"><a href="#1、静态代理" class="headerlink" title="1、静态代理"></a>1、静态代理</h3><p>由程序员创建代理类或特定工具自动生成源代码再对其编译。在程序运行前代理类的.class文件就已经存在了。<br><strong>优点：</strong></p><p>代理使客户端不需要知道实现类是什么，怎么做的，而客户端只需知道代理即可（解耦合），对于如上的客户端代码，newUserManagerImpl()可以应用工厂将它隐藏，如上只是举个例子而已。</p><p><strong>缺点：</strong><br>理类和委托类实现了相同的接口，代理类通过委托类实现了相同的方法。这样就出现了大量的代码重复。如果接口增加一个方法，除了所有实现类需要实现这个方法外，所有代理类也需要实现此方法。增加了代码维护的复杂度。</p><p>2）代理对象只服务于一种类型的对象，如果要服务多类型的对象。势必要为每一种对象都进行代理，静态代理在程序规模稍大时就无法胜任了</p><h3 id="2、动态代理"><a href="#2、动态代理" class="headerlink" title="2、动态代理"></a>2、动态代理</h3><p>动态代理是在运行时，通过反射机制实现动态代理，并且能够代理各种类型的对象</p><p>在程序运行时运用反射机制动态创建而成。<br>在Java中要想实现动态代理机制，需要java.lang.reflect.InvocationHandler  接口和 java.lang.reflect.Proxy 类的支持</p><p>java.lang.reflect.InvocationHandler接口的定义如下：</p><pre><code>//Object proxy:被代理的对象  //Method method:要调用的方法  //Object[] args:方法调用时所需要参数  public interface InvocationHandler &#123;      public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;  &#125;  </code></pre><p>java.lang.reflect.Proxy类的定义如下：</p><pre><code>//CLassLoader loader:类的加载器//Class&lt;?&gt; interfaces:得到全部的接口//InvocationHandler h:得到InvocationHandler接口的子类的实例public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException</code></pre><p>具体实现：</p><pre><code class="java">//动态代理类只能代理接口（不支持抽象类），代理类都需要实现InvocationHandler类，实现invoke方法。该invoke方法就是调用被代理接口的所有方法时需要调用的，该invoke方法返回的值是被代理接口的一个实现类   public class LogHandler implements InvocationHandler &#123;     // 目标对象    private Object targetObject;    //绑定关系，也就是关联到哪个接口（与具体的实现类绑定）的哪些方法将被调用时，执行invoke方法。                public Object newProxyInstance(Object targetObject)&#123;        this.targetObject=targetObject;        //该方法用于为指定类装载器、一组接口及调用处理器生成动态代理类实例          //第一个参数指定产生代理对象的类加载器，需要将其指定为和目标对象同一个类加载器        //第二个参数要实现和目标对象一样的接口，所以只需要拿到目标对象的实现接口        //第三个参数表明这些被拦截的方法在被拦截时需要执行哪个InvocationHandler的invoke方法        //根据传入的目标返回一个代理对象        return Proxy.newProxyInstance(targetObject.getClass().getClassLoader(),                targetObject.getClass().getInterfaces(),this);    &#125;    @Override    //关联的这个实现类的方法被调用时将被执行    /*InvocationHandler接口的方法，proxy表示代理，method表示原对象被调用的方法，args表示方法的参数*/    public Object invoke(Object proxy, Method method, Object[] args)            throws Throwable &#123;        System.out.println(&quot;start--&gt;&gt;&quot;);        for(int i=0;i&lt;args.length;i++)&#123;            System.out.println(args[i]);        &#125;        Object ret=null;        try&#123;            /*原对象方法调用前处理日志信息*/            System.out.println(&quot;satrt--&gt;&gt;&quot;);                        //调用目标方法            ret=method.invoke(targetObject, args);            /*原对象方法调用后处理日志信息*/            System.out.println(&quot;success--&gt;&gt;&quot;);        &#125;catch(Exception e)&#123;            e.printStackTrace();            System.out.println(&quot;error--&gt;&gt;&quot;);            throw e;        &#125;        return ret;    &#125; &#125;</code></pre><p><strong>优点：</strong><br>我们可以通过LogHandler代理不同类型的对象，如果我们把对外的接口都通过动态代理来实现，那么所有的函数调用最终都会经过invoke函数的转发，因此我们就可以在这里做一些自己想做的操作，比如日志系统、事务、拦截器、权限控制等。这也就是AOP(面向切面编程)的基本原理。</p><h3 id="静态代理和动态代理区别："><a href="#静态代理和动态代理区别：" class="headerlink" title="静态代理和动态代理区别："></a>静态代理和动态代理区别：</h3><p>动态代理与静态代理相比较，最大的好处是接口中声明的所有方法都被转移到调用处理器一个集中的方法中处理（InvocationHandler.invoke）。这样，在接口方法数量比较多的时候，我们可以进行灵活处理，而不需要像静态代理那样每一个方法进行中转。而且动态代理的应用使我们的类职责更加单一，复用性更强</p><h2 id="Mybatis工作原理"><a href="#Mybatis工作原理" class="headerlink" title="Mybatis工作原理"></a>Mybatis工作原理</h2><ol><li>Mybatis在运行时，首先通过dom4j(还有其他XML解析技术)解析XML来解析你的SqlMapConfig文件，转换成input stream流。</li><li>使用input stream中的数据取出来生成一个Configuration配置对象，把Configuration中的参数取出来创建mysql connect对象，并构建SqlSessionFactory。</li><li>SqlSessionFactory单例模式创建SqlSession对象</li><li>SqlSession对象需要传入 .class 参数，并返回一个 .class接口的动态代理对象，供你调用</li><li>可以对这个对象进行操作，在执行时通过反射和动态代理来执行你的语句。</li></ol><h3 id="pojo对象"><a href="#pojo对象" class="headerlink" title="pojo对象"></a>pojo对象</h3><p>使用ognl对象解析对象字段的值，#{} 或 ${}括号中的值是pojo属性名称</p><h3 id="Mysql配置文件"><a href="#Mysql配置文件" class="headerlink" title="Mysql配置文件"></a>Mysql配置文件</h3><p>Mysql在Window中不区分大小写，在linx中区分大小写</p><h2 id="mybatis实体类和数据库的字段不匹配两种解决方案："><a href="#mybatis实体类和数据库的字段不匹配两种解决方案：" class="headerlink" title="mybatis实体类和数据库的字段不匹配两种解决方案："></a>mybatis实体类和数据库的字段不匹配两种解决方案：</h2><h3 id="一、更改xml配置文件sql语句和参数"><a href="#一、更改xml配置文件sql语句和参数" class="headerlink" title="一、更改xml配置文件sql语句和参数"></a>一、更改xml配置文件sql语句和参数</h3><pre><code class="xml">&lt;insert&gt;insert into table (username,Sex) values(#&#123;username&#125;,#&#123;UserSex&#125;);&lt;/insert&gt;&lt;select&gt;select username,sex as UserSex from user;&lt;/select&gt;</code></pre><h3 id="二、ResultMap"><a href="#二、ResultMap" class="headerlink" title="二、ResultMap"></a>二、ResultMap</h3><p>上面那种方法语句多的时候太过于繁琐</p><p>可以使用resultMap</p><pre><code class="xml">    &lt;!--左边的是实体类字段，右边的是数据库字段，一一对应--&gt;    &lt;!-- 配置 查询结果的列名和实体类的属性名的对应关系 ，这样写不区分大小写--&gt;    &lt;resultMap id=&quot;userMap&quot; type=&quot;uSeR&quot;&gt;        &lt;!-- 主键字段的对应 --&gt;        &lt;id property=&quot;userId&quot; column=&quot;id&quot;&gt;&lt;/id&gt;        &lt;!--非主键字段的对应--&gt;        &lt;result property=&quot;userName&quot; column=&quot;username&quot;&gt;&lt;/result&gt;        &lt;result property=&quot;userAddress&quot; column=&quot;address&quot;&gt;&lt;/result&gt;        &lt;result property=&quot;userSex&quot; column=&quot;sex&quot;&gt;&lt;/result&gt;        &lt;result property=&quot;userBirthday&quot; column=&quot;birthday&quot;&gt;&lt;/result&gt;    &lt;/resultMap&gt;    &lt;!-- 根据queryVo的条件查询用户 --&gt;    &lt;select id=&quot;findUserByVo&quot; parameterType=&quot;com.itheima.domain.QueryVo&quot; resultMap=&quot;userMap&quot;&gt;        select * from user where username like #&#123;user.username&#125;    &lt;/select&gt;</code></pre><h4 id="注意事项-1"><a href="#注意事项-1" class="headerlink" title="注意事项"></a>注意事项</h4><p>在使用ResuleMap时，在返回值定义要使用resultMap，而不是使用ResultType!  否则会找不到你这个类型而报错</p><h2 id="创建自定义注解"><a href="#创建自定义注解" class="headerlink" title="创建自定义注解"></a>创建自定义注解</h2><p>创建一个接口</p><pre><code class="xml">@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface Select &#123;    /**     * 配置SQL语句的     * @return     */    String value();&#125;</code></pre><p>在代码上方使用注解</p><pre><code>@Target说明了 Annotation 所修饰的对象范围：Annotation可被用于 packages、types（类、接口、枚举、Annotation类型）、类型成员（方法、构造方法、成员变量、枚举值）、方法参数和本地变量（如循环变量、catch参数）。在Annotation类型的声明中使用了target可更加明晰其修饰的目标。　　作用：用于描述注解的使用范围（即：被描述的注解可以用在什么地方）　　取值(ElementType)有：　　　　1.CONSTRUCTOR:用于描述构造器　　　　　　　　2.FIELD:用于描述域　　　　　　　　3.LOCAL_VARIABLE:用于描述局部变量　　　　　　　　4.METHOD:用于描述方法　　　　　　　　5.PACKAGE:用于描述包　　　　　　　　6.PARAMETER:用于描述参数　　　　　　　　7.TYPE:用于描述类、接口(包括注解类型) 或enum声明　　　　@Retention：　　@Retention定义了该 Annotation 被保留的时间长短：某些Annotation仅出现在源代码中，而被编译器丢弃；而另一些却被编译在class文件中；编译在class文件中的Annotation可能会被虚拟机忽略，而另一些在class被装载时将被读取（请注意并不影响class的执行，因为Annotation与class在使用上是被分离的）。使用这个meta-Annotation可以对 Annotation的“生命周期”限制。　　作用：表示需要在什么级别保存该注释信息，用于描述注解的生命周期（即：被描述的注解在什么范围内有效）　　取值（RetentionPoicy）有：　　　　1.SOURCE:在源文件中有效（即源文件保留）　　　　2.CLASS:在class文件中有效（即class保留）　　　　3.RUNTIME:在运行时有效（即运行时保留）　　　　　　　　@Documented:　　@Documented用于描述其它类型的annotation应该被作为被标注的程序成员的公共API，因此可以被例如javadoc此类的工具文档化。Documented是一个标记注解，没有成员。　　@Inherited：　　@Inherited 元注解是一个标记注解，@Inherited阐述了某个被标注的类型是被继承的。如果一个使用了@Inherited修饰的annotation类型被用于一个class，则这个annotation将被用于该class的子类。　　注意：@Inherited annotation类型是被标注过的class的子类所继承。类并不从它所实现的接口继承annotation，方法并不从它所重载的方法继承annotation。　　当@Inherited annotation类型标注的annotation的Retention是RetentionPolicy.RUNTIME，则反射API增强了这种继承性。如果我们使用java.lang.reflect去查询一个@Inherited annotation类型的annotation时，反射代码检查将展开工作：检查class和其父类，直到发现指定的annotation类型被发现，或者到达类继承结构的顶层。</code></pre><h2 id="JNDI数据源"><a href="#JNDI数据源" class="headerlink" title="JNDI数据源"></a>JNDI数据源</h2><p>JNDI（java naming and directory interface） 模仿的window系统的注册表。</p><p>注册表是kv类型的。k是绝对路径+名称，v是存放的数据。</p><p>JNDI中存放的就是对象。</p><p>JNDI视频没看，后期要看可以看58.59这两个视频</p><h2 id="XML标签导入外部配置文件"><a href="#XML标签导入外部配置文件" class="headerlink" title="XML标签导入外部配置文件"></a>XML标签导入外部配置文件</h2><h4 id="resource方式"><a href="#resource方式" class="headerlink" title="resource方式"></a>resource方式</h4><p>resuource来指定classpath的下的配置文件</p><pre><code>&lt;!-- 配置properties--&gt;&lt;properties resource=&quot;jdbcConfig.properties&quot;&gt;&lt;/properties&gt;&lt;dataSource type=&quot;POOLED&quot;&gt;    &lt;property name=&quot;driver&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;&gt;&lt;/property&gt;    &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;&gt;&lt;/property&gt;    &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;&gt;&lt;/property&gt;    &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;/property&gt;&lt;/dataSource&gt;</code></pre><h4 id="url绝对路径的方式"><a href="#url绝对路径的方式" class="headerlink" title="url绝对路径的方式"></a>url绝对路径的方式</h4><pre><code>&lt;properties url=&quot;G:\BaiduNetdiskDownload\第三天代码\day03_eesy_01datasourceAndTx\src\main\resources\jdbcConfig.properties&quot;&gt;&lt;/properties&gt;&lt;dataSource type=&quot;POOLED&quot;&gt;    &lt;property name=&quot;driver&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;&gt;&lt;/property&gt;    &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot;&gt;&lt;/property&gt;    &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot;&gt;&lt;/property&gt;    &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;/property&gt;&lt;/dataSource&gt;</code></pre><h2 id="实体类和实体类接口别名"><a href="#实体类和实体类接口别名" class="headerlink" title="实体类和实体类接口别名"></a>实体类和实体类接口别名</h2><h4 id="TypeAliases"><a href="#TypeAliases" class="headerlink" title="TypeAliases"></a>TypeAliases</h4><p>在使用标签的时候，返回值中写全路径，很繁琐</p><p>可以使用typeAliases来去进行取别名<br>虽然配置了，但一次一个类，太麻烦</p><pre><code> &lt;typeAliases&gt;        &lt;!--typeAlias用于配置别名。type属性指定的是实体类全限定类名。alias属性指定别名，当指定了别名就再区分大小写 --&gt;        &lt;typeAlias type=&quot;com.itheima.domain.User&quot; alias=&quot;user&quot;&gt;&lt;/typeAlias&gt; &lt;/typeAliases&gt;</code></pre><h4 id="Package"><a href="#Package" class="headerlink" title="Package"></a>Package</h4><p>使用上面的方式进行取别名，但是如果实体类太多，工作还是很繁琐，就可以使用<package>标签来完成<br>直接配置一个包里面所有的类的别名</package></p><pre><code> &lt;typeAliases&gt; &lt;!--可以指定一个包，里面的类别名全部都是是类的名称，不区分大小写--&gt; &lt;/typeAliases&gt;&lt;package name=&quot;com.itheima.domain&quot;&gt;&lt;/package&gt; &lt;/typeAliases&gt;&lt;!--也可以写在mappers中，package是指定dao接口所在的包，指定了之后不需要写mapper和resource了或者class了--&gt;&lt;mappers&gt;        &lt;!--  &lt;mapper resource=&quot;com/mybatisexercise/dao/UserDao.xml&quot;/&gt;        &lt;mapper class=&quot;com.mybatisexercise.dao.UserDao&quot;/&gt;--&gt;    &lt;package name=&quot;com.exercise.dao&quot;&gt;&lt;/package&gt;    &lt;/mappers&gt; </code></pre><h3 id="注意事项-2"><a href="#注意事项-2" class="headerlink" title="注意事项"></a>注意事项</h3><p>上面两个注解只能在SqlMapConfig.xml中使用，不能在和实体类对应的xml文件中使用</p><h2 id="Mybatis连接池"><a href="#Mybatis连接池" class="headerlink" title="Mybatis连接池"></a>Mybatis连接池</h2><h4 id="什么是连接池？"><a href="#什么是连接池？" class="headerlink" title="什么是连接池？"></a>什么是连接池？</h4><p>连接池可以减少我们获取链接所消耗的时间和开销</p><p>线程池，就是一个线程安全的集合来存储着连接对象。还实现了队列的特性：先进先出</p><h2 id="Mybatis连接池-1"><a href="#Mybatis连接池-1" class="headerlink" title="Mybatis连接池"></a>Mybatis连接池</h2><h3 id="三种类型的连接池"><a href="#三种类型的连接池" class="headerlink" title="三种类型的连接池"></a>三种类型的连接池</h3><p>主配置文件SQLMapConfig.xml中的dataSource标签，type就是表示使用何种连接池的方式</p><pre><code>type的取值：POOLED采用用了传统的javax.sql.DataSource规范的连接池mybatis中有针对规范的实现UNPOOLED采用了传统获取连接的方式，虽然也实现了javax.sql.DataSource接口，但是没有使用池的思想。他每次使用都会创建一个新的连接JNDI采用了服务器提供的JNDI技术实现的，来获取DataSource对象，不同服务器能拿到的DataSource对象是不一样的。注意：如果不是Web或者Maven的war工程是不能使用的，我们实际开发中使用的是Tomcat服务器，采用的连接池使用的是dbcp连接池 &lt;dataSource type=&quot;POOLED&quot;&gt;    &lt;!-- 配置连接数据库的4个基本信息 --&gt;    &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;    &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/eesy_mybatis&quot;/&gt;    &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt;    &lt;property name=&quot;password&quot; value=&quot;1234&quot;/&gt;&lt;/dataSource&gt;</code></pre><h2 id="Mybatis事务"><a href="#Mybatis事务" class="headerlink" title="Mybatis事务"></a>Mybatis事务</h2><h3 id="什么是事务？事物的四大特性"><a href="#什么是事务？事物的四大特性" class="headerlink" title="什么是事务？事物的四大特性"></a>什么是事务？事物的四大特性</h3><p> 对数据库的事务而言，应该具有以下几点：创建（create）、提交（commit）、回滚（rollback）、关闭（close）。对应地，MyBatis将事务抽象成了Transaction接口：其接口定义如下：<br> <img src="https://img-blog.csdnimg.cn/img_convert/30e58e0af991a2ea4a2b3aa9577b5d97.png" alt="img"><br> 不考虑隔离性会产生的问题</p><h3 id="事务的四种隔离机制"><a href="#事务的四种隔离机制" class="headerlink" title="事务的四种隔离机制"></a>事务的四种隔离机制</h3><blockquote><p>原文章连接：<a href="https://www.cnblogs.com/ubuntu1/p/8999403.html">https://www.cnblogs.com/ubuntu1/p/8999403.html</a></p></blockquote><p>数据库事务的隔离级别有4种，由低到高分别为<strong>Read uncommitted</strong> 、<strong>Read committed</strong> 、<strong>Repeatable read</strong> 、<strong>Serializable</strong> 。而且，在事务的并发操作中可能会出现脏读，不可重复读，幻读。</p><h4 id="Read-uncommitted"><a href="#Read-uncommitted" class="headerlink" title="Read uncommitted"></a>Read uncommitted</h4><p>读未提交，顾名思义，就是一个事务可以读取另一个未提交事务的数据。</p><p>事例：老板要给程序员发工资，程序员的工资是3.6万&#x2F;月。但是发工资时老板不小心按错了数字，按成3.9万&#x2F;月，该钱已经打到程序员的户口，但是事务还没有提交，就在这时，程序员去查看自己这个月的工资，发现比往常多了3千元，以为涨工资了非常高兴。但是老板及时发现了不对，马上回滚差点就提交了的事务，将数字改成3.6万再提交。</p><p>分析：实际程序员这个月的工资还是3.6万，但是程序员看到的是3.9万。他看到的是老板还没提交事务时的数据。这就是脏读</p><p><strong>那怎么解决可能的不可重复读问题？Repeatable read ！</strong></p><h4 id="Read-committed"><a href="#Read-committed" class="headerlink" title="Read committed"></a>Read committed</h4><p>读提交，顾名思义，就是一个事务要等另一个事务提交后才能读取数据。</p><p>事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（程序员事务开启），收费系统事先检测到他的卡里有3.6万，就在这个时候！！程序员的妻子要把钱全部转出充当家用，并提交。当收费系统准备扣款时，再检测卡里的金额，发现已经没钱了（第二次检测金额当然要等待妻子转出金额事务提交完）。程序员就会很郁闷，明明卡里是有钱的…</p><p>分析：这就是读提交，若有事务对数据进行更新（UPDATE）操作时，读操作事务要等待这个更新操作事务提交后才能读取数据，可以解决脏读问题。但在这个事例中，出现了一个事务范围内两个相同的查询却返回了不同数据，这就是不可重复读。</p><h4 id="Repeatable-read"><a href="#Repeatable-read" class="headerlink" title="Repeatable read"></a>Repeatable read</h4><p>重复读，就是在开始读取数据（事务开启）时，不再允许修改操作</p><p>事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（事务开启，不允许其他事务的UPDATE修改操作），收费系统事先检测到他的卡里有3.6万。这个时候他的妻子不能转出金额了。接下来收费系统就可以扣款了。</p><p>分析：重复读可以解决不可重复读问题。写到这里，应该明白的一点就是，不可重复读对应的是修改，即UPDATE操作。但是可能还会有幻读问题。因为幻读问题对应的是插入INSERT操作，而不是UPDATE操作。</p><p>什么时候会出现幻读？</p><p>事例：程序员某一天去消费，花了2千元，然后他的妻子去查看他今天的消费记录（全表扫描FTS，妻子事务开启），看到确实是花了2千元，就在这个时候，程序员花了1万买了一部电脑，即新增INSERT了一条消费记录，并提交。当妻子打印程序员的消费记录清单时（妻子事务提交），发现花了1.2万元，似乎出现了幻觉，这就是幻读。</p><p><strong>那怎么解决幻读问题？Serializable！</strong></p><h4 id="Serializable-序列化"><a href="#Serializable-序列化" class="headerlink" title="Serializable 序列化"></a>Serializable 序列化</h4><p>Serializable 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。</p><p>值得一提的是：大多数数据库默认的事务隔离级别是Read committed，比如Sql Server , Oracle。Mysql的默认隔离级别是Repeatable read。</p><h2 id="Mybatis中基于XML配置的动态SQL语句的拼接"><a href="#Mybatis中基于XML配置的动态SQL语句的拼接" class="headerlink" title="Mybatis中基于XML配置的动态SQL语句的拼接"></a>Mybatis中基于XML配置的动态SQL语句的拼接</h2><h4 id="多条件查询"><a href="#多条件查询" class="headerlink" title="多条件查询"></a>多条件查询</h4><p>mappers配置文件中的几个标签</p><pre><code>&lt;if&gt;&lt;where&gt;&lt;foreach&gt;&lt;sql&gt;</code></pre><p>下面介绍他们的使用：</p><h4 id="where标签"><a href="#where标签" class="headerlink" title="where标签"></a>where标签</h4><pre><code>不用标签正常是&lt;select id=&quot;findUserByCondition&quot; resultMap=&quot;UserMap&quot; parameterType=&quot;com.itheima.domain.User&quot;&gt;            select * from user where 1=1                &lt;if test=&quot;userName!=null&quot;&gt;                   and username=#&#123;username&#125;                &lt;/if&gt;                &lt;if test=&quot;userSex!=null&quot;&gt;                   and sex=#&#123;userSex&#125;                &lt;/if&gt;&lt;/select&gt;用了where标签就是&lt;select id=&quot;findUserByCondition&quot; resultMap=&quot;UserMap&quot; parameterType=&quot;com.itheima.domain.User&quot;&gt;            select * from user            &lt;where&gt;                &lt;if test=&quot;userName!=null&quot;&gt;                   and username=#&#123;username&#125;                &lt;/if&gt;                &lt;if test=&quot;userSex!=null&quot;&gt;                   and sex=#&#123;userSex&#125;                &lt;/if&gt;            &lt;/where&gt;&lt;/select&gt;</code></pre><h4 id="if标签"><a href="#if标签" class="headerlink" title="if标签"></a>if标签</h4><pre><code>不用if标签，无法进行判断到底有没有传参数在多条件查询时必须使用这个标签！    &lt;select id=&quot;findUserByCondition&quot; resultMap=&quot;UserMap&quot; parameterType=&quot;com.itheima.domain.User&quot;&gt;            select * from user where 1=1            &lt;where&gt;                &lt;if test=&quot;userName!=null&quot;&gt;                   and username=#&#123;username&#125;                &lt;/if&gt;                &lt;if test=&quot;userSex!=null&quot;&gt;                   and sex=#&#123;userSex&#125;                &lt;/if&gt;            &lt;/where&gt;    &lt;/select&gt;</code></pre><h4 id="foreach标签"><a href="#foreach标签" class="headerlink" title="foreach标签"></a>foreach标签</h4><pre><code>遍历拼接字符的一个标签    &lt;select id=&quot;findUserInIds&quot; resultMap=&quot;userMap&quot; parameterType=&quot;QueryVo&quot;&gt;        select * from user        &lt;where&gt;            &lt;if test=&quot;ids != null and ids.size()&gt;0&quot;&gt;                &lt;foreach collection=&quot;ids&quot; open=&quot;and id in (&quot; close=&quot;)&quot; item=&quot;uid&quot; separator=&quot;,&quot;&gt;                    #&#123;uid&#125;                &lt;/foreach&gt;            &lt;/if&gt;        &lt;/where&gt;    &lt;/select&gt;</code></pre><h2 id="Mybatis多表查询"><a href="#Mybatis多表查询" class="headerlink" title="Mybatis多表查询"></a>Mybatis多表查询</h2><h4 id="一对一"><a href="#一对一" class="headerlink" title="一对一"></a>一对一</h4><p>实体和实体是一对一的关系，一对一使用查询使用</p><pre><code>&lt;resultMap id=&quot;userMap&quot; type=&quot;account&quot;&gt;&lt;!--这个是主键--&gt; &lt;id property=&quot;id&quot; cloumn=&quot;aid&quot;&gt;&lt;/id&gt;&lt;result property=&quot;uid&quot; column=&quot;uid&quot;&gt;&lt;/result&gt;&lt;result property=&quot;money&quot; column=&quot;money&quot;&gt;&lt;/result&gt;&lt;association property=&quot;user&quot; column=&quot;uid&quot; javaType=&quot;User&quot;&gt;&lt;id property=&quot;id&quot; cloumn=&quot;id&quot;&gt;&lt;/id&gt;&lt;result property=&quot;user&quot; column=&quot;username&quot;&gt;&lt;/result&gt;&lt;result property=&quot;address&quot; column=&quot;address&quot;&gt;&lt;/result&gt;&lt;result property=&quot;sex&quot; column=&quot;sex&quot;&gt;&lt;/result&gt;&lt;result property=&quot;birthday&quot; column=&quot;birthday&quot;&gt;&lt;/result&gt;&lt;result&gt;&lt;/association&gt;&lt;/resultMap&gt;</code></pre><h4 id="一对多"><a href="#一对多" class="headerlink" title="一对多"></a>一对多</h4><p>一对多，多的那部分实体类使用集合的方式来装数据(list，array等)</p><pre><code>&lt;resultMap id=&quot;userMap&quot; type=&quot;user&quot;&gt;&lt;!--这个是主键--&gt; &lt;id property=&quot;id&quot; cloumn=&quot;id&quot;&gt;&lt;/id&gt;&lt;result property=&quot;username&quot; column=&quot;username&quot;&gt;&lt;/result&gt;&lt;result property=&quot;address&quot; column=&quot;address&quot;&gt;&lt;/result&gt;&lt;result property=&quot;sex&quot; column=&quot;sex&quot;&gt;&lt;/result&gt;&lt;result property=&quot;birthday&quot; column=&quot;birthday&quot;&gt;&lt;/result&gt;    &lt;!--实体对象中定义的集合名字，oftype是集合中装填的类型，这里取别名了--&gt;&lt;collection property=&quot;accounts&quot; ofType=&quot;account&quot;&gt;&lt;id column=&quot;aid&quot; property=&quot;id&quot;&gt;&lt;/id&gt;&lt;result property=&quot;uid&quot; column=&quot;uid&quot;&gt;&lt;/result&gt;&lt;result property=&quot;money&quot; column=&quot;money&quot;&gt;&lt;/result&gt;&lt;/collection&gt;&lt;/resultMap&gt;</code></pre><h4 id="多对多"><a href="#多对多" class="headerlink" title="多对多"></a>多对多</h4><pre><code>&lt;!--这个其实和一对多是一样的，把其中一个拿出来就行了--&gt; &lt;!--定义role表的ResultMap--&gt;    &lt;resultMap id=&quot;roleMap&quot; type=&quot;role&quot;&gt;        &lt;id property=&quot;roleId&quot; column=&quot;rid&quot;&gt;&lt;/id&gt;        &lt;result property=&quot;roleName&quot; column=&quot;role_name&quot;&gt;&lt;/result&gt;        &lt;result property=&quot;roleDesc&quot; column=&quot;role_desc&quot;&gt;&lt;/result&gt;        &lt;collection property=&quot;users&quot; ofType=&quot;user&quot;&gt;            &lt;id column=&quot;id&quot; property=&quot;id&quot;&gt;&lt;/id&gt;            &lt;result column=&quot;username&quot; property=&quot;username&quot;&gt;&lt;/result&gt;            &lt;result column=&quot;address&quot; property=&quot;address&quot;&gt;&lt;/result&gt;            &lt;result column=&quot;sex&quot; property=&quot;sex&quot;&gt;&lt;/result&gt;            &lt;result column=&quot;birthday&quot; property=&quot;birthday&quot;&gt;&lt;/result&gt;        &lt;/collection&gt;    &lt;/resultMap&gt;</code></pre><h2 id="Mybatis延迟加载"><a href="#Mybatis延迟加载" class="headerlink" title="Mybatis延迟加载"></a>Mybatis延迟加载</h2><h4 id="什么是延迟加载？"><a href="#什么是延迟加载？" class="headerlink" title="什么是延迟加载？"></a>什么是延迟加载？</h4><p>在真正使用数据是再发起查询，不用的时候不查。按需加载（懒加载）</p><pre><code>&lt;!--必须写column属性，用户根据id查询时，所需要的参数的值select是表示调用的方法--&gt;    &lt;!-- 定义封装account和user的resultMap --&gt;&lt;resultMap id=&quot;accountUserMap&quot; type=&quot;account&quot;&gt;    &lt;id property=&quot;id&quot; column=&quot;id&quot;&gt;&lt;/id&gt;    &lt;result property=&quot;uid&quot; column=&quot;uid&quot;&gt;&lt;/result&gt;    &lt;result property=&quot;money&quot; column=&quot;money&quot;&gt;&lt;/result&gt;    &lt;!-- 一对一的关系映射：配置封装user的内容    select属性指定的内容：查询用户的唯一标识：用什么去查    column属性指定的内容：用户根据id查询时，所需要的参数的值    --&gt;        &lt;association property=&quot;user&quot; column=&quot;uid&quot; javaType=&quot;user&quot; select=&quot;com.itheima.dao.IUserDao.findById&quot;&gt;&lt;/association&gt;&lt;/resultMap&gt;&lt;!--同时还要开启延迟加载的开关,在sqlMapConfig中配置为true--&gt;&lt;!--配置参数--&gt;&lt;settings&gt;    &lt;!--开启Mybatis支持延迟加载--&gt;    &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt;    &lt;setting name=&quot;aggressiveLazyLoading&quot; value=&quot;false&quot;&gt;&lt;/setting&gt;&lt;/settings&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20201125200813570.png#pic_center" alt="在这里插入图片描述"></p><h4 id="什么是立即加载？"><a href="#什么是立即加载？" class="headerlink" title="什么是立即加载？"></a>什么是立即加载？</h4><p>不管用不用，只要一调用就立马发起查询</p><p>对应四种表关系，一对一，多对一，一对一，多对多。</p><p><strong>一对多，多对多：</strong>通常情况下采用延迟加载</p><p><strong>一对一，多对一：</strong>通常情况下使用立即加载</p><h2 id="Mybatis的缓存"><a href="#Mybatis的缓存" class="headerlink" title="Mybatis的缓存"></a>Mybatis的缓存</h2><h4 id="什么是缓存？"><a href="#什么是缓存？" class="headerlink" title="什么是缓存？"></a>什么是缓存？</h4><p>存在于内存中的临时数据，下次查询时，不需要再次查询，从而快速的获取数据。</p><h4 id="为什么使用缓存？缓存的配置"><a href="#为什么使用缓存？缓存的配置" class="headerlink" title="为什么使用缓存？缓存的配置"></a>为什么使用缓存？缓存的配置</h4><p>减少和数据库的交互次数，提高执行效率。</p><h4 id="什么样数据适用缓存，什么样数据不能使用？"><a href="#什么样数据适用缓存，什么样数据不能使用？" class="headerlink" title="什么样数据适用缓存，什么样数据不能使用？"></a>什么样数据适用缓存，什么样数据不能使用？</h4><p><strong>适用缓存：</strong>经常查询的，不经常改变的。</p><p>数据的正确与否队最终结果影响不大。</p><p><strong>不适用缓存：</strong></p><p>经常改变的数据</p><p>数据的正确与否队最终结果影响大</p><h3 id="Mybatis的一级缓存和二级缓存"><a href="#Mybatis的一级缓存和二级缓存" class="headerlink" title="Mybatis的一级缓存和二级缓存"></a>Mybatis的一级缓存和二级缓存</h3><h5 id="一级缓存："><a href="#一级缓存：" class="headerlink" title="一级缓存："></a>一级缓存：</h5><p>它指的是Mybatis中SqlSession对象的缓存，<strong>默认是开启的</strong></p><p>当我们执行查询后，查询的结果会同时存入到sql’session为我们提供一块区域</p><p>该区域的结构是一个map，当我们再次查询相同的数据时，mybatis会先去sqlsessin中查询是否有，有的话直接拿出来。</p><p>当sqlsession过期后，缓存也就消失了</p><pre><code>清除一级缓存,下面两个都可以清除缓存调用sqlsession.clearCache()sqlsession.close()一级缓存在调用sqlsession的修改，添加，删除，commit(),close()等方法，就会清空一级缓存缓存的同步一级缓存在调用sqlsession的修改，添加，删除，commit(),close()等方法，就会清空一级缓存，再次查询时就会重新查询</code></pre><h5 id="二级缓存："><a href="#二级缓存：" class="headerlink" title="二级缓存："></a>二级缓存：</h5><p>它指的是Mybatis中SqlSessionFactory对象的缓存。有同一个sqlSessionFactory创建的sqlsession共享缓存内容。二级缓存默认是不开启的。</p><p>二级缓存存放的是数据，不是对象！也就是对象会消除，数据不会，在查询二级缓存时，会新建对象指向数据，所以不同sqlsession使用同一个二级缓存时不是同一个对象</p><pre><code>使用步骤：第一步：让Mybatis框架支持二级缓存(在SQLMapConfig.xml中配置)第二步：让当前映射文件支持二级缓存（在IUserDao.xml中配置）&lt;cache/&gt;第三步：让当前的操作支持二级缓存，（在select标签中配置）&lt;select id=&quot;findById&quot; parameterType=&quot;INT&quot; resultType=&quot;user&quot; UserCache=&quot;true&quot;&gt;select * from user where id =#&#123;id&#125;&lt;/select&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/2020112520150923.png#pic_center" alt="在这里插入图片描述"></p><h1 id="Mybatis注解开发"><a href="#Mybatis注解开发" class="headerlink" title="Mybatis注解开发"></a>Mybatis注解开发</h1><p>没看</p><p>从72开始看，看到75</p><p>注解开发只会用注解的方式省略掉实体类对应的.xml文件，sqlmapconfig.xml文件依然存在</p><h3 id="单表CRUD代理Dao方式"><a href="#单表CRUD代理Dao方式" class="headerlink" title="单表CRUD代理Dao方式"></a>单表CRUD代理Dao方式</h3><p>crud中一共有四个注解 @Insert @Update @Delete @Select</p><p>使用时直接写在你的实体接口上面写上直接调用即可</p><pre><code>public interface IUserDao &#123;    /**     * 查询所有用户     * @return     */    @Select(&quot;select * from user&quot;)    List&lt;User&gt; findAll();    /**     * 保存用户     * @param user     */    @Insert(&quot;insert into user(username,address,sex,birthday)values(#&#123;username&#125;,#&#123;address&#125;,#&#123;sex&#125;,#&#123;birthday&#125;)&quot;)    void saveUser(User user);    /**     * 更新用户     * @param user     */    @Update(&quot;update user set username=#&#123;username&#125;,sex=#&#123;sex&#125;,birthday=#&#123;birthday&#125;,address=#&#123;address&#125; where id=#&#123;id&#125;&quot;)    void updateUser(User user);    /**     * 删除用户     * @param userId     */    @Delete(&quot;delete from user where id=#&#123;id&#125; &quot;)    void deleteUser(Integer userId);    /**     * 根据id查询用户     * @param userId     * @return     */    @Select(&quot;select * from user  where id=#&#123;id&#125; &quot;)    User findById(Integer userId);    /**     * 根据用户名称模糊查询     * @param username     * @return     *///    @Select(&quot;select * from user where username like #&#123;username&#125; &quot;)    @Select(&quot;select * from user where username like &#39;%$&#123;value&#125;%&#39; &quot;)    List&lt;User&gt; findUserByName(String username);    /**     * 查询总用户数量     * @return     */    @Select(&quot;select count(*) from user &quot;)    int findTotalUser();&#125;</code></pre><h3 id="注意事项-3"><a href="#注意事项-3" class="headerlink" title="注意事项"></a>注意事项</h3><p>如果注解和XML两种方式都存在，然后在sqlMapConfig.xml中配置选择其中一种，是会报错的。</p><p>Mybatis要求只能存在一种方式<img src="https://img-blog.csdnimg.cn/20201125202030728.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="注解开发实体类和数据库字段名称不匹配的情况"><a href="#注解开发实体类和数据库字段名称不匹配的情况" class="headerlink" title="注解开发实体类和数据库字段名称不匹配的情况"></a>注解开发实体类和数据库字段名称不匹配的情况</h3><h4 id="Results注解"><a href="#Results注解" class="headerlink" title="@Results注解"></a>@Results注解</h4><pre><code>//这里的id是让别人来引用的，不用再写一遍了@Results(id=&quot;userMap&quot;,value=&#123;            @Result(id=true,column = &quot;id&quot;,property = &quot;userId&quot;),            @Result(column = &quot;username&quot;,property = &quot;userName&quot;),            @Result(column = &quot;address&quot;,property = &quot;userAddress&quot;),            @Result(column = &quot;sex&quot;,property = &quot;userSex&quot;),            @Result(column = &quot;birthday&quot;,property = &quot;userBirthday&quot;),            @Result(property = &quot;accounts&quot;,column = &quot;id&quot;,                    many = @Many(select = &quot;com.itheima.dao.IAccountDao.findAccountByUid&quot;,                                fetchType = FetchType.LAZY))    &#125;)    /**     * 根据id查询用户     * @param userId     * @return     *///这里就引用了上面写好的Results    @Select(&quot;select * from user  where id=#&#123;id&#125; &quot;)    @ResultMap(&quot;userMap&quot;)    //@ResultMap(&quot;userMap&quot;)    //上面的整体的写法，但在只有一个参数的时候可以省略掉    User findById(Integer userId);</code></pre><h4 id="一对一-1"><a href="#一对一-1" class="headerlink" title="一对一"></a>一对一</h4><pre><code>    /**     * 查询所有用户     * @return     */    @Select(&quot;select * from user&quot;)    @Results(id=&quot;userMap&quot;,value=&#123;            @Result(id=true,column = &quot;id&quot;,property = &quot;userId&quot;),            @Result(column = &quot;username&quot;,property = &quot;userName&quot;),            @Result(column = &quot;address&quot;,property = &quot;userAddress&quot;),            @Result(column = &quot;sex&quot;,property = &quot;userSex&quot;),            @Result(column = &quot;birthday&quot;,property = &quot;userBirthday&quot;),            @Result(property = &quot;accounts&quot;,column = &quot;id&quot;,                    many = @Many(select = &quot;com.itheima.dao.IAccountDao.findAccountByUid&quot;,                                fetchType = FetchType.LAZY))    &#125;)    List&lt;User&gt; findAll();</code></pre><h4 id="多表查询操作"><a href="#多表查询操作" class="headerlink" title="多表查询操作"></a>多表查询操作</h4><p>mybatis<br>33 34中流程原理<br>preparedstatement</p>]]></content>
      
      
      <categories>
          
          <category> Java框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java框架 </tag>
            
            <tag> Mybatis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring常用注解</title>
      <link href="/java/spring/spring-chang-yong-zhu-jie-zong-jie/"/>
      <url>/java/spring/spring-chang-yong-zhu-jie-zong-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="SpringBootApplication"><a href="#SpringBootApplication" class="headerlink" title="@SpringBootApplication"></a>@SpringBootApplication</h2><p>是Sprnig Boot项目的核心注解，目的是开启自动配置</p><p>实现了以下三个注解的功能：</p><p><strong>@EnableAutoConfiguration</strong>：启用Spring Boot的自动配置机制</p><p><strong>@ComponentScan</strong>：启用@Component对应用程序所在的软件包的扫描</p><p><strong>@Configuration</strong>：允许在上下文中注册额外的bean或导入其他配置类</p><h2 id="Enable"><a href="#Enable" class="headerlink" title="@Enable*"></a>@Enable*</h2><p>启用什么什么服务，里面会import导入配置类</p><h2 id="Table"><a href="#Table" class="headerlink" title="@Table"></a>@Table</h2><p>spring @Table注解  作用是 ： 声明此对象映射到数据库的数据表，通过它可以为实体指定表(talbe)</p><p>常用的两个属性：<br>1、name 用来命名 当前实体类 对应的数据库 表的名字 </p><pre><code class="java">@Table(name = &quot;tab_user&quot;）</code></pre><p>2、uniqueConstraints 用来批量命名唯一键<br>其作用等同于多个：@Column(unique &#x3D; true)</p><pre><code class="java">@Table(name = &quot;tab_user&quot;,uniqueConstraints = &#123;@UniqueConstraint(columnNames=&#123;&quot;uid&quot;,&quot;email&quot;&#125;)&#125;)</code></pre><h2 id="Basic"><a href="#Basic" class="headerlink" title="@Basic"></a>@Basic</h2><p>@Basic表示一个简单的属性到数据库表的字段的映射,对于没有任何标注的getXxxx()方法,默认 即为 </p><pre><code class="java">@Basic　　(1)、FetchType.LAZY：懒加载，加载一个实体时，定义懒加载的属性不会马上从数据库中加载。　　(2)、FetchType.EAGER：急加载，加载一个实体时，定义急加载的属性会立即从数据库中加载。</code></pre><pre><code class="java">　　示例:@Basic(fetch=FetchType,optional=true)public String getAddress() &#123; return address; &#125;</code></pre><h2 id="Column"><a href="#Column" class="headerlink" title="@Column"></a>@Column</h2><p>@Column描述了数据库表中该字段的详细定义,这对于根据JPA注解生成数据库表结构的工具非常 有作用.</p><pre><code class="java">    @Column:　　name:表示数据库表中该字段的名称,默认情形属性名称一致　　nullable:表示该字段是否允许为null,默认为true　　unique:表示该字段是否是唯一标识,默认为false　　length:表示该字段的大小,仅对String类型的字段有效　　insertable:表示在ORM框架执行插入操作时,该字段是否应出现INSETRT语句中,默认为true　　updateable:表示在ORM框架执行更新操作时,该字段是否应该出现在UPDATE语句中,默认为 true.对于一经创建就不可以更改的字段,该属性非常有用,如对于birthday字段.　　columnDefinition:表示该字段在数据库中的实际类型.通常ORM框架可以根据属性类型自动判 断数据库中字段的类型,但是对于Date类型仍无法确定数据库中字段类型究竟是DATE,TIME还是 TIMESTAMP.此外,String的默认映射类型为VARCHAR,如果要将String类型映射到特定数据库的 BLOB或TEXT字段类型,该属性非常有用.</code></pre><pre><code class="java">　　示例:　　@Column(name=&quot;BIRTH&quot;,nullable=&quot;false&quot;,columnDefinition=&quot;DATE&quot;)　　public String getBithday() &#123; return birthday; &#125;</code></pre><h2 id="id"><a href="#id" class="headerlink" title="@id"></a>@id</h2><p>@Id 标注用于声明一个实体类的属性映射为数据库的主键列。<br>该属性通常置于属性声明语句之前，可与声明语句同行，也可写在单独行上。<br>@Id标注也可置于属性的getter方法之前。</p><pre><code class="java">　　示例:@Table(name=&quot;admin&quot;)public class Customer &#123;    @Id    private Integer id;    private String name;    private String email;    private int age;        @Basic    @Column(name=&quot;id&quot;)    public Integer getId() &#123;        return id;    &#125;    public void setId(Integer id) &#123;        this.id = id;    &#125;</code></pre><h2 id="Scope"><a href="#Scope" class="headerlink" title="@Scope"></a>@Scope</h2><p>用于指定scope作用域的（用在类上）</p><h2 id="Configuration"><a href="#Configuration" class="headerlink" title="@Configuration"></a>@Configuration</h2><p><a href="https://blog.csdn.net/qiuz1024/article/details/100530260">@Configuration推荐文章连接</a><br>从Spring3.0，@Configuration用于定义配置类，可替换xml配置文件，被注解的类内部包含有一个或多个被@Bean注解的方法，这些方法将会被AnnotationConfigApplicationContext或AnnotationConfigWebApplicationContext类进行扫描，并用于构建bean定义，初始化Spring容器。</p><pre><code class="java">　　示例:　　@Configurationpublic class TestConfiguration &#123;    public TestConfiguration() &#123;        System.out.println(&quot;TestConfiguration容器启动初始化。。。&quot;);    &#125;&#125;@Configurationpublic class TestConfiguration &#123;    public TestConfiguration() &#123;        System.out.println(&quot;TestConfiguration容器启动初始化。。。&quot;);    &#125;    // @Bean注解注册bean,同时可以指定初始化和销毁方法    // @Bean(name=&quot;testBean&quot;,initMethod=&quot;start&quot;,destroyMethod=&quot;cleanUp&quot;)    @Bean    @Scope(&quot;prototype&quot;)    public TestBean testBean() &#123;        return new TestBean();    &#125;&#125;</code></pre><h2 id="Bean"><a href="#Bean" class="headerlink" title="@Bean"></a>@Bean</h2><p>Spring的@Bean注解用于告诉方法，产生一个Bean对象，然后这个Bean对象交给Spring管理。产生这个Bean对象的方法Spring只会调用一次，随后这个Spring将会将这个Bean对象放在自己的IOC容器中。</p><pre><code class="java">@Bean:value： name属性的别名，在不需要其他属性时使用，也就是说value 就是默认值name： 此bean 的名称，或多个名称，主要的bean的名称加别名。如果未指定，则bean的名称是带注解方法的名称。如果指定了，方法的名称就会忽略，如果没有其他属性声明的话，bean的名称和别名可能通过value属性配置initMethod ：指定初始化方法             destroyMethod：指定销毁的方法    PS：相当于xml文件中 init-method &amp;destroy-method属性</code></pre><h2 id="token相关注解："><a href="#token相关注解：" class="headerlink" title="token相关注解："></a>token相关注解：</h2><p>下面两个注解是token的请求相关的，官网文档地址：<br><a href="https://docs.spring.io/spring-security-oauth2-boot/docs/2.2.0.RELEASE/reference/html5/">https://docs.spring.io/spring-security-oauth2-boot/docs/2.2.0.RELEASE/reference/html5/</a></p><h3 id="EnableAuthorizationServer"><a href="#EnableAuthorizationServer" class="headerlink" title="@EnableAuthorizationServer"></a>@EnableAuthorizationServer</h3><p>启用授权服务器，默认情况下，@EnableAuthorizationServer授予客户端访问客户端凭据的权限<br>与其他Spring Boot@Enable注释类似，可以将@EnableAuthorizationServer注释添加到main方法的类中</p><pre><code class="java">　　示例:　　@EnableAuthorizationServer@SpringBootApplicationpublic class SimpleAuthorizationServerApplication &#123;    public static void main(String[] args) &#123;        SpringApplication.run(SimpleAuthorizationServerApplication, args);    &#125;&#125;</code></pre><h3 id="EnableResourceServer"><a href="#EnableResourceServer" class="headerlink" title="@EnableResourceServer"></a>@EnableResourceServer</h3><p>指定用于验证承载令牌的策略，添加此注释会添加OAuth2AuthenticationProcessingFilter，尽管它还需要进行其他配置才能知道如何适当地处理和验证令牌。</p><p>与其他Spring Boot@Enable注释类似，可以将@EnableResourceServer注释添加到main方法的类中</p><pre><code class="java">　　示例:　　@EnableResourceServer@SpringBootApplicationpublic class SimpleAuthorizationServerApplication &#123;    public static void main(String[] args) &#123;        SpringApplication.run(SimpleAuthorizationServerApplication, args);    &#125;&#125;</code></pre><h3 id="EnableWebSecurity"><a href="#EnableWebSecurity" class="headerlink" title="@EnableWebSecurity"></a>@EnableWebSecurity</h3><p>参考文章：<a href="https://blog.csdn.net/andy_zhang2007/article/details/90023901">https://blog.csdn.net/andy_zhang2007&#x2F;article&#x2F;details&#x2F;90023901</a><br>首先,EnableWebSecurity注解是个组合注解,他的注解中,又使用了@EnableGlobalAuthentication注解:<br>@EnableWebSecurity是Spring Security用于启用Web安全的注解。典型的用法是该注解用在某个Web安全配置类上(实现了接口WebSecurityConfigurer或者继承自WebSecurityConfigurerAdapter)。典型的使用例子如下 :</p><pre><code class="java">　　示例:　　 @Configuration @EnableWebSecurity public class MyWebSecurityConfiguration extends WebSecurityConfigurerAdapter &#123;        @Override        public void configure(WebSecurity web) throws Exception &#123;                web.ignoring()                // Spring Security should completely ignore URLs starting with /resources/                                .antMatchers(&quot;/resources/**&quot;);        &#125;        @Override        protected void configure(HttpSecurity http) throws Exception &#123;                http.authorizeRequests().antMatchers(&quot;/public/**&quot;).permitAll().anyRequest()                                .hasRole(&quot;USER&quot;).and()                                // Possibly more configuration ...                                .formLogin() // enable form based log in                                // set permitAll for all URLs associated with Form Login                                .permitAll();        &#125;        @Override        protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123;                auth                // enable in memory based authentication with a user named &quot;user&quot; and &quot;admin&quot;                .inMemoryAuthentication().withUser(&quot;user&quot;).password(&quot;password&quot;).roles(&quot;USER&quot;)                                .and().withUser(&quot;admin&quot;).password(&quot;password&quot;).roles(&quot;USER&quot;, &quot;ADMIN&quot;);        &#125;        // Possibly more overridden methods ... &#125;</code></pre><h2 id="Qualifier"><a href="#Qualifier" class="headerlink" title="@Qualifier"></a>@Qualifier</h2><p>使用@Qualifier明确指定使用那个实现类了。</p><h2 id="Autowired"><a href="#Autowired" class="headerlink" title="@Autowired"></a>@Autowired</h2><p>@Autowired 注释，它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作<br>它是按照类型来自动进行标注</p><pre><code class="java">　　示例:　　@Servicepublic class UserService &#123;    @Autowired    private UserRepository userRepository;    public void save()&#123;        userRepository.save();    &#125;&#125;</code></pre><h2 id="Component"><a href="#Component" class="headerlink" title="@Component"></a>@Component</h2><p>实现bean的注入<br>针对不同的使用场景所采取的特定功能化的注解。</p><h2 id="Service"><a href="#Service" class="headerlink" title="@Service"></a>@Service</h2><p>一般使用@Service注解标记这个类属于业务逻辑层。<br>就是针对不同的使用场景所采取的特定功能化的注解。</p><h2 id="Controller"><a href="#Controller" class="headerlink" title="@Controller"></a>@Controller</h2><p>这个注解主要告诉Spring这个类作为控制器，可以看做标记为暴露给前端的入口。@Controller 用于标记在一个类上，使用它标记的类就是一个SpringMVC Controller 对象。分发处理器将会扫描使用了该注解的类的方法。通俗来说，被Controller标记的类就是一个控制器，这个类中的方法，就是相应的动作。<br>就是针对不同的使用场景所采取的特定功能化的注解。</p><h2 id="Repository"><a href="#Repository" class="headerlink" title="@Repository"></a>@Repository</h2><p>这个注解用来标识这个类是用来直接访问数据库的，dao层使用@repository注解。</p><h2 id="RestController"><a href="#RestController" class="headerlink" title="@RestController"></a>@RestController</h2><p>@Controller+@ResponseBody</p><h2 id="ResponseBody"><a href="#ResponseBody" class="headerlink" title="@ResponseBody"></a>@ResponseBody</h2><p>@ResponseBody的作用其实是将java对象转为json格式的数据。</p><p>@responseBody注解的作用是将controller的方法返回的对象通过适当的转换器转换为指定的格式之后，写入到response对象的body区，通常用来返回JSON数据或者是XML数据。</p><h2 id="RequestParam"><a href="#RequestParam" class="headerlink" title="@RequestParam"></a>@RequestParam</h2><p>注解@RequestParam接收的参数是来自HTTP请求体或请求url的QueryString中。<br>@RequestParam有三个配置参数：</p><p>required 表示是否必须，默认为 true，必须。<br>defaultValue 可设置请求参数的默认值。<br>value 为接收url的参数名（相当于key值）。<br>@RequestParam用来处理 Content-Type 为 application&#x2F;x-www-form-urlencoded 编码的内容，Content-Type默认为该属性。@RequestParam也可用于其它类型的请求，例如：POST、DELETE等请求。</p><h2 id="RequestBody"><a href="#RequestBody" class="headerlink" title="@RequestBody"></a>@RequestBody</h2><p>注解@RequestBody接收的参数是来自requestBody中，即请求体。一般用于处理非 Content-Type: application&#x2F;x-www-form-urlencoded编码格式的数据，比如：application&#x2F;json、application&#x2F;xml等类型的数据。<br>通俗的讲，就是会把你传入的json字符解析成该参数类型的Javabean对象。</p><p>GET 方式无请求体，所以 @RequestBody 接收数据时，前端必须是 POST 方式进行提交，然后给页面的数据默认也是 json</p><p>同一个方法中，@RequestBody 与 @RequestParam() 可以同时使用，前者最多只能有一个，后者可以有多个</p><h2 id="GetMapping"><a href="#GetMapping" class="headerlink" title="@GetMapping"></a>@GetMapping</h2><p>@GetMapping 注解将 HTTP GET 请求映射到特定的处理程序方法。</p><h2 id="PostMapping"><a href="#PostMapping" class="headerlink" title="@PostMapping"></a>@PostMapping</h2><p>@GetMapping 注解将 HTTP POST 请求映射到特定的处理程序方法。</p><h2 id="PutMapping"><a href="#PutMapping" class="headerlink" title="@PutMapping"></a>@PutMapping</h2><p>@GetMapping 注解将 HTTP PUT 请求映射到特定的处理程序方法。</p><h2 id="DeleteMapping"><a href="#DeleteMapping" class="headerlink" title="@DeleteMapping"></a>@DeleteMapping</h2><p>@GetMapping 注解将 HTTP Delete 请求映射到特定的处理程序方法。</p><h2 id="Select"><a href="#Select" class="headerlink" title="@Select"></a>@Select</h2><p>在Mapper层使用@Select注解，以此省略掉mapper.xml文件</p><pre><code class="java">　　示例:　　　　@Select(&quot;&lt;script&gt;&quot;            +&quot;select * from mi_taobao where 1=1&quot;            +&quot;&lt;if test=&#39;status != null&#39;&gt;&quot;            +&quot;and status = #&#123;status&#125;&quot;            +&quot;&lt;/if&gt;&quot;            +&quot;&lt;/script&gt;&quot;)    public List&lt;Taobao&gt; getTaobao(@Param(&quot;status&quot;) Integer status);    @Select(&#123; &quot;SELECT &quot;                + &quot; a.id, &quot;                + &quot; a.role_name roleName, &quot;                + &quot; a.enabled, &quot;                + &quot; a.create_by createBy, &quot;                + &quot; a.create_time createTime &quot;            + &quot; FROM &quot;                + &quot; sys_role a &quot;            + &quot; WHERE &quot;            + &quot; a.id = #&#123;roleId&#125;&quot; &#125;)    SysRole selectSysRoleById(Long roleId);</code></pre><h2 id="MapperScan"><a href="#MapperScan" class="headerlink" title="@MapperScan"></a>@MapperScan</h2><p>指定要变成实现类的接口所在的包，然后包下面的所有接口在编译之后都会生成相应的实现类<br>在Springboot启动类上面添加</p><pre><code class="java">　　示例:　　@SpringBootApplication//@EnableEurekaClient@MapperScan(&quot;com.winter.dao&quot;)public class SpringbootMybatisDemoApplication &#123;    public static void main(String[] args) &#123;        SpringApplication.run(SpringbootMybatisDemoApplication.class, args);    &#125;&#125;多个包扫描：@SpringBootApplication  @MapperScan(&#123;&quot;com.kfit.demo&quot;,&quot;com.kfit.user&quot;&#125;)  public class App &#123;      public static void main(String[] args) &#123;         SpringApplication.run(App.class, args);      &#125;  &#125; </code></pre><h2 id="Data"><a href="#Data" class="headerlink" title="@Data"></a>@Data</h2><p>作用在实体类上，生成对应的getset方法，需要导入lombok包</p><h2 id="NoArgsConstructor"><a href="#NoArgsConstructor" class="headerlink" title="@NoArgsConstructor"></a>@NoArgsConstructor</h2><p>作用在实体类上，生成对应的无参构造方法，需要导入lombok包</p><h2 id="AllArgsConstructor"><a href="#AllArgsConstructor" class="headerlink" title="@AllArgsConstructor"></a>@AllArgsConstructor</h2><p>作用在实体类上，生成对应的有参构造方法，需要导入lombok包</p><h2 id="Param"><a href="#Param" class="headerlink" title="@Param"></a>@Param</h2><p>作为Dao层的注解，作用是用于传递参数，从而可以与SQL中的的字段名相对应，一般在2&#x3D;&lt;参数数&lt;&#x3D;5时使用最佳。</p><pre><code class="java">　　示例:　　public List&lt;Role&gt; findRoleByAnnotation(@Param(&quot;roleName&quot;) String roleName, @Param(&quot;note&quot;) String note);&lt;select id=&quot;findRoleByMap&quot; parameterType=&quot;map&quot; resultType=&quot;role&quot;&gt;    SELECT id,name FROM t_role    WHERE roleName=#&#123;roleName&#125;    AND note=#&#123;note&#125;&lt;select&gt;</code></pre><h2 id="Resource"><a href="#Resource" class="headerlink" title="@Resource"></a>@Resource</h2><p>@Resource和@Autowired注解都是用来实现依赖注入的。只是@AutoWried按by type自动注入，而@Resource默认按byName自动注入。</p><p>@Resource有两个重要属性，分别是name和type</p><p>spring将name属性解析为bean的名字，而type属性则被解析为bean的类型。所以如果使用name属性，则使用byName的自动注入策略，如果使用type属性则使用byType的自动注入策略。如果都没有指定，则通过反射机制使用byName自动注入策略。</p><pre><code class="java">　　示例:　　@Resource (name= &quot;baseDao&quot; )private BaseDao baseDao;</code></pre><h2 id="ControllerAdvice"><a href="#ControllerAdvice" class="headerlink" title="@ControllerAdvice"></a>@ControllerAdvice</h2><p>是一个增强的 Controller。使用这个 Controller ，可以实现三个方面的功能：</p><ol><li>全局异常处理</li><li>全局数据绑定</li><li>全局数据预处理<br>异常集中处理，更好的使业务逻辑与异常处理剥离开；其是对Controller层进行拦截</li></ol><p>注意事项: </p><p>一个Controller下多个@ExceptionHandler上的异常类型不能出现一样的，否则运行时抛异常.</p><p>   @ExceptionHandler method mapped for;</p><p>@ExceptionHandler下方法返回值类型支持多种，常见的ModelAndView，@ResponseBody注解标注，ResponseEntity等类型都OK.</p><pre><code class="java">　　示例:　　@ControllerAdvice@ResponseBodypublic class ErrorController &#123;&#125;</code></pre><h2 id="ExceptionHandler"><a href="#ExceptionHandler" class="headerlink" title="@ExceptionHandler"></a>@ExceptionHandler</h2><p>统一处理某一类异常，从而能够减少代码重复率和复杂度</p><pre><code class="java">　　示例:　　/** * 统一错误处理 */@ControllerAdvice@ResponseBodypublic class ErrorController &#123;    @ExceptionHandler(Exception.class)    public RequestResults customException(Exception e) &#123;        RequestResults results = new RequestResults();        results.setCode(RequestStatusEnum.SYSTEM_ERROR.getCode());        results.setMsg(RequestStatusEnum.SYSTEM_ERROR.getMsg());        results.setSystemErrorMsg(e.getMessage());        results.setData(e.getMessage());        e.printStackTrace();        return results;    &#125;&#125;</code></pre><h2 id="ResponseStatus"><a href="#ResponseStatus" class="headerlink" title="@ResponseStatus"></a>@ResponseStatus</h2><p>可以将某种异常映射为HTTP状态码</p><h2 id="AOP注解："><a href="#AOP注解：" class="headerlink" title="AOP注解："></a>AOP注解：</h2><h3 id="Aspect"><a href="#Aspect" class="headerlink" title="@Aspect"></a>@Aspect</h3><p>作用是把当前类标识为一个切面供容器读取</p><h3 id="Before"><a href="#Before" class="headerlink" title="@Before"></a>@Before</h3><p>前置通知, 在方法执行之前执行</p><h3 id="After"><a href="#After" class="headerlink" title="@After"></a>@After</h3><p>后置通知, 在方法执行之后执行，final增强，不管是抛出异常或者正常退出都会执行</p><h3 id="AfterRunning"><a href="#AfterRunning" class="headerlink" title="@AfterRunning"></a>@AfterRunning</h3><p>返回通知, 在方法返回结果之后执行，方法正常退出时执行</p><h3 id="AfterThrowing"><a href="#AfterThrowing" class="headerlink" title="@AfterThrowing"></a>@AfterThrowing</h3><p>异常通知, 在方法抛出异常之后<br>异常抛出增强，相当于ThrowsAdvice</p><h3 id="Around"><a href="#Around" class="headerlink" title="@Around"></a>@Around</h3><p>环绕通知, 围绕着方法执行</p><pre><code class="java">@Aspect@Componentpublic class PostDataAspect &#123;    @Pointcut(&quot;@annotation(org.springframework.web.bind.annotation.PostMapping)&quot;)    public void post() &#123;&#125;    @Around(&quot;post()&quot;)    public Object around(ProceedingJoinPoint point) throws Throwable &#123;        //执行方法        Object result = point.proceed();        //如果是返回的结果对象就直接返回        if(result instanceof RequestResults)&#123;            return result;        &#125;        return RequestResults.success(result);    &#125;&#125;</code></pre><h3 id="Pointcut"><a href="#Pointcut" class="headerlink" title="@Pointcut"></a>@Pointcut</h3><p>Pointcut是植入Advice的触发条件。每个Pointcut的定义包括2部分，一是表达式，二是方法签名。方法签名必须是 public及void型。可以将Pointcut中的方法看作是一个被Advice引用的助记符，因为表达式不直观，因此我们可以通过方法签名的方式为 此表达式命名。因此Pointcut中的方法只需要方法签名，而不需要在方法体内编写实际代码。<br>切入点，指定说明匹配的方法或者是</p><pre><code class="java">　　示例:    定义了这个    @Pointcut(&quot;@annotation(org.springframework.web.bind.annotation.PostMapping)&quot;)    public void post() &#123;&#125;　    定义了这个包及子包下面的所有方法进行匹配    public class AopPointcutClass &#123;    @Pointcut(&quot;execution(* com.spring.service..*(..))&quot;)    public void logsMean()&#123;&#125;    &#125;</code></pre><h3 id="AfterReturning"><a href="#AfterReturning" class="headerlink" title="@AfterReturning"></a>@AfterReturning</h3><p>后置增强，相当于AfterReturningAdvice，方法正常退出时执行</p><h2 id="ComponentScan"><a href="#ComponentScan" class="headerlink" title="@ComponentScan"></a>@ComponentScan</h2><h2 id><a href="#" class="headerlink" title></a></h2><pre><code class="java">　　示例:1.创建一个配置类，在配置类上添加 @ComponentScan 注解。该注解默认会扫描该类所在的包下所有的配置类，相当于之前的 &lt;context:component-scan&gt;。@ComponentScan(value = &quot;io.mieux.controller&quot;)public class BeanConfig &#123;&#125;2.basePackageClasses属性会去扫描类所在包下的所有组件，而不是指定某个组件！  basePackages 属性会扫描指定的组件，也可以指定当前类的所有组件  但得写成：&lt;context:component-scan base-package=&quot;controller.**&quot; /&gt;  综上， 可以分析出，**匹配任意class文件和包，而*只能匹配包，因此无法扫描到包下的类，因此也就无法被Spring管理。@ComponentScan(basePackages = &#123;        &quot;com.common.config.aspect&quot;        ,&quot;com.common.config.error&quot;        ,&quot;com.common.config.auth&quot;&#125;, basePackageClasses = AutoDriveApplication.class)</code></pre><h2 id="Primary"><a href="#Primary" class="headerlink" title="@Primary"></a>@Primary</h2><p>@Primary 告诉spring 在犹豫的时候优先选择哪一个具体的实现。<br>在有多个实现类在自动注入时，</p><h2 id="PostConstruct"><a href="#PostConstruct" class="headerlink" title="@PostConstruct"></a>@PostConstruct</h2><p>用于指定初始化方法（用在方法上）</p><h2 id="PreDestory"><a href="#PreDestory" class="headerlink" title="@PreDestory"></a>@PreDestory</h2><p>用于指定销毁方法（用在方法上）</p><h2 id="EnableWebSocket"><a href="#EnableWebSocket" class="headerlink" title="@EnableWebSocket"></a>@EnableWebSocket</h2><h2 id="ServerEndpoint"><a href="#ServerEndpoint" class="headerlink" title="@ServerEndpoint"></a>@ServerEndpoint</h2><p>这个注解指明了Encoders and Decoders（编码器和解码器）</p><h2 id="KafkaListener"><a href="#KafkaListener" class="headerlink" title="@KafkaListener"></a>@KafkaListener</h2><p>@KafkaListeners是@KafkaListener的Container Annotation，这也是jdk8的新特性之一，注解可以重复标注。</p><pre><code class="java">@KafkaListener(topics = &#123; &quot;topic1&quot;,&quot;topic2&quot;,&quot;topic3&quot;&#125;)    public void listen(ConsumerRecord&lt;?, ?&gt; consumerRecord) &#123;    &#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> Java框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java框架 </tag>
            
            <tag> Spring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring框架学习</title>
      <link href="/java/spring/spring-de-jian-dan-shi-yong-kuang-jia-er/"/>
      <url>/java/spring/spring-de-jian-dan-shi-yong-kuang-jia-er/</url>
      
        <content type="html"><![CDATA[<h1 id="Spring框架概述"><a href="#Spring框架概述" class="headerlink" title="Spring框架概述"></a>Spring框架概述</h1><h3 id="Spring是什么"><a href="#Spring是什么" class="headerlink" title="Spring是什么"></a>Spring是什么</h3><p>Spring是分层的Java se&#x2F;EE 应用full-stack轻量级开源框架，以及ioc(控制反转)和AOP（面向切面编程）为内核，提供了展现层SpringMVC和持久层Spring JDBC以及业务层事务管理等众多的企业级应用技术，还能整合开源世界众多著名的第三方框架和类库。逐渐成为使用最多的Java EE企业应用开源框架</p><h3 id="Spring两大核心"><a href="#Spring两大核心" class="headerlink" title="Spring两大核心"></a>Spring两大核心</h3><p>IOC和AOP</p><h3 id="Spring的优势"><a href="#Spring的优势" class="headerlink" title="Spring的优势"></a>Spring的优势</h3><ol><li>方便解耦，简化开发</li><li>声明式事务的支持</li><li>方便程序的测试</li><li>方便集成各种优秀框架</li><li>降低javaEE API使用难度</li></ol><h3 id="Spring体系结构"><a href="#Spring体系结构" class="headerlink" title="Spring体系结构"></a>Spring体系结构</h3><p><img src="https://img-blog.csdnimg.cn/20201125203447667.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="程序的耦合及解耦"><a href="#程序的耦合及解耦" class="headerlink" title="程序的耦合及解耦"></a>程序的耦合及解耦</h3><p>使用反射来创建对象，而避免使用new关键字，spring框架主要就是通过ioc来达到解耦效果的，而mysql是用来封装各种操作，将过程简化(个人理解)</p><h3 id="曾经案例中问题"><a href="#曾经案例中问题" class="headerlink" title="曾经案例中问题"></a>曾经案例中问题</h3><p>业务层调用持久层，都会new一个实例，这是强耦合的关系，如何解决这个问题</p><h3 id="工厂模式解耦"><a href="#工厂模式解耦" class="headerlink" title="工厂模式解耦"></a>工厂模式解耦</h3><p>使用这种方式，可以减少他们的依赖，使用类加载器来进行对象加载，减少他们的耦合度，工厂模式有单例模式和多例模式两种，<br>使用工厂模式有多利和单例两种模式，单例模式中属性尽量保证他们的范围是方法范围而不是类的范围，否则后面可能有产生异常<br>使用类加载器配置文件可以是xml和properties的</p><pre><code class="java">/** * 一个创建Bean对象的工厂 * * Bean：在计算机英语中，有可重用组件的含义。 * JavaBean：用java语言编写的可重用组件。 *      javabean &gt;  实体类 * *   它就是创建我们的service和dao对象的。 * *   第一个：需要一个配置文件来配置我们的service和dao *           配置的内容：唯一标识=全限定类名（key=value) *   第二个：通过读取配置文件中配置的内容，反射创建对象 * *   我的配置文件可以是xml也可以是properties */public class BeanFactory &#123;    //定义一个Properties对象    private static Properties props;    //定义一个Map,用于存放我们要创建的对象。我们把它称之为容器    private static Map&lt;String,Object&gt; beans;    //使用静态代码块为Properties对象赋值    static &#123;        try &#123;            //实例化对象            props = new Properties();            //获取properties文件的流对象            InputStream in = BeanFactory.class.getClassLoader().getResourceAsStream(&quot;bean.properties&quot;);            props.load(in);            //实例化容器            beans = new HashMap&lt;String,Object&gt;();            //取出配置文件中所有的Key            Enumeration keys = props.keys();            //遍历枚举            while (keys.hasMoreElements())&#123;                //取出每个Key                String key = keys.nextElement().toString();                //根据key获取value                String beanPath = props.getProperty(key);                //反射创建对象                Object value = Class.forName(beanPath).newInstance();                //把key和value存入容器中                beans.put(key,value);            &#125;        &#125;catch(Exception e)&#123;            throw new ExceptionInInitializerError(&quot;初始化properties失败！&quot;);        &#125;    &#125;    /**     * 根据bean的名称获取对象     * @param beanName     * @return     */    public static Object getBean(String beanName)&#123;        return beans.get(beanName);    &#125;&#125;</code></pre><h1 id="IOC"><a href="#IOC" class="headerlink" title="IOC"></a>IOC</h1><h2 id="IOC概念和spring中的IOC"><a href="#IOC概念和spring中的IOC" class="headerlink" title="IOC概念和spring中的IOC"></a>IOC概念和spring中的IOC</h2><pre><code>控制反转，原来app和资源之间都是直接联系的，二控制反转引入后，由工厂来和资源联系并把资源转到app中，实现了应用和资源之间的关系带来的好处减少了程序之间的耦合</code></pre><h2 id="Spring中基于Xml的IOC环境搭建"><a href="#Spring中基于Xml的IOC环境搭建" class="headerlink" title="Spring中基于Xml的IOC环境搭建"></a>Spring中基于Xml的IOC环境搭建</h2><ol><li><p>先导入spring的依赖</p><pre><code class="xml"> &lt;dependencies&gt;     &lt;dependency&gt;         &lt;groupId&gt;org.springframework&lt;/groupId&gt;         &lt;artifactId&gt;spring-context&lt;/artifactId&gt;         &lt;version&gt;5.0.2.RELEASE&lt;/version&gt;     &lt;/dependency&gt; &lt;/dependencies&gt;</code></pre></li><li><p>在spring.xml中配置</p></li></ol><p>使用时需要在xml配置文件中添加的头部文件</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;       xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans   http://www.springframework.org/schema/beans/spring-beans.xsd   http://www.springframework.org/schema/context   http://www.springframework.org/schema/context/spring-context.xsd   http://www.springframework.org/schema/aop   http://www.springframework.org/schema/aop/spring-aop.xsd   http://www.springframework.org/schema/tx   http://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt;      &lt;!--开启注解的扫描，希望处理service和dao，controller不需要Spring框架去处理--&gt;    &lt;context:component-scan base-package=&quot;cn.itcast&quot; &gt;        &lt;!--配置哪些注解不扫描--&gt;        &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot; /&gt;    &lt;/context:component-scan&gt;        &lt;!--配置Spring框架声明式事务管理--&gt;    &lt;!--配置事务管理器--&gt;    &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt;        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;    &lt;/bean&gt;    &lt;!--配置事务通知--&gt;    &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt;        &lt;tx:attributes&gt;            &lt;tx:method name=&quot;find*&quot; read-only=&quot;true&quot;/&gt;            &lt;tx:method name=&quot;*&quot; isolation=&quot;DEFAULT&quot;/&gt;        &lt;/tx:attributes&gt;    &lt;/tx:advice&gt;    &lt;!--配置AOP增强--&gt;    &lt;aop:config&gt;        &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut=&quot;execution(* cn.itcast.service.impl.*ServiceImpl.*(..))&quot;/&gt;    &lt;/aop:config&gt;    &lt;/beans&gt;</code></pre><ol start="3"><li>获取到spring的ioc核心容器</li></ol><pre><code class="java">/**     * 获取spring的Ioc核心容器，并根据id获取对象     *     * ApplicationContext的三个常用实现类：     *      ClassPathXmlApplicationContext：它可以加载类路径下的配置文件，要求配置文件必须在类路径下。不在的话，加载不了。(更常用)     *      FileSystemXmlApplicationContext：它可以加载磁盘任意路径下的配置文件(必须有访问权限）     *     *      AnnotationConfigApplicationContext：它是用于读取注解创建容器的，前两个是指定配置文件路径的，这个是配置注解的方式     *     * 核心容器的两个接口引发出的问题：     *  ApplicationContext:     单例对象适用              采用此接口     *      它在构建核心容器时，创建对象采取的策略是采用立即加载的方式。也就是说，只要一读取完配置文件马上就创建配置文件中配置的对象。     *     *  BeanFactory:            多例对象使用     *      它在构建核心容器时，创建对象采取的策略是采用延迟加载的方式。也就是说，什么时候根据id获取对象了，什么时候才真正的创建对象。     * @param args     */        //--------ApplicationContext----------    public static void main(String[] args) &#123;        //1.获取核心容器对象        ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;);//        ApplicationContext ac = new FileSystemXmlApplicationContext(&quot;C:\\Users\\zhy\\Desktop\\bean.xml&quot;);        //2.根据id获取Bean对象        IAccountService as  = (IAccountService)ac.getBean(&quot;accountService&quot;);        IAccountDao adao = ac.getBean(&quot;accountDao&quot;,IAccountDao.class);        System.out.println(as);        System.out.println(adao);        as.saveAccount();        //--------BeanFactory----------//        Resource resource = new ClassPathResource(&quot;bean.xml&quot;);//        BeanFactory factory = new XmlBeanFactory(resource);//        IAccountService as  = (IAccountService)factory.getBean(&quot;accountService&quot;);//        System.out.println(as);    &#125;</code></pre><h2 id="Spring核心容器创建-两个接口的区别"><a href="#Spring核心容器创建-两个接口的区别" class="headerlink" title="Spring核心容器创建 两个接口的区别"></a>Spring核心容器创建 两个接口的区别</h2><p><strong>ApplicationContext</strong><br>构建核心容器，采用的策略是采用立即加载的方式，单例对象适用接口<br>更多的是采用此接口</p><p><strong>BeanFactory</strong><br>构建核心容器时，采用的是延迟加载的方式。多例对象适用接口</p><pre><code class="xml">&lt;!--把对象的创建交给spring来管理--&gt;    spring对bean的管理细节    1.创建bean的三种方式    2.bean对象的作用范围    3.bean对象的生命周期</code></pre><h2 id="依赖注入-dependency-injection"><a href="#依赖注入-dependency-injection" class="headerlink" title="依赖注入(dependency injection)"></a>依赖注入(dependency injection)</h2><pre><code class="xml"> spring中的依赖注入        依赖注入：            Dependency Injection        IOC的作用：            降低程序间的耦合（依赖关系）        依赖关系的管理：            以后都交给spring来维护        在当前类需要用到其他类的对象，由spring为我们提供，我们只需要在配置文件中说明        依赖关系的维护：            就称之为依赖注入。         依赖注入：            能注入的数据：有三类                基本类型和String                其他bean类型（在配置文件中或者注解配置过的bean）                复杂类型/集合类型             注入的方式：有三种                第一种：使用构造函数提供                第二种：使用set方法提供                第三种：使用注解提供</code></pre><h2 id="三种注入方式"><a href="#三种注入方式" class="headerlink" title="三种注入方式"></a>三种注入方式</h2><h3 id="bean标签的属性"><a href="#bean标签的属性" class="headerlink" title="bean标签的属性"></a>bean标签的属性</h3><p>bean的作用范围调整<br>    bean标签的  <strong>scope</strong>  属性：<br>        作用：用于指定  bean 的作用范围<br>        取值： 常用的就是单例的和多例的<br>            <strong>singleton</strong>：单例的（默认值）<br>            <strong>prototype</strong>：多例的<br>            <strong>request</strong>：作用于web应用的请求范围<br>            <strong>session</strong>：作用于web应用的会话范围<br>            <strong>global-session</strong>：作用于集群环境的会话范围（全局会话范围），当不是集群环境时，它就是session</p><pre><code class="xml">&lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot; scope=&quot;prototype&quot;&gt;&lt;/bean&gt;</code></pre><h4 id="bean对象的生命周期"><a href="#bean对象的生命周期" class="headerlink" title="bean对象的生命周期"></a>bean对象的生命周期</h4><p><strong>单例对象</strong></p><pre><code>        出生：当容器创建时对象出生        活着：只要容器还在，对象一直活着        死亡：容器销毁，对象消亡        总结：单例对象的生命周期和容器相同</code></pre><p><strong>多例对象</strong></p><pre><code>        出生：当我们使用对象时spring框架为我们创建        活着：对象只要是在使用过程中就一直活着。        死亡：当对象长时间不用，且没有别的对象引用时，由Java的垃圾回收器回收</code></pre><pre><code class="xml">&lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;      scope=&quot;prototype&quot; init-method=&quot;init&quot; destroy-method=&quot;destroy&quot;&gt;&lt;/bean&gt;</code></pre><p>初始化方法和关闭方法</p><h3 id="1、构造函数注入"><a href="#1、构造函数注入" class="headerlink" title="1、构造函数注入"></a>1、构造函数注入</h3><pre><code class="xml">&lt;!--构造函数注入：        使用的标签:constructor-arg        标签出现的位置：bean标签的内部        标签中的属性            type：用于指定要注入的数据的数据类型，该数据类型也是构造函数中某个或某些参数的类型            index：用于指定要注入的数据给构造函数中指定索引位置的参数赋值。索引的位置是从0开始            name：用于指定给构造函数中指定名称的参数赋值                                        常用的            =============以上三个用于指定给构造函数中哪个参数赋值===============================            value：用于提供基本类型和String类型的数据            ref：用于指定其他的bean类型数据。它指的就是在spring的Ioc核心容器中出现过的bean对象        优势：            在获取bean对象时，注入数据是必须的操作，否则对象无法创建成功。        弊端：            改变了bean对象的实例化方式，使我们在创建对象时，如果用不到这些数据，也必须提供。    --&gt;    &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt;        &lt;constructor-arg name=&quot;name&quot; value=&quot;泰斯特&quot;&gt;&lt;/constructor-arg&gt;        &lt;constructor-arg name=&quot;age&quot; value=&quot;18&quot;&gt;&lt;/constructor-arg&gt;        &lt;constructor-arg name=&quot;birthday&quot; ref=&quot;now&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt;    &lt;!-- 配置一个日期对象 --&gt;    &lt;bean id=&quot;now&quot; class=&quot;java.util.Date&quot;&gt;&lt;/bean&gt;//bean中通过ref来引用外部的bean</code></pre><h3 id="2、set方法注入"><a href="#2、set方法注入" class="headerlink" title="2、set方法注入"></a>2、set方法注入</h3><pre><code class="xml">  &lt;!-- set方法注入                更常用的方式        涉及的标签：property        出现的位置：bean标签的内部        标签的属性            name：用于指定注入时所调用的set方法名称            value：用于提供基本类型和String类型的数据            ref：用于指定其他的bean类型数据。它指的就是在spring的Ioc核心容器中出现过的bean对象        优势：            创建对象时没有明确的限制，可以直接使用默认构造函数        弊端：            如果有某个成员必须有值，则获取对象是有可能set方法没有执行。    --&gt;        &lt;bean id=&quot;accountService2&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl2&quot;&gt;        &lt;property name=&quot;name&quot; value=&quot;TEST&quot; &gt;&lt;/property&gt;        &lt;property name=&quot;age&quot; value=&quot;21&quot;&gt;&lt;/property&gt;        &lt;property name=&quot;birthday&quot; ref=&quot;now&quot;&gt;&lt;/property&gt;    &lt;/bean&gt;    &lt;!-- 配置一个日期对象 --&gt;    &lt;bean id=&quot;now&quot; class=&quot;java.util.Date&quot;&gt;&lt;/bean&gt;    //这里使用的是property这个标签来进行</code></pre><h3 id="3、复杂类型的依赖注入"><a href="#3、复杂类型的依赖注入" class="headerlink" title="3、复杂类型的依赖注入"></a>3、复杂类型的依赖注入</h3><pre><code class="xml">&lt;!-- 复杂类型的注入/集合类型的注入        用于给List结构集合注入的标签：            list array set        用于个Map结构集合注入的标签:            map  props        结构相同，标签可以互换    --&gt;        &lt;bean id=&quot;accountService3&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl3&quot;&gt;        &lt;property name=&quot;myStrs&quot;&gt;            &lt;set&gt;                &lt;value&gt;AAA&lt;/value&gt;                &lt;value&gt;BBB&lt;/value&gt;                &lt;value&gt;CCC&lt;/value&gt;            &lt;/set&gt;        &lt;/property&gt;        &lt;property name=&quot;myList&quot;&gt;            &lt;array&gt;                &lt;value&gt;AAA&lt;/value&gt;                &lt;value&gt;BBB&lt;/value&gt;                &lt;value&gt;CCC&lt;/value&gt;            &lt;/array&gt;        &lt;/property&gt;        &lt;property name=&quot;mySet&quot;&gt;            &lt;list&gt;                &lt;value&gt;AAA&lt;/value&gt;                &lt;value&gt;BBB&lt;/value&gt;                &lt;value&gt;CCC&lt;/value&gt;            &lt;/list&gt;        &lt;/property&gt;        &lt;property name=&quot;myMap&quot;&gt;            &lt;props&gt;                &lt;prop key=&quot;testC&quot;&gt;ccc&lt;/prop&gt;                &lt;prop key=&quot;testD&quot;&gt;ddd&lt;/prop&gt;            &lt;/props&gt;        &lt;/property&gt;        &lt;property name=&quot;myProps&quot;&gt;            &lt;map&gt;                &lt;entry key=&quot;testA&quot; value=&quot;aaa&quot;&gt;&lt;/entry&gt;                &lt;entry key=&quot;testB&quot;&gt;                    &lt;value&gt;BBB&lt;/value&gt;                &lt;/entry&gt;            &lt;/map&gt;        &lt;/property&gt;    &lt;/bean&gt;</code></pre><h2 id="创建Bean的三种方式"><a href="#创建Bean的三种方式" class="headerlink" title="创建Bean的三种方式"></a>创建Bean的三种方式</h2><h3 id="第一种方式：使用默认构造函数创建。"><a href="#第一种方式：使用默认构造函数创建。" class="headerlink" title="第一种方式：使用默认构造函数创建。"></a>第一种方式：使用默认构造函数创建。</h3><p>在spring的配置文件中使用bean标签，配以id和class属性之后，且没有其他属性和标签时。<br>采用的就是默认构造函数创建bean对象，此时如果类中没有默认构造函数，则对象无法创建。</p><pre><code class="xml">&lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt;&lt;/bean&gt;</code></pre><h3 id="第二种方式：-使用普通工厂中的方法创建对象（使用某个类中的方法创建对象，并存入spring容器）"><a href="#第二种方式：-使用普通工厂中的方法创建对象（使用某个类中的方法创建对象，并存入spring容器）" class="headerlink" title="第二种方式： 使用普通工厂中的方法创建对象（使用某个类中的方法创建对象，并存入spring容器）"></a>第二种方式： 使用普通工厂中的方法创建对象（使用某个类中的方法创建对象，并存入spring容器）</h3><pre><code class="xml">&lt;bean id=&quot;instanceFactory&quot; class=&quot;com.itheima.factory.InstanceFactory&quot;&gt;&lt;/bean&gt;&lt;bean id=&quot;accountService&quot; factory-bean=&quot;instanceFactory&quot; factory-method=&quot;getAccountService&quot;&gt;&lt;/bean&gt;</code></pre><h3 id="第三种方式：使用工厂中的静态方法创建对象（使用某个类中的静态方法创建对象，并存入spring容器"><a href="#第三种方式：使用工厂中的静态方法创建对象（使用某个类中的静态方法创建对象，并存入spring容器" class="headerlink" title="第三种方式：使用工厂中的静态方法创建对象（使用某个类中的静态方法创建对象，并存入spring容器)"></a>第三种方式：使用工厂中的静态方法创建对象（使用某个类中的静态方法创建对象，并存入spring容器)</h3><pre><code class="xml">&lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.factory.StaticFactory&quot; factory-method=&quot;getAccountService&quot;&gt;&lt;/bean&gt;</code></pre><h2 id="Spring中ioc常用注解"><a href="#Spring中ioc常用注解" class="headerlink" title="Spring中ioc常用注解"></a>Spring中ioc常用注解</h2><h3 id="在Spring容器中注入类的："><a href="#在Spring容器中注入类的：" class="headerlink" title="在Spring容器中注入类的："></a>在Spring容器中注入类的：</h3><h4 id="Component"><a href="#Component" class="headerlink" title="@Component:"></a>@Component:</h4><ul><li><p>作用：用于把当前类对象存入spring容器中</p></li><li><p>属性：</p></li><li><p>value：用于指定bean的id。当我们不写时，它的默认值是当前类名，且首字母改小写。</p></li></ul><h4 id="Controller："><a href="#Controller：" class="headerlink" title="@Controller："></a>@Controller：</h4><p>一般用在表现层</p><h4 id="Service："><a href="#Service：" class="headerlink" title="@Service："></a>@Service：</h4><p>一般用在业务层</p><h4 id="Repository："><a href="#Repository：" class="headerlink" title="@Repository："></a>@Repository：</h4><p>一般用在持久层</p><p>以上三个注解他们的作用和属性与Component是一模一样。</p><p>他们三个是spring框架为我们提供明确的三层使用的注解，使我们的三层对象更加清晰<br>当不属于这三层的就可以使用component来调用</p><h3 id="用于注入数据的："><a href="#用于注入数据的：" class="headerlink" title="用于注入数据的："></a>用于注入数据的：</h3><ul><li><pre><code> 他们的作用就和在xml配置文件中的bean标签中写一个&lt;property&gt;标签的作用是一样的</code></pre></li></ul><h4 id="Autowired"><a href="#Autowired" class="headerlink" title="@Autowired:"></a>@Autowired:</h4><ul><li>作用：自动按照类型注入。只要容器中有唯一的一个bean对象类型和要注入的变量类型匹配，就可以注入成功</li></ul><p>如果ioc容器中没有任何bean的类型和要注入的变量类型匹配，则报错。</p><p>如果Ioc容器中有多个类型匹配时：</p><ul><li>出现位置：</li></ul><p>可以是变量上，也可以是方法上</p><ul><li>细节：</li></ul><p>在使用注解注入时，set方法就不是必须的了。</p><h4 id="Qualifier"><a href="#Qualifier" class="headerlink" title="@Qualifier:"></a>@Qualifier:</h4><ul><li><p>作用：</p><p>在按照类中注入的基础之上再按照名称注入。它在给类成员注入时不能单独使用。但是在给方法参数注入时可以（稍后我们讲）</p></li><li><p>属性：</p></li></ul><p>value：用于指定注入bean的id。</p><h4 id="Resource"><a href="#Resource" class="headerlink" title="@Resource"></a>@Resource</h4><ul><li><p>作用：直接按照bean的id注入。它可以独立使用</p></li><li><p>属性：</p></li></ul><p>name：用于指定bean的id。</p><p>以上三个注入都只能注入其他bean类型的数据，而基本类型和String类型无法使用上述注解实现。</p><p>   另外，集合类型的注入只能通过XML来实现。</p><h4 id="Value"><a href="#Value" class="headerlink" title="@Value"></a>@Value</h4><ul><li><p>作用：用于注入基本类型和String类型的数据</p></li><li><p>属性：</p></li></ul><p>value：用于指定数据的值。它可以使用spring中SpEL(也就是spring的el表达式）</p><p>SpEL的写法：${表达式}   </p><h3 id="用于改变作用范围的："><a href="#用于改变作用范围的：" class="headerlink" title="用于改变作用范围的："></a>用于改变作用范围的：</h3><p>他们的作用就和在bean标签中使用scope属性实现的功能是一样的</p><h4 id="Scope"><a href="#Scope" class="headerlink" title="@Scope"></a>@Scope</h4><ul><li><p>作用：用于指定bean的作用范围</p></li><li><p>属性：</p></li></ul><p>value：指定范围的取值。常用取值：singleton prototype</p><h3 id="和生命周期相关："><a href="#和生命周期相关：" class="headerlink" title="和生命周期相关："></a>和生命周期相关：</h3><p>他们的作用就和在bean标签中使用init-method和destroy-methode的作用是一样的</p><h4 id="PreDestroy"><a href="#PreDestroy" class="headerlink" title="@PreDestroy"></a>@PreDestroy</h4><p><strong>作用</strong>：用于指定销毁方法</p><h4 id="PostConstruct"><a href="#PostConstruct" class="headerlink" title="@PostConstruct"></a>@PostConstruct</h4><p><strong>作用</strong>：用于指定初始化方法 </p><h2 id="Spring一些新注解的使用"><a href="#Spring一些新注解的使用" class="headerlink" title="Spring一些新注解的使用"></a>Spring一些新注解的使用</h2><h4 id="Configuration"><a href="#Configuration" class="headerlink" title="@Configuration"></a>@Configuration</h4><ul><li><pre><code>作用：指定当前类是一个配置类</code></pre></li><li><pre><code>细节：当配置类作为AnnotationConfigApplicationContext对象创建的参数时，该注解可以不写。</code></pre></li></ul><h4 id="ComponentScan"><a href="#ComponentScan" class="headerlink" title="@ComponentScan"></a>@ComponentScan</h4><ul><li><pre><code> 作用：用于通过注解指定spring在创建容器时要扫描的包(范围)</code></pre></li><li><pre><code> 属性：</code></pre></li><li><pre><code> value：它和basePackages的作用是一样的，都是用于指定创建容器时要扫描的包。</code></pre></li><li><pre><code> 我们使用此注解就等同于在xml中配置了:</code></pre></li><li><pre><code> &lt;context:component-scan base-package=&quot;com.itheima&quot;&gt;&lt;/context:component-scan&gt;</code></pre></li></ul><h4 id="Bean"><a href="#Bean" class="headerlink" title="@Bean"></a>@Bean</h4><ul><li><pre><code> 作用：用于把当前方法的返回值作为bean对象存入spring的ioc容器中</code></pre></li><li><pre><code> 属性:</code></pre></li><li><pre><code> name:用于指定bean的id。当不写时，默认值是当前方法的名称</code></pre></li><li><pre><code> 细节：</code></pre></li><li><pre><code> 当我们使用注解配置方法时，如果方法有参数，spring框架会去容器中查找有没有可用的bean对象。</code></pre></li><li><pre><code> 查找的方式和Autowired注解的作用是一样的</code></pre></li></ul><h4 id="Import"><a href="#Import" class="headerlink" title="@Import"></a>@Import</h4><ul><li><pre><code> 作用：用于导入其他的配置类</code></pre></li><li><pre><code> 属性：</code></pre></li><li><pre><code> value：用于指定其他配置类的字节码。</code></pre></li><li><pre><code> 当我们使用Import的注解之后，有Import注解的类就父配置类，而导入的都是子配置类</code></pre></li></ul><h4 id="PropertySource"><a href="#PropertySource" class="headerlink" title="@PropertySource"></a>@PropertySource</h4><ul><li><pre><code> 作用：用于指定properties文件的位置</code></pre></li><li><pre><code> 属性：</code></pre></li><li><pre><code> value：指定文件的名称和路径。</code></pre></li><li><pre><code> 关键字：classpath，表示类路径下</code></pre></li></ul><pre><code class="java">//@Configuration@ComponentScan(&quot;com.itheima&quot;)@Import(JdbcConfig.class)@PropertySource(&quot;classpath:jdbcConfig.properties&quot;)public class SpringConfiguration &#123;&#125;</code></pre><h2 id="使用纯注解的方式实现ioc案例"><a href="#使用纯注解的方式实现ioc案例" class="headerlink" title="使用纯注解的方式实现ioc案例"></a>使用纯注解的方式实现ioc案例</h2><pre><code>@ComponentScan(&quot;com.itheima&quot;)@Import(JdbcConfig.class)@PropertySource(&quot;classpath:jdbcConfig.properties&quot;)public class SpringConfiguration &#123;&#125;</code></pre><h2 id="DBUtils"><a href="#DBUtils" class="headerlink" title="DBUtils"></a>DBUtils</h2><h3 id="DBUtils是什么，有什么作用？"><a href="#DBUtils是什么，有什么作用？" class="headerlink" title="DBUtils是什么，有什么作用？"></a>DBUtils是什么，有什么作用？</h3><p>持久层选择DBUtils</p><p>DBUtils简化了JDBC的开发步骤，使得我们可以用更少量的代码实现连接数据库的功能</p><p>JavaBean是一个用于封装数据的类，在与数据库连接之中，JavaBean其的作用是将获取的数据库的记录封装到JavaBean中。特性如下：</p><ol><li>需要实现接口：java.io.Serializable ，可以省略不写。</li><li>提供私有字段：private 类型 字段名;</li><li>提供getter&#x2F;setter方法：</li><li>提供无参构造</li></ol><p>获取getter&#x2F;setter方法，在类中右键-&gt;Source-&gt;Generate Getters and Setters</p><h3 id="DBUtils使用"><a href="#DBUtils使用" class="headerlink" title="DBUtils使用"></a>DBUtils使用</h3><p>DBUtils封装了JDBC的操作，核心功能如下：<br>Dbutils三个核心功能介绍</p><ol><li>QueryRunner中提供对sql语句操作的API.</li><li>ResultSetHandler接口，用于定义select操作后，怎样封装结果集.</li><li>DbUtils类是一个工具类，定义了关闭资源与事务处理的方法</li></ol><h3 id="QueryRunner核心类："><a href="#QueryRunner核心类：" class="headerlink" title="QueryRunner核心类："></a>QueryRunner核心类：</h3><ol><li>QueryRunner(DataSource ds) ；传入参数为连接池</li><li>update(String sql, Object… params) ，执行insert update delete操作</li><li>query(String sql, ResultSetHandler rsh, Object… params) ，执行 select操作</li></ol><h2 id="spring和junit的整合"><a href="#spring和junit的整合" class="headerlink" title="spring和junit的整合"></a>spring和junit的整合</h2><pre><code class="java">/** * 使用Junit单元测试：测试我们的配置 * Spring整合junit的配置 *      1、导入spring整合junit的jar(坐标) *      2、使用Junit提供的一个注解把原有的main方法替换了，替换成spring提供的 *             @Runwith *      3、告知spring的运行器，spring和ioc创建是基于xml还是注解的，并且说明位置 *          @ContextConfiguration *                  locations：指定xml文件的位置，加上classpath关键字，表示在类路径下 *                  classes：指定注解类所在地位置 * *   当我们使用spring 5.x版本的时候，要求junit的jar必须是4.12及以上 */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = SpringConfiguration.class)public class AccountServiceTest &#123;    @Autowired    private IAccountService as = null;    @Test    public void testFindAll() &#123;        //3.执行方法        List&lt;Account&gt; accounts = as.findAllAccount();        for(Account account : accounts)&#123;            System.out.println(account);        &#125;    &#125;    @Test    public void testFindOne() &#123;        //3.执行方法        Account account = as.findAccountById(1);        System.out.println(account);    &#125;</code></pre><h2 id="动态代理"><a href="#动态代理" class="headerlink" title="动态代理"></a>动态代理</h2><p>动态代理没听明白<br>视频：123-131   还需要看java基础的反射和基础知识</p><p><strong>特点</strong>：字节码随用随创建，随用随加载<br><strong>作用</strong>：不修改源码的基础上对方法增强<br><strong>分类</strong>：<br>基于接口的动态代理<br>基于子类的动态代理</p><h3 id="动态代理1：基于接口的动态代理"><a href="#动态代理1：基于接口的动态代理" class="headerlink" title="动态代理1：基于接口的动态代理"></a>动态代理1：基于接口的动态代理</h3><p>基于接口的动态代理：<br>涉及的类：Proxy<br>提供者：JDK官方<br>如何创建代理对象：<br>使用Proxy类中的newProxyInstance方法<br>创建代理对象的要求：</p><p><strong>被代理类最少实现一个接口，如果没有则不能使用</strong></p><p><strong>newProxyInstance</strong>   方法的参数：</p><p>ClassLoader：类加载器<br>    它是用于加载代理对象字节码的。和被代理对象使用相同的类加载器。固定写法。</p><p>Class[]：字节码数组<br>    它是用于让代理对象和被代理对象有相同方法。固定写法。</p><p>InvocationHandler：用于提供增强的代码<br>    它是让我们写如何代理。我们一般都是些一个该接口的实现类，通常情况下都是匿名内部类，但不是必须的。</p><p>此接口的实现类都是谁用谁写。</p><pre><code class="java">IProducer proxyProducer = (IProducer) Proxy.newProxyInstance(producer.getClass().getClassLoader(),                producer.getClass().getInterfaces(),                new InvocationHandler() &#123;                    /**                     * 作用：执行被代理对象的任何接口方法都会经过该方法                     * 方法参数的含义                     * @param proxy   代理对象的引用                     * @param method  当前执行的方法                     * @param args    当前执行方法所需的参数                     * @return        和被代理对象方法有相同的返回值                     * @throws Throwable                     */                    @Override                    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;                        //提供增强的代码                        Object returnValue = null;                        //1.获取方法执行的参数                        Float money = (Float)args[0];                        //2.判断当前方法是不是销售                        if(&quot;saleProduct&quot;.equals(method.getName())) &#123;                            returnValue = method.invoke(producer, money*0.8f);                        &#125;                        return returnValue;                    &#125;                &#125;);        proxyProducer.saleProduct(10000f);</code></pre><h3 id="动态代理2：基于子类的动态代理"><a href="#动态代理2：基于子类的动态代理" class="headerlink" title="动态代理2：基于子类的动态代理"></a>动态代理2：基于子类的动态代理</h3><p><strong>特点</strong>：字节码随用随创建，随用随加载<br><strong>作用</strong>：不修改源码的基础上对方法增强</p><p>基于子类的动态代理：<br>涉及的类：Enhancer<br>提供者：第三方cglib库<br>如何创建代理对象：<br>使用Enhancer类中的create方法<br>创建代理对象的要求：<br>被代理类不能是最终类<br>create方法的参数：</p><ul><li><p>Class：字节码<br>它是用于指定被代理对象的字节码。</p></li><li><p>Callback：用于提供增强的代码<br>它是让我们写如何代理。我们一般都是些一个该接口的实现类，通常情况下都是匿名内部类，但不是必须的。<br> 此接口的实现类都是谁用谁写。</p></li></ul><p>我们一般写的都是该接口的子接口实现类：MethodInterceptor </p><pre><code class="java">Producer cglibProducer = (Producer)Enhancer.create(producer.getClass(), new MethodInterceptor() &#123; @Overridepublic Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123;//提供增强的代码    Object returnValue = null;     //1.获取方法执行的参数     Float money = (Float)args[0];     //2.判断当前方法是不是销售     if(&quot;saleProduct&quot;.equals(method.getName())) &#123;           returnValue = method.invoke(producer, money*0.8f);    &#125;    return returnValue;    &#125;    &#125;);        cglibProducer.saleProduct(12000f);</code></pre><h1 id="AOP"><a href="#AOP" class="headerlink" title="AOP"></a>AOP</h1><p>面向切面编程<br>作用：在程序运行期间，不修改源码的基础上对方法进行增强<br>优势：减少重复代码，提高开发效率，维护方便</p><p>是否实现了接口来判断<br>基于接口和基于子类的两种方式来动态代理</p><h2 id="Spring-Aop相关术语"><a href="#Spring-Aop相关术语" class="headerlink" title="Spring Aop相关术语"></a>Spring Aop相关术语</h2><p><strong>连接点</strong><br>方法就是连接点</p><p><strong>切入点</strong><br>被增强的方法就是切入点<br>所有的切入点都是连接点，但不是所有的连接点都是切入点</p><p><strong>通知和增强</strong><br>环绕通知是指整个方法调用</p><p><strong>四种通知类型</strong><br>前置通知<br>后置通知<br>异常通知<br>最终通知</p><p><strong>目标对象</strong><br>被代理对象</p><p><strong>织入</strong><br>加入增强功能的过程</p><p><strong>代理对象</strong><br>就是增强后的的代理类</p><p><strong>引介</strong><br>建立接入点方法</p><p><strong>切面</strong><br>切入点和通知(引介)的集合<br>就是一个抽象的概念，一步一步的执行，进行切分就是切入</p><h2 id="Spring中AOP配置"><a href="#Spring中AOP配置" class="headerlink" title="Spring中AOP配置"></a>Spring中AOP配置</h2><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;    xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;    xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans        https://www.springframework.org/schema/beans/spring-beans.xsd        http://www.springframework.org/schema/aop        https://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt;    &lt;!-- an HTTP Session-scoped bean exposed as a proxy --&gt;    &lt;bean id=&quot;userPreferences&quot; class=&quot;com.something.UserPreferences&quot; scope=&quot;session&quot;&gt;        &lt;!-- instructs the container to proxy the surrounding bean --&gt;        &lt;aop:scoped-proxy/&gt;     &lt;/bean&gt;    &lt;!-- a singleton-scoped bean injected with a proxy to the above bean --&gt;    &lt;bean id=&quot;userService&quot; class=&quot;com.something.SimpleUserService&quot;&gt;        &lt;!-- a reference to the proxied userPreferences bean --&gt;        &lt;property name=&quot;userPreferences&quot; ref=&quot;userPreferences&quot;/&gt;    &lt;/bean&gt;&lt;/beans&gt;</code></pre><h2 id="Spring中标签的AOP配置"><a href="#Spring中标签的AOP配置" class="headerlink" title="Spring中标签的AOP配置"></a>Spring中标签的AOP配置</h2><pre><code>spring中基于XML的AOP配置步骤        1、把通知Bean也交给spring来管理        2、使用aop:config标签表明开始AOP的配置        3、使用aop:aspect标签表明配置切面                id属性：是给切面提供一个唯一标识                ref属性：是指定通知类bean的Id。        4、在aop:aspect标签的内部使用对应标签来配置通知的类型               我们现在示例是让printLog方法在切入点方法执行之前之前：所以是前置通知               aop:before：表示配置前置通知                    method属性：用于指定Logger类中哪个方法是前置通知                    pointcut属性：用于指定切入点表达式，该表达式的含义指的是对业务层中哪些方法增强</code></pre><h2 id="切入点表达式的写法"><a href="#切入点表达式的写法" class="headerlink" title="切入点表达式的写法"></a>切入点表达式的写法</h2><pre><code class="xml">切入点表达式的写法：                关键字：execution(表达式)                表达式：                    访问修饰符  返回值  包名.包名.包名...类名.方法名(参数列表)                标准的表达式写法：                    public void com.itheima.service.impl.AccountServiceImpl.saveAccount()                访问修饰符可以省略                    void com.itheima.service.impl.AccountServiceImpl.saveAccount()                返回值可以使用通配符，表示任意返回值                    * com.itheima.service.impl.AccountServiceImpl.saveAccount()                包名可以使用通配符，表示任意包。但是有几级包，就需要写几个*.                    * *.*.*.*.AccountServiceImpl.saveAccount())                包名可以使用..表示当前包及其子包                    * *..AccountServiceImpl.saveAccount()                类名和方法名都可以使用*来实现通配                    * *..*.*()                参数列表：                    可以直接写数据类型：                        基本类型直接写名称           int                        引用类型写包名.类名的方式   java.lang.String                    可以使用通配符表示任意类型，但是必须有参数                    可以使用..表示有无参数均可，有参数可以是任意类型                全通配写法：                    * *..*.*(..)                实际开发中切入点表达式的通常写法：                    切到业务层实现类下的所有方法                        * com.itheima.service.impl.*.*(..)                                    &lt;bean id=&quot;logger&quot; class=&quot;com.itheima.utils.Logger&quot;&gt;&lt;/bean&gt;    &lt;!--配置AOP--&gt;    &lt;aop:config&gt;        &lt;!--配置切面 --&gt;        &lt;aop:aspect id=&quot;logAdvice&quot; ref=&quot;logger&quot;&gt;            &lt;!-- 配置通知的类型，并且建立通知方法和切入点方法的关联--&gt;            &lt;aop:before method=&quot;printLog&quot; pointcut=&quot;execution(* com.itheima.service.impl.*.*(..))&quot;&gt;&lt;/aop:before&gt;        &lt;/aop:aspect&gt;    &lt;/aop:config&gt;</code></pre><h2 id="Aop通知的四种类型"><a href="#Aop通知的四种类型" class="headerlink" title="Aop通知的四种类型"></a>Aop通知的四种类型</h2><p>基于XML的aop配置</p><pre><code class="xml">    &lt;bean id=&quot;accountService&quot; class=&quot;com.itheima.service.impl.AccountServiceImpl&quot;&gt;&lt;/bean&gt;    &lt;!-- 配置Logger类 --&gt;    &lt;bean id=&quot;logger&quot; class=&quot;com.itheima.utils.Logger&quot;&gt;&lt;/bean&gt;    &lt;!--配置AOP--&gt;    &lt;aop:config&gt;        &lt;!-- 配置切入点表达式 id属性用于指定表达式的唯一标识。expression属性用于指定表达式内容              此标签写在aop:aspect标签内部只能当前切面使用。              它还可以写在aop:aspect外面，此时就变成了所有切面可用          --&gt;        &lt;aop:pointcut id=&quot;pt1&quot; expression=&quot;execution(* com.itheima.service.impl.*.*(..))&quot;&gt;&lt;/aop:pointcut&gt;        &lt;!--配置切面 --&gt;        &lt;aop:aspect id=&quot;logAdvice&quot; ref=&quot;logger&quot;&gt;            &lt;!-- 配置前置通知：在切入点方法执行之前执行            &lt;aop:before method=&quot;beforePrintLog&quot; pointcut-ref=&quot;pt1&quot; &gt;&lt;/aop:before&gt;--&gt;            &lt;!-- 配置后置通知：在切入点方法正常执行之后值。它和异常通知永远只能执行一个            &lt;aop:after-returning method=&quot;afterReturningPrintLog&quot; pointcut-ref=&quot;pt1&quot;&gt;&lt;/aop:after-returning&gt;--&gt;            &lt;!-- 配置异常通知：在切入点方法执行产生异常之后执行。它和后置通知永远只能执行一个            &lt;aop:after-throwing method=&quot;afterThrowingPrintLog&quot; pointcut-ref=&quot;pt1&quot;&gt;&lt;/aop:after-throwing&gt;--&gt;            &lt;!-- 配置最终通知：无论切入点方法是否正常执行它都会在其后面执行            &lt;aop:after method=&quot;afterPrintLog&quot; pointcut-ref=&quot;pt1&quot;&gt;&lt;/aop:after&gt;--&gt;            &lt;!-- 配置环绕通知 详细的注释请看Logger类中--&gt;            &lt;aop:around method=&quot;aroundPringLog&quot; pointcut-ref=&quot;pt1&quot;&gt;&lt;/aop:around&gt;        &lt;/aop:aspect&gt;    &lt;/aop:config&gt;    环绕通知     * 问题：     *      当我们配置了环绕通知之后，切入点方法没有执行，而通知方法执行了。     * 分析：     *      通过对比动态代理中的环绕通知代码，发现动态代理的环绕通知有明确的切入点方法调用，而我们的代码中没有。     * 解决：     *      Spring框架为我们提供了一个接口：ProceedingJoinPoint。()，此方法就相当于明确调用切入点方法。     *      该接口可以作为环绕通知的方法参数，在程序执行时，spring框架会为我们提供该接口的实现类供我们使用。     *     * spring中的环绕通知：     *      它是spring框架为我们提供的一种可以在代码中手动控制增强方法何时执行的方式。                  public Object aroundPringLog(ProceedingJoinPoint pjp)&#123;        Object rtValue = null;        try&#123;            Object[] args = pjp.getArgs();//得到方法执行所需的参数            System.out.println(&quot;Logger类中的aroundPringLog方法开始记录日志了。。。前置&quot;);            rtValue = pjp.proceed(args);//明确调用业务层方法（切入点方法）            System.out.println(&quot;Logger类中的aroundPringLog方法开始记录日志了。。。后置&quot;);            return rtValue;        &#125;catch (Throwable t)&#123;            System.out.println(&quot;Logger类中的aroundPringLog方法开始记录日志了。。。异常&quot;);            throw new RuntimeException(t);        &#125;finally &#123;            System.out.println(&quot;Logger类中的aroundPringLog方法开始记录日志了。。。最终&quot;);        &#125;    &#125;</code></pre><h2 id="Spring注解的AOP配置"><a href="#Spring注解的AOP配置" class="headerlink" title="Spring注解的AOP配置"></a>Spring注解的AOP配置</h2><pre><code class="java">@Component(&quot;logger&quot;)@Aspect//表示当前类是一个切面类public class Logger &#123;    @Pointcut(&quot;execution(* com.itheima.service.impl.*.*(..))&quot;)    private void pt1()&#123;&#125;    /**     * 前置通知     */    @Before(&quot;pt1()&quot;)    public  void beforePrintLog()&#123;        System.out.println(&quot;前置通知Logger类中的beforePrintLog方法开始记录日志了。。。&quot;);    &#125;    /**     * 后置通知     */    @AfterReturning(&quot;pt1()&quot;)    public  void afterReturningPrintLog()&#123;        System.out.println(&quot;后置通知Logger类中的afterReturningPrintLog方法开始记录日志了。。。&quot;);    &#125;    /**     * 异常通知     */    @AfterThrowing(&quot;pt1()&quot;)    public  void afterThrowingPrintLog()&#123;        System.out.println(&quot;异常通知Logger类中的afterThrowingPrintLog方法开始记录日志了。。。&quot;);    &#125;    /**     * 最终通知     */    @After(&quot;pt1()&quot;)    public  void afterPrintLog()&#123;        System.out.println(&quot;最终通知Logger类中的afterPrintLog方法开始记录日志了。。。&quot;);    &#125;&#125;</code></pre><h2 id="注解的方式实现环绕增强"><a href="#注解的方式实现环绕增强" class="headerlink" title="注解的方式实现环绕增强"></a>注解的方式实现环绕增强</h2><pre><code class="java">@Component(&quot;logger&quot;)@Aspect//表示当前类是一个切面类public class Logger &#123;    @Pointcut(&quot;execution(* com.itheima.service.impl.*.*(..))&quot;)    private void pt1()&#123;&#125;        @Around(&quot;pt1()&quot;)    public Object aroundPringLog(ProceedingJoinPoint pjp)&#123;        Object rtValue = null;        try&#123;            Object[] args = pjp.getArgs();//得到方法执行所需的参数            System.out.println(&quot;Logger类中的aroundPringLog方法开始记录日志了。。。前置&quot;);            rtValue = pjp.proceed(args);//明确调用业务层方法（切入点方法）            System.out.println(&quot;Logger类中的aroundPringLog方法开始记录日志了。。。后置&quot;);            return rtValue;        &#125;catch (Throwable t)&#123;            System.out.println(&quot;Logger类中的aroundPringLog方法开始记录日志了。。。异常&quot;);            throw new RuntimeException(t);        &#125;finally &#123;            System.out.println(&quot;Logger类中的aroundPringLog方法开始记录日志了。。。最终&quot;);        &#125;    &#125;</code></pre><h2 id="注解的方式aop环绕增强启动类"><a href="#注解的方式aop环绕增强启动类" class="headerlink" title="注解的方式aop环绕增强启动类"></a>注解的方式aop环绕增强启动类</h2><pre><code class="java">@EnableAspectJautoProxy@Configuration@ComponentScan(basePackage=&quot;com.eeee&quot;)/** * 使用Junit单元测试：测试我们的配置 */@RunWith(SpringJUnit4ClassRunner.class)public class AccountServiceTest &#123;        @Autowired    private  IAccountService as;    @Test    public void testTransfer()&#123;        as.transfer(&quot;aaa&quot;,&quot;bbb&quot;,100f);    &#125;&#125;</code></pre><h1 id="事务控制"><a href="#事务控制" class="headerlink" title="事务控制"></a>事务控制</h1><h2 id="Spring中的jdbcTemlate以及Spring事务控制"><a href="#Spring中的jdbcTemlate以及Spring事务控制" class="headerlink" title="Spring中的jdbcTemlate以及Spring事务控制"></a>Spring中的jdbcTemlate以及Spring事务控制</h2><p>JDBCTemplate<br>spring.core包下的工具类</p><p>JDBCDaoSupper  自己定义一个公共类<br>抽取dao层的公共代码<br>xml方式注入方式    可以使用继承的方式来减少重复的代码<br>使用注解的方式     就直接autowired就行</p><p>如果使用了spring中的jdbcdaosupper类<br>再使用注解的方式注入就变得麻烦了</p><h2 id="注解的方式实现aop环境配置"><a href="#注解的方式实现aop环境配置" class="headerlink" title="注解的方式实现aop环境配置"></a>注解的方式实现aop环境配置</h2><pre><code class="xml">   &lt;!--开启spring对注解AOP的支持--&gt;    &lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt;</code></pre><h2 id="基于XML的声明式事务控制配置步骤"><a href="#基于XML的声明式事务控制配置步骤" class="headerlink" title="基于XML的声明式事务控制配置步骤"></a>基于XML的声明式事务控制配置步骤</h2><h3 id="1、配置事务管理器"><a href="#1、配置事务管理器" class="headerlink" title="1、配置事务管理器"></a>1、配置事务管理器</h3><h3 id="2、配置事务的通知"><a href="#2、配置事务的通知" class="headerlink" title="2、配置事务的通知"></a>2、配置事务的通知</h3><p>此时我们需要导入事务的约束 tx名称空间和约束，同时也需要aop的<br>使用 tx:advice 标签配置事务通知</p><p><strong>属性</strong>：</p><p><strong>id</strong>：给事务通知起一个唯一标识</p><p><strong>transaction-manager</strong>：给事务通知提供一个事务管理器引用</p><h3 id="3、配置AOP中的通用切入点表达式"><a href="#3、配置AOP中的通用切入点表达式" class="headerlink" title="3、配置AOP中的通用切入点表达式"></a>3、配置AOP中的通用切入点表达式</h3><h3 id="4、建立事务通知和切入点表达式的对应关系"><a href="#4、建立事务通知和切入点表达式的对应关系" class="headerlink" title="4、建立事务通知和切入点表达式的对应关系"></a>4、建立事务通知和切入点表达式的对应关系</h3><h3 id="5、配置事务的属性"><a href="#5、配置事务的属性" class="headerlink" title="5、配置事务的属性"></a>5、配置事务的属性</h3><p>是在事务的通知tx:advice标签的内部</p><pre><code class="xml">&lt;!-- 配置事务管理器 --&gt;    &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt;        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt;    &lt;/bean&gt;     &lt;!-- 配置事务的通知--&gt;    &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt;    &lt;!-- 配置事务的属性                isolation：用于指定事务的隔离级别。默认值是DEFAULT，表示使用数据库的默认隔离级别。                propagation：用于指定事务的传播行为。默认值是REQUIRED，表示一定会有事务，增删改的选择。查询方法可以选择SUPPORTS。                read-only：用于指定事务是否只读。只有查询方法才能设置为true。默认值是false，表示读写。                timeout：用于指定事务的超时时间，默认值是-1，表示永不超时。如果指定了数值，以秒为单位。                rollback-for：用于指定一个异常，当产生该异常时，事务回滚，产生其他异常时，事务不回滚。没有默认值。表示任何异常都回滚。                no-rollback-for：用于指定一个异常，当产生该异常时，事务不回滚，产生其他异常时事务回滚。没有默认值。表示任何异常都回滚。        --&gt;        &lt;tx:attributes&gt;            &lt;tx:method name=&quot;*&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot;/&gt;            &lt;tx:method name=&quot;find*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot;&gt;&lt;/tx:method&gt;        &lt;/tx:attributes&gt;    &lt;/tx:advice&gt; &lt;!-- 配置aop--&gt;    &lt;aop:config&gt;        &lt;!-- 配置切入点表达式--&gt;        &lt;aop:pointcut id=&quot;pt1&quot; expression=&quot;execution(* com.itheima.service.impl.*.*(..))&quot;&gt;&lt;/aop:pointcut&gt;        &lt;!--建立切入点表达式和事务通知的对应关系 --&gt;        &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;pt1&quot;&gt;&lt;/aop:advisor&gt;    &lt;/aop:config&gt;</code></pre><h2 id="基于注解配置事务管理器"><a href="#基于注解配置事务管理器" class="headerlink" title="基于注解配置事务管理器"></a>基于注解配置事务管理器</h2><pre><code class="xml"> &lt;!-- 配置数据源--&gt;    &lt;bean id=&quot;dataSource&quot; class=&quot;org.springframework.jdbc.datasource.DriverManagerDataSource&quot;&gt;        &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt;        &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/eesy&quot;&gt;&lt;/property&gt;        &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt;        &lt;property name=&quot;password&quot; value=&quot;1234&quot;&gt;&lt;/property&gt;    &lt;/bean&gt;    &lt;!-- spring中基于XML的声明式事务控制配置步骤        1、配置事务管理器        2、配置事务的通知                此时我们需要导入事务的约束 tx名称空间和约束，同时也需要aop的                使用tx:advice标签配置事务通知                    属性：                        id：给事务通知起一个唯一标识                        transaction-manager：给事务通知提供一个事务管理器引用        3、配置AOP中的通用切入点表达式        4、建立事务通知和切入点表达式的对应关系        5、配置事务的属性               是在事务的通知tx:advice标签的内部     --&gt;         &lt;!-- 配置事务管理器 --&gt;    &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt;        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt;    &lt;/bean&gt;    &lt;!-- 配置事务的通知--&gt;    &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt;            &lt;!-- 配置事务的属性                isolation：用于指定事务的隔离级别。默认值是DEFAULT，表示使用数据库的默认隔离级别。                propagation：用于指定事务的传播行为。默认值是REQUIRED，表示一定会有事务，增删改的选择。查询方法可以选择SUPPORTS。                read-only：用于指定事务是否只读。只有查询方法才能设置为true。默认值是false，表示读写。                timeout：用于指定事务的超时时间，默认值是-1，表示永不超时。如果指定了数值，以秒为单位。                rollback-for：用于指定一个异常，当产生该异常时，事务回滚，产生其他异常时，事务不回滚。没有默认值。表示任何异常都回滚。                no-rollback-for：用于指定一个异常，当产生该异常时，事务不回滚，产生其他异常时事务回滚。没有默认值。表示任何异常都回滚。        --&gt;        &lt;tx:attributes&gt;            &lt;tx:method name=&quot;*&quot; propagation=&quot;REQUIRED&quot; read-only=&quot;false&quot;/&gt;            &lt;tx:method name=&quot;find*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot;&gt;&lt;/tx:method&gt;        &lt;/tx:attributes&gt;    &lt;/tx:advice&gt;    &lt;!-- 配置aop--&gt;    &lt;aop:config&gt;        &lt;!-- 配置切入点表达式--&gt;        &lt;aop:pointcut id=&quot;pt1&quot; expression=&quot;execution(* com.itheima.service.impl.*.*(..))&quot;&gt;&lt;/aop:pointcut&gt;        &lt;!--建立切入点表达式和事务通知的对应关系 --&gt;        &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;pt1&quot;&gt;&lt;/aop:advisor&gt;    &lt;/aop:config&gt;</code></pre><h3 id="在你的业务层加上-Transactional注解"><a href="#在你的业务层加上-Transactional注解" class="headerlink" title="在你的业务层加上@Transactional注解"></a>在你的业务层加上@Transactional注解</h3><pre><code class="java">@Service(&quot;accountService&quot;)@Transactional(propagation= Propagation.SUPPORTS,readOnly=true)//只读型事务的配置public class AccountServiceImpl implements IAccountService&#123;    &#125;</code></pre><h3 id="测试类"><a href="#测试类" class="headerlink" title="测试类"></a>测试类</h3><pre><code class="java">@Configuration@ComponentScan(&quot;com.itheima&quot;)@Import(&#123;JdbcConfig.class,TransactionConfig.class&#125;)@PropertySource(&quot;jdbcConfig.properties&quot;)@EnableTransactionManagement  //EnableTransactionManagementpublic class SpringConfiguration &#123;&#125;/** * 使用Junit单元测试：测试我们的配置 */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes= SpringConfiguration.class)public class AccountServiceTest &#123;    @Autowired    private  IAccountService as;    @Test    public  void testTransfer()&#123;        as.transfer(&quot;aaa&quot;,&quot;bbb&quot;,100f);    &#125;&#125;</code></pre><h2 id="编程式事物控制"><a href="#编程式事物控制" class="headerlink" title="编程式事物控制"></a>编程式事物控制</h2><p>没看，视频156-158</p>]]></content>
      
      
      <categories>
          
          <category> Java框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java -jar脚本模板</title>
      <link href="/linux/shell-jiao-ben/java-jar-jiao-ben-mo-ban/"/>
      <url>/linux/shell-jiao-ben/java-jar-jiao-ben-mo-ban/</url>
      
        <content type="html"><![CDATA[<h2 id="java-jar-脚本"><a href="#java-jar-脚本" class="headerlink" title="java -jar 脚本"></a>java -jar 脚本</h2><p>首先了解下1和2在Linux中代表什么<br>在Linux系统中0 1 2是一个文件描述符</p><table><thead><tr><th>名称</th><th>代码</th><th>操作符</th><th>Java中表示</th><th>Linux 下文件描述符（Debian 为例)</th></tr></thead><tbody><tr><td>标准输入(stdin)</td><td>0</td><td>&lt; 或 &lt;&lt;</td><td><a href="http://system.in/">System.in</a></td><td>&#x2F;dev&#x2F;stdin -&gt; &#x2F;proc&#x2F;self&#x2F;fd&#x2F;0 -&gt; &#x2F;dev&#x2F;pts&#x2F;0</td></tr><tr><td>标准输出(stdout)</td><td>1</td><td>&gt;, &gt;&gt;, 1&gt; 或 1&gt;&gt;</td><td>System.out</td><td>&#x2F;dev&#x2F;stdout -&gt; &#x2F;proc&#x2F;self&#x2F;fd&#x2F;1 -&gt; &#x2F;dev&#x2F;pts&#x2F;0</td></tr><tr><td>标准错误输出(stderr)</td><td>2</td><td>2&gt; 或 2&gt;&gt;</td><td>System.err</td><td>&#x2F;dev&#x2F;stderr -&gt; &#x2F;proc&#x2F;self&#x2F;fd&#x2F;2 -&gt; &#x2F;dev&#x2F;pts&#x2F;0</td></tr></tbody></table><p>2&gt;&amp;1 的意思就是将<strong>标准错误重定向到标准输出</strong>。这里标准输出已经重定向到了 &#x2F;dev&#x2F;null。那么标准错误也会输出到&#x2F;dev&#x2F;null</p><pre><code class="shell">#!/bin/bash#description: 启动重启server服务#端口号，根据此端口号确定pid1PORT1=8080PORT2=8081PORT3=8082PORT4=8083#启动命令所在目录HOME=&#39;/usr/local/src/large-screen/jars&#39;LOGHOME=&#39;/usr/local/src/large-screen/logs&#39;#查询出监听了PORT端口TCP协议的程序pid1=`netstat -anp|grep $PORT1|awk &#39;&#123;printf $7&#125;&#39;|cut -d/ -f1`pid2=`netstat -anp|grep $PORT2|awk &#39;&#123;printf $7&#125;&#39;|cut -d/ -f1`pid3=`netstat -anp|grep $PORT3|awk &#39;&#123;printf $7&#125;&#39;|cut -d/ -f1`pid4=`netstat -anp|grep $PORT4|awk &#39;&#123;printf $7&#125;&#39;|cut -d/ -f1`start()&#123;   if [ -n &quot;$pid1&quot; ]; then      echo &quot;server already start,pid1:$pid1&quot;      return 0   fi   #进入命令所在目录   cd $HOME   nohup java -jar $HOME/auto-drive-0.0.1-SNAPSHOT.jar &gt; $LOGHOME/server.log 2&gt;&amp;1 &amp;   #启动聊天服务器 把日志输出到HOME目录的server.log文件中   echo &quot;start at port:$PORT1&quot;         if [ -n &quot;$pid2&quot; ]; then      echo &quot;server already start,pid2:$pid2&quot;      return 0   fi   cd $HOME   nohup java -jar $HOME/test-operation-0.0.1-SNAPSHOT.jar &gt; $LOGHOME/operation.log 2&gt;&amp;1 &amp;   echo &quot;start at port:$PORT2&quot;      if [ -n &quot;$pid3&quot; ]; then      echo &quot;server already start,pid3:$pid3&quot;      return 0   fi   #进入命令所在目录   cd $HOME   nohup java -jar $HOME/test-service-0.0.1-SNAPSHOT.jar &gt; $LOGHOME/service.log 2&gt;&amp;1 &amp;   echo &quot;start at port:$PORT3&quot;   if [ -n &quot;$pid4&quot; ]; then      echo &quot;server already start,pid4:$pid4&quot;      return 0   fi   #进入命令所在目录   cd $HOME   nohup java -jar $HOME/vehicle-coord-0.0.1-SNAPSHOT.jar &gt; $LOGHOME/coord.log 2&gt;&amp;1 &amp;   echo &quot;start at port:$PORT4&quot;&#125;stop()&#123;   if [ -z &quot;$pid1&quot; ]; then      echo &quot;not find program on port1:$PORT1&quot;      return 0   fi   kill -9 $pid1   rm -rf $pid1   echo &quot;kill program use signal 2,pid1:$pid1&quot;      if [ -z &quot;$pid2&quot; ]; then      echo &quot;not find program on port2:$PORT2&quot;      return 0   fi   kill -9 $pid2   rm -rf $pid2   echo &quot;kill program use signal 2,pid2:$pid2&quot;      if [ -z &quot;$pid3&quot; ]; then      echo &quot;not find program on port3:$PORT3&quot;      return 0   fi   kill -9 $pid3   rm -rf $pid3   echo &quot;kill program use signal 2,pid3:$pid3&quot;      if [ -z &quot;$pid4&quot; ]; then      echo &quot;not find program on port4:$PORT4&quot;      return 0   fi   kill -9 $pid4   rm -rf $pid4   echo &quot;kill program use signal 2,pid4:$pid4&quot;&#125;status()&#123;   if [ -z &quot;$pid1&quot; ]; then      echo &quot;not find program on port1:$PORT1&quot;   else      echo &quot;program is running,pid1:$pid1&quot;   fi      if [ -z &quot;$pid2&quot; ]; then      echo &quot;not find program on port1:$PORT2&quot;   else      echo &quot;program is running,pid1:$pid2&quot;   fi      if [ -z &quot;$pid3&quot; ]; then      echo &quot;not find program on port1:$PORT3&quot;   else      echo &quot;program is running,pid1:$pid3&quot;   fi      if [ -z &quot;$pid4&quot; ]; then      echo &quot;not find program on port1:$PORT4&quot;   else      echo &quot;program is running,pid1:$pid4&quot;   fi&#125;case $1 in   start)      start   ;;   stop)      stop   ;;   restart)      $0 stop      sleep 2      $0 start    ;;   status)      status   ;;   *)      echo &quot;Usage: &#123;start|stop|status&#125;&quot;   ;;esacexit 0</code></pre>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>所有机器运行相同命令</title>
      <link href="/linux/shell-jiao-ben/suo-you-ji-qi-yun-xing-xiang-tong-ming-ling/"/>
      <url>/linux/shell-jiao-ben/suo-you-ji-qi-yun-xing-xiang-tong-ming-ling/</url>
      
        <content type="html"><![CDATA[<h2 id="所有节点运行相同命令脚本"><a href="#所有节点运行相同命令脚本" class="headerlink" title="所有节点运行相同命令脚本"></a>所有节点运行相同命令脚本</h2><p>前提条件：需要将profile文件内容放到 bashrc 文件中，在<code>shell学习笔记</code> 中有配置说明。</p><p>写的时候注意，中间没有逗号</p><pre><code class="shell">#! /bin/bashfor i in hadoop1 hadoop2 hadoop3do        echo --------- $i ----------        ssh $i &quot;$*&quot;done</code></pre><p>i  是循环的变量（hadoop1,hadoop2,hadoop3），do是要干的事情，done是结束语句，这三个是shell中for循环一体的</p>]]></content>
      
      
      <categories>
          
          <category> Shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringBoot</title>
      <link href="/java/spring/spring-boot/springboot-kuang-jia-wu/"/>
      <url>/java/spring/spring-boot/springboot-kuang-jia-wu/</url>
      
        <content type="html"><![CDATA[<h1 id="SpringBoot"><a href="#SpringBoot" class="headerlink" title="SpringBoot"></a>SpringBoot</h1><h2 id="原有Spring优缺点分析"><a href="#原有Spring优缺点分析" class="headerlink" title="原有Spring优缺点分析"></a>原有Spring优缺点分析</h2><p>Spring是java企业版j2ee的轻量级代替品。</p><p>虽然Spring的组件代码是轻量级的，但它的配置却是重量级的。一开始，Spring用XML配置，而且是很多XML配 置。Spring 2.5引入了基于注解的组件扫描，这消除了大量针对应用程序自身组件的显式XML配置。Spring 3.0引入 了基于Java的配置，这是一种类型安全的可重构配置方式，可以代替XML。</p><p>所有这些配置都代表了开发时的损耗。因为在思考Spring特性配置和解决业务问题之间需要进行思维切换，所以编 写配置挤占了编写应用程序逻辑的时间。和所有框架一样，Spring实用，但与此同时它要求的回报也不少。</p><p>除此之外，项目的依赖管理也是一件耗时耗力的事情。在环境搭建时，需要分析要导入哪些库的坐标，而且还需要分析导入与之有依赖关系的其他库的坐标，一旦选错了依赖的版本，随之而来的不兼容问题就会严重阻碍项目的开发进度 。</p><h2 id="SpringBoot常用注解"><a href="#SpringBoot常用注解" class="headerlink" title="SpringBoot常用注解"></a>SpringBoot常用注解</h2><h4 id="SpringBootApplication"><a href="#SpringBootApplication" class="headerlink" title="@SpringBootApplication"></a>@SpringBootApplication</h4><p>表明这是个启动类</p><p>项目在tomcat部署并启动，项目的扫描范围它默认是启动类的包以及它的子包</p><pre><code class="java">@SpringBootApplication@EnableAutoConfiguration@SpringBootConfiguration@ComponentScan</code></pre><h4 id="RestController"><a href="#RestController" class="headerlink" title="@RestController"></a>@RestController</h4><p>返回前端请求类型并指明这是个控制类</p><p>里面包含了ResponseBody和Controller两个标签</p><h4 id="EnableAutoConfiguration"><a href="#EnableAutoConfiguration" class="headerlink" title="@EnableAutoConfiguration"></a>@EnableAutoConfiguration</h4><p>自动配置的环境注解</p><h4 id="SpringBootConfiguration"><a href="#SpringBootConfiguration" class="headerlink" title="@SpringBootConfiguration"></a>@SpringBootConfiguration</h4><p>配置文件</p><h4 id="获取配置信息的注解"><a href="#获取配置信息的注解" class="headerlink" title="获取配置信息的注解"></a>获取配置信息的注解</h4><h5 id="Value"><a href="#Value" class="headerlink" title="@Value"></a>@Value</h5><p>方法和参数上可以使用</p><p>通过@Value注解将配置文件的值映射到Spring管理的Bean字段上面</p><h5 id="ConfigurationProperties"><a href="#ConfigurationProperties" class="headerlink" title="@ConfigurationProperties"></a>@ConfigurationProperties</h5><pre><code>@ConfigurationProperties(prefix = &quot;person&quot;)指明配置文件中的前缀同时属性要提供get set方法</code></pre>]]></content>
      
      
      <categories>
          
          <category> Java框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java框架 </tag>
            
            <tag> Spring </tag>
            
            <tag> Spring boot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据漂移解决方式</title>
      <link href="/shu-ju-cang-ku/shu-ju-piao-yi/"/>
      <url>/shu-ju-cang-ku/shu-ju-piao-yi/</url>
      
        <content type="html"><![CDATA[<p>版权声明：本文为CSDN博主「嗨胖」的原创文章<br>原文链接：<a href="https://blog.csdn.net/weixin_39714046/article/details/93661755">https://blog.csdn.net/weixin_39714046/article/details/93661755</a></p><h2 id="什么是数据漂移？"><a href="#什么是数据漂移？" class="headerlink" title="什么是数据漂移？"></a>什么是数据漂移？</h2><p>通常我们把从源系统同步进入数据仓库的第一层数据成为ODS层数据，我们公司目前只有ODS一层，虽说只有一层，但是仍然有有一个顽疾：数据漂移，简单来说就是ODS表的同一个业务日期数据中包含前一天或者后一天凌晨附近的数据或者丢失当天的变更数据。更新表来说会丢失变更数据，流水表一般会丢失上一天数据，或者说上一天数据漂移到下一天。</p><p><strong>通常时间戳字段分为四类：</strong></p><p>（1）数据库表中用来表示数据记录更新时间的时间戳字段（假设这类字段叫modified_time）；</p><p>（2）数据库日志中用来表示数据记录更新时间的时间戳字段（假设这类字段叫log_time）；</p><p>（3）数据库表中用于记录具体业务过程发生时间的时间戳字段（假设这类字段叫proc_time）；</p><p>（4）标识数据记录被抽取到时间的时间戳字段（假设这类字段叫extract_time）当然这种还是比较少用的；</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><ol start="0"><li><p>多等一段时间在执行统计数据的脚本，直到他完成数据的采集</p></li><li><p>多获取一段时间的数据</p></li></ol><p>ODS层保证数据之多不少，根据不同业务时间proc_time 来限制。但是这种方式会有一些数据误差，例如一个订单是当天支付的，但是第二天凌晨申请退款关闭了该订单，那么这条记录的订单状态会被更新，下游在统计支付订单状态时会出现错误。</p><ol start="2"><li>通过多个时间戳字段限制时间来获取相对准确的数据</li></ol><p> 首先根据log-time分别冗余前一天最后15分钟的数据和后一天凌晨开始15分钟数据，并用modified_time过滤非当天数据，确保数据不会因为系统问题而被遗漏</p><ol start="3"><li>第二天15分钟之后可能还是没有完全的获取到当天数据，将他们使用flume分流到另一个文件中。后期在重新统计一次。</li><li>在可以舍去数据的前提下，等待一段时间，如果还有没有过来的数据就进行舍弃。可以使用flume拦截器来实现这个功能。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS Gateway 报错： Failed to delete file or dir [/tmp/.hdfs-nfs]： it still exists</title>
      <link href="/cdh/hdfs-gateway-bao-cuo-failed-to-delete-file-or-dir-tmp.hdfs-nfs-it-still-exists/"/>
      <url>/cdh/hdfs-gateway-bao-cuo-failed-to-delete-file-or-dir-tmp.hdfs-nfs-it-still-exists/</url>
      
        <content type="html"><![CDATA[<p>tmp没有权限，所有Gateway节点都弄</p><pre><code>chmod 777 /tmp </code></pre><p>重启Hdfs</p><p>原来的权限是</p><pre><code>drwxrwxrwt</code></pre><p>这是只允许root用户来操作的目录<br>执行完后  权限会改成</p><pre><code>drwxrwxrw</code></pre><p>变成了所有用户可以操作的目录</p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH5.16.2安装Kafka4.1.0</title>
      <link href="/cdh/kafka/cdh5.16.2-an-zhuang-kafka4.1.0/"/>
      <url>/cdh/kafka/cdh5.16.2-an-zhuang-kafka4.1.0/</url>
      
        <content type="html"><![CDATA[<p>安装Kafka，需要下载Parcel包</p><p>Kafka.parcel下载地址：<a href="http://archive.cloudera.com/kafka/parcels/4.1.0/">http://archive.cloudera.com/kafka/parcels/4.1.0/</a></p><p><img src="https://img-blog.csdnimg.cn/20201112092356320.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h2><p>将下面的三个文件上传到&#x2F;opt&#x2F;cloudera&#x2F;parcel-repo目录下</p><p><img src="https://img-blog.csdnimg.cn/2020111209344464.png#pic_center" alt="在这里插入图片描述"><br>把sha1改成sha</p><p>在7180页面进行解压和激活Kafka服务</p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH5.16.2安装Spark2.4</title>
      <link href="/cdh/spark/cdh5.16.2-an-zhuang-spark2.4/"/>
      <url>/cdh/spark/cdh5.16.2-an-zhuang-spark2.4/</url>
      
        <content type="html"><![CDATA[<p>安装了好几次才成功，做做笔记<br>CDH安装时，用的离线安装，下载parcel包进行操作<br><img src="https://img-blog.csdnimg.cn/20201112085329647.png#pic_center" alt="在这里插入图片描述"><br>上面的这四个都得下载下来，manifest.json和parcel的可以在一起下载</p><p>Spark2.4.0下载地址：<a href="https://archive.cloudera.com/spark2/parcels/2.4.0.cloudera2/">https://archive.cloudera.com/spark2/parcels/2.4.0.cloudera2/</a><br><img src="https://img-blog.csdnimg.cn/20201112085711967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>Yarn的下载地址：<a href="http://archive.cloudera.com/spark2/csd/">http://archive.cloudera.com/spark2/csd/</a><br><img src="https://img-blog.csdnimg.cn/20201112085835980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>选择对应版本的进行下载<br>下载完毕进行上传</p><h2 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h2><p>将下面的三个文件上传到<code>/opt/cloudera/parcel-repo</code>目录下<br><img src="https://img-blog.csdnimg.cn/20201112090025357.png#pic_center" alt="在这里插入图片描述"><br>将.parcel.sha1改成.parcel.sha</p><pre><code>mv SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012-el7.parcel.sha1 SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012-el7.parcel.sha</code></pre><p>将下面的jar包上传到&#x2F;opt&#x2F;cloudera&#x2F;csd目录下<br><img src="https://img-blog.csdnimg.cn/20201112090443374.png#pic_center" alt="在这里插入图片描述"><br>打开7180监控页面，点击右上角<br><img src="https://img-blog.csdnimg.cn/20201112090916437.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>点击检查新的Parcel，出来Spark2，然后进行分配，分配完成后再点一下进行激活，这是单独的两个步骤<br><img src="https://img-blog.csdnimg.cn/20201112091106611.png#pic_center" alt="选择Spark2，"><br><img src="https://img-blog.csdnimg.cn/2020111209133729.png#pic_center" alt="在这里插入图片描述"></p><p>激活成功之后，进行Spark的安装<br><img src="https://img-blog.csdnimg.cn/20201112091409833.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20201112091437667.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>安装完就好了</p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Canal配置方式发送数据到Kafka</title>
      <link href="/canal/canal1.1.4-shu-ju-zhi-jie-fa-song-kafka-topic/"/>
      <url>/canal/canal1.1.4-shu-ju-zhi-jie-fa-song-kafka-topic/</url>
      
        <content type="html"><![CDATA[<h1 id="Canal修改配置文件"><a href="#Canal修改配置文件" class="headerlink" title="Canal修改配置文件"></a>Canal修改配置文件</h1><p>在原有Canal已经启动运行成功的情况下，</p><p>停掉服务，找到这个配置文件中对应项进行修改：</p><h2 id="第一个配置文件"><a href="#第一个配置文件" class="headerlink" title="第一个配置文件"></a>第一个配置文件</h2><blockquote><p>vim  &#x2F;opt&#x2F;canal&#x2F;conf&#x2F;canal.properties</p></blockquote><pre><code># 配置zkcanal.zkServers = hadoop1:2181,hadoop2:2181,hadoop3:2181# 设置成rowcanal.instance.binlog.format = ROW# 可选项: tcp(默认), kafka, RocketMQcanal.serverMode = kafka# 填写你的kafka地址和端口，可以不用都写canal.mq.servers = hadoop1:9092,hadoop2:9092,hadoop3:9092# 配置你的mysql地址，数据库的名称不用改，它默认全是这个名字，刚开始改名了，后来嫌麻烦改回来了# 涉及到配置账号密码的都设置好别漏掉一个canal.instance.tsdb.url = jdbc:mysql://hadoop1:3306/canal_tsdbcanal.instance.tsdb.dbUsername = canalcanal.instance.tsdb.dbPassword = 123456#这里要配置成mysql的，不变的话链接mysql会报 java.sql.SQLExceptionjava.sql.SQL：driverClass org.h2.Driver# canal.instance.tsdb.spring.xml = classpath:spring/tsdb/h2-tsdb.xml  canal.instance.tsdb.spring.xml = classpath:spring/tsdb/mysql-tsdb.xml</code></pre><h2 id="第二个配置文件"><a href="#第二个配置文件" class="headerlink" title="第二个配置文件"></a>第二个配置文件</h2><blockquote><p>vim  &#x2F;opt&#x2F;canal&#x2F;conf&#x2F;example&#x2F;instance.properties</p></blockquote><pre><code># 配置mysqlcanal.instance.tsdb.url=jdbc:mysql://hadoop1:3306/canal_tsdbcanal.instance.tsdb.dbUsername=canalcanal.instance.tsdb.dbPassword=123456# 配置过滤，可以在这里指定表和库，只获取他们的binlogcanal.instance.filter.regex=.*\\..*# 配置多topic，在这里可以指定你的数据放kafka的哪个topic，还可以自己指定topic的名称，不存在的topic会自动创建canal.mq.dynamicTopic=exercise_topic:exercise\\.user,exercise_topic2:exercise2\\.user2</code></pre><p>上面配置文件中的特殊参数配置说明可以在下面进行查看</p><p>conf&#x2F;example&#x2F;instance.properties配置文件中的<code>canal.instance.filter.regex=</code>  属性：</p><p>mysql 数据解析关注的表，Perl正则表达式.多个正则之间以逗号(,)分隔，转义符需要双斜杠(\)<br>常见例子：</p><ol><li>所有表：.* or .<em>\..</em></li><li>canal schema下所有表： canal\..*</li><li>canal下的以canal打头的表：canal\.canal.*</li><li>canal schema下的一张表：canal.test1</li><li>多个规则组合使用：canal\..*,mysql.test1,mysql.test2 (逗号分隔)<br>注意：此过滤条件只针对row模式的数据有效(ps. mixed&#x2F;statement因为不解析sql，所以无法准确提取tableName进行过滤)</li></ol><p>conf&#x2F;example&#x2F;instance.properties配置文件的<code>canal.mq.dynamicTopic</code> 表达式说明：</p><p><strong>canal 1.1.3</strong>版本之后, 支持配置格式：schema 或 schema.table，多个配置之间使用逗号或分号分隔：</p><ul><li>例子1：test.test 指定匹配的单表，发送到以test_test为名字的topic上</li><li>例子2：... 匹配所有表，则每个表都会发送到各自表名的topic上</li><li>例子3：test 指定匹配对应的库，一个库的所有表都会发送到库名的topic上</li><li>例子4：test.* 指定匹配的表达式，针对匹配的表会发送到各自表名的topic上</li><li>例子5：test,test1.test1，指定多个表达式，会将test库的表都发送到test的topic上，test1.test1的表发送到对应的test1_test1 topic上，其余的表发送到默认的canal.mq.topic值<br>为满足更大的灵活性，允许对匹配条件的规则指定发送的topic名字，配置格式：topicName:schema 或 topicName:schema.table：</li><li>例子1: test:test.test 指定匹配的单表，发送到以test为名字的topic上</li><li>例子2: test:... 匹配所有表，因为有指定topic，则每个表都会发送到test的topic下</li><li>例子3: test:test 指定匹配对应的库，一个库的所有表都会发送到test的topic下</li><li>例子4：testA:test.* 指定匹配的表达式，针对匹配的表会发送到testA的topic下</li><li>例子5：test0:test,test1:test1.test1，指定多个表达式，会将test库的表都发送到test0的topic下，test1.test1的表发送到对应的test1的topic下，其余的表发送到默认的canal.mq.topic值</li></ul><p>conf&#x2F;example&#x2F;instance.properties配置文件的<code>canal.mq.partitionHash</code> 表达式说明：<br>canal 1.1.3版本之后, 支持配置格式：schema.table:pk1^pk2，多个配置之间使用逗号分隔：</p><ul><li>例子1：test.test:pk1^pk2 指定匹配的单表，对应的hash字段为pk1 + pk2</li><li>例子2：...:id 正则匹配，指定所有正则匹配的表对应的hash字段为id</li><li>例子3：...:pk 正则匹配，指定所有正则匹配的表对应的hash字段为表主键(自动查找)</li><li>例子4: 匹配规则啥都不写，则默认发到0这个partition上</li><li>例子5：... ，不指定pk信息的正则匹配，将所有正则匹配的表,对应的hash字段为表名。按表hash: 一张表的所有数据可以发到同一个分区，不同表之间会做散列 (会有热点表分区过大问题)</li><li>例子6: test.test:id,...* , 针对test的表按照id散列,其余的表按照table散列</li><li>注意：设置匹配规则，多条匹配规则之间是按照顺序进行匹配(命中一条规则就返回)</li></ul><h2 id="启动Canal"><a href="#启动Canal" class="headerlink" title="启动Canal"></a>启动Canal</h2><p>配置完成，启动canal，然后查看日志</p><pre><code>tail -10 logs/canal/canal.logtail -10 logs/example/example.log</code></pre><p>没有报错信息，启动成功！</p><p>查看kafka的topic列表</p><pre><code>kafka-topics --list --bootstrap-server hadoop1:9092,hadoop2:9092,hadoop3:9092</code></pre><p>没有我要的发数据的topic</p><p>打开mysql执行一句delete或者insert<br>再次查看kafka的topics，发现已经生成了topic<br>进行消费数据</p><pre><code>kafka-console-consumer --bootstrap-server hadoop1:9092,hadoop2:9092,hadoop3:9092 --from-beginning --topic exercise_topic</code></pre><p>可以看到消费到了数据<br><img src="https://img-blog.csdnimg.cn/2020111115374725.png#pic_center" alt="在这里插入图片描述"><br>配置好用好这个就不用写canal客户端代码了<br>到这里就完成了，如果有什么没有表述清楚，可以告知，会改。</p><p>下面是我整体的配置文件</p><p>canal.properties  整体配置文件：</p><pre><code>########################################################## common argument############################################################### tcp bind ipcanal.ip =# register ip to zookeepercanal.register.ip =canal.port = 11111canal.metrics.pull.port = 11112# canal instance user/passwd# canal.user = canal# canal.passwd = E3619321C1A937C46A0D8BD1DAC39F93B27D4458# canal admin config#canal.admin.manager = 127.0.0.1:8089canal.admin.port = 11110canal.admin.user = admincanal.admin.passwd = 4ACFE3202A5FF5CF467898FC58AAB1D615029441canal.zkServers = hadoop1:2181,hadoop2:2181,hadoop3:2181# flush data to zkcanal.zookeeper.flush.period = 1000canal.withoutNetty = false# tcp, kafka, RocketMQ# canal.serverMode = tcpcanal.serverMode = kafka# flush meta cursor/parse position to filecanal.file.data.dir = $&#123;canal.conf.dir&#125;canal.file.flush.period = 1000## memory store RingBuffer size, should be Math.pow(2,n)canal.instance.memory.buffer.size = 16384## memory store RingBuffer used memory unit size , default 1kbcanal.instance.memory.buffer.memunit = 1024 ## meory store gets mode used MEMSIZE or ITEMSIZEcanal.instance.memory.batch.mode = MEMSIZEcanal.instance.memory.rawEntry = true## detecing configcanal.instance.detecting.enable = false#canal.instance.detecting.sql = insert into retl.xdual values(1,now()) on duplicate key update x=now()canal.instance.detecting.sql = select 1canal.instance.detecting.interval.time = 3canal.instance.detecting.retry.threshold = 3canal.instance.detecting.heartbeatHaEnable = false# support maximum transaction size, more than the size of the transaction will be cut into multiple transactions deliverycanal.instance.transaction.size =  1024# mysql fallback connected to new master should fallback timescanal.instance.fallbackIntervalInSeconds = 60# network configcanal.instance.network.receiveBufferSize = 16384canal.instance.network.sendBufferSize = 16384canal.instance.network.soTimeout = 30# binlog filter configcanal.instance.filter.druid.ddl = truecanal.instance.filter.query.dcl = falsecanal.instance.filter.query.dml = falsecanal.instance.filter.query.ddl = falsecanal.instance.filter.table.error = falsecanal.instance.filter.rows = falsecanal.instance.filter.transaction.entry = false# binlog format/image check#canal.instance.binlog.format = ROW,STATEMENT,MIXEDcanal.instance.binlog.format = ROW canal.instance.binlog.image = FULL,MINIMAL,NOBLOB# binlog ddl isolationcanal.instance.get.ddl.isolation = false# parallel parser configcanal.instance.parser.parallel = true## concurrent thread number, default 60% available processors, suggest not to exceed Runtime.getRuntime().availableProcessors()#canal.instance.parser.parallelThreadSize = 16## disruptor ringbuffer size, must be power of 2canal.instance.parser.parallelBufferSize = 256# table meta tsdb infocanal.instance.tsdb.enable = truecanal.instance.tsdb.dir = $&#123;canal.file.data.dir:../conf&#125;/$&#123;canal.instance.destination:&#125;canal.instance.tsdb.url = jdbc:h2:$&#123;canal.instance.tsdb.dir&#125;/h2;CACHE_SIZE=1000;MODE=MYSQL;canal.instance.tsdb.dbUsername = canalcanal.instance.tsdb.dbPassword = 123456# dump snapshot interval, default 24 hourcanal.instance.tsdb.snapshot.interval = 24# purge snapshot expire , default 360 hour(15 days)canal.instance.tsdb.snapshot.expire = 360# aliyun ak/sk , support rds/mqcanal.aliyun.accessKey =canal.aliyun.secretKey =########################################################## destinations##############################################################canal.destinations = example# conf root dircanal.conf.dir = ../conf# auto scan instance dir add/remove and start/stop instancecanal.auto.scan = truecanal.auto.scan.interval = 5#canal.instance.tsdb.spring.xml = classpath:spring/tsdb/h2-tsdb.xmlcanal.instance.tsdb.spring.xml = classpath:spring/tsdb/mysql-tsdb.xmlcanal.instance.global.mode = springcanal.instance.global.lazy = falsecanal.instance.global.manager.address = $&#123;canal.admin.manager&#125;#canal.instance.global.spring.xml = classpath:spring/memory-instance.xmlcanal.instance.global.spring.xml = classpath:spring/file-instance.xml#canal.instance.global.spring.xml = classpath:spring/default-instance.xml###########################################################      MQ      ###############################################################canal.mq.servers = hadoop1:9092,hadoop2:9092,hadoop3:9092canal.mq.retries = 2canal.mq.batchSize = 16384canal.mq.maxRequestSize = 1048576canal.mq.lingerMs = 100canal.mq.bufferMemory = 33554432canal.mq.canalBatchSize = 50canal.mq.canalGetTimeout = 100canal.mq.flatMessage = truecanal.mq.compressionType = nonecanal.mq.acks = all#canal.mq.properties. =canal.mq.producerGroup = test# Set this value to &quot;cloud&quot;, if you want open message trace feature in aliyun.canal.mq.accessChannel = local# aliyun mq namespace#canal.mq.namespace =###########################################################     Kafka Kerberos Info    ###############################################################canal.mq.kafka.kerberos.enable = falsecanal.mq.kafka.kerberos.krb5FilePath = &quot;../conf/kerberos/krb5.conf&quot;canal.mq.kafka.kerberos.jaasFilePath = &quot;../conf/kerberos/jaas.conf&quot;</code></pre><p>instance.properties  整体配置文件：</p><pre><code>################################################### mysql serverId , v1.0.26+ will autoGen# canal.instance.mysql.slaveId=0# enable gtid use true/falsecanal.instance.gtidon=false# position infocanal.instance.master.address=hadoop1:3306canal.instance.master.journal.name=canal.instance.master.position=canal.instance.master.timestamp=canal.instance.master.gtid=# rds oss binlogcanal.instance.rds.accesskey=canal.instance.rds.secretkey=canal.instance.rds.instanceId=# table meta tsdb infocanal.instance.tsdb.enable=true#canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb#canal.instance.tsdb.dbUsername=canal#canal.instance.tsdb.dbPassword=canalcanal.instance.tsdb.url=jdbc:mysql://hadoop1:3306/canal_tsdbcanal.instance.tsdb.dbUsername=canalcanal.instance.tsdb.dbPassword=123456#canal.instance.standby.address =#canal.instance.standby.journal.name =#canal.instance.standby.position =#canal.instance.standby.timestamp =#canal.instance.standby.gtid=# username/passwordcanal.instance.dbUsername=canalcanal.instance.dbPassword=123456canal.instance.connectionCharset = UTF-8# enable druid Decrypt database passwordcanal.instance.enableDruid=false#canal.instance.pwdPublicKey=MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBALK4BUxdDltRRE5/zXpVEVPUgunvscYFtEip3pmLlhrWpacX7y7GCMo2/JM6LeHmiiNdH1FWgGCpUfircSwlWKUCAwEAAQ==# table regex#canal.instance.filter.regex=.*\\..*canal.instance.filter.regex=exercise.user# table black regexcanal.instance.filter.black.regex=# table field filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2)#canal.instance.filter.field=test1.t_product:id/subject/keywords,test2.t_company:id/name/contact/ch# mq configcanal.mq.topic=example# dynamic topic route by schema or table regex#canal.mq.dynamicTopic=mytest1.user,mytest2\\..*,.*\\..*canal.mq.partition=0# hash partition config#canal.mq.partitionsNum=3#canal.mq.partitionHash=test.table:id^name,.*\\..*#################################################</code></pre>]]></content>
      
      
      <categories>
          
          <category> Canal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Canal </tag>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CM报错：Unable to build ntityManagerFactory</title>
      <link href="/cdh/cm-bao-cuo/"/>
      <url>/cdh/cm-bao-cuo/</url>
      
        <content type="html"><![CDATA[<p>今天CDH突然报错，先重启了一下CM，然后在重试，结果还是不行，WebUI打不开</p><p>报错内容</p><pre><code>2020-11-10 13:32:36,827 ERROR 100851968@scm-web-6023:org.mortbay.log: /cmf/j_spring_security_checkjava.lang.RuntimeException: javax.persistence.PersistenceException: org.hibernate.exception.GenericJDBCException: Could not open connectionorg.springframework.beans.factory.BeanCreationException: Error creating bean with name &#39;com.cloudera.server.cmf.TrialState&#39;: Cannot resolve reference to bean &#39;entityManagerFactoryBean&#39; while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#39;entityManagerFactoryBean&#39;: FactoryBean threw exception on object creation; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: cmf.server] Unable to build EntityManagerFactory........</code></pre><p>在网上查了没查出什么结果</p><p>就把agent和server的日志都看了看，就是连接不上Mysql数据库，而且权限都配置好了<br>然后发现了一篇博客</p><blockquote><p><a href="https://www.cnblogs.com/zlslch/p/7266101.html">https://www.cnblogs.com/zlslch/p/7266101.html</a></p></blockquote><p>参考里面的一部分，浏览了里面提到的cm-5.16.2&#x2F;etc&#x2F;cloudera-scm-server&#x2F;db.properties,…..等文件之后，发现里面存在的是CM连接数据库的配置信息<br>这些信息是之前用过的(初始化了多次数据库)，把这些我改成了我现在的用户和密码后，重启CM</p><pre><code>/opt/cm-5.16.2/etc/init.d/cloudera-scm-agent restart/opt/cm-5.16.2/etc/init.d/cloudera-scm-server restart</code></pre><p>再次 浏览器连接 7180  进行查看，成功</p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Centos7下Canal1.1.4安装</title>
      <link href="/canal/centos7-xia-canal1.1.4-de-an-zhuang/"/>
      <url>/canal/centos7-xia-canal1.1.4-de-an-zhuang/</url>
      
        <content type="html"><![CDATA[<h1 id="Canal是什么？"><a href="#Canal是什么？" class="headerlink" title="Canal是什么？"></a>Canal是什么？</h1><p>canal [kə’næl]，译意为水道&#x2F;管道&#x2F;沟渠，主要用途是基于 MySQL 数据库增量日志解析，提供增量数据订阅和消费<br><img src="https://img-blog.csdnimg.cn/20201110125412686.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>工作原理</strong></p><p>canal 模拟 MySQL slave 的交互协议，伪装自己为 MySQL slave ，向 MySQL master 发送 dump 协议<br>MySQL master 收到 dump 请求，开始推送 binary log 给 slave (即 canal )<br>canal 解析 binary log 对象(原始为 byte 流)</p><h2 id="安装前的准备-配置Mysql"><a href="#安装前的准备-配置Mysql" class="headerlink" title="安装前的准备 配置Mysql"></a>安装前的准备 配置Mysql</h2><p>Canal是模仿mysql的slave，需要读取mysql的binlog文件，Mysql默认是没有开启binlog的，要先开启日志</p><p>vim &#x2F;etc&#x2F;my.cnf</p><pre><code>[mysqld]log-bin=mysql-bin # 开启 binlogbinlog-format=ROW # 选择 ROW 模式server_id=1 # 配置 MySQL replaction 需要定义，不要和 canal 的 slaveId 重复</code></pre><p>配置完成后</p><p>查看binlog的状态</p><pre><code>show VARIABLES like &#39;log_bin&#39;</code></pre><p><img src="https://img-blog.csdnimg.cn/20201110132056354.png#pic_center" alt="在这里插入图片描述"></p><p>&#x2F;&#x2F;查看binlog日志</p><pre><code>show binary logs</code></pre><p><img src="https://img-blog.csdnimg.cn/20201110132112497.png#pic_center" alt="在这里插入图片描述"></p><p>&#x2F;&#x2F;查看master状态</p><pre><code>show master status ;</code></pre><p><img src="https://img-blog.csdnimg.cn/20201110132122230.png#pic_center" alt="在这里插入图片描述"><br>注意：针对阿里云 RDS for MySQL , 默认打开了 binlog , 并且账号默认具有 binlog dump 权限 , 不需要任何权限或者 binlog 设置,可以直接跳过这一步</p><h2 id="Mysql-binlog的三种格式"><a href="#Mysql-binlog的三种格式" class="headerlink" title="Mysql  binlog的三种格式"></a>Mysql  binlog的三种格式</h2><p>1） statement<br>语句级，binlog会记录每次一执行写操作的语句。<br>相对row模式节省空间，但是可能产生不一致性，比如<br>update tt set create_date&#x3D;now()<br>如果用binlog日志进行恢复，由于执行时间不同可能产生的数据就不同。<br>优点： 节省空间<br>缺点： 有可能造成数据不一致。<br>2） row<br>行级， binlog会记录每次操作后每行记录的变化。<br>优点：保持数据的绝对一致性。因为不管sql是什么，引用了什么函数，他只记录执行后的效果。<br>缺点：占用较大空间。</p><p>3） mixed<br>statement的升级版，一定程度上解决了，因为一些情况而造成的statement模式不一致问题<br>在某些情况下譬如：<br>当函数中包含 UUID() 时；<br>包含 AUTO_INCREMENT 字段的表被更新时；<br>执行 INSERT DELAYED 语句时；<br>用 UDF 时；<br>会按照 ROW的方式进行处理<br>优点：节省空间，同时兼顾了一定的一致性。<br>缺点：还有些极个别情况依旧会造成不一致，另外statement和mixed对于需要对binlog的监控的情况都不方便。</p><h2 id="重启你的Mysql服务"><a href="#重启你的Mysql服务" class="headerlink" title="重启你的Mysql服务"></a>重启你的Mysql服务</h2><pre><code>service mysql restart</code></pre><h2 id="进入你的mysql，添加canal使用的用户"><a href="#进入你的mysql，添加canal使用的用户" class="headerlink" title="进入你的mysql，添加canal使用的用户"></a>进入你的mysql，添加canal使用的用户</h2><pre><code>use mysql;在mysql创建一个用户create user &#39;canal&#39;@&#39;%&#39; identified by &#39;123456&#39;;修改密码update user set authentication_string=password(&quot;123456&quot;) where user=&quot;canal&quot;;对用户进行授权grant all privileges on *.* to &#39;canal&#39;@&#39;localhost&#39; identified by &#39;123456&#39; with grant option;GRANT ALL PRIVILEGES ON *.* TO &#39;canal&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;;刷新权限FLUSH PRIVILEGES;</code></pre><p>添加完毕后，测试是否可以正常连接，如果不测试的话，到时候canal报错还得看日志</p><h2 id="下载Canal"><a href="#下载Canal" class="headerlink" title="下载Canal"></a>下载Canal</h2><p>下载Canal的包：<a href="https://github.com/alibaba/canal/releases/tag/canal-1.1.4">官网下载地址</a></p><p>下载上传后解压包</p><pre><code>mkdir canaltar -zxvf canal.deployer-1.1.4.tar.gz -C /opt/canal</code></pre><p>会生成四个文件夹<br><img src="https://img-blog.csdnimg.cn/20201110130532710.png#pic_center" alt="在这里插入图片描述"></p><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>vim conf&#x2F;example&#x2F;instance.properties</p><pre><code>## mysql serverIdcanal.instance.mysql.slaveId = 1234#position info，需要改成自己的数据库信息canal.instance.master.address = hadoop1:3306 canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = #canal.instance.standby.address = #canal.instance.standby.journal.name =#canal.instance.standby.position = #canal.instance.standby.timestamp = #username/password，需要改成自己的数据库信息canal.instance.dbUsername = canalcanal.instance.dbPassword = 123456canal.instance.defaultDatabaseName =canal.instance.connectionCharset = UTF-8#table regexcanal.instance.filter.regex = .\*\\\\..\*</code></pre><p>如果需要修改Canal的端口号可以在conf&#x2F;canal.properties中修改</p><p>vim conf&#x2F;canal.properties<br><img src="https://img-blog.csdnimg.cn/20201110130951663.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="启动Canal服务端"><a href="#启动Canal服务端" class="headerlink" title="启动Canal服务端"></a>启动Canal服务端</h2><pre><code>sh bin/startup.sh</code></pre><h2 id="查看日志"><a href="#查看日志" class="headerlink" title="查看日志"></a>查看日志</h2><p>查看Server 日志</p><pre><code>vim  logs/canal/canal.log</code></pre><p>查看 instance 的日志</p><pre><code>vim  logs/example/example.log</code></pre><p>没有报错就说明启动没有问题</p><h2 id="客户端pom文件配置"><a href="#客户端pom文件配置" class="headerlink" title="客户端pom文件配置"></a>客户端pom文件配置</h2><p>服务端启动完成后使用代码访问服务端<br>pom文件中导包</p><pre><code>    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt;            &lt;artifactId&gt;canal.client&lt;/artifactId&gt;            &lt;version&gt;1.1.4&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;</code></pre><h2 id="客户端代码"><a href="#客户端代码" class="headerlink" title="客户端代码"></a>客户端代码</h2><pre><code>import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.protocol.Message;import com.alibaba.otter.canal.protocol.CanalEntry.Column;import com.alibaba.otter.canal.protocol.CanalEntry.Entry;import com.alibaba.otter.canal.protocol.CanalEntry.EntryType;import com.alibaba.otter.canal.protocol.CanalEntry.EventType;import com.alibaba.otter.canal.protocol.CanalEntry.RowChange;import com.alibaba.otter.canal.protocol.CanalEntry.RowData;import org.springframework.beans.factory.InitializingBean;import org.springframework.stereotype.Component;import java.net.InetSocketAddress;import java.util.List;@Componentpublic class CannalClient implements InitializingBean &#123;    public static void main(String args[]) &#123;        // 创建链接  连接地址是安装canal的地址，端口默认11111，example是他的配置文件的topic名称，账号密码是自己设置的        CanalConnector connector = CanalConnectors.newSingleConnector(new InetSocketAddress(&quot;hadoop1&quot;,                11111), &quot;example&quot;, &quot;canal&quot;, &quot;123456&quot;);        int batchSize = 1000;        int emptyCount = 0;        try &#123;            connector.connect();            connector.subscribe(&quot;.*\\..*&quot;);            connector.rollback();            int totalEmptyCount = 120;            while (emptyCount &lt; totalEmptyCount) &#123;                Message message = connector.getWithoutAck(batchSize); // 获取指定数量的数据                long batchId = message.getId();                int size = message.getEntries().size();                if (batchId == -1 || size == 0) &#123;                    emptyCount++;                    System.out.println(&quot;empty count : &quot; + emptyCount);                    try &#123;                        Thread.sleep(1000);                    &#125; catch (InterruptedException e) &#123;                    &#125;                &#125; else &#123;                    emptyCount = 0;                    // System.out.printf(&quot;message[batchId=%s,size=%s] \n&quot;, batchId, size);                    printEntry(message.getEntries());                &#125;                connector.ack(batchId); // 提交确认                // connector.rollback(batchId); // 处理失败, 回滚数据            &#125;            System.out.println(&quot;empty too many times, exit&quot;);        &#125; finally &#123;            connector.disconnect();        &#125;    &#125;    private static void printEntry(List&lt;Entry&gt; entrys) &#123;        for (Entry entry : entrys) &#123;            if (entry.getEntryType() == EntryType.TRANSACTIONBEGIN || entry.getEntryType() == EntryType.TRANSACTIONEND) &#123;                continue;            &#125;            RowChange rowChage = null;            try &#123;                rowChage = RowChange.parseFrom(entry.getStoreValue());            &#125; catch (Exception e) &#123;                throw new RuntimeException(&quot;ERROR ## parser of eromanga-event has an error , data:&quot; + entry.toString(),                        e);            &#125;            EventType eventType = rowChage.getEventType();            System.out.println(String.format(&quot;================&amp;gt; binlog[%s:%s] , name[%s,%s] , eventType : %s&quot;,                    entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(),                    entry.getHeader().getSchemaName(), entry.getHeader().getTableName(),                    eventType));            for (RowData rowData : rowChage.getRowDatasList()) &#123;                if (eventType == EventType.DELETE) &#123;                    printColumn(rowData.getBeforeColumnsList());                &#125; else if (eventType == EventType.INSERT) &#123;                    printColumn(rowData.getAfterColumnsList());                &#125; else &#123;                    System.out.println(&quot;-------&amp;gt; before&quot;);                    printColumn(rowData.getBeforeColumnsList());                    System.out.println(&quot;-------&amp;gt; after&quot;);                    printColumn(rowData.getAfterColumnsList());                &#125;            &#125;        &#125;    &#125;    private static void printColumn(List&lt;Column&gt; columns) &#123;        for (Column column : columns) &#123;            System.out.println(column.getName() + &quot; : &quot; + column.getValue() + &quot;    update=&quot; + column.getUpdated());        &#125;    &#125;    @Override    public void afterPropertiesSet() throws Exception &#123;    &#125;&#125;</code></pre><p>在启动后可以操作mysql的数据库，插入删除数据看看是否有数据被打印出来</p><p>没有说明白的地方可以查看官方文档<a href="https://github.com/alibaba/canal/wiki/QuickStart">https://github.com/alibaba/canal/wiki/QuickStart</a></p><h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><h4 id="Canal使用时必备的条件？"><a href="#Canal使用时必备的条件？" class="headerlink" title="Canal使用时必备的条件？"></a>Canal使用时必备的条件？</h4><ol><li><p>mysql要先开启binlog写入功能，配置 binlog-format 为 ROW 模式。</p></li><li><p>添加canal账号，并赋予权限。</p></li><li><p>下载canal到你的机器上，并启动服务。</p></li></ol><h4 id="Canal原理？"><a href="#Canal原理？" class="headerlink" title="Canal原理？"></a>Canal原理？</h4><ul><li>canal 模拟 MySQL slave 的交互协议，伪装自己为 MySQL slave ，向 MySQL master 发送dump 协议</li><li>MySQL master 收到 dump 请求，开始推送 binary log 给 slave (即 canal )</li><li>canal 解析 binary log 对象(原始为 byte 流)</li></ul>]]></content>
      
      
      <categories>
          
          <category> Canal </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Canal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop1导入数据hdsf/hive</title>
      <link href="/cdh/sqoop/sqoop1-dao-ru-hdfs-hive-shu-ju/"/>
      <url>/cdh/sqoop/sqoop1-dao-ru-hdfs-hive-shu-ju/</url>
      
        <content type="html"><![CDATA[<h1 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h1><h2 id="Sqoop是什么"><a href="#Sqoop是什么" class="headerlink" title="Sqoop是什么"></a>Sqoop是什么</h2><p>sqoop是apache旗下一款“Hadoop和关系数据库服务器之间传送数据”的工具。sqoop通过mr导入导出，底层是只有m没有r的mr任务</p><p>导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统；</p><p>导出数据：从Hadoop的文件系统中导出数据到关系数据库</p><p>CDH先安装了Sqoop2，以为用法一样，只是结构不一样了，后来发现用法完全不一样，又把sqoop给装了回来，还复习了一波<br>hadoop1    是我的Mysql安装机器<br>exercise    Mysql中测试的的数据库<br>test_table   Mysql  exercise数据库的测试表</p><h2 id="手动安装sqoop"><a href="#手动安装sqoop" class="headerlink" title="手动安装sqoop"></a>手动安装sqoop</h2><p>这里我是CDH直接安装的，下面安装的步骤是apache版本过程</p><p>1、上传解压sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar包</p><pre><code>tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar</code></pre><p>2、修改etc&#x2F;profile文件</p><pre><code>export JAVA_HOME=/root/Downloads/jdk1.8.0_161export PATH=$PATH:$JAVA_HOME/binexport HADOOP_HOME=/root/Downloads/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport ZOOKEEPER_HOME=/root/Downloads/zookeeper-3.4.5export PATH=$PATH:$ZOOKEEPER_HOME/binexport HIVE_HOME=/root/Downloads/apache-hive-1.2.0-binexport PATH=$PATH:$HIVE_HOME/binexport FLUME_HOME=/root/Downloads/apache-flume-1.6.0-binexport PATH=$PATH:$FLUME_HOME/binexport HBASE_HOME=/root/Downloads/hbase-1.2.6export PATH=$PATH:$HBASE_HOME/binexport SQOOP_HOME=/root/Downloads/sqoop-1.4.6.bin__hadoop-2.0.4-alphaexport PATH=$PATH:$SQOOP_HOME/bin</code></pre><p>source &#x2F;etc&#x2F;profile 来刷新配置文件来使生效</p><p>3、在sqoop下的lib中添加mysql控制包，是负责sql能够识别的</p><pre><code>cd /home/hadoop/hive-1.2.1/libcp mysql-connector-java-5.1.24-bin.jar /home/hadoop/sqoop-1.4.6/lib</code></pre><p>4、修改conf&#x2F;sqoop-env.sh</p><p>mv sqoop-env-template.sh sqoop-env.sh</p><pre><code class="sh">export HADOOP_COMMON_HOME=/opt/modules/hadoop#Set path to where hadoop-*-core.jar is availableexport HADOOP_MAPRED_HOME=/opt/modules/hadoop#Set the path to where bin/hive is availableexport HIVE_HOME=/opt/modules/hiveexport ZOOKEEPER_HOME=/opt/modules/zookeeper#Set the path for where zookeper config dir isexport ZOOCFGDIR=/opt/modules/zookeeper#set the path to where bin/hbase is availableexport HBASE_HOME=/opt/modules/hbase</code></pre><p>5、测试</p><pre><code>sqoop version</code></pre><h3 id><a href="#" class="headerlink" title></a></h3><h2 id="底层工作机制"><a href="#底层工作机制" class="headerlink" title="底层工作机制"></a>底层工作机制</h2><p>将导入或导出命令翻译成 MapReduce 程序来实现</p><p>在翻译出的 MapReduce 中主要是对InputFormat 和 </p><p>OutputFormat 进行定制<img src="/cdh/sqoop/sqoop1-dao-ru-hdfs-hive-shu-ju/640" alt="img"></p><h2 id="参数选项"><a href="#参数选项" class="headerlink" title="参数选项"></a>参数选项</h2><table><thead><tr><th>选项</th><th>含义说明</th></tr></thead><tbody><tr><td>–append</td><td>将数据追加到HDFS上一个已存在的数据集上</td></tr><tr><td>–as-avrodatafile</td><td>将数据导入到Avro数据文件</td></tr><tr><td>–as-sequencefile</td><td>将数据导入到SequenceFile</td></tr><tr><td>–as-textfile</td><td>将数据导入到普通文本文件（默认）</td></tr><tr><td>–boundary-query</td><td><statement>边界查询，用于创建分片（InputSplit）</statement></td></tr><tr><td>–columns &lt;col,col,col…&gt;</td><td>从表中导出指定的一组列的数据</td></tr><tr><td>—delete-target-dir</td><td>如果指定目录存在，则先删除掉</td></tr><tr><td>–direct</td><td>使用直接导入模式（优化导入速度）</td></tr><tr><td>–direct-split-size <n></n></td><td>分割输入stream的字节大小（在直接导入模式下）</td></tr><tr><td>–fetch-size <n></n></td><td>从数据库中批量读取记录数</td></tr><tr><td>–inline-lob-limit <n></n></td><td>设置内联的LOB对象的大小</td></tr><tr><td>-m,–num-mappers <n></n></td><td>使用n个map任务并行导入数据</td></tr><tr><td>-e,–query <statement></statement></td><td>导入的查询语句</td></tr><tr><td>–split-by <column-name></column-name></td><td>指定按照哪个列去分割数据，不同数据交给不同maptask执行</td></tr><tr><td>–table <table-name></table-name></td><td>导入的源表表名</td></tr><tr><td>–target-dir <dir></dir></td><td>导入HDFS的目标路径</td></tr><tr><td>–warehouse-dir <dir></dir></td><td>HDFS存放表的根路径</td></tr><tr><td>–where <where clause></where></td><td>指定导出时所使用的查询条件</td></tr><tr><td>-z,–compress</td><td>启用压缩</td></tr><tr><td>–compression-codec <c></c></td><td>指定Hadoop的codec方式（默认gzip）</td></tr><tr><td>–null-string <null-string></null-string></td><td>如果指定列为字符串类型，使用指定字符串替换值为null的该类列的值</td></tr><tr><td>–null-non-string <null-string></null-string></td><td>如果指定列为非字符串类型，使用指定字符串替换值为null的该类列的值</td></tr><tr><td>这里的<code>--table</code>和 <code>--query</code>两者是可以替换的。在使用query时，可以不使用sql，改成–table的形式</td><td></td></tr><tr><td>使用query时，必须得在Sql语句后面加上&#x2F;<code>$CONDITIONS </code></td><td>外面单引号可以不加，但是双引号必须加，双引号在shell中有特殊含义</td></tr><tr><td><code>$CONDITIONS </code>的作用可以参考以下博客</td><td></td></tr><tr><td><a href="https://www.cnblogs.com/youngchaolin/p/12271211.html">https://www.cnblogs.com/youngchaolin/p/12271211.html</a></td><td></td></tr></tbody></table><h2 id="查询mysql的数据库"><a href="#查询mysql的数据库" class="headerlink" title="查询mysql的数据库"></a>查询mysql的数据库</h2><pre><code>sqoop list-databases \--connect &quot;jdbc:mysql://hadoop1:3306/&quot; \--username root \--password 123456 </code></pre><h2 id="查看指定数据库的表"><a href="#查看指定数据库的表" class="headerlink" title="查看指定数据库的表"></a>查看指定数据库的表</h2><pre><code>sqoop list-tables \--connect &quot;jdbc:mysql://hadoop1:3306/exercise&quot; \--username root \--password 123456</code></pre><h2 id="Sqoop执行SQL语句"><a href="#Sqoop执行SQL语句" class="headerlink" title="Sqoop执行SQL语句"></a>Sqoop执行SQL语句</h2><pre><code>sqoop eval -connect jdbc:mysql://hadoop1:3306/mysql -username root -password 123456 -query &quot;select * from user&quot;</code></pre><h2 id="把mysql表的表结构导入到hive中"><a href="#把mysql表的表结构导入到hive中" class="headerlink" title="把mysql表的表结构导入到hive中"></a>把mysql表的表结构导入到hive中</h2><p>&#x2F;&#x2F;fields-terminated-by 使用非键入字符：’\001’</p><pre><code>sqoop create-hive-table \--connect &quot;jdbc:mysql://hadoop1:3306/exercise?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot; \--table traveling_track \--username root  \--password 123456 \--hive-table exercise.hive_traveling_track \--fields-terminated-by &quot;\001&quot; \--lines-terminated-by &quot;\n&quot;;</code></pre><h2 id="把mysl数据导入Hdfs中"><a href="#把mysl数据导入Hdfs中" class="headerlink" title="把mysl数据导入Hdfs中"></a>把mysl数据导入Hdfs中</h2><pre><code class="shell">sqoop import \--connect &quot;jdbc:mysql://hadoop1:3306/exercise?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot; \--username root \--password 123456 \--query &quot;select * from traveling_track where \$CONDITIONS&quot; \--target-dir /user/hive_traveling_track \--split-by raw_add_time \--direct \--null-string &#39;\\N&#39; \--null-non-string &#39;\\N&#39;</code></pre><p>上述语句还可以指定mr的任务个数    -m </p><h2 id="把mysql的数据导入到hive中-覆盖数据"><a href="#把mysql的数据导入到hive中-覆盖数据" class="headerlink" title="把mysql的数据导入到hive中  覆盖数据"></a>把mysql的数据导入到hive中  覆盖数据</h2><pre><code>sqoop import \--connect &quot;jdbc:mysql://hadoop1:3306/exercise?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot; \--username root \--password 123456 \--query &quot;select * from traveling_track where id = 392855 and \$CONDITIONS&quot; \--fields-terminated-by &#39;,&#39; \--target-dir \hive_traveling_track_temp_data \--num-mappers 1 \--hive-import \--hive-overwrite \--hive-database exercise \--hive-table hive_traveling_track</code></pre><h2 id="把mysql的数据-条件导入-到hive中"><a href="#把mysql的数据-条件导入-到hive中" class="headerlink" title="把mysql的数据 条件导入 到hive中"></a>把mysql的数据 条件导入 到hive中</h2><p>可以使用 –columns 或 –query 这两个参数来指定哪些列或者数据</p><p>！使用query 后，不要使用 –table、–where、–columns了</p><pre><code class="shell">sqoop import \--connect &quot;jdbc:mysql://hadoop1:3306/exercise?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot; \--username root \--password 123456 \--query &quot;select * from traveling_track where id = 392855 and \$CONDITIONS&quot; \--fields-terminated-by &#39;,&#39; \--target-dir \hive_traveling_track_temp_data \--num-mappers 1 \--hive-import \--hive-overwrite \--hive-database exercise \--hive-table hive_traveling_track//--columns &quot;字段1，字段2，字段3&quot; \//--query &#39;SQL  and $CONDITIONS&#39;-m 2</code></pre><h2 id="增量导入数据"><a href="#增量导入数据" class="headerlink" title="增量导入数据"></a>增量导入数据</h2><p>增量数据：和昨天比，新增的数据和改变的数据，这些数据和昨天的数据进行滚动合并</p><h3 id="1-可以通过–query-来手动限制条件，需要在业务表中有相应的字段，不推荐使用"><a href="#1-可以通过–query-来手动限制条件，需要在业务表中有相应的字段，不推荐使用" class="headerlink" title="1.可以通过–query 来手动限制条件，需要在业务表中有相应的字段，不推荐使用"></a>1.可以通过–query 来手动限制条件，需要在业务表中有相应的字段，不推荐使用</h3><h3 id="2-append增量导入数据到Hdfs，然后hive中load加载数据"><a href="#2-append增量导入数据到Hdfs，然后hive中load加载数据" class="headerlink" title="2.append增量导入数据到Hdfs，然后hive中load加载数据"></a>2.append增量导入数据到Hdfs，然后hive中load加载数据</h3><p><strong>适用情况：表的数据类似hive的事实表，不会发生改变只有新增数据的表。</strong> </p><pre><code>sqoop import \--connect &quot;jdbc:mysql://hadoop1:3306/exercise?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot; \--username root \--password 123456 \--table traveling_track \--target-dir /user/temp_traveling_track \--split-by id \--m 6  \--incremental append \--check-column id \--last-value 392855</code></pre><p>–check-column：用指定字段去检查是否符合增量导入的条件<br>–incremental：指定增量导入的模式,可选 <code>append</code>  |  <code>lastmodified</code><br>–last-value：跟–check-column 配合使用,上次导入的最后一个值,元数据中所有大于last-value的值都会被导入</p><p>–m 使用<em>n个</em>map任务并行导入</p><p>–split-by <column-name>  用于拆分工作单元的表格列，默认只能数字进行切分，默认情况下使用文本切分会报错，可通过添加参数使他不报错：</column-name></p><pre><code>sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true  \</code></pre><p>–as-avrodatafile 将数据导入 Avro 数据文件<br>–as-sequencefile将数据导入 SequenceFiles<br>–as-textfile          以纯文本形式导入数据（默认）</p><h3 id="3-lastmodified增量导入数据到Hdfs，然后hive中load加载数据"><a href="#3-lastmodified增量导入数据到Hdfs，然后hive中load加载数据" class="headerlink" title="3.lastmodified增量导入数据到Hdfs，然后hive中load加载数据"></a>3.lastmodified增量导入数据到Hdfs，然后hive中load加载数据</h3><p>适用情况：新增数据的同时老数据还会发生改变.</p><p>这种增量导入方式不支持直接写入hive！需要自己在hive中load</p><pre><code>sqoop import \--connect &quot;jdbc:mysql://hadoop1:3306/exercise?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot; \--username root \--password 123456 \--table traveling_track \--target-dir /user/temp_traveling_track \--split-by id \-m 4 \--incremental lastmodified \--merge-key id \--check-column raw_add_time \--last-value &quot;2020-07-31 09:31:51&quot;;</code></pre><p><code>--check-column</code>: 必须是timestamp列  时间列（int）<br><code>--incremental lastmodified</code>: 设置为最后改动模式，基于时间列的增量导入（将时间列大于等于阈值的所有数据增量导入Hadoop）<br><code>--merge-key</code>: 合并列（主键，合并键值相同的记录）必须是唯一主键。导入后的数据如果需要跟存量进行合并，使用此参数。 导入的增量数据会将新旧数据进行合并，不会保留。<br><code>--last-value</code>: 所有<strong>大于</strong>最后一个时间的数据都会被更新</p><h2 id="Sqoop合并新老数据"><a href="#Sqoop合并新老数据" class="headerlink" title="Sqoop合并新老数据"></a>Sqoop合并新老数据</h2><p>不建议使用这个功能，不灵活</p><pre><code class="shell">sqoop codegen \--connect &quot;jdbc:mysql://hadoop1:3306/exercise?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot; \--username root \--password 123456 \--table traveling_track \--bindir /user/temp_traveling_track \--class-name id \--fields-terminated-by &quot;,&quot; \--null-string &#39;\\N&#39; \--null-non-string &#39;\\N&#39;//数据库地址//账号//密码//表名称//生成的jar包存放地址//生成的类名称//数据分隔符</code></pre><pre><code class="shell">sqoop merge \--new-data /sqoopdata/stu1--onto /sqoopdata/stu0  \--target-dir /sqoopdata/stu_all  \--jar-file /opt/apps/code/stu/Stu.jar \--class-name Stu \--merge-Key id//最新数据（增量）//到 老数据  （全量）//放到哪个路径//jar包的位置//类的名称//合并的key</code></pre><h2 id="Sqoop与HBase"><a href="#Sqoop与HBase" class="headerlink" title="Sqoop与HBase"></a>Sqoop与HBase</h2><h3 id="mysql导入hbase表中"><a href="#mysql导入hbase表中" class="headerlink" title="mysql导入hbase表中"></a>mysql导入hbase表中</h3><pre><code class="shell">#-column-family &#39;&#39;指定列族名sqoop import -connect jdbc:mysql://linux03:3306/hehe -username root -password 123456 root-table student -hbase-table student2 -column-family &#39;per data&#39; -hbase-row-key id -m 1 </code></pre><h2 id="Sqoop的Append和Lastmodified的区别"><a href="#Sqoop的Append和Lastmodified的区别" class="headerlink" title="Sqoop的Append和Lastmodified的区别"></a>Sqoop的Append和Lastmodified的区别</h2><p>Lastmodified 和Append模式的区别：<br><code>Append</code>模式处理不了更新数据，而<code>Lastmodified</code>模式可以</p><h2 id="Sqoop导入hdfs的null值处理"><a href="#Sqoop导入hdfs的null值处理" class="headerlink" title="Sqoop导入hdfs的null值处理"></a>Sqoop导入hdfs的null值处理</h2><p>数据库表中的null值 经过  sqoop导入hdfs中后 再文件中也显示null</p><p>而hive加载数据会认为null是字符串，hive中的null是\N</p><p>通过下列参数处理：</p><p>–null-string ‘\\N’</p><p>–null-string ‘\\N’</p><p>(两条\\) 第一条是转义字符</p><table><thead><tr><th>–null-string <null-string></null-string></th><th>如果指定列为字符串类型，使用指定字符串替换值为null的该类列的值</th></tr></thead><tbody><tr><td>–null-non-string <null-string></null-string></td><td>如果指定列为非字符串类型，使用指定字符串替换值为null的该类列的值</td></tr></tbody></table><h2 id="Sqoop导出RDbms的null值处理"><a href="#Sqoop导出RDbms的null值处理" class="headerlink" title="Sqoop导出RDbms的null值处理"></a>Sqoop导出RDbms的null值处理</h2><table><thead><tr><th>–input-null-string <null-string></null-string></th><th>如果指定列为字符串类型，使用指定字符串替换值为null的该类列的值</th></tr></thead><tbody><tr><td>–input-null-non-string <null-string></null-string></td><td>如果指定列为非字符串类型，使用指定字符串替换值为null的该类列的值</td></tr></tbody></table><h2 id="把hive数据导出到mysql"><a href="#把hive数据导出到mysql" class="headerlink" title="把hive数据导出到mysql"></a>把hive数据导出到mysql</h2><p><strong>表必须存在！</strong></p><pre><code class="shell">sqoop export \--connect &quot;jdbc:mysql://hadoop1:3306/exercise?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot; \--username root \--password 123456 \--table dau_t \--input-fields-terminated-by &#39;,&#39; \--export-dir &#39;/user/hive/warehouse/dau_t&#39; \--batch  #以batch模式执行sql --地址 --账号 --密码 --表名 --hdfs分隔符 --hdfs文件位置 --batch模式执行</code></pre><h2 id="数据导出到mysql，更新模式选择"><a href="#数据导出到mysql，更新模式选择" class="headerlink" title="数据导出到mysql，更新模式选择"></a>数据导出到mysql，更新模式选择</h2><p><strong>控制新旧数据导到mysql时，选择更新模式</strong></p><p><strong>–update-mode</strong> 如果选择updateonly，只会对mysql中已存在的id数据进行更新，不存在的id数据不会插入了</p><p>123 、345、123</p><p><strong>–update-mode</strong> 如果选择allowinsert，既会更新已存在id数据，也会插入新的id数据</p><p>123、345、12345</p><pre><code class="shell"> sqoop export \ --connect &quot;jdbc:mysql://hadoop1:3306/exercise?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot; \ --username root \ --password 123456 \ --table dau_t \ --export-dir &#39;/export3&#39; \ --input-null-string &#39;\\N&#39; \ --input-null-non-stirng &#39;\\N&#39; \ --update-mode allowinsert \ --update-key id \ --batch  --地址 --账号 --密码 --表名 --hdfs地址 --null值处理 --null string处理 --更新模式 --key --batch模式</code></pre>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Yarn的学习笔记</title>
      <link href="/cdh/yarn/yarn/"/>
      <url>/cdh/yarn/yarn/</url>
      
        <content type="html"><![CDATA[<h2 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h2><h3 id="Yarn是什么？"><a href="#Yarn是什么？" class="headerlink" title="Yarn是什么？"></a>Yarn是什么？</h3><p>Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的 Hadoop <strong>资源管理器</strong>，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。</p><h3 id="Yarn的组件"><a href="#Yarn的组件" class="headerlink" title="Yarn的组件"></a>Yarn的组件</h3><p><a href="https://www.cnblogs.com/TiePiHeTao/p/66b3f9a60ed97359681c7cb97f1e3627.html">https://www.cnblogs.com/TiePiHeTao/p/66b3f9a60ed97359681c7cb97f1e3627.html</a></p><h4 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a><strong>ResourceManager</strong></h4><p>sourceManager负责整个集群的资源管理和分配，是一个全局的资源管理系统。<strong>它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，ASM）。</strong></p><p>NodeManager以心跳的方式向ResourceManager汇报资源使用情况（目前主要是CPU和内存的使用情况）。RM只接受NM的资源回报信息，对于具体的资源处理则交给NM自己处理。</p><p>YARN Scheduler根据application的请求为其分配资源，不负责application job的监控、追踪、运行状态反馈、启动等工作（<strong>仅负责分配资源</strong>）。</p><h4 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h4><p>NodeManager是每个节点上的资源和任务管理器，它是管理这台机器的代理，负责该节点程序的运行，以及该节点资源的管理和监控。YARN集群每个节点都运行一个NodeManager。</p><p>NodeManager定时向ResourceManager汇报本节点资源（CPU、内存）的使用情况和Container的运行状态。当ResourceManager宕机时NodeManager自动连接RM备用节点。</p><p>NodeManager接收并处理来自ApplicationMaster的Container启动、停止等各种请求。</p><h4 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h4><p>用户提交的<strong>每个应用程序均包含一个ApplicationMaster</strong>，它可以运行在ResourceManager以外的机器上。</p><p>负责与RM调度器协商以获取资源（用Container表示）。</p><p>将得到的任务进一步分配给内部的任务(资源的二次分配)。</p><p>与NM通信以启动&#x2F;停止任务。</p><p>监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。</p><h4 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h4><p>资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示，Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需要设计新的调度器，YARN提供了多种直接可用的调度器，比如Fair Scheduler和Capacity Scheduler等。</p><h3 id="Yarn的内存"><a href="#Yarn的内存" class="headerlink" title="Yarn的内存"></a>Yarn的内存</h3><p>yarn的内存默认设置是8G，可以灵活的调整他的大小</p><p>运行一个任务默认最大内存也是8G，这个也是需要调整的</p><h3 id="Yarn执行的流程"><a href="#Yarn执行的流程" class="headerlink" title="Yarn执行的流程"></a>Yarn执行的流程</h3><p><a href="https://www.cnblogs.com/Transkai/p/10549923.html">https://www.cnblogs.com/Transkai/p/10549923.html</a></p><p><img src="/cdh/yarn/yarn/1565437165939.png" alt="1565437165939"></p><p>2、如果是yarn平台，对resoucemanager提交作业审请<br>3、resourcemanager返回一个jobid和数据保存目录（hdfs:&#x2F;&#x2F;xxx&#x2F;staging&#x2F;xxx）<br>4、客户端根据返回数据保存目录路径，将job.split、job.xml、jar文件提交到hdfs:&#x2F;&#x2F;xxx&#x2F;staging&#x2F;xxx目录<br>5、提交数据资源之后，客户端对resouremanager提交任务运行<br>6、resourcemanager将任务存储任务队列<br>7、resourcemanager发送命令nodemanager处理从任务取出的任务<br>8、nodemanager往resourcemanageer审请我要创建一个app master<br>a、在nodemanager创建一个container，再启动app master<br>9、app master读取数据切片处理方案<br>10、app master往resourcemanager审请运行资源<br>11、resourcemanager往空闲的nodemanager主机发送指令，要创建Container<br>12、app master往nodemanger发送运行指令，container运行任务。</p><h3 id="Yarn调度器"><a href="#Yarn调度器" class="headerlink" title="Yarn调度器"></a>Yarn调度器</h3><p><a href="https://www.cnblogs.com/lenmom/p/11285273.html">https://www.cnblogs.com/lenmom/p/11285273.html</a></p><h5 id="1、FIFO-Scheduler-先进先出调度器"><a href="#1、FIFO-Scheduler-先进先出调度器" class="headerlink" title="1、FIFO Scheduler(先进先出调度器)"></a><strong>1、FIFO Scheduler(先进先出调度器)</strong></h5><p>先进先出，但不适合资源公平性</p><p>FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其它应用被阻塞。在共享集群中，更适合采用Capacity Scheduler或Fair Scheduler，这两个调度器都允许大任务和小任务在提交的同时获得一定的系统资源。下面“Yarn调度器对比图”展示了这几个调度器的区别，从图中可以看出，<strong>在FIFO 调度器中，小任务会被大任务阻塞</strong>。</p><p><img src="/cdh/yarn/yarn/31857-20190801204346575-976593023.png" alt="img"></p><h5 id="2、Capacity-Scheduler-容量调度器"><a href="#2、Capacity-Scheduler-容量调度器" class="headerlink" title="2、Capacity Scheduler(容量调度器)"></a><strong>2、Capacity Scheduler(容量调度器)</strong></h5><p><img src="/cdh/yarn/yarn/31857-20190801204818038-843718141.png" alt="img"></p><p>yarn-site.xml中默认配置的资源调度器。</p><p>独立的专门队列保证小作业也可以提交后就启动，队列容量是专门保留的</p><p>以整个集群的利用率为代价，与FIFO比，大作业执行的时间要长</p><p>Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。用这个资源调度器，就可以配置yarn资源队列，这个后面后介绍用到。</p><h5 id="3、FairS-scheduler-公平调度器"><a href="#3、FairS-scheduler-公平调度器" class="headerlink" title="3、FairS scheduler(公平调度器)"></a><strong>3、FairS scheduler(公平调度器)</strong></h5><p>不需要预留资源，调度器可以在运行的作业之间动态平衡资源，大作业启动时，因为是唯一运行的，所以获得集群的所有资源，之后小作业启动时，被分配到集群的一半的资源，这样每个作业都能公平共享资源</p><p>Fair调度器的设计目标是为所有的应用分配公平的资源（对公平的定义可以通过参数来设置）</p><p>举个例子，假设有两个用户A和B，他们分别拥有一个队列。当A启动一个job而B没有任务时，A会获得全部集群资源；当B启动一个job后，A的job会继续运行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个job并且其它job还在运行，则它将会和B的第一个job共享B这个队列的资源，也就是B的两个job会用于四分之一的集群资源，而A的job仍然用于集群一半的资源，结果就是资源最终在两个用户之间平等的共享。在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。<br>a) 公平调度器，就是能够共享整个集群的资源<br>b) 不用预先占用资源，每一个作业都是共享的<br>c) 每当提交一个作业的时候，就会占用整个资源。如果再提交一个作业，那么第一个作业就会分给第二个作业一部分资源，第一个作业也就释放一部分资源。再提交其他的作业时，也同理。。。。也就是说每一个作业进来，都有机会获取资源。</p><h4 id="调度器之间的区别"><a href="#调度器之间的区别" class="headerlink" title="调度器之间的区别"></a>调度器之间的区别</h4><p>**FIFO Scheduler(先进先出调度器)**不用进行配置，小任务会被大任务阻塞。</p><p>**Capacity Scheduler(容量调度器)**，是yarn的默认调度器，会预先占用一定集群资源，小任务一个执行队列，大任务一个执行队列</p><p>**FairS scheduler(公平调度器)**，能够共享整个集群资源，不用预先占用集群资源，每一个作业都是共享的</p><h4 id="Fair-Scheduler与Capacity-Scheduler区别"><a href="#Fair-Scheduler与Capacity-Scheduler区别" class="headerlink" title="Fair Scheduler与Capacity Scheduler区别"></a>Fair Scheduler与Capacity Scheduler区别</h4><p><strong>资源公平共享</strong>：在每个队列中，Fair Scheduler可选择按照FIFO、Fair或DRF策略为应用程序分配资源。Fair策略即平均分配，默认情况下，每个队列采用该方式分配资源<br><strong>支持资源抢占</strong>：当某个队列中有剩余资源时，调度器会将这些资源共享给其他队列，而当该队列中有新的应用程序提交时，调度器要为它回收资源。为了尽可能降低不必要的计算浪费，调度器采用了先等待再强制回收的策略，即如果等待一段时间后尚有未归还的资源，则会进行资源抢占；从那些超额使用资源的队列中杀死一部分任务，进而释放资源<br><strong>负载均衡</strong>：Fair Scheduler提供了一个基于任务数的负载均衡机制，该机制尽可能将系统中的任务均匀分配到各个节点上。此外，用户也可以根据自己的需求设计负载均衡机制<br><strong>调度策略灵活配置</strong>：Fiar Scheduler允许管理员为每个队列单独设置调度策略（当前支持FIFO、Fair或DRF三种）<br>提高小应用程序响应时间：由于采用了最大最小公平算法，小作业可以快速获取资源并运行完成</p><h3 id="Yarn的历史服务器"><a href="#Yarn的历史服务器" class="headerlink" title="Yarn的历史服务器"></a>Yarn的历史服务器</h3><p><a href="https://blog.csdn.net/HoldBelief/article/details/79565573">MR History Server与Spark History Server</a>的对比</p><p>向yarn集群上提交任务，但是任务执行完后，我们就不能再查看log文件了。此时我们可以开启historyserver，实现yarn集群上历史任务的保存。</p><p>历史服务器可以常看已经运行完的mapreduce的提交时间，用论多少个Reduce等信息</p><h5 id="配置历史服务器"><a href="#配置历史服务器" class="headerlink" title="配置历史服务器"></a>配置历史服务器</h5><h6 id="1-vim-mapred-site-xml"><a href="#1-vim-mapred-site-xml" class="headerlink" title="1.vim mapred-site.xml"></a>1.vim mapred-site.xml</h6><p>这个文件配置mapreduce 的 historyserver 服务</p><pre><code class="xml">&lt;!-- History Server的地址 --&gt;&lt;property&gt;    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;    &lt;value&gt;127.0.0.1:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- History Server的WebUI地址 --&gt;&lt;property&gt;    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;    &lt;value&gt;127.0.0.1:19888&lt;/value&gt;&lt;/property&gt;&lt;!-- 存放MR运行完成日志的最终目录 --&gt;&lt;property&gt;    &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt;    &lt;value&gt;/history/done&lt;/value&gt;&lt;/property&gt;</code></pre><h6 id="2-vim-yarn-site-xml"><a href="#2-vim-yarn-site-xml" class="headerlink" title="2.vim yarn-site.xml"></a>2.vim yarn-site.xml</h6><p>yarn是负责根据containerid或者applicationid,查询日志.</p><p>将文件地址存放在hdfs中，设定的目录要存在！</p><pre><code class="xml">&lt;!-- 启用日志聚合功能，默认值false --&gt;&lt;property&gt;    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 在HDFS上聚合的日志最多保存多长时间，默认值：-1，即永久保存 --&gt; &lt;property&gt;    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;    &lt;value&gt;2592000&lt;/value&gt;&lt;/property&gt;&lt;!-- MR History Server的webUI地址 --&gt;&lt;property&gt;    &lt;name&gt;yarn.log.server.url&lt;/name&gt;    &lt;value&gt;http://node01:19888/jobhistory/logs&lt;/value&gt;&lt;/property&gt;&lt;!-- 当应用程序运行结束后，日志被转移到的HDFS目录（启用日志聚合后该参数生效），默认值/tmp/logs --&gt;&lt;property&gt;    &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;    &lt;value&gt;hdfs://node01:9000/user/hpe/yarn-logs/&lt;/value&gt;&lt;/property&gt;</code></pre><h5 id="启动历史服务器"><a href="#启动历史服务器" class="headerlink" title="启动历史服务器"></a>启动历史服务器</h5><p>在默认情况下，Hadoop历史服务器是没有启动的</p><pre><code class="shell">mr-jobhistory-daemon.sh start historyserver   --启动历史服务器默认端口  19888</code></pre><h3 id="Yarn调度器配置文件"><a href="#Yarn调度器配置文件" class="headerlink" title="Yarn调度器配置文件"></a><strong>Yarn</strong>调度器<strong>配置</strong>文件</h3><p>yarn资源调度器是在<strong>yarn-site.xml</strong>中配置</p><h3 id="Yarn的Uber模式"><a href="#Yarn的Uber模式" class="headerlink" title="Yarn的Uber模式"></a>Yarn的Uber模式</h3><p>Yarn的默认配置会禁用uber组件，即不允许JVM重用（不允许container重用）。我们先看看在这种情况下，Yarn是如何执行一个MapReduce job的。首先，Resource Manager里的Application Manager会为每一个application(比如一个用户提交的MapReduce Job)在NodeManager里面申请一个container，然后在该container里面启动一个Application Master。container在Yarn中是分配资源的容器(内存、cpu、硬盘等)，它启动时便会相应启动一个JVM。此时，Application Master便陆续为application包含的每一个task(一个Map task或Reduce task)向Resource Manager申请一个container。等每得到一个container后，便要求该container所属的NodeManager将此container启动，然后就在这个container里面执行相应的task。等这个task执行完后，这个container便会被NodeManager收回，而container所拥有的JVM也相应地被退出。</p><p>申请——》启动——》运行——》回收</p><p>用户可以通过启用uber组件来允许JVM重用——即在同一个container里面依次执行多个task。在mapred-site.xml文件中，改变一下几个参数的配置即可启用uber的方法：</p><h4 id="Uber配置文件"><a href="#Uber配置文件" class="headerlink" title="Uber配置文件"></a>Uber配置文件</h4><p>在maped-site.xml文件添加</p><p><img src="/cdh/yarn/yarn/1565710519732.png" alt="1565710519732"></p><h3 id="Yarn-分布式资源管理系统"><a href="#Yarn-分布式资源管理系统" class="headerlink" title="Yarn 分布式资源管理系统"></a>Yarn 分布式资源管理系统</h3><p>yarn能够给长短应用运行时提供资源分配</p><h3 id="RPC协议"><a href="#RPC协议" class="headerlink" title="RPC协议"></a>RPC协议</h3><p>远程过程调用</p><p><img src="/cdh/yarn/yarn/1565525258071.png" alt="1565525258071"></p><h5 id="长应用"><a href="#长应用" class="headerlink" title="长应用"></a>长应用</h5><p>通常情况下，永不停止运行</p><h5 id="短应用"><a href="#短应用" class="headerlink" title="短应用"></a>短应用</h5><p>短时间内会运行结束的程序</p><p>JobTracker是主节点的进程 </p><p>TaskTracker是从节点的进程 ，管理mr任务 </p><p><img src="/cdh/yarn/yarn/1565437244367.png" alt="1565437244367"></p><h3 id="一个程序运行需要3点"><a href="#一个程序运行需要3点" class="headerlink" title="一个程序运行需要3点"></a>一个程序运行需要3点</h3><p>数据，资源，mr（jar包）</p>]]></content>
      
      
      <categories>
          
          <category> Yarn </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Yarn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH5.16.2 Spark安装后启动失败</title>
      <link href="/cdh/spark/cdh5.16-spark2-an-zhuang-cheng-gong-dan-qi-dong-shi-bai/"/>
      <url>/cdh/spark/cdh5.16-spark2-an-zhuang-cheng-gong-dan-qi-dong-shi-bai/</url>
      
        <content type="html"><![CDATA[<p>参考文章<a href="https://blog.csdn.net/nieji3057/article/details/79416531">https://blog.csdn.net/nieji3057/article/details/79416531</a><br>CDHspark升级成Spark2之后，在启动时spark后面都要加一个2，</p><pre><code>spark-shell   -&gt;  spark2-shell</code></pre><p>启动Spark后会报一个错</p><pre><code>Permission denied: user=root, access=WRITE, inode=&quot;/user&quot;</code></pre><p>这是权限不对，这里已经是root，应该是最高权限了，怎么不对<br>在CDH中，hdfs用户才是最高的权限，你的HDFS目录中的spark目录所属用户是hdfs，root没有权限访问也就导致了报错<br>需要把你的Hdfs路径的Spark权限改成spark用户的</p><pre><code>sudo -u hdfs hadoop fs -chown root /user</code></pre><p>在命令行中重试</p><pre><code>spark2-shell</code></pre><p><img src="https://img-blog.csdnimg.cn/20201109090350137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDY3NTg0NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>成了</p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka2.2.0命令</title>
      <link href="/cdh/kafka/kafka2.2.cdh-ming-ling-xing-ming-ling/"/>
      <url>/cdh/kafka/kafka2.2.cdh-ming-ling-xing-ming-ling/</url>
      
        <content type="html"><![CDATA[<p>Kafka安装完成启动后，测试连不起来，发现版本比以前高，他们的语法有点变化，zookeeper这个语法被淘汰掉了，因为新版本不存放offset在zk上了。使用–bootstrap-server来指定broker的位置</p><blockquote><p>查看所有topic</p></blockquote><pre><code>kafka-topics --list --bootstrap-server hadoop1:9092,hadoop2:9092,hadoop3:9092</code></pre><blockquote><p>生产者</p></blockquote><pre><code>kafka-console-producer --broker-list hadoop1:9092,hadoop2:9092,hadoop3:9092 --topic TP_LABEL</code></pre><blockquote><p>消费者</p></blockquote><pre><code>kafka-console-consumer --bootstrap-server hadoop1:9092,hadoop2:9092,hadoop3:9092 --topic TP_LABEL</code></pre><blockquote><p>消费者-从头消费数据</p></blockquote><pre><code>kafka-console-consumer --bootstrap-server hadoop1:9092,hadoop2:9092,hadoop3:9092 --from-beginning --topic TP_LABEL</code></pre><blockquote><p>查看topic主题详细信息</p></blockquote><pre><code>kafka-topics  --bootstrap-server hadoop1:9092,hadoop2:9092,hadoop3:9092 --topic TP_LABEL --describe</code></pre><blockquote><p>查看消费者组</p></blockquote><pre><code>kafka-consumer-groups --bootstrap-server hadoop1:9092,hadoop2:9092,hadoop3:9092 --list</code></pre><blockquote><p>查看指定消费者组详细信息(CONSUMER-ID HOST CLIENT-ID PARTITIONS ASSIGNMENT)</p></blockquote><pre><code>kafka-consumer-groups --bootstrap-server hadoop1:9092,hadoop2:9092,hadoop3:9092 --group cloudera_mirrormaker --describe --members --verboseCONSUMER-ID                                                 HOST            CLIENT-ID              #PARTITIONS     ASSIGNMENTcloudera_mirrormaker-0-1d1a8103-3e9d-43c4-9a34-09f677fd049a /10.130.210.246 cloudera_mirrormaker-0 0               -cloudera_mirrormaker-0-a6168ff9-e579-4536-b92a-fc6aaf82b8c1 /10.130.210.245 cloudera_mirrormaker-0 0               -cloudera_mirrormaker-0-a110eabd-5191-49a0-acf4-dbdc7e9a602f /10.130.210.247 cloudera_mirrormaker-0 0               -</code></pre><blockquote><p>查看指定消费者组详细信息(CONSUMER-ID HOST CLIENT-ID PARTITIONS)</p></blockquote><pre><code>kafka-consumer-groups --bootstrap-server hadoop1:9092,hadoop2:9092,hadoop3:9092 --group cloudera_mirrormaker --describe --members//CONSUMER-ID                                                 HOST            CLIENT-ID              #PARTITIONS     //cloudera_mirrormaker-0-1d1a8103-3e9d-43c4-9a34-09f677fd049a /10.130.210.246 cloudera_mirrormaker-0 0               //cloudera_mirrormaker-0-a6168ff9-e579-4536-b92a-fc6aaf82b8c1 /10.130.210.245 cloudera_mirrormaker-0 0               //cloudera_mirrormaker-0-a110eabd-5191-49a0-acf4-dbdc7e9a602f /10.130.210.247 cloudera_mirrormaker-0 0 </code></pre><blockquote><p>查看指定消费者组详细信息(TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID)</p></blockquote><pre><code>kafka-consumer-groups --bootstrap-server hadoop1:9092,hadoop2:9092,hadoop3:9092 --group cloudera_mirrormaker --describe//TOPIC               PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID//xxx_trajectory 0          143693805       143693905       100             consumer-1-02f47385-5496-4094-9513-09c0b66b47bc /59.202.28.243  consumer-1//xxx_trajectory 1          137186261       137186348       87              consumer-1-02f47385-5496-4094-9513-09c0b66b47bc /59.202.28.243  consumer-1</code></pre><blockquote><p>查看指定消费者组详细信息(COORDINATOR(ID) ASSIGNMENT-STRATEGY STATE MEMBERS)</p></blockquote><pre><code>kafka-consumer-groups --bootstrap-server hadoop1:9092,hadoop2:9092,hadoop3:9092 --group cloudera_mirrormaker --describe --state//COORDINATOR (ID)          ASSIGNMENT-STRATEGY       STATE                #MEMBERS//hadoop3:9092 (63)         range                     Stable               3</code></pre>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka笔记</title>
      <link href="/cdh/kafka/kafka/"/>
      <url>/cdh/kafka/kafka/</url>
      
        <content type="html"><![CDATA[<h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><h3 id="Kafka是什么？"><a href="#Kafka是什么？" class="headerlink" title="Kafka是什么？"></a>Kafka是什么？</h3><p>Kafka是一个<strong>分布式</strong>的基于<strong>发布&#x2F;订阅模式</strong>的<strong>消息队列，</strong>主要应用于大数据实时处理领域。kafka0.9是一个过渡点。</p><h3 id="Kafka端口号"><a href="#Kafka端口号" class="headerlink" title="Kafka端口号"></a>Kafka端口号</h3><p>9092 Kafka通信端口号</p><h3 id="消息队列有什么作用？"><a href="#消息队列有什么作用？" class="headerlink" title="消息队列有什么作用？"></a>消息队列有什么作用？</h3><p>1、异步处理，使用消息队列当中间件，进行异步处理，更好的用户体验</p><p><img src="/cdh/kafka/kafka/image-20200408195438360.png" alt="image-20200408195438360"></p><p>2、秒杀系统</p><p>使用消息队列进行来削峰，消除峰值，先进先出，减少秒杀系统的处理压力，只需要取出先进来的前多少名就解决了秒杀系统的高并发</p><p><img src="/cdh/kafka/kafka/image-20200408200056295.png" alt="image-20200408200056295"></p><p>3、解耦</p><p>使用消息队列解耦，案例flume，flume中间件channel就是一个中间件，上下兼容，进行了解耦，如果没有这个中间件，source和sink各自适配，将会非常麻烦</p><p><img src="/cdh/kafka/kafka/image-20200408200035955.png" alt="image-20200408200035955"></p><h3 id="消息队列的两种模式"><a href="#消息队列的两种模式" class="headerlink" title="消息队列的两种模式"></a>消息队列的两种模式</h3><p>（1）<strong>点对点模式</strong>（一对一，消费者主动拉取数据，消息收到后消息清除）<strong>消息是一次性的</strong>，已经不怎么用了</p><p>消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。</p><p>消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。<strong>Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费</strong></p><p><img src="/cdh/kafka/kafka/image-20200408200325374.png" alt="image-20200408200325374"></p><p>（2）<strong>发布&#x2F;订阅模式</strong>（一对多，消费者消费数据之后不会清除消息）是常用的模式,不是一次性的,数据保存在磁盘上</p><p>消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。<strong>和点对点方式不同，发布到topic的消息会被所有订阅者消费。</strong></p><h3 id="Kafka的基本架构"><a href="#Kafka的基本架构" class="headerlink" title="Kafka的基本架构"></a>Kafka的基本架构</h3><p><img src="/cdh/kafka/kafka/image-20200910155746550.png" alt="image-20200910155746550"></p><h3 id="Kafka各个组件作用"><a href="#Kafka各个组件作用" class="headerlink" title="Kafka各个组件作用"></a>Kafka各个组件作用</h3><p>1）<strong>Producer</strong> ：消息生产者，就是向kafka broker发消息的客户端；</p><p>2）<strong>Consumer</strong> ：消息消费者，向kafka broker取消息的客户端；</p><p>3）<strong>Consumer Group （CG）</strong>：消费者组，由<strong>多个consumer组成</strong>。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</p><p>4）<strong>Broker</strong> ：一台kafka服务器就是一个broker。<strong>一个集群由多个broker组成</strong>。一个broker可以容纳多个topic。</p><p>5）<strong>Topic</strong> ：可以理解为一个队列，生产者和消费者面向的都是一个topic；</p><p>6）<strong>Partition</strong>：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；</p><p>7）<strong>Replica</strong>：副本，为防止集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，<strong>一个topic的每个分区都有若干个副本，一个leader和若干个follower。</strong></p><p>8）<strong>leader</strong>：每个分区多个副本的“主”，<strong>生产者发送数据的对象，以及消费者消费数据的对象都是leader。</strong></p><p>9）<strong>follower</strong>：每个分区多个副本中的“从”，实时从leader中同步数据，<strong>保持和leader数据的同步。leader发生故障时，某个follower会成为新的follower。</strong></p><h3 id="Kafka基本概念"><a href="#Kafka基本概念" class="headerlink" title="Kafka基本概念"></a>Kafka基本概念</h3><p>Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。</p><p><strong>topic是逻辑上的概念，而partition是物理上的概念</strong>，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。</p><p>offset是一个long型的数字，我们通过这个offset可以确定一条在该partition下的唯一消息。在partition下面是保证了有序性，但是在topic下面没有保证有序性。</p><p>消费者组中的每个消费者，都会实时记录自己消费到了哪个offset ，以便出错恢复时，从上次的位置继续消费。</p><h3 id="Kafka生产者"><a href="#Kafka生产者" class="headerlink" title="Kafka生产者"></a>Kafka生产者</h3><h4 id="一、分区策略"><a href="#一、分区策略" class="headerlink" title="一、分区策略"></a>一、分区策略</h4><h5 id="1）分区的原因"><a href="#1）分区的原因" class="headerlink" title="1）分区的原因"></a><strong>1</strong>）分区的原因</h5><p>（1）<strong>方便在集群中扩展</strong>，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</p><p>（2）<strong>可以提 高并发</strong>，因为可以以Partition为单位读写了。</p><p>（3）负载均衡，防止热点数据。</p><h5 id="2）分区的规则"><a href="#2）分区的规则" class="headerlink" title="2）分区的规则"></a><strong>2</strong>）分区的规则</h5><p>我们需要将producer发送的数据封装成一个<strong>ProducerRecord</strong>对象。</p><p><img src="/cdh/kafka/kafka/image-20200910164708408.png" alt="image-20200910164708408"></p><p>（1）指明 partition 的情况下，直接将指明的值直接作为 partiton 值；<br>（2）没有指明 partition 值但有 key 的情况下，将 key 的 <strong>hash 值</strong>与 topic 的 partition 数进行取余得到 partition 值；<br>（3）既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。(黏性分区规则)</p><h4 id="二、Producer发送数据可靠性保证"><a href="#二、Producer发送数据可靠性保证" class="headerlink" title="二、Producer发送数据可靠性保证"></a>二、Producer发送数据可靠性保证</h4><p>为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到<strong>ack</strong>，就会进行下一轮的发送，否则重新发送数据</p><p><img src="/cdh/kafka/kafka/image-20200910164935464.png" alt="image-20200910164935464"></p><h5 id="1）副本数据同步策略"><a href="#1）副本数据同步策略" class="headerlink" title="1）副本数据同步策略"></a><strong>1）副本数据同步策略</strong></h5><table><thead><tr><th><strong>方案</strong></th><th><strong>优点</strong></th><th><strong>缺点</strong></th></tr></thead><tbody><tr><td><strong>半数以上完成同步，就发送ack</strong></td><td>延迟低</td><td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td></tr><tr><td><strong>全部完成同步，才发送ack</strong></td><td>选举新的leader时，容忍n台节点的故障，需要n+1个副本</td><td>延迟高</td></tr></tbody></table><p><strong>Kafka选择了第二种方案，原因如下：</strong></p><p>1.同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</p><p>2.虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</p><h5 id="2）ISR"><a href="#2）ISR" class="headerlink" title="2）ISR"></a>2）ISR</h5><p>副本同步队列</p><p>Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给follower发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的leader。</p><h5 id="3）ack应答机制"><a href="#3）ack应答机制" class="headerlink" title="3）ack应答机制"></a><strong>3</strong>）ack应答机制</h5><p>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。<br>所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。<br>acks参数配置：<br>acks：<br><strong>0</strong>：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；<br><strong>1</strong>：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据</p><p><strong>-1</strong>（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复。</p><img src="/cdh/kafka/kafka/Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200910165946929.png" alt="image-20200910165946929" style="zoom: 67%;"><img src="/cdh/kafka/kafka/Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200910170004399.png" alt="image-20200910170004399" style="zoom:67%;"><h5 id="4）故障处理细节"><a href="#4）故障处理细节" class="headerlink" title="4）故障处理细节"></a><strong>4</strong>）故障处理细节</h5><p><img src="/cdh/kafka/kafka/image-20200910170106287.png" alt="image-20200910170106287"></p><h5 id="（1）follower故障"><a href="#（1）follower故障" class="headerlink" title="（1）follower故障"></a>（1）follower故障</h5><p>follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。</p><h5 id="（2）leader故障"><a href="#（2）leader故障" class="headerlink" title="（2）leader故障"></a>（2）leader故障</h5><p>leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。<strong>（1<strong><strong>）follower</strong></strong>故障</strong></p><p>follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该<strong>follower<strong><strong>的LEO</strong></strong>大于等于该Partition****的HW</strong>，即follower追上leader之后，就可以重新加入ISR了。</p><p><strong>（2<strong><strong>）leader</strong></strong>故障</strong></p><p>leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。</p><p><strong>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</strong></p><h4 id="三、Exactly-Once语义"><a href="#三、Exactly-Once语义" class="headerlink" title="三、Exactly Once语义"></a>三、Exactly Once语义</h4><p>对于某些比较重要的消息，我们需要保证exactly once语义，即<strong>保证每条消息被发送且仅被发送一次。</strong><br>在0.11版本之后，Kafka引入了幂等性机制（idempotent），配合acks &#x3D; -1时的at least once语义，实现了producer到broker的exactly once语义。<br>idempotent + at least once &#x3D; exactly once<br>使用时，只需将enable.idempotence属性设置为true，kafka自动将acks属性设为-1。</p><p>kafka幂等性原理：<a href="https://mp.weixin.qq.com/s/EQvY_VyZc-k8SB2c3O6kLA">https://mp.weixin.qq.com/s/EQvY_VyZc-k8SB2c3O6kLA</a></p><h3 id="Kafka消费者"><a href="#Kafka消费者" class="headerlink" title="Kafka消费者"></a>Kafka消费者</h3><h4 id="一、消费方式"><a href="#一、消费方式" class="headerlink" title="一、消费方式"></a>一、消费方式</h4><p><strong>consumer采用pull（拉）模式从broker中读取数据。</strong></p><p>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</p><p><strong>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。</strong></p><h4 id="二、分区分配策略"><a href="#二、分区分配策略" class="headerlink" title="二、分区分配策略"></a>二、分区分配策略</h4><p>一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。<br>Kafka有两种分配策略，一是roundrobin，一是range。</p><h5 id="1）roundrobin"><a href="#1）roundrobin" class="headerlink" title="1）roundrobin"></a>1）roundrobin</h5><h5 id="2）range"><a href="#2）range" class="headerlink" title="2）range"></a>2）range</h5><h4 id="三、offset的维护"><a href="#三、offset的维护" class="headerlink" title="三、offset的维护"></a>三、offset的维护</h4><p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。<br>Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets。</p><p>也可以手动维护offset，一般保存在mysql中，因为他有事务，一般将保存offset和保存数据的代码写在一个事务当中，实现精准一次消费。</p><h3 id="Kafka为什么快"><a href="#Kafka为什么快" class="headerlink" title="Kafka为什么快?"></a>Kafka为什么快?</h3><p>1)顺序写磁盘</p><p>顺序写磁盘，写的过程是一直追加到文件末端，，顺序写之所以快，是因为其省去了大量磁头寻址的时间。顺序写磁盘比内存都快！</p><p>2）零拷贝技术</p><p><a href="https://cloud.tencent.com/developer/article/1421266">https://cloud.tencent.com/developer/article/1421266</a></p><p><img src="/cdh/kafka/kafka/image-20200728105930977.png" alt="image-20200728105930977"></p><h3 id="Kafka如何保证consumer只消费一次？"><a href="#Kafka如何保证consumer只消费一次？" class="headerlink" title="Kafka如何保证consumer只消费一次？"></a>Kafka如何保证consumer只消费一次？</h3><p>在consumer消费的时候，在处理完成数据后在提交offset，而不是接收到数据就发送offset </p><h3 id="Zookeeper在Kafka中的作用"><a href="#Zookeeper在Kafka中的作用" class="headerlink" title="Zookeeper在Kafka中的作用"></a>Zookeeper在Kafka中的作用</h3><p>Kafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。<br>Controller的管理工作都是依赖于Zookeeper的。<br>以下为partition的leader选举过程：</p><p><img src="/cdh/kafka/kafka/image-20200910174852060.png" alt="image-20200910174852060"></p><h3 id="文件存储机制"><a href="#文件存储机制" class="headerlink" title="文件存储机制"></a>文件存储机制</h3><p><img src="/cdh/kafka/kafka/640" alt="img"></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NfkyB9ueApWd5FLFTctA28U1bOvvpgMrqpicIC844Q6ibHkSezAHGYTw1TwMRO5eqx95e98kpia0zbA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p>由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment(逻辑概念，等于index+log文件)。</p><p>每个partition(目录)相当于一个巨型文件被平均分配到多个大小相等的segment(片段)数据文件中（每个segment文件中消息数量不一定相等），这种特性也方便old segment的删除，即方便已被消费的消息的清理，提高磁盘的利用率。每个partition只需要支持顺序读写就行，segment的文件生命周期由服务端配置参数（log.segment.bytes，log.roll.{ms,hours}等若干参数）决定。</p><p>每个segment对应两个文件——“.index”文件和“.log”文件。分别表示为segment索引文件和数据文件（引入索引文件的目的就是便于利用二分查找快速定位message位置）。这两个文件的命令规则为：partition全局的第一个segment从0开始，后续每个segment文件名以当前segment的第一条消息的offset命名，数值大小为64位，20位数字字符长度，没有数字用0填充。</p><p>这些文件位于一个文件夹下（partition目录），该文件夹的命名规则为：topic名称+分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。</p><p>index和log文件以当前segment的第一条消息的offset命名。下图为index文件和log文件的结构示意图。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NfkyB9ueApWd5FLFTctA28gJ1sqCIcMnEnVhDWT4d9HozwdJdXYTXV6NmnRWJfUU0icXiaoib8Ewu3g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><h4 id="index和log文件详解"><a href="#index和log文件详解" class="headerlink" title="index和log文件详解"></a>index和log文件详解</h4><p>.index索引文件存储大量的索引信息，.log数据文件存储大量消息数据（Message）,索引文件中的元数据指向对应数据文件中Message的物理偏移地址。以index索引文件中的元数据3,497为例，依次在数据文件中表示第三个Message（在全局Partition中表示第368772个message），以及该消息的物理偏移地址为497.</p><p>索引和日志文件内部的关系，如图：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NfkyB9ueApWd5FLFTctA287Rm6Cyg5OJqynqkIIywcFt1MpguSIIdk8iagxP1mJCdzo0fT8wwr8Kg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><h4 id="message的结构"><a href="#message的结构" class="headerlink" title="message的结构"></a>message的结构</h4><p>Segment的Log文件由多个Message组成，下面详细说明Message的物理结构，如图：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NfkyB9ueApWd5FLFTctA28SBCpWxUia9YQJ3VgTkVSFfrt0oy2v8T5DWspRXMDJExrZ90hYNWetyw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>参数说明:</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NfkyB9ueApWd5FLFTctA28D5olWpbLOibtiarrw9aQeoovW177zMhOORzokYd2eBasRwZmlL3IY4vA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p>这里Kafka主要给sparkstream提供服务, 数据提交到kafka，sparkstream处理数据从kafka中拉取数据，能处理多少就拉取多少，有缓存的作用</p><p>kafka在<strong>存储数据的时候都是存到磁盘中，一对多的</strong>，消息是持久化的，不是一次性的<strong>，默认存储时间1个星期</strong></p><p>kafka集群的机器节点(Broker)相当于一个大分区，这里三个分区组成一个topic(队列)，topic的存储分布到多个broker上，启到负载均衡的作用，为了可靠性，分区后的topic拥有副本，副本保存在不同的节点上，防止全部丢失，每个分区多有很多副本，但只有一个副本是leader，副本的从节点不参与服务,.kafka只有主节点能够进行数据的存储和读取消息，从节点就是起到备份的作用，在主节点故障时,成为新的leader.</p><p>consumer随机从kafka里面读取，一个组中的consumer都会有一个标识所以一个组的consumer不会重复读取(消费)数据。尽量保证一个线程对应一个分区</p><p>在kafka中,一个partition中的消息只会被group中的一个consumer消费****(同一时刻)****；</p><p><strong>一个Topic中的每个partions，只会被一个”订阅者”中的一个consumer消费，不过一个consumer可以同时消费多个partitions中的消息。</strong></p><p><strong>*kafka只能保证一个partition中的消息被某个consumer消费时是顺序的；事实上，从Topic角度来说,当有多个partitions时,消息仍不是全局有序的。*</strong></p><h3 id="Kafka0-11的安装"><a href="#Kafka0-11的安装" class="headerlink" title="Kafka0.11的安装"></a>Kafka0.11的安装</h3><p>1、下载kafka的jar包</p><p><a href="http://kafka.apache.org/downloads.html">http://kafka.apache.org/downloads.html</a></p><p>2、上传到集群然后解压  三台</p><pre><code>tar -zxvf kafka_2.11-0.11.0.0.tgz </code></pre><p>3、kafka目录下创建datadir文件夹  三台</p><pre><code>mkdir logs</code></pre><p>4、修改配置文件</p><p>kafka_2.11-0.11.0.2&#x2F;config  修改这个配置文件，三台</p><p><strong>每个节点上的broker.id 必须不同</strong>，不能重复</p><pre><code class="properties"># The id of the broker. This must be set to a unique integer for each broker.#这个id三台机器必须不相同broker.id=0# Switch to enable topic deletion or not, default value is falsedelete.topic.enable=truenum.network.threads=3# The number of threads that the server uses for processing requests, which may include disk I/Onum.io.threads=8# The send buffer (SO_SNDBUF) used by the socket serversocket.send.buffer.bytes=102400# The receive buffer (SO_RCVBUF) used by the socket serversocket.receive.buffer.bytes=102400# The maximum size of a request that the socket server will accept (protection against OOM)socket.request.max.bytes=104857600############################# Log Basics ############################## A comma seperated list of directories under which to store log fileslog.dirs=/install/kafka_2.11-0.11.0.2/datadir# The default number of log partitions per topic. More partitions allow greater# parallelism for consumption, but this will also result in more files across# the brokers.num.partitions=1# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.# This value is recommended to be increased for installations with data dirs located in RAID array.num.recovery.threads.per.data.dir=1############################# Internal Topic Settings  ############################## The replication factor for the group metadata internal topics &quot;__consumer_offsets&quot; and &quot;__transaction_state&quot;# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.offsets.topic.replication.factor=1transaction.state.log.replication.factor=1transaction.state.log.min.isr=1# The minimum age of a log file to be eligible for deletion due to agelog.retention.hours=168# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining# segments don&#39;t drop below log.retention.bytes. Functions independently of log.retention.hours.#log.retention.bytes=1073741824# The maximum size of a log segment file. When this size is reached a new log segment will be created.log.segment.bytes=1073741824# The interval at which log segments are checked to see if they can be deleted according# to the retention policieslog.retention.check.interval.ms=300000############################# Zookeeper ############################## Zookeeper connection string (see zookeeper docs for details).# This is a comma separated host:port pairs, each corresponding to a zk# server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;.# You can also append an optional chroot string to the urls to specify the# root directory for all kafka znodes.zookeeper.connect=Linux01:2181,Linux02:2181,Linux03:2181# Timeout in ms for connecting to zookeeperzookeeper.connection.timeout.ms=6000############################# Group Coordinator Settings ############################## The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.# The default value for this is 3 seconds.# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.group.initial.rebalance.delay.ms=0</code></pre><p>5、修改&#x2F;etc&#x2F;profile文件</p><p>修改完成后source 刷新配置文件</p><pre><code class="properties">export SPARK_HOME=/install/spark-2.2.0-bin-hadoop2.7export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbinexport KAFKA_HOME=/install/kafka_2.11-0.11.0.2export PATH=$PATH:$KAFKA_HOME/bin</code></pre><h3 id="Kafka的命令行操作"><a href="#Kafka的命令行操作" class="headerlink" title="Kafka的命令行操作"></a>Kafka的命令行操作</h3><h4 id="1、Kafka集群的启动"><a href="#1、Kafka集群的启动" class="headerlink" title="1、Kafka集群的启动"></a>1、Kafka集群的启动</h4><p>在kafka的根目录下，三台</p><pre><code class="scala">bin/kafka-server-start.sh -daemon config/server.properties</code></pre><h4 id="2、Kafka集群的关闭"><a href="#2、Kafka集群的关闭" class="headerlink" title="2、Kafka集群的关闭"></a>2、Kafka集群的关闭</h4><p>在kafka的根目录下，三台</p><pre><code class="scala">bin/kafka-server-stop.sh stop</code></pre><p><strong>在kafka的根目录下</strong></p><h4 id="3、当前服务器中的所有topic"><a href="#3、当前服务器中的所有topic" class="headerlink" title="3、当前服务器中的所有topic"></a><strong>3、当前服务器中的所有topic</strong></h4><pre><code class="scala">bin/kafka-topics.sh --zookeeper Linux02:2181 --list//查看所有的topic</code></pre><h4 id="4、创建topic"><a href="#4、创建topic" class="headerlink" title="4、创建topic"></a><strong>4、创建topic</strong></h4><pre><code class="scala">bin/kafka-topics.sh --zookeeper Linux02:2181 --create --topic first --partitions 3 --replication-factor 3 //创建first节点  3个分区  1个分区三个副本//--topic 定义topic名//--replication-factor  定义副本数//--partitions  定义分区数</code></pre><h4 id="5、删除topic"><a href="#5、删除topic" class="headerlink" title="5、删除topic"></a><strong>5、删除topic</strong></h4><pre><code class="scala">bin/kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first//需要server.properties中设置delete.topic.enable=true否则只是标记删除。//--topic 定义topic名//--replication-factor  定义副本数//--partitions  定义分区数</code></pre><h4 id="6、发送消息"><a href="#6、发送消息" class="headerlink" title="6、发送消息"></a><strong>6、发送消息</strong></h4><p>如果只发送，需要指定消费信息</p><pre><code>bin/kafka-console-producer.sh --broker-list Linux02:9092 --topic first&gt;hello world&gt;agjasidjdj jsidjfj</code></pre><h4 id="7、消费消息"><a href="#7、消费消息" class="headerlink" title="7、消费消息"></a><strong>7、消费消息</strong></h4><p>–from-beginning   加上这个从头开始读取，没有这个会按照offset的位置来读取</p><pre><code class="scala">bin/kafka-console-consumer.sh --bootstrap-server Linux02:9092 --from-beginning --topic first//--from-beginning：会把主题中以往所有的数据都读取出来。</code></pre><h4 id="8、查看某个Topic的详情"><a href="#8、查看某个Topic的详情" class="headerlink" title="8、查看某个Topic的详情"></a><strong>8、查看某个Topic的详情</strong></h4><pre><code class="scala">bin/kafka-topics.sh --zookeeper Linux02:2181 \--describe --topic first</code></pre><p><img src="/cdh/kafka/kafka/image-20200409204342145.png" alt="image-20200409204342145"></p><h4 id="9、修改分区数"><a href="#9、修改分区数" class="headerlink" title="9、修改分区数"></a><strong>9、修改分区数</strong></h4><pre><code class="scala">bin/kafka-topics.sh --zookeeper hadoop102:2181 --alter --topic first --partitions 6//只能增加分区不能减少分区</code></pre><h3 id="集群的基准测试-压测"><a href="#集群的基准测试-压测" class="headerlink" title="集群的基准测试(压测)"></a>集群的基准测试(压测)</h3><p>Kafka压测</p><p>用Kafka官方自带的脚本，对Kafka进行压测。Kafka压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。 cpu可以加 内存可以调 网络带宽太贵了</p><p>kafka-consumer-perf-test.sh</p><p>kafka-producer-perf-test.sh</p><p>2）Kafka Producer压力测试</p><p>（1）在&#x2F;opt&#x2F;modules&#x2F;kafka&#x2F;bin目录下面有这两个文件。我们来测试一下</p><pre><code>[admin@hadoop-yarn kafka]$ bin/kafka-producer-perf-test.sh --topic test --record-size 100 --num-records 100000 --throughput 1000 --producer-props bootstrap.servers=bw66:9092,bw67:9092,bw68:9092</code></pre><p>说明：record-size是一条信息有多大，单位是字节。num-records是总共发送多少条信息。throughput 是每秒多少条信息。（吞吐量）</p><p>（2）Kafka会打印下面的信息</p><pre><code>5000 records sent, 999.4 records/sec (0.10 MB/sec), 1.9 ms avg latency, 254.0 max latency.5002 records sent, 1000.4 records/sec (0.10 MB/sec), 0.7 ms avg latency, 12.0 max latency.5001 records sent, 1000.0 records/sec (0.10 MB/sec), 0.8 ms avg latency, 4.0 max latency.5000 records sent, 1000.0 records/sec (0.10 MB/sec), 0.7 ms avg latency, 3.0 max latency.5000 records sent, 1000.0 records/sec (0.10 MB/sec), 0.8 ms avg latency, 5.0 max latency.100000 records sent, 999.930005 records/sec (0.10 MB/sec), 1.38 ms avg latency, 809.00 ms max latency, 1 ms 50th, 2 ms 95th, 20 ms 99th, 81 ms 99.9th.</code></pre><p>参数解析：本例中一共写入10w条消息，每秒向Kafka写入了<strong>0.10MB</strong>的数据，平均是999.99条消息&#x2F;秒，每次写入的平均延迟为1.38毫秒，最大的延迟为809毫秒。</p><p>这就能看出来 ： 每秒中0.1M能否满足我们的需求 延迟时间能否接受？</p><p><strong>这个测试环境</strong> <strong>要和实际情况</strong> <strong>联系起来</strong> <strong>你以后每天要产生一亿条</strong> <strong>那你就陪<strong><strong>1</strong></strong>亿</strong> </p><p><strong>数据得根据实际情况来！！！！！</strong></p><p>3）Kafka Consumer压力测试</p><p> 注意：消费的速度一定要大于生产的速度 否则数据就会有堆积</p><p>Consumer的测试，如果这四个指标（IO，CPU，内存，网络）都不能改变，考虑增加分区数来提升性能。</p><pre><code>[admin@hadoop-yarn kafka]$ bin/kafka-consumer-perf-test.sh --zookeeper bw66:2181,bw67:2181,bw68:2181 --topic test --fetch-size 10000 --messages 10000000 --threads 1</code></pre><p>参数说明：</p><p>–zookeeper 指定zookeeper的链接信息</p><p>–topic 指定topic的名称</p><p>–fetch-size 指定每次fetch的数据的大小</p><p>–messages 总共要消费的消息个数</p><p>测试结果说明：</p><p>start.time, <strong>end.time,</strong> data.consumed.in.MB, <strong>MB.sec,</strong> data.consumed.in.nMsg**, nMsg.sec**</p><p>2019-02-19 20:29:07:566, <strong>2019-02-19 20:29:12:170,</strong> 9.5368, <strong>2.0714,</strong> 100010, <strong>21722.4153</strong></p><p><strong>开始测试时间，测试结束数据，最大吞吐率</strong>9.5368MB&#x2F;s，平均每秒消费<strong>2.0714MB&#x2F;s****，最大每秒消费</strong>100010条，平均每秒消费<strong>21722.4153****条。</strong></p><h3 id="项目经验之Kafka机器数量计算"><a href="#项目经验之Kafka机器数量计算" class="headerlink" title="项目经验之Kafka机器数量计算"></a>项目经验之Kafka机器数量计算</h3><p>注意 副本数基本设置为2<br>$$<br>Kafka机器数量（经验公式）&#x3D;2<em>（峰值生产速度</em>副本数&#x2F;100）+1<br>$$</p><h4 id="kafka节点有几台？"><a href="#kafka节点有几台？" class="headerlink" title="kafka节点有几台？"></a>kafka节点有几台？</h4><p>先要预估一天大概产生多少数据，然后用Kafka自带的生产压测（只测试Kafka的写入速度，保证数据不积压），计算出峰值生产速度。再根据设定的副本数，就能预估出需要部署Kafka的数量。</p><p>100&#x3D;100*1024&#x2F;3600&#x2F;24</p><p>比如我们采用压力测试测出写入的速度是10M&#x2F;s一台，峰值的业务数据的速度是50M&#x2F;s。副本数为2。</p><p>Kafka机器数量&#x3D;2<em>（50</em>2&#x2F;100）+ 1&#x3D;3台  这个kafka集群能够抗住50M&#x2F;s的数据 已经非常快了！！ <strong>在正常的企业 3台kafka肯定是够了 ！！</strong></p><h2 id="Kafka与传统消息队列优缺点"><a href="#Kafka与传统消息队列优缺点" class="headerlink" title="Kafka与传统消息队列优缺点"></a>Kafka与传统消息队列优缺点</h2><h2 id="KafkaAPI"><a href="#KafkaAPI" class="headerlink" title="KafkaAPI"></a>KafkaAPI</h2><h3 id="Producer-API"><a href="#Producer-API" class="headerlink" title="Producer API"></a>Producer API</h3><h4 id="一、消息发送流程"><a href="#一、消息发送流程" class="headerlink" title="一、消息发送流程"></a>一、消息发送流程</h4><p><strong>Kafka的Producer发送消息采用的是异步发送的方式。</strong></p><p><strong>在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，以及一个线程共享变量——RecordAccumulator。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker</strong>。</p><p><img src="/cdh/kafka/kafka/image-20200910175224088.png" alt="image-20200910175224088"></p><p><strong>相关参数：</strong><br>batch.size：只有数据积累到batch.size之后，sender才会发送数据。<br>linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。</p><h4 id="二、异步发送API"><a href="#二、异步发送API" class="headerlink" title="二、异步发送API"></a>二、异步发送API</h4><p>1）导入依赖</p><pre><code class="xml">&lt;dependency&gt;&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;&lt;version&gt;0.11.0.0&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>2）编写代码<br>需要用到的类：</p><pre><code>KafkaProducer：需要创建一个生产者对象，用来发送数据ProducerConfig：获取所需的一系列配置参数ProducerRecord：每条数据都要封装成一个ProducerRecord对象</code></pre><h5 id="1-不带回调函数的API"><a href="#1-不带回调函数的API" class="headerlink" title="1.不带回调函数的API"></a>1.不带回调函数的API</h5><pre><code class="java">package com.atguigu.kafka;import org.apache.kafka.clients.producer.*;import java.util.Properties;import java.util.concurrent.ExecutionException;public class CustomProducer &#123;    public static void main(String[] args) throws ExecutionException, InterruptedException &#123;        Properties props = new Properties();        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);//kafka集群，broker-list        props.put(&quot;acks&quot;, &quot;all&quot;);        props.put(&quot;retries&quot;, 1);//重试次数        props.put(&quot;batch.size&quot;, 16384);//批次大小        props.put(&quot;linger.ms&quot;, 1);//等待时间        props.put(&quot;buffer.memory&quot;, 33554432);//RecordAccumulator缓冲区大小        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);        for (int i = 0; i &lt; 100; i++) &#123;            producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), Integer.toString(i)));        &#125;        producer.close();    &#125;&#125;</code></pre><h5 id="2-带回调函数的API"><a href="#2-带回调函数的API" class="headerlink" title="2.带回调函数的API"></a>2.带回调函数的API</h5><p><img src="/cdh/kafka/kafka/image-20200910191728376.png" alt="image-20200910191728376"></p><p><strong>注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</strong></p><p>回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是RecordMetadata和Exception，如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。</p><pre><code class="java">package com.atguigu.kafka;import org.apache.kafka.clients.producer.*;import java.util.Properties;import java.util.concurrent.ExecutionException;public class CustomProducer &#123;    public static void main(String[] args) throws ExecutionException, InterruptedException &#123;        Properties props = new Properties();        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);//kafka集群，broker-list        props.put(&quot;acks&quot;, &quot;all&quot;);        props.put(&quot;retries&quot;, 1);//重试次数        props.put(&quot;batch.size&quot;, 16384);//批次大小        props.put(&quot;linger.ms&quot;, 1);//等待时间        props.put(&quot;buffer.memory&quot;, 33554432);//RecordAccumulator缓冲区大小        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);        for (int i = 0; i &lt; 100; i++) &#123;            producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), Integer.toString(i)), new Callback() &#123;                //回调函数，该方法会在Producer收到ack时调用，为异步调用                @Override                public void onCompletion(RecordMetadata metadata, Exception exception) &#123;                    if (exception == null) &#123;                        System.out.println(&quot;success-&gt;&quot; + metadata.offset());                    &#125; else &#123;                        exception.printStackTrace();                    &#125;                &#125;            &#125;);        &#125;        producer.close();    &#125;&#125;</code></pre><h4 id="三、同步发送API"><a href="#三、同步发送API" class="headerlink" title="三、同步发送API"></a>三、同步发送API</h4><p>同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。</p><p>由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，只需在调用Future对象的get方发即可。</p><pre><code class="java">package com.atguigu.kafka;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;import java.util.concurrent.ExecutionException;public class CustomProducer &#123;    public static void main(String[] args) throws ExecutionException, InterruptedException &#123;        Properties props = new Properties();        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);//kafka集群，broker-list        props.put(&quot;acks&quot;, &quot;all&quot;);        props.put(&quot;retries&quot;, 1);//重试次数        props.put(&quot;batch.size&quot;, 16384);//批次大小        props.put(&quot;linger.ms&quot;, 1);//等待时间        props.put(&quot;buffer.memory&quot;, 33554432);//RecordAccumulator缓冲区大小        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);        for (int i = 0; i &lt; 100; i++) &#123;            producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), Integer.toString(i))).get();        &#125;        producer.close();    &#125;&#125;</code></pre><h3 id="Consumer-API"><a href="#Consumer-API" class="headerlink" title="Consumer API"></a>Consumer API</h3><p>Consumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。</p><p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</p><p>所以offset的维护是Consumer消费数据是必须考虑的问题。</p><h4 id="一、手动提交offset"><a href="#一、手动提交offset" class="headerlink" title="一、手动提交offset"></a>一、手动提交offset</h4><p>1）导入依赖</p><pre><code class="xml">&lt;dependency&gt;&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;&lt;version&gt;0.11.0.0&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>2）编写代码<br>需要用到的类：</p><pre><code>KafkaConsumer：需要创建一个消费者对象，用来消费数据ConsumerConfig：获取所需的一系列配置参数ConsuemrRecord：每条数据都要封装成一个ConsumerRecord对象</code></pre><pre><code class="java">package com.atguigu.kafka;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.Arrays;import java.util.Properties;public class CustomConsumer &#123;    public static void main(String[] args) &#123;        Properties props = new Properties();        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);        props.put(&quot;group.id&quot;, &quot;test&quot;);//消费者组，只要group.id相同，就属于同一个消费者组        props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);//自动提交offset               props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);        consumer.subscribe(Arrays.asList(&quot;first&quot;));        while (true) &#123;            ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);            for (ConsumerRecord&lt;String, String&gt; record : records) &#123;                System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value());            &#125;            consumer.commitSync();        &#125;    &#125;&#125;</code></pre><p>3）代码分析：</p><p>手动提交offset的方法有两种：分别是<strong>commitSync（同步提交）</strong>和<strong>commitAsync（异步提交）</strong>。两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；不同点是，commitSync会失败重试，一直到提交成功（如果由于不可恢复原因导致，也会提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。</p><p>4）数据重复消费问题</p><p><img src="/cdh/kafka/kafka/image-20200910193111686.png" alt="image-20200910193111686"></p><h4 id="二、自动提交offset"><a href="#二、自动提交offset" class="headerlink" title="二、自动提交offset"></a>二、自动提交offset</h4><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。<br>自动提交offset的相关参数：<br><strong>enable.auto.commit</strong>：是否开启自动提交offset功能<br><strong>auto.commit.interval.ms</strong>：自动提交offset的时间间隔</p><p>以下为自动提交offset的代码：</p><pre><code class="java">package com.atguigu.kafka;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.Arrays;import java.util.Properties;public class CustomConsumer &#123;    public static void main(String[] args) &#123;        Properties props = new Properties();        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);        props.put(&quot;group.id&quot;, &quot;test&quot;);        props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);        props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);        props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);        consumer.subscribe(Arrays.asList(&quot;first&quot;));        while (true) &#123;            ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);            for (ConsumerRecord&lt;String, String&gt; record : records)                System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value());        &#125;    &#125;&#125;</code></pre><h4 id="三、重新分区问题"><a href="#三、重新分区问题" class="headerlink" title="三、重新分区问题"></a>三、重新分区问题</h4><pre><code class="java">public class CustomOffsetConsumer &#123;    public static void main(String[] args) &#123;        Properties props = new Properties();        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);        props.put(&quot;group.id&quot;, &quot;test&quot;);//消费者组，只要group.id相同，就属于同一个消费者组        props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);//自动提交offset        props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);        consumer.subscribe(Arrays.asList(&quot;first&quot;), new ConsumerRebalanceListener() &#123;            //提交当前负责的分区的offset            @Override            public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123;            &#125;            //定位新分配的分区的offset            @Override            public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123;                for (TopicPartition partition : partitions) &#123;                    Long offset = getPartitionOffset(partition);                    consumer.seek(partition,offset);                &#125;            &#125;        &#125;);        while (true) &#123;            ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);            for (ConsumerRecord&lt;String, String&gt; record : records) &#123;                System.out.printf(&quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value());                TopicPartition topicPartition = new TopicPartition(record.topic(), record.partition());                commitOffset(topicPartition,record.offset()+1);            &#125;        &#125;    &#125;    private static void commitOffset(TopicPartition topicPartition, long l) &#123;    &#125;    private static Long getPartitionOffset(TopicPartition partition) &#123;        return null;    &#125;&#125;</code></pre><h3 id="自定义Interceptor"><a href="#自定义Interceptor" class="headerlink" title="自定义Interceptor"></a>自定义Interceptor</h3><h4 id="拦截器原理"><a href="#拦截器原理" class="headerlink" title="拦截器原理"></a>拦截器原理</h4><p>Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。<br>对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如<strong>修改消息</strong>等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。</p><p>Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括：<br><strong>（1）configure(configs)</strong><br>获取配置信息和初始化数据时调用。<br><strong>（2）onSend(ProducerRecord)：</strong><br>该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算。<br><strong>（3）onAcknowledgement(RecordMetadata, Exception)：</strong><br><strong>该方法会在消息从RecordAccumulator成功发送到Kafka Broker之后，或者在发送过程中失败时调用</strong>。并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率。<br><strong>（4）close：</strong><br>关闭interceptor，主要用于执行一些资源清理工作<br>如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外<strong>倘若指定了多个interceptor，则producer将按照指定顺序调用它们</strong>，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。</p><h4 id="拦截器案例"><a href="#拦截器案例" class="headerlink" title="拦截器案例"></a>拦截器案例</h4><p>需求：</p><p>实现一个简单的双interceptor组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部；第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。</p><p><img src="/cdh/kafka/kafka/image-20200910225802854.png" alt="image-20200910225802854"></p><h4 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h4><h5 id="（1）增加时间戳拦截器"><a href="#（1）增加时间戳拦截器" class="headerlink" title="（1）增加时间戳拦截器"></a>（1）增加时间戳拦截器</h5><pre><code class="java">package com.atguigu.kafka.interceptor;import java.util.Map;import org.apache.kafka.clients.producer.ProducerInterceptor;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; &#123;    @Override    public void configure(Map&lt;String, ?&gt; configs) &#123;    &#125;    @Override    public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123;        // 创建一个新的record，把时间戳写入消息体的最前部        return new ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(),                System.currentTimeMillis() + &quot;,&quot; + record.value().toString());    &#125;    @Override    public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123;    &#125;    @Override    public void close() &#123;    &#125;&#125;</code></pre><h5 id="（2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器"><a href="#（2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器" class="headerlink" title="（2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器"></a>（2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器</h5><pre><code class="java">package com.atguigu.kafka.interceptor;import java.util.Map;import org.apache.kafka.clients.producer.ProducerInterceptor;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt;&#123;    private int errorCounter = 0;    private int successCounter = 0;    @Override    public void configure(Map&lt;String, ?&gt; configs) &#123;            &#125;    @Override    public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123;         return record;    &#125;    @Override    public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123;        // 统计成功和失败的次数        if (exception == null) &#123;            successCounter++;        &#125; else &#123;            errorCounter++;        &#125;    &#125;    @Override    public void close() &#123;        // 保存结果        System.out.println(&quot;Successful sent: &quot; + successCounter);        System.out.println(&quot;Failed sent: &quot; + errorCounter);    &#125;&#125;</code></pre><h5 id="（3）producer主程序"><a href="#（3）producer主程序" class="headerlink" title="（3）producer主程序"></a>（3）producer主程序</h5><pre><code class="java">package com.atguigu.kafka.interceptor;import java.util.ArrayList;import java.util.List;import java.util.Properties;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;public class InterceptorProducer &#123;    public static void main(String[] args) throws Exception &#123;        // 1 设置配置信息        Properties props = new Properties();        props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;);        props.put(&quot;acks&quot;, &quot;all&quot;);        props.put(&quot;retries&quot;, 0);        props.put(&quot;batch.size&quot;, 16384);        props.put(&quot;linger.ms&quot;, 1);        props.put(&quot;buffer.memory&quot;, 33554432);        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);                // 2 构建拦截链        List&lt;String&gt; interceptors = new ArrayList&lt;&gt;();        interceptors.add(&quot;com.atguigu.kafka.interceptor.TimeInterceptor&quot;); interceptors.add(&quot;com.atguigu.kafka.interceptor.CounterInterceptor&quot;);         props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);                 String topic = &quot;first&quot;;        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);                // 3 发送消息        for (int i = 0; i &lt; 10; i++) &#123;                        ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, &quot;message&quot; + i);            producer.send(record);        &#125;                 // 4 一定要关闭producer，这样才会调用interceptor的close方法        producer.close();    &#125;&#125;</code></pre><h5 id="（4）测试"><a href="#（4）测试" class="headerlink" title="（4）测试"></a>（4）测试</h5><p>（1）在kafka上启动消费者，然后运行客户端java程序。</p><pre><code class="java">[atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh \--bootstrap-server hadoop102:9092 --from-beginning --topic first1501904047034,message01501904047225,message11501904047230,message21501904047234,message31501904047236,message41501904047240,message51501904047243,message61501904047246,message71501904047249,message81501904047252,message9</code></pre><h2 id="Kafka监控组件"><a href="#Kafka监控组件" class="headerlink" title="Kafka监控组件"></a>Kafka监控组件</h2><h3 id="Kafka-Monitor"><a href="#Kafka-Monitor" class="headerlink" title="Kafka Monitor"></a>Kafka Monitor</h3><p>1.上传jar包KafkaOffsetMonitor-assembly-0.4.6.jar到集群</p><p>2.在&#x2F;opt&#x2F;module&#x2F;下创建kafka-offset-console文件夹</p><p>3.将上传的jar包放入刚创建的目录下</p><p>4.在&#x2F;opt&#x2F;module&#x2F;kafka-offset-console目录下创建启动脚本start.sh，内容如下：</p><pre><code class="sh">#!/bin/bashjava -cp KafkaOffsetMonitor-assembly-0.4.6-SNAPSHOT.jar \com.quantifind.kafka.offsetapp.OffsetGetterWeb \--offsetStorage kafka \--kafkaBrokers hadoop102:9092,hadoop103:9092,hadoop104:9092 \--kafkaSecurityProtocol PLAINTEXT \--zk hadoop102:2181,hadoop103:2181,hadoop104:2181 \--port 8086 \--refresh 10.seconds \--retain 2.days \--dbName offsetapp_kafka &amp;</code></pre><p>5.在&#x2F;opt&#x2F;module&#x2F;kafka-offset-console目录下创建mobile-logs文件夹</p><pre><code class="shell">mkdir /opt/module/kafka-offset-console/mobile-logs</code></pre><p>6.启动KafkaMonitor</p><pre><code class="shell">./start.sh</code></pre><p>7.登录页面hadoop102:8086端口查看详情</p><h3 id="Kafka-Manager"><a href="#Kafka-Manager" class="headerlink" title="Kafka Manager"></a>Kafka Manager</h3><p>1.上传压缩包kafka-manager-1.3.3.15.zip到集群</p><p>2.解压到&#x2F;opt&#x2F;module</p><p>3.修改配置文件conf&#x2F;application.conf</p><p>kafka-manager.zkhosts&#x3D;”kafka-manager-zookeeper:2181”</p><p>修改为：</p><p>kafka-manager.zkhosts&#x3D;”hadoop102:2181,hadoop103:2181,hadoop104:2181”</p><p>4.启动kafka-manager</p><p>bin&#x2F;kafka-manager</p><p>5.登录hadoop102:9000页面查看详细信息</p><h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><h4 id="1-Kafka中的ISR、AR又代表什么？"><a href="#1-Kafka中的ISR、AR又代表什么？" class="headerlink" title="1.Kafka中的ISR、AR又代表什么？"></a>1.Kafka中的ISR、AR又代表什么？</h4><pre><code>ISR：与leader同步的follower集合AR：分区的全部副本</code></pre><h4 id="2-Kafka中的HW、LEO等分别代表什么？"><a href="#2-Kafka中的HW、LEO等分别代表什么？" class="headerlink" title="2.Kafka中的HW、LEO等分别代表什么？"></a>2.Kafka中的HW、LEO等分别代表什么？</h4><pre><code>HW:分区副本中最小的偏移量LEO：一个分区中所有副本最小的偏移量</code></pre><h4 id="3-Kafka中是怎么体现消息顺序性的？"><a href="#3-Kafka中是怎么体现消息顺序性的？" class="headerlink" title="3.Kafka中是怎么体现消息顺序性的？"></a>3.Kafka中是怎么体现消息顺序性的？</h4><pre><code>kafka的每个分区数据是有序的，消费者消费数据都有offset，能够保证数据有序性kafka写入数据可以指定partition，将相同的key发送到同一个partition里，保证业务相同的key有序。</code></pre><h4 id="4-Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？"><a href="#4-Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？" class="headerlink" title="4.Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？"></a>4.Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？</h4><h4 id><a href="#" class="headerlink" title></a><img src="/cdh/kafka/kafka/jo6w29tl4b.png" alt="jo6w29tl4b"></h4><pre><code>处理顺序：拦截器-》序列化器-》分区器</code></pre><h4 id="5-Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？"><a href="#5-Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？" class="headerlink" title="5.Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？"></a>5.Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？</h4><pre><code>整体架构：Producer生产者启动main西安城，数据经过拦截器，序列化器，分区器后，通过异步调用RecordAccumulator的sender西安城来实现消息的批次发送到topic</code></pre><h4 id="6-“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？"><a href="#6-“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？" class="headerlink" title="6.“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？"></a>6.“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？</h4><pre><code>正确。</code></pre><h4 id="7-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1？"><a href="#7-消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1？" class="headerlink" title="7.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？"></a>7.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1？</h4><pre><code>offset+1</code></pre><h4 id="8-有哪些情形会造成重复消费？"><a href="#8-有哪些情形会造成重复消费？" class="headerlink" title="8.有哪些情形会造成重复消费？"></a>8.有哪些情形会造成重复消费？</h4><pre><code>d</code></pre><h4 id="9-那些情景会造成消息漏消费？"><a href="#9-那些情景会造成消息漏消费？" class="headerlink" title="9.那些情景会造成消息漏消费？"></a>9.那些情景会造成消息漏消费？</h4><pre><code></code></pre><h4 id="10-当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？"><a href="#10-当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？" class="headerlink" title="10.当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？"></a>10.当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？</h4><p>​    1）会在zookeeper中的&#x2F;brokers&#x2F;topics节点下创建一个新的topic节点，如：&#x2F;brokers&#x2F;topics&#x2F;first<br>​    2）触发Controller的监听程序<br>​    3）kafka Controller 负责topic的创建工作，并更新metadata cache<br>11.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？<br>12.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？<br>13.Kafka有内部的topic吗？如果有是什么？有什么所用？<br>14.Kafka分区分配的概念？<br>15.简述Kafka的日志目录结构？<br>16.如果我指定了一个offset，Kafka Controller怎么查找到对应的消息？<br>17.聊一聊Kafka Controller的作用？<br>18.Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？<br>19.失效副本是指什么？有那些应对措施？<br>20.Kafka的那些设计让它有如此高的性能？<br>21.扩展问题，怎么保证exactly one语义</p><h4 id="11-kafka和其他的消息队列有什么区别？"><a href="#11-kafka和其他的消息队列有什么区别？" class="headerlink" title="11. kafka和其他的消息队列有什么区别？"></a>11. kafka和其他的消息队列有什么区别？</h4><pre><code>kafka是分布式的消息队列，用来缓存解耦削峰异步请求等作用，其他的消息队列如rocketmq，activemq等消息队列都是全局有序的，先进先出。但是kafka虽然有先进先出的特性，但他有topic的概念，分区进行存储，每个分区的数据是有序的，无法保证topic整体的数据有序性其他的消息队列是严格的先进先出的，是全局有序的。</code></pre><h4 id="12-kafka怎么保证消费者不丢失数据？"><a href="#12-kafka怎么保证消费者不丢失数据？" class="headerlink" title="12. kafka怎么保证消费者不丢失数据？"></a>12. kafka怎么保证消费者不丢失数据？</h4><p>处理完毕数据后在进行offset的提交，虽然有可能会重复消费。</p><h4 id="13-Kafka事务"><a href="#13-Kafka事务" class="headerlink" title="13. Kafka事务"></a>13. Kafka事务</h4><p>Kafka 从 0.11 版本开始支持了事务机制。具体来说，Kafka 生产者在同一个事务内提交到多个分区的消息，要么同时成功，要么同时失败。这一保证在生产者运行时出现异常甚至宕机重启之后仍然成立。</p><h4 id="14-多条数据如何放入同一个topic的分区中？"><a href="#14-多条数据如何放入同一个topic的分区中？" class="headerlink" title="14.多条数据如何放入同一个topic的分区中？"></a>14.多条数据如何放入同一个topic的分区中？</h4><ol><li><p>可以使用自定义分区器。</p></li><li><p>默认条件下，指定相同的key即可放松到同一个partition中。</p></li></ol><h4 id="15-kafka怎么保证精准一次推送？"><a href="#15-kafka怎么保证精准一次推送？" class="headerlink" title="15.kafka怎么保证精准一次推送？"></a>15.kafka怎么保证精准一次推送？</h4><p>旧版本中（0.11之前）：</p><p>生产者端：ack设置成-1</p><p>消费者端：取消自动提交offset，在消费完毕后进行提交offset。</p><p>新版本中（0.11之后）：</p><p>kafka 0.11.0.0版本引入了idempotent  producer机制，在这个机制中同一消息可能被producer发送多次，但是在broker端只会写入一次，他为每一条消息编号去重，而且对kafka开销影响不大。</p><p>如何设置开启呢？ 需要设置producer端的新参数  enable.idempotent  为true。</p><h4 id="16-kafka怎么保证精准一次消费？"><a href="#16-kafka怎么保证精准一次消费？" class="headerlink" title="16.kafka怎么保证精准一次消费？"></a>16.kafka怎么保证精准一次消费？</h4><p><a href="https://blog.csdn.net/wangsl754/article/details/107479977">https://blog.csdn.net/wangsl754/article/details/107479977</a></p><ol><li><strong>方案一：利用关系型数据库的事务进行处理</strong></li><li><strong>方案二：手动提交偏移量+开启幂等性处理</strong></li></ol><p>但是如果数据保存了，没等偏移量提交进程挂了，数据会被<strong>重复消费</strong>。怎么办？那就要把数据的保存做成幂等性保存。即同一批数据反复保存多次，数据不会翻倍，保存一次和保存一百次的效果是一样的。如果能做到这个，就达到了幂等性保存，就不用担心数据会重复了。</p>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo搭建个人博客</title>
      <link href="/hexo-da-jian-ge-ren-bo-ke/"/>
      <url>/hexo-da-jian-ge-ren-bo-ke/</url>
      
        <content type="html"><![CDATA[<h3 id="1、github上注册账号"><a href="#1、github上注册账号" class="headerlink" title="1、github上注册账号"></a>1、github上注册账号</h3><p>创建一个Repository 版本库</p><h3 id="2、安装hexo-需要先安装node-js和git"><a href="#2、安装hexo-需要先安装node-js和git" class="headerlink" title="2、安装hexo   需要先安装node.js和git"></a>2、安装hexo   需要先安装node.js和git</h3><p>hexo文档中有详细安装所需要的node和git的过程<br>hexo官方文档地址：<a href="https://hexo.io/zh-cn/docs/">https://hexo.io/zh-cn/docs/</a><br>廖雪峰老师的git教程：<a href="https://www.liaoxuefeng.com/wiki/896043488029600/896954074659008">https://www.liaoxuefeng.com/wiki/896043488029600/896954074659008</a>  </p><p>注意：这里的 node.js 的来源可以切换成中国镜像进行下载，<a href="https://npm.taobao.org/mirrors/node">https://npm.taobao.org/mirrors/node</a> </p><h3 id="3、初始化建立项目"><a href="#3、初始化建立项目" class="headerlink" title="3、初始化建立项目"></a>3、初始化建立项目</h3><p>在指定位置开始建立一个html项目 </p><ul><li>cmd 命令</li></ul><pre><code>hexo init MyBlog</code></pre><p>会生成一个MyBlog的文件夹,里面存放着项目的数据</p><p><img src="/hexo-da-jian-ge-ren-bo-ke/image-20210116134755671.png" alt="image-20210116134755671"></p><pre><code>npm install//安装一些node.js模块，可能报warn警告，不影响结果，可以忽略</code></pre><p>完成后的目录结构：</p><pre><code>.├── _config.yml├── package.json├── scaffolds├── source|   ├── _drafts|   └── _posts└── themes//_config.yml   是主要的配置文件//scaffolds是模板文件，创建文章会按照这个文件来生成模板//source是存放用户资源，你的文章就是存在这里的_posts 目录下，    source/_drafts保存你的草稿文件，但是你的草稿文件不会显示在页面上，可以publish将草稿推到posts下，显示它                        或者加上 --draft 参数，来预览草稿    source/_drafts//themes是主题，选个好看的主题下载下来，然后再_config.yml中配置好后就可以使用了</code></pre><h3 id="4、-config-yml文件配置"><a href="#4、-config-yml文件配置" class="headerlink" title="4、_config.yml文件配置"></a>4、_config.yml文件配置</h3><p>这是官网的文档配置的详细说明</p><pre><code>https://hexo.io/zh-cn/docs/configuration</code></pre><h3 id="5、设置文章的分类和标签"><a href="#5、设置文章的分类和标签" class="headerlink" title="5、设置文章的分类和标签"></a>5、设置文章的分类和标签</h3><p>分类有顺序性，并且不支持多个同级</p><pre><code>//指定分类，life是diary的子类categories:  - Diary  - Life//添加多个分类categories:- [Diary, PlayStation]- [Diary, Games]- [Life]此时这篇文章同时包括三个分类： PlayStation 和 Games 分别都是父分类 Diary 的子分类，同时 Life 是一个没有子分类的分类。</code></pre><p>标签没有顺序性，支持多个同级别显示</p><h3 id="6、配置git地址"><a href="#6、配置git地址" class="headerlink" title="6、配置git地址"></a>6、配置git地址</h3><pre><code class="yml"># URL## If your site is put in a subdirectory, set url as &#39;http://example.com/child&#39; and root as &#39;/child/&#39;#在这里设置github的访问地址，以及资源存放目录设置url: https://star-hash.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults:pretty_urls:  trailing_index: true # Set to false to remove trailing &#39;index.html&#39; from permalinks  trailing_html: true # Set to false to remove trailing &#39;.html&#39; from permalinks# Deployment## Docs: https://hexo.io/docs/one-command-deploymentdeploy:   type: git  repo: https://github.com/star-hash/zwhblog.cn  branch: master</code></pre><h3 id="7、设置中文和时区"><a href="#7、设置中文和时区" class="headerlink" title="7、设置中文和时区"></a>7、设置中文和时区</h3><pre><code># Sitetitle: StartLight_Blog   #文章标题subtitle: &#39;zwh的博客&#39; #副标题description: &#39;&#39; #文章描述keywords: #关键词author: zhangwenhui      #作者language: zh-CN          #语言timezone: &#39;Asia/Shanghai&#39;#时区</code></pre><h3 id="6、新建博客并提交"><a href="#6、新建博客并提交" class="headerlink" title="6、新建博客并提交"></a>6、新建博客并提交</h3><pre><code>hexo new 名字</code></pre><p>然后打开文件进行写博客</p><pre><code>hexo generate  #生成静态文件hexo deploy    #上传到git上面</code></pre><h3 id="常见错误："><a href="#常见错误：" class="headerlink" title="常见错误："></a>常见错误：</h3><h4 id="ERROR-Deployer-not-found-git"><a href="#ERROR-Deployer-not-found-git" class="headerlink" title="ERROR Deployer not found: git"></a><code>ERROR Deployer not found: git</code></h4><p>如果提交报这个错，需要下载一个插件</p><pre><code>npm install hexo-deployer-git</code></pre><p>提交到github报这个错  需要安装这个插件，在重新hexo d<br>原文连接：<a href="https://www.cnblogs.com/codecheng99/p/12380810.html">https://www.cnblogs.com/codecheng99/p/12380810.html</a></p><p><img src="/hexo-da-jian-ge-ren-bo-ke/image-20210116134830928.png" alt="image-20210116134830928"></p><h4 id="github-上，css、js-文件加载不出来"><a href="#github-上，css、js-文件加载不出来" class="headerlink" title="github 上，css、js 文件加载不出来"></a>github 上，css、js 文件加载不出来</h4><pre><code class="yml">#修改hexo中 _config.yml 文件：#要修改对，否则找不到你的路径url: https://star-hash.github.ioroot: /zwhblog.cn/</code></pre><p>之后 clean、generate、deploy 重试</p><h3 id="本地运行"><a href="#本地运行" class="headerlink" title="本地运行"></a>本地运行</h3><pre><code>$ hexo g # 生成$ hexo s # 启动服务hexo s是开启本地预览服务，打开浏览器访问 http://localhost:4000 即可看到内容</code></pre><h2 id="后续补充"><a href="#后续补充" class="headerlink" title="后续补充"></a>后续补充</h2><h3 id="hexo博客无法显示图片问题"><a href="#hexo博客无法显示图片问题" class="headerlink" title="hexo博客无法显示图片问题"></a>hexo博客无法显示图片问题</h3><ol><li>设置md文件的图片位置设置，每一个md文件都在相应位置生成一个文件夹，存放自己的图片</li><li>配置根目录的_config.yml</li></ol><pre><code class="yml">post_asset_folder: true</code></pre><ol start="3"><li><p>添加hexo的中文转拼音插件</p><pre><code>npm i hexo-permalink-pinyin --save</code></pre><p>配置根目录的_config.yml</p><pre><code class="yml">permalink_pinyin:  enable: true  separator: &#39;-&#39; # default: &#39;-&#39;</code></pre></li></ol><p><a href="https://cloud.tencent.com/developer/article/1952241">https://cloud.tencent.com/developer/article/1952241</a></p>]]></content>
      
      
      <categories>
          
          <category> hexo搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo搭建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive/Spark小文件的处理</title>
      <link href="/xiao-wen-jian-chu-li/"/>
      <url>/xiao-wen-jian-chu-li/</url>
      
        <content type="html"><![CDATA[<p>解决hive&#x2F;Spark的大量小文件问题</p><p>原文链接：<a href="https://mp.weixin.qq.com/s/m4NPnZaKJMXKrTwtZoOQeQ">https://mp.weixin.qq.com/s/m4NPnZaKJMXKrTwtZoOQeQ</a></p><h3 id="1、hive解决小文件问题"><a href="#1、hive解决小文件问题" class="headerlink" title="1、hive解决小文件问题"></a>1、hive解决小文件问题</h3><p>小文件问题存在程序执行之前<br>程序可能会需要读取多个数据源的数据，或者动态分区中生成了大量的小文件，再或者上一个mr执行完成的结果文件，<br>这些数据以分大量的小文件形式分布在磁盘中，如果按照默认的形式去执行，那么这些小文件将会每个都产生一个mr任务，而mr任务的启动和销毁都会消耗大量的资源，导致集群运行缓慢，或者直接导致集群崩溃<br>而且mr的执行比mr的初始化时间和销毁时间短</p><p>解决问题的思路：<br>1、少用动态分区，<br>在使用动态分区的时候，使用distribute by 来进行数据的划分（将map端的数据按照指定字段来进行划分到不同的reduce来进行处理）</p><p>2、上面的方式运行完毕后，发现每个动态分区的目录中都只有一个文件，这是因为没有指定每个文件的大小和reduce的个数，在hive-site.xml配置文件中配置他们的文件数量</p><pre><code class="xml">-- 在 map only 的任务结束时合并小文件set hive.merge.mapfiles = true;-- 在 MapReduce 的任务结束时合并小文件set hive.merge.mapredfiles = true;-- 作业结束时合并文件的大小 set hive.merge.size.per.task = 256000000;-- 每个Map最大输入大小(这个值决定了合并后文件的数量) set mapred.max.split.size=256000000;   -- 每个reducer的大小， 默认是1G，输入文件如果是10G，那么就会起10个reducer；set hive.exec.reducers.bytes.per.reducer=1073741824;set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</code></pre><h3 id="2、Spark的优化小文件"><a href="#2、Spark的优化小文件" class="headerlink" title="2、Spark的优化小文件"></a>2、Spark的优化小文件</h3><p>sparksql并行度默认是200，在使用了group by join等产生shuffle算子后会产生大量的小文件，可以使用repartition&#x2F;coalesce算子来进行优化</p><p>coalesce算子在不产生shuffle的情况下进行数据分区的合并，但是内存不足会oom</p><p>常用情况：</p><p>一般有使用到Spark进行完业务处理后，为了避免小文件问题，对RDD&#x2F;DataFrame进行分区的缩减，避免写入HDFS有大量的小文件问题，从而给HDFS的NameNode内存造成大的压力，而调用coalesce，实则源码调用的是case class Repartition shuffle参数为false的，默认是不走shuffle的。</p><ol><li>假设当前spark作业的提交参数是num-executor 10 ，executor-core 2，那么就会有20个Task同时并行，如果对最后结果DataFrame进行coalesce操作缩减为(10)，最后也就只会生成10个文件，也表示只会运行10个task，就会有大量executor空跑，cpu core空转的情况；</li><li>而且coalesce的分区缩减是全在内存里进行处理，如果当前处理的数据量过大，这样很容易就导致程序OOM异常</li><li>如果 coalesce 前的分区数小于 后预想得到的分区数，coalesce就不会起作用，也不会进行shuffle，因为父RDD和子RDD是窄依赖</li></ol><p>Repartition Hint可以增加或减少分区数量，它执行数据的完全shuffle，并确保数据平均分配。</p><p>常用情况：</p><p>上游数据分区数据分布不均匀，才会对RDD&#x2F;DataFrame等数据集进行重分区，将数据重新分配均匀</p><p>假设原来有N个分区，现在repartition(M)的参数传为M，</p><p>​而 N &lt; M ，则会根据HashPartitioner （key的hashCode % M）进行数据的重新划分</p><p>​而 N  远大于 M ，那么还是建议走repartition，这样所有的executor都会运作起来，效率更高，如果还是走coalesce，假定参数是1，那么即使原本申请了10个executor，那么最后执行的也只会有1个executor。</p><h3 id="3、Hadoop-HAR归档"><a href="#3、Hadoop-HAR归档" class="headerlink" title="3、Hadoop HAR归档"></a>3、Hadoop HAR归档</h3><p>Hadoop Archive是一种特殊的归档格式，Hadoop Archive映射到文件系统目录，一个HAR以扩展名.har结尾，一个HAR目录包含元数据（以_index和_masterindex的形式）和data（part- *）文件。 _index文件包含文件名称，这些文件时归档的一部分，并且包含这些文件在归档中的位置。</p><p>对于已经产生小文件的hive表可以使用har归档，而且Hive提供了原生支持：</p><pre><code>set  hive.archive.enabled=  true ;set  hive.archive.har.parentdir.settable=  true ;set  har.partfile.size=256000000;</code></pre><p>在配置项设置完之后，就可以使用以下命令进行归档。</p><pre><code>ALTER TABLE table_name ARCHIVE PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)ALTER TABLE srcpart ARCHIVE PARTITION(ds=&#39;2008-04-08&#39;, hr=&#39;12&#39;)</code></pre><p>也可以对已归档的分区恢复为原文件。</p><pre><code>ALTER TABLE srcpart UNARCHIVE PARTITION(ds=&#39;2008-04-08&#39;, hr=&#39;12&#39;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 小文件优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小文件优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume</title>
      <link href="/flume/flume/"/>
      <url>/flume/flume/</url>
      
        <content type="html"><![CDATA[<h3 id="Flume是什么"><a href="#Flume是什么" class="headerlink" title="Flume是什么"></a>Flume是什么</h3><p>非官方中文文档地址：<a href="https://flume.liyifeng.org/#hdfs-sink">https://flume.liyifeng.org/#hdfs-sink</a></p><p>是cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统</p><p>日志收集</p><p>数据处理</p><h4 id="什么要用flume"><a href="#什么要用flume" class="headerlink" title="什么要用flume"></a>什么要用flume</h4><p>将客户端的请求经过负载均衡，写入到日志文件中，<strong>通过flume采集日志放到flume中，监控变化</strong>，etl对hdfs中的日志数据进行清洗和转换，然后可以放入到hbase或hive中，统计数据的结果(mr,spark,flink)，结果经过sqoop放入到MySQL(mongodb,redis)中，返回给客户端进行展示结果</p><h4 id="高可靠性"><a href="#高可靠性" class="headerlink" title="高可靠性"></a>高可靠性</h4><p>当节点出现故障时，日志能被传送到其他节点上而不会丢失</p><p>三种级别的可靠性保障，从强到弱排序</p><p>end-to-end （收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除，如果数据发送失败，可以重新发送）</p><p>Store on failure（这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送）</p><p>Best effort（数据发送到接收方后，不会进行确认）</p><h3 id="安装flume"><a href="#安装flume" class="headerlink" title="安装flume"></a>安装flume</h3><p>1、上传flume的tar包</p><p>2、解压tar包</p><pre><code>tar -zxvf apache-fluem....</code></pre><p>3、配置etc环境变量</p><pre><code>export JAVA_HOME=/root/Downloads/jdk1.8.0_161export PATH=$PATH:$JAVA_HOME/binexport HADOOP_HOME=/root/Downloads/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport ZOOKEEPER_HOME=/root/Downloads/zookeeper-3.4.5export PATH=$PATH:$ZOOKEEPER_HOME/binexport HIVE_HOME=/root/Downloads/apache-hive-1.2.0-binexport PATH=$PATH:$HIVE_HOME/binexport FLUME_HOME=/root/Downloads/apache-flume-1.6.0-binexport PATH=$PATH:$FLUME_HOME/bin</code></pre><p>4、刷新配置文件</p><pre><code>source /etc/profile</code></pre><p>5、验证是否成功</p><pre><code>flume-ng version</code></pre><p>6、更改flume的conf下的flume-env.sh</p><p>主要设置JAVA_HOME的变量设置(带env的都是组件的运行环境配置)</p><pre><code>export JAVA_HOME=/root/Downloads/jdk1.8.0_161</code></pre><h3 id="flume的版本"><a href="#flume的版本" class="headerlink" title="flume的版本"></a>flume的版本</h3><p>flume的逻辑分三层架构</p><p>agent,collector,storage</p><h4 id="flume-og"><a href="#flume-og" class="headerlink" title="flume og"></a>flume og</h4><p>og采用了多mast的方式，保证数据一致性而引入了zookeeper，保存配置数据，zookeeper本身可以保证配置数据地一致性和高可用，发生变化zookeeper通知flume master 节点，master间使用gossip协议同步数据</p><h5 id="flume-og的特点"><a href="#flume-og的特点" class="headerlink" title="flume og的特点"></a>flume og的特点</h5><p>三种角色</p><p>代理节点（agent）、收集节点（collector）、主节点（master）</p><p>agent从各个数据源收集日志数据，将收集到的数据集中到collector，然后手机节点汇总hdfs。master负责管理agent，collector的活动</p><p>agent、collector都成为node，node角色根据配置的不同分为logical node（逻辑节点）、physical node（物理节点）</p><p>agent、collector由source、sink组成，代表在当前节点数据是从source传送到sink</p><h4 id="flume-ng"><a href="#flume-ng" class="headerlink" title="flume ng"></a>flume ng</h4><p>ng取消了集中管理配置的zookeeper和master，是一个纯粹的传输工具。</p><p>flume ng的另一个不同点是读入数据和写出数据由不同的工作线程处理(称为runner).在flume ng中读入线程同样做写出工作（除了故障重试），如果写出速度太慢的话（不是完全失败），他将阻塞flume接收数据的能力。这种异步的设计使读入线程可以顺畅的工作，而无需关注下游的任何问题</p><h5 id="flume-ng的特点"><a href="#flume-ng的特点" class="headerlink" title="flume ng的特点"></a>flume ng的特点</h5><p>只有一种角色</p><p>代理节点（agent）</p><p>没有collector、master节点，这是最核心的变化</p><p>去除了physical nodes(物理节点)、logical nodes(逻辑节点)的概念和相关内容</p><p>agent节点的组成也发生了变化，flume ng的agent由source、sink、Channel组成</p><h4 id="Flume-逻辑上分三层架构"><a href="#Flume-逻辑上分三层架构" class="headerlink" title="Flume 逻辑上分三层架构"></a>Flume 逻辑上分三层架构</h4><p>Agent，collector，storage</p><p>Agent<br>用于采集数据，agent 是 flume 中存储数据流的地方，同时 agent 会将产生的数据传输到 collector<br>Coolector<br>Collector 的作用是坚多个 agent 的数据汇总后，加载到 storage 中，多个 collector 之间遵循负载均衡规则<br>Storage<br>Storage 是存储系统，可以是一个普通 file，也可以是 HDFS，HIVE，Hbase 等。<br>————————————————<br>版权声明：本文为CSDN博主「郑斯道」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/zhengshidao/article/details/78312299">https://blog.csdn.net/zhengshidao/article/details/78312299</a></p><h4 id="flume-ng-的组件"><a href="#flume-ng-的组件" class="headerlink" title="flume ng 的组件"></a>flume ng 的组件</h4><h5 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h5><p>flume以agent是flume的最小执行单位。一个agent就是一个jvm，单个agent由source、sink和channel三大组件构成，agent是产生数据流的地方</p><p><img src="/flume/flume/999804-20171108130603872-780242084.png" alt="img"></p><h4 id="1、source（单线程）"><a href="#1、source（单线程）" class="headerlink" title="1、source（单线程）"></a>1、source（单线程）</h4><p>完成对日志数据的采集，分成transtion和event打入channel</p><p>Event： 一个数据单元，消息头和消息体组成。（Events可以是日志记录、 avro 对象等。）可以选择配置batchsize和batchtimeout两个参数</p><p>一个source可以接入多个channel。</p><p><img src="/flume/flume/999804-20171108130931325-512757774.png" alt="img"></p><h6 id="source-avro-类型"><a href="#source-avro-类型" class="headerlink" title="source  avro 类型"></a>source  avro 类型</h6><p>avro是监控的本地文件</p><p>vim avro.conf</p><pre><code class="properties">a1.sources = r1a1.sinks = k1a1.channels = c1#取个别名a1.sources.r1.type = avroa1.sources.r1.bind = localhosta1.sources.r1.port = 6666#类型、地址、端口号，端口号只要不被占用就好a1.sinks.k1.type = logger#sink的类型时logger(不产生实体文件，只在控制台打印)a1.channels.c1.type = memorya1.channels.c1.capacity = 1000#channel最大存储event的数量是1000a1.channels.c1.transactionCapacity = 100#每次送到sink的event数量是100a1.sources.r1.channels = c1a1.sinks.k1.channel = c1                 </code></pre><p>启动命令</p><pre><code>flume-ng agent -n a1 -c . -f avro.conf -Dflume.root.logger=INFO,console#-c配置文件的所在位置#-fflume定义组件的配置文件#-n启动agent的名称-Dflume.root.logger  设置logger日志的级别，一般测试的时候</code></pre><p>监控文件</p><pre><code>flume-ng avro-client -c /root/Downloads/apache-flume-1.6.0-bin/conf -H linux01 -p 6666 -F /ssss.txt#avro-client   数据远程发送到客户端#-c   flume启动所依赖的配置文件的目录#-H   指定主机#-p   端口号#-F   指定要发送的文件</code></pre><p>进行追加数据来测试是否成功</p><pre><code>echo adf &gt;&gt; /ssss.txt</code></pre><h6 id="source-exec"><a href="#source-exec" class="headerlink" title="source  exec"></a>source  exec</h6><p>exec是监控指定文件，是一个单线程的，只能监控一个文件</p><p>不支持断点续传，需要手动生成一个索引文件来实现断点续传</p><p>vim file_exec.conf</p><pre><code class="properties">a1.sources=r1a1.sinks=k1a1.channels=c1a1.sources.r1.type=execa1.sources.r1.command=tail -f /root/ssss.txt#tail -f 是通过文件inode编号来实时获取文件尾部的数据  command是exec类型必须配置的参数#tail -F 是通过文件名来实时获取文件尾部数据的a1.channels.c1.type=memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity=100a1.sinks.k1.type=loggera1.sources.r1.channels=c1a1.sinks.k1.channel=c1</code></pre><p>启动命令</p><pre><code>flume-ng agent -n a1 -c . -f file_exec.conf -Dflume.root.logger=INFO,console#-c配置文件的所在位置#-fflume定义组件的配置文件#-n启动agent的名称#Dflume.root.logger 设置日志等级 </code></pre><p>在启动或因为已经在配置文件中设置好监控文件，直接进行测试即可</p><pre><code>echo adf &gt;&gt; /ssss.txt</code></pre><h6 id="source-tcp"><a href="#source-tcp" class="headerlink" title="source tcp"></a>source tcp</h6><p>读取日志文件产生event，支持tcp和udp两种协议</p><p>vim tcp.conf</p><pre><code class="properties">a1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type = syslogtcpa1.sources.r1.bind = localhosta1.sources.r1.port = 6666a1.sinks.k1.type = loggera1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><p>启动命令</p><pre><code>flume-ng agent -n a1 -c . -f tcp.conf -Dflume.root.logger=INFO,console#-c配置文件的所在位置#-fflume定义组件的配置文件#-n启动agent的名称</code></pre><p>tcp在测试时，需要安装nc的rpm包</p><pre><code>rpm -ivh nc-1.84-22....</code></pre><p>测试</p><pre><code>nc localhost 6666#跟上面配置文件中端口号一致</code></pre><h6 id="source-http"><a href="#source-http" class="headerlink" title="source http"></a>source http</h6><p> HTTP post或get方式的数据源，支持json、blob表示形式</p><p>vim http.conf</p><pre><code class="properties">a1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type = org.apache.flume.source.http.HTTPSourcea1.sources.r1.port = 6666a1.sinks.k1.type = loggera1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><p>进行测试</p><pre><code>curl -X post -d &#39;[&#123;&quot;headers&quot;:&#123;&quot;a1&quot;:&quot;a&quot;&#125;,&quot;body&quot;:&quot;i am a log&quot;&#125;]&#39; linux01:6666</code></pre><p>结果显示</p><p><img src="/flume/flume/1568806782986.png" alt="1568806782986"></p><h6 id="source-spooldir"><a href="#source-spooldir" class="headerlink" title="source spooldir"></a>source spooldir</h6><p>监控目录下的文件变化，一次性，在采集后不可修改，文件后缀名变为.COMPLETED，此时不能再对文件内容进行改变，否则flume会抛出异常</p><p>即spool source类型只适合采集一次写入。<strong>不支持断点续传</strong></p><p>vim spool.conf</p><pre><code class="properties">a1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type = spooldira1.sources.r1.spoolDir=/flumee#在此指定的文件夹必须存在a1.sources.r1.fileHeader=truea1.sinks.k1.type = loggera1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><p>启动</p><pre><code>mkdir /flumee//创建文件夹flume-ng agent -n a1 -c . -f spool.conf -Dflume.root.logger=INFO,console#-c配置文件的所在位置#-fflume定义组件的配置文件#-n启动agent的名称</code></pre><p>测试</p><p>在配置文件中指定的文件夹中修改文件就会触发监控</p><h6 id="source-Taildir"><a href="#source-Taildir" class="headerlink" title="source Taildir"></a>source Taildir</h6><p>flume1.7才出来的，建议用1.8，1.8修复了一个taildirsource的bug</p><p><strong>taildirsouce可以自动保存处理出局的偏移量，支持断点续传和故障转移，还可以同时监控多个目录，是多线程的</strong>，文件改变名称也不会重复采集，因为是根据inode号来进行采集的。</p><pre><code class="properties">a1.sources = r1a1.channels = c1a1.sources.r1.type = TAILDIRa1.sources.r1.channels = c1a1.sources.r1.positionFile = /var/log/flume/taildir_position.json# 被监控的文件夹目录集合，这些文件夹下的文件都会被监控，多个用空格分隔a1.sources.r1.filegroups = f1 f2#被监控文件夹的绝对路径。正则表达式（注意不会匹配文件系统的目录）只是用来匹配文件名a1.sources.r1.filegroups.f1 = /var/log/test1/example.log#f就是head的key，f1是valuea1.sources.r1.headers.f1.f = f1a1.sources.r1.filegroups.f2 = /var/log/test2/.*log.*a1.sources.r1.headers.f2.f = f2a1.sources.r1.headers.f2.f3 = f3#添加绝对路径的头文件fileheadera1.sources.r1.fileHeader = true#自定义fileheader的key的名称#a1.sources.ri.fileHeaderKey = aa</code></pre><h4 id="2、channel（缓存区，一次性）"><a href="#2、channel（缓存区，一次性）" class="headerlink" title="2、channel（缓存区，一次性）"></a>2、channel（缓存区，一次性）</h4><p>主要提供队列功能，对source提供的数据进行简单的缓存，是连接Source和Sink的组件，还可以对source和sink之间进行解耦</p><p>可以把Channel看作是一个缓冲区，它将保存事件直到Sink处理完该事件。</p><p>处理场景：source端数据高峰sink端速度比source端慢，这时候channel就起到了它的作用。</p><p><img src="/flume/flume/999804-20171108131248653-2068131817.png" alt="img"></p><h6 id="channel-memory-类型-常用"><a href="#channel-memory-类型-常用" class="headerlink" title="channel memory 类型(常用)"></a>channel memory 类型(常用)</h6><pre><code class="properties">a1.sources = r1a1.sinks = k1a1.channels = c1#取个别名a1.sources.r1.type = avroa1.sources.r1.bind = localhosta1.sources.r1.port = 6666#类型、地址、端口号，端口号只要不被占用就好a1.sinks.k1.type = loggera1.channels.c1.type = memorya1.channels.c1.capacity = 1000#一次输入的容量a1.channels.c1.transactionCapacity = 100#事务的一次输出量a1.sources.r1.channels = c1a1.sinks.k1.channel = c1    </code></pre><h6 id="channel-file"><a href="#channel-file" class="headerlink" title="channel file"></a>channel file</h6><p>数据保存在磁盘中,一般用这个，不用memory，memory不安全还占资源</p><p>vim file-channel.conf</p><pre><code class="properties">a1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type = spooldira1.sources.r1.spoolDir=/flumeea1.sinks.k1.type = loggera1.channels.c1.type = filea1.channels.c1.dataDir=/flumedata#数据存储的文件夹a1.channels.c1.checkpointDir=/flumecke#检查点的目录## 下面的配置不要产生大量小文件a1.channels.c1.maxFileSize = 2146435071  #存储的最大文件的大小a1.channels.c1.capacity = 1000000   #缓存的容量a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 </code></pre><h6 id="channel-file的优化"><a href="#channel-file的优化" class="headerlink" title="channel file的优化"></a>channel file的优化</h6><p>1）通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。</p><p>2）checkpointDir和backupCheckpointDir也尽量配置在不同硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据</p><h6 id="channel-kafka"><a href="#channel-kafka" class="headerlink" title="channel kafka"></a>channel kafka</h6><p>直接往kafka中放入数据，省去了sink，提高了效率</p><h6 id="FileChannel和MemoryChannel区别"><a href="#FileChannel和MemoryChannel区别" class="headerlink" title="FileChannel和MemoryChannel区别"></a>FileChannel和MemoryChannel区别</h6><p>MemoryChannel传输数据速度更快，但因为数据保存在JVM的堆内存中，Agent进程挂掉会导致数据丢失，适用于对数据质量要求不高的需求。</p><p>FileChannel传输速度相对于Memory慢，但数据安全保障高，Agent进程挂掉也可以从失败中恢复数据。</p><h4 id="3、sink（单线程）"><a href="#3、sink（单线程）" class="headerlink" title="3、sink（单线程）"></a>3、sink（单线程）</h4><p>取出channel的数据，进行相应的存储文件系统，数据库，或者提交到远程服务器</p><p>Sink负责持久化日志或者把事件推向另一个Source。</p><p>一个sink只能接入一个channel。</p><p><img src="/flume/flume/999804-20171108131516809-1930573599.png" alt="img"></p><h6 id="sink-hdfs"><a href="#sink-hdfs" class="headerlink" title="sink hdfs"></a>sink hdfs</h6><p>生成的监控文件在hdfs中，在浏览器中 linux:50070  来查看结果，hdfs dfs -text 路径  查看数据</p><p>vim hdfs-sink.conf </p><pre><code class="properties">@sink必配项a1.sources=r1a1.sinks=k1@a1.channels=c1a1.sources.r1.type=spooldira1.sources.r1.spoolDir=/flumeea1.sources.r1.fileHeader=truea1.channels.c1.type=filea1.channels.c1.checkpointDir=/flume_checkpointa1.channels.c1.dataDir=/flume_data@a1.sinks.k1.type=hdfs@a1.sinks.k1.hdfs.path = hdfs://linux01:9000/flumehhaaa1.sinks.k1.hdfs.filePrefix = 1811Aa1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 1a1.sinks.k1.hdfs.roundUnit = minute#文件压缩编码a1.sinks.k1.hdfs.codeC=snappy#文件类型，不压缩a1.sinks.k1.hdfs.fileType=DataStreama1.sinks.k1.hdfs.writeFormat=Text##下面这3个是文件夹下的文件的控制，谁先满足按谁来#这三个属性一定要配a1.sinks.k1.hdfs.rollInterval=0a1.sinks.k1.hdfs.rollSize=1024a1.sinks.k1.hdfs.rollCount=0a1.sinks.k1.hdfs.idleTimeout=30type        sinks的类型#path hdfs://linux01:9000/flumehaha  HDFS路径#filePrefix 保存数据文件的前缀名#round是否启用时间上的”舍弃”，开启后会取整#roundvalue时间上进行“舍弃”的值#roundunit时间上进行”舍弃”的单位，包含：second,minute,hour#filetype  文件格式，包括：SequenceFile, DataStream,CompressedStre，#writeformat  写 sequence 文件的格式。包含：Text, Writable（默认）#rollinterval  间隔多长将临时文件滚动成最终目标文件，单位：秒,正常一个小时，0就是不使用这个策略。#rollsize    当临时文件达到多少（单位：bytes）时，滚动成目标文件，0就是不使用这个策略。#rolllcount   当events数据达到该数量时候，将临时文件滚动成目标文件，0就是不使用这个策略。#idleTimeout   当目前被打开的临时文件在该参数指定的时间（秒）内，没有任何数据写入，则将该临时文件关闭并重命名成目标文件(闲置N秒后，关闭当前文件（去掉.tmp后缀）)a1.sources.r1.channels=c1a1.sinks.k1.channel=c1</code></pre><p>启动</p><pre><code>flume-ng agent -n a1 -c . -f hdfs-sink.conf -Dflume.root.logger=info,console</code></pre><h6 id="sink-file"><a href="#sink-file" class="headerlink" title="sink file"></a>sink file</h6><p>存储数据到本地文件系统</p><p>vim file-sink.conf</p><pre><code class="properties">a1.sources=r1a1.sinks=k1a1.channels=c1a1.sources.r1.type=spooldira1.sources.r1.spoolDir=/flumeea1.channels.c1.type=memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity=100a1.sinks.k1.type=file_rolla1.sinks.k1.sink.directory=/flog#flog文件夹必须存在，建空文件存放在此目录下a1.sinks.k1.sink.serializer=TEXT#序列化器  输出的格式时texta1.sources.r1.channels=c1a1.sinks.k1.channel=c1</code></pre><p>启动</p><pre><code>flume-ng agent -n a1 -c . -f file-sink.conf -Dflume.root.logger=info,console</code></pre><h6 id="sink-avro"><a href="#sink-avro" class="headerlink" title="sink avro"></a>sink avro</h6><p>vim ftof-sink.conf</p><pre><code class="properties">a1.sources=r1a1.sinks=k1a1.channels=c1a1.sources.r1.type=spooldira1.sources.r1.spoolDir=/flume_loga1.channels.c1.type=memorya1.channels.c1.capacity=1000a1.channels.transactionCapacity=100a1.sinks.k1.type=avroa1.sinks.k1.hostname=192.168.130.107a1.sinks.k1.port=8888a1.sources.r1.channels=c1a1.sinks.k1.channel=c1</code></pre><p>启动</p><pre><code>flume-ng agent -n a1 -c . -f ftof-sink.conf -Dflume.root.logger=info,console</code></pre><h6 id="sink-kafka"><a href="#sink-kafka" class="headerlink" title="sink kafka"></a>sink kafka</h6><pre><code class="properties">a1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type = netcata1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44444a1.sources.r1.channels = c1a1.channels.c1.type = memory# capacity:容量a1.channels.c1.capacity = 1000# transactionCapacity:事务的容量a1.channels.c1.transactionCapacity = 100a1.sinks.k1.channel = c1a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink# kafak集群a1.sinks.k1.kafka.bootstrap.servers = cenos7-1:9092,cenos7-2:9092,cenos7-3:9092,centos7-4:9092# kafka主题a1.sinks.k1.kafka.topic = bigdata1a1.sinks.k1.kafka.flumeBatchSize = 20a1.sinks.k1.kafka.producer.acks = 1</code></pre><h6 id="sink-HDFS存入大量小文件，有什么影响？"><a href="#sink-HDFS存入大量小文件，有什么影响？" class="headerlink" title="sink HDFS存入大量小文件，有什么影响？"></a>sink HDFS存入大量小文件，有什么影响？</h6><p><strong>元数据层面：</strong>每个小文件都有一份元数据，其中包括文件路径，文件名，所有者，所属组，权限，创建时间等，这些信息都保存在Namenode内存中。所以小文件过多，会占用Namenode服务器大量内存，影响Namenode性能和使用寿命.</p><p><strong>计算层面：</strong>默认情况下MR会对每个小文件启用一个Map任务计算，非常影响计算性能。同时也影响磁盘寻址时间。</p><h6 id="sink-hdfs怎么防止写入大量小文件？"><a href="#sink-hdfs怎么防止写入大量小文件？" class="headerlink" title="sink hdfs怎么防止写入大量小文件？"></a>sink hdfs怎么防止写入大量小文件？</h6><p>官方默认的这三个参数配置写入HDFS后会产生小文件，hdfs.rollInterval、hdfs.rollSize、hdfs.rollCount</p><p>基于以上hdfs.rollInterval&#x3D;3600，hdfs.rollSize&#x3D;134217728，hdfs.rollCount &#x3D;0（event个数），hdfs.roundValue&#x3D;10，hdfs.roundUnit&#x3D; second（这两个是控制目录）几个参数综合作用，效果如下：</p><p>（1）tmp文件在达到128M时会滚动生成正式文件</p><p>（2）tmp文件创建超10秒时会滚动生成正式文件</p><p>举例：在2018-01-01 05:23的时侯sink接收到数据，那会产生如下tmp文件：</p><p>&#x2F;home&#x2F;20180101&#x2F;admin.201801010520.tmp 时间到了 把tmp去掉 </p><p>即使文件内容没有达到128M，也会在05:33时滚动生成正式文件</p><h4 id="分布式flume配置"><a href="#分布式flume配置" class="headerlink" title="分布式flume配置"></a>分布式flume配置</h4><p>linux01节点的flume负责拉取webserver数据，同时发送给linux02和linux03节点，03  04节点flume接收到linux01的数据并将数据以日志的形式下沉到各自的控制台</p><h6 id="linux01配置文件"><a href="#linux01配置文件" class="headerlink" title="linux01配置文件"></a>linux01配置文件</h6><pre><code class="properties">a1.sources=r1a1.channels=c1 c2a1.sinks=k1 k2a1.sources.r1.type=execa1.sources.r1.command=tail -F /aa.txta1.channels.c1.type=memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity=100a1.channels.c2.type=memorya1.channels.c2.capacity=1000a1.channels.c2.transactionCapacity=100a1.sinks.k1.type=avroa1.sinks.k1.hostname=linux02a1.sinks.k1.port=6666a1.sinks.k2.type=avroa1.sinks.k2.hostname=linux03a1.sinks.k2.port=6665a1.sources.r1.channels=c1 c2a1.sinks.k1.channel=c1a1.sinks.k2.channel=c2</code></pre><h6 id="linux02配置文件"><a href="#linux02配置文件" class="headerlink" title="linux02配置文件"></a>linux02配置文件</h6><pre><code class="properties">a1.sources=r1a1.channels=c1a1.sinks=k1a1.sources.r1.type=avroa1.sources.r1.bind=linux02a1.sources.r1.port=6666a1.channels.c1.type=memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity=100a1.sinks.k1.type=loggera1.sources.r1.channels=c1a1.sinks.k1.channel=c1</code></pre><h6 id="linux03配置文件"><a href="#linux03配置文件" class="headerlink" title="linux03配置文件"></a>linux03配置文件</h6><pre><code class="properties">a1.sources=r1a1.channels=c1a1.sinks=k1a1.sources.r1.type=avroa1.sources.r1.bind=linux03a1.sources.r1.port=6665a1.channels.c1.type=memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity=100a1.sinks.k1.type=loggera1.sources.r1.channels=c1a1.sink.k1.channel=c1</code></pre><p>先启动linux02和linux03，然后再启动linux01</p><pre><code>flume-ng agent -n a1 -c . -f ftof-sink.conf -Dflume.root.logger=INFO,console//-c  配置文件的位置//-f  flume定义组件的配置文件名//-n  启动agent的名称 该名称在配置文件中定义//-Dflume.root.logger 信息打印到控制台</code></pre><p>在linux01中追加数据，linux02,linux03会监控到数据</p><h4 id="flume的运行单位"><a href="#flume的运行单位" class="headerlink" title="flume的运行单位"></a>flume的运行单位</h4><p>flume的最小运行单位是Agent</p><h4 id="flume的最小传输单位"><a href="#flume的最小传输单位" class="headerlink" title="flume的最小传输单位"></a>flume的最小传输单位</h4><p>flume的最小传输单位是event(事件)</p><h4 id="Flume的启动构建过程"><a href="#Flume的启动构建过程" class="headerlink" title="Flume的启动构建过程"></a>Flume的启动构建过程</h4><p>先构造channel，在构建sink，最后是source。</p><h4 id="Event-事件-的组成"><a href="#Event-事件-的组成" class="headerlink" title="Event(事件)的组成"></a>Event(事件)的组成</h4><p>一行文本内容会被反序列化成一个event </p><p>event是传输数据的核心，他的组成有<strong>headers</strong>和<strong>body</strong>两部分组成：Header是一个集合 Map[String,String]用于携带一些KV类型的元数据（标识，描述等），body部分可以是string和<strong>byte[]字节数组</strong>，body装载着具体的数据内容，headers部分是interceptor,可以存放一些数据属性,header默认是空的</p><p> 从source获取数据后会先封装成event，然后将event发送到channel，sink从channel拿event消费 </p><h4 id="interceptor-拦截器）"><a href="#interceptor-拦截器）" class="headerlink" title="interceptor(拦截器）"></a>interceptor(拦截器）</h4><p>拦截器可以在运行过程中根据开发人员的标准来修改和删除event。他是在source到chananl的过程中发挥作用的</p><h5 id="Timestamp-Interceptor"><a href="#Timestamp-Interceptor" class="headerlink" title="Timestamp Interceptor"></a>Timestamp Interceptor</h5><p>时间戳拦截器，属性中的时间戳会抽取过过来放到event的head中，key默认是timestamp</p><h5 id="Host-Interceptor"><a href="#Host-Interceptor" class="headerlink" title="Host Interceptor"></a>Host Interceptor</h5><p>head中添加你的host主机信息</p><h5 id="Static-Interceptor"><a href="#Static-Interceptor" class="headerlink" title="Static Interceptor"></a>Static Interceptor</h5><p>静态拦截器，key和value都是不会变得</p><h5 id="Remove-Header-Interceptor"><a href="#Remove-Header-Interceptor" class="headerlink" title="Remove Header Interceptor"></a>Remove Header Interceptor</h5><p>将head内容去掉的一个拦截器</p><h5 id="UUID-Interceptor"><a href="#UUID-Interceptor" class="headerlink" title="UUID Interceptor"></a>UUID Interceptor</h5><p>随机生成一个uuid</p><p>…..</p><p>拦截器的位置在source传入到channel中时发挥作用，当我们为Source指定拦截器后，我们在拦截器中会得到event，根据需求我们可以对event进行保留还是抛弃，抛弃的数据不会进入Channel中。</p><h4 id="Source选择器"><a href="#Source选择器" class="headerlink" title="Source选择器"></a>Source选择器</h4><p>选择器在source粗到chananl过程中发挥作用，在拦截器之后，处理拦截器的数据</p><p>Source选择器有两种：</p><p><strong>replicating</strong>(默认)</p><p>复制的方式</p><p>replicating是发给下级所有的channels</p><pre><code class="sh">a1.sources = r1a1.channels = c1 c2 c3a1.sources.r1.selector.type = replicatinga1.sources.r1.channels = c1 c2 c3#可选的channel，失败了也不会报错a1.sources.r1.selector.optional = c3</code></pre><p><strong>multiplexing</strong></p><p>multiplexing是根据选择自己配置把不同的数据发送给不同的channels</p><p>根据event的header中的kv来决定这条消息写入到哪个channel中，选择配置一个映射关系：</p><pre><code>a1.sources = r1a1.channels = c1 c2 c3 c4a1.sources.r1.selector.type = multiplexinga1.sources.r1.selector.header = state#key是state，#v是cz的放入c1#v是us的放入c2 c3#其他的放入到c4里面a1.sources.r1.selector.mapping.CZ = c1a1.sources.r1.selector.mapping.US = c2 c3a1.sources.r1.selector.default = c4</code></pre><pre><code>a1.sources.r1.selector.header = statea1.sources.r1.selector.mapping.CZ = c1这就意味着header中的value为CZ的话，这条消息就会被写入到c1这个channel</code></pre><p>（判断header数据，然后把不同event传输给不同的配置对象）</p><p>可以通过拦截器来再header中添加键值对，如果是启动日志(“topic”,”topic_start)，如果是事件日志，(“topic”,”topic_event)</p><p>拦截器可以通过关键字来进行判断日志类型</p><h4 id="Sink的故障转移和负载均衡"><a href="#Sink的故障转移和负载均衡" class="headerlink" title="Sink的故障转移和负载均衡"></a>Sink的故障转移和负载均衡</h4><p><strong>Sink组逻辑处理器</strong></p><p>你可以把多个sink分成一个组， 这时候 <a href="https://flume.liyifeng.org/#sink">Sink组逻辑处理器</a> 可以对这同一个组里的几个sink进行负载均衡或者其中一个sink发生故障后将输出Event的任务转移到其他的sink上。</p><h5 id="故障转移"><a href="#故障转移" class="headerlink" title="故障转移"></a>故障转移</h5><p>故障转移机制的工作原理是将故障sink降级到一个池中，在池中为它们分配冷却期（超时时间），在重试之前随顺序故障而增加。 Sink成功发送事件后，它将恢复到实时池。sink具有与之相关的优先级，数值越大，优先级越高。 如果在发送Event时Sink发生故障，会继续尝试下一个具有最高优先级的sink。 例如，在优先级为80的sink之前激活优先级为100的sink。如果未指定优先级，则根据配置中的顺序来选取。</p><pre><code class="properties">a1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2a1.sinkgroups.g1.processor.type = failovera1.sinkgroups.g1.processor.priority.k1 = 5a1.sinkgroups.g1.processor.priority.k2 = 10a1.sinkgroups.g1.processor.maxpenalty = 10000</code></pre><h5 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h5><p>负载均衡Sink 选择器提供了在多个sink上进行负载均衡流量的功能。 它维护一个活动sink列表的索引来实现负载的分配。 默认支持了轮询（<code>round_robin</code>）和随机（<code>random</code>）两种选择机制分配负载。 默认是轮询，可以通过配置来更改。也可以从 <em>AbstractSinkSelector</em> 继承写一个自定义的选择器。</p><ul><li><p>工作时，此选择器使用其配置的选择机制选择下一个sink并调用它。 如果所选sink无法正常工作，则处理器通过其配置的选择机制选择下一个可用sink。 此实现不会将失败的Sink列入黑名单，而是继续乐观地尝试每个可用的Sink。</p><p>如果所有sink调用都失败了，选择器会将故障抛给sink的运行器。</p></li></ul><p>如果backoff设置为true则启用了退避机制，失败的sink会被放入黑名单，达到一定的超时时间后会自动从黑名单移除。 如从黑名单出来后sink仍然失败，则再次进入黑名单而且超时时间会翻倍，以避免在无响应的sink上浪费过长时间。 如果没有启用退避机制，在禁用此功能的情况下，发生sink传输失败后，会将本次负载传给下一个sink继续尝试，因此这种情况下是不均衡的。</p><pre><code class="properties">a1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2a1.sinkgroups.g1.processor.type = load_balancea1.sinkgroups.g1.processor.backoff = truea1.sinkgroups.g1.processor.selector = random</code></pre><h3 id="自定义source、sink和interceptor"><a href="#自定义source、sink和interceptor" class="headerlink" title="自定义source、sink和interceptor"></a>自定义source、sink和interceptor</h3><p> <a href="https://blog.csdn.net/wangshuminjava/article/details/83550950">https://blog.csdn.net/wangshuminjava/article/details/83550950</a> </p><h4 id="一、自定义source"><a href="#一、自定义source" class="headerlink" title="一、自定义source"></a>一、自定义source</h4><p> 自定义Source，自定义的Event需要继承PollableSource （轮训拉取）或者EventDrivenSource （事件驱动），另外还需要实现Configurable接口。</p><p>PollableSource或者EventDrivenSource的区别在于：PollableSource是通过线程不断去调用process方法，主动拉取消息，而EventDrivenSource是需要触发一个调用机制，即被动等待。 Configurable接口：便于项目中初始化某些配置用的。 </p><h5 id="1、编写自定义source的代码"><a href="#1、编写自定义source的代码" class="headerlink" title="1、编写自定义source的代码"></a>1、编写自定义source的代码</h5><pre><code class="java">public class CustomSource extends AbstractSource implements Configurable,PollableSource&#123;    @Override    public long getBackOffSleepIncrement() &#123;        // TODO Auto-generated method stub        return 0;    &#125;    @Override    public long getMaxBackOffSleepInterval() &#123;        // TODO Auto-generated method stub        return 0;    &#125;    @Override    public Status process() throws EventDeliveryException &#123;         Random random = new Random();        int randomNum = random.nextInt(100);        String text = &quot;Hello world&quot; + random.nextInt(100);        HashMap&lt;String, String&gt; header = new HashMap&lt;String,String&gt;();        header.put(&quot;id&quot;,Integer.toString(randomNum));        this.getChannelProcessor()            .processEvent(EventBuilder.withBody(text,Charset.forName(&quot;UTF-8&quot;),header));                     return Status.READY;    &#125;    @Override    public void configure(Context arg0) &#123;     &#125;&#125;</code></pre><h5 id="2、编写配置文件"><a href="#2、编写配置文件" class="headerlink" title="2、编写配置文件"></a>2、编写配置文件</h5><pre><code class="conf"># 指定Agent的组件名称  a1.sources = r1  a1.sinks = k1  a1.channels = c1   # 指定Flume source(要监听的路径)  a1.sources.r1.type = com.harderxin.flume.test.MySource   # 指定Flume sink  a1.sinks.k1.type = file_roll  # sink的输出目录，根据自己情况定义a1.sinks.k1.sink.directory = /home/hadoop/sinkFolder   # 指定Flume channel  a1.channels.c1.type = memory  a1.channels.c1.capacity = 1000  a1.channels.c1.transactionCapacity = 100 # 绑定source和sink到channel上  a1.sources.r1.channels = c1  a1.sinks.k1.channel = c1  </code></pre><h5 id="3、项目打jar包，放在flume的lib目录下"><a href="#3、项目打jar包，放在flume的lib目录下" class="headerlink" title="3、项目打jar包，放在flume的lib目录下"></a>3、项目打jar包，放在flume的lib目录下</h5><p>(bin目录网上说成功不了)</p><p>在bin目录下执行</p><pre><code>flume-ng agent -conf conf -conf-file ../conf/custom_source.conf -name a1 </code></pre><h4 id="二、自定义sink"><a href="#二、自定义sink" class="headerlink" title="二、自定义sink"></a>二、自定义sink</h4><h5 id="1、编写自定义类继承-AbstractSink-实现-Configurable-接口"><a href="#1、编写自定义类继承-AbstractSink-实现-Configurable-接口" class="headerlink" title="1、编写自定义类继承 AbstractSink  实现 Configurable 接口"></a>1、编写自定义类继承 AbstractSink  实现 Configurable 接口</h5><pre><code class="java">public class CustomSink extends AbstractSink implements Configurable&#123;     @Override    public Status process() throws EventDeliveryException &#123;         Status status = Status.READY;        Transaction trans = null;        try &#123;            Channel channel = getChannel();            trans = channel.getTransaction();            trans.begin();            for(int i= 0;i &lt; 100 ;i++) &#123;                Event event = channel.take();                if(event == null) &#123;                    status = status.BACKOFF;                    break;                &#125;else &#123;                    String body = new String(event.getBody());                    System.out.println(body);                &#125;            &#125;            trans.commit();         &#125;catch (Exception e) &#123;            if(trans != null) &#123;                trans.commit();            &#125;            e.printStackTrace();        &#125;finally &#123;            if(trans != null) &#123;                trans.close();            &#125;        &#125;        return status;    &#125;     @Override    public void configure(Context arg0) &#123;     &#125;   &#125;</code></pre><h5 id="2、配置文件"><a href="#2、配置文件" class="headerlink" title="2、配置文件"></a>2、配置文件</h5><pre><code class="conf">a1.sources = r1a1.sinks = k1a1.channels = c1 # Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 44444 # Describe the sinka1.sinks.k1.type = com.caoxufeng.MyCustom.CustomSink # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 10000a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><h5 id="3、项目打成jar包，放入flume下的-lib目录下"><a href="#3、项目打成jar包，放入flume下的-lib目录下" class="headerlink" title="3、项目打成jar包，放入flume下的 lib目录下"></a>3、项目打成jar包，放入flume下的 lib目录下</h5><pre><code>flume-ng agent -conf conf -conf-file ../conf/custom_sink.conf -name a1 </code></pre><h4 id="三、自定义Interceptor（拦截器）"><a href="#三、自定义Interceptor（拦截器）" class="headerlink" title="三、自定义Interceptor（拦截器）"></a>三、自定义Interceptor（拦截器）</h4><p>Flume中拦截器的作用就是对于event中header的部分可以按需塞入一些属性，当然你如果想要处理event的body内容，也是可以的，但是event的body内容是系统下游阶段真正处理的内容，如果让Flume来修饰body的内容的话，那就是强耦合了，这就违背了当初使用Flume来解耦的初衷了。</p><h5 id="1、编写自定义类实现Interceptor接口"><a href="#1、编写自定义类实现Interceptor接口" class="headerlink" title="1、编写自定义类实现Interceptor接口"></a>1、编写自定义类实现Interceptor接口</h5><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;    &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt;    &lt;version&gt;1.9.0&lt;/version&gt;&lt;dependency&gt;</code></pre><p>执行过程：内部类config()-》内部类build()-&gt;外部类</p><pre><code class="java">public class CustomInterceptor implements Interceptor&#123;     private final String headerKey;    private static final String CONF_HEADER_KEY = &quot;header&quot;;    private static final String DEFAULT_HEADER = &quot;count&quot;;    private final AtomicLong currentCount;     public CustomInterceptor(Context ctx) &#123;        headerKey = ctx.getString(CONF_HEADER_KEY,DEFAULT_HEADER);        currentCount = new AtomicLong();    &#125;     //运行前的初始化，一般不需要实现    @Override    public void initialize() &#123;        // TODO Auto-generated method stub      &#125;    //)处理单个event    @Override    public Event intercept(Event event) &#123;         long count = currentCount.incrementAndGet();        event.getHeaders().put(headerKey, String.valueOf(count));        return event;    &#125;    //批量处理event，循环出路一面的interceptor(Event event)    @Override    public List&lt;Event&gt; intercept(List&lt;Event&gt; events) &#123;        for(Event e:events) &#123;            intercept(e);        &#125;        return events;    &#125;    @Override    public void close() &#123;           &#125;    public static class CounterInterceptorBuilder implements Builder &#123;        private Context ctx;         //用户构建的一个拦截器实例        @Override        public Interceptor build() &#123;            return new CustomInterceptor(ctx);        &#125;         //获取参数的入口，通过context获取配置好的参数        @Override        public void configure(Context context) &#123;            this.ctx = context;        &#125;    &#125;&#125;</code></pre><h5 id="2、编写配置文件-1"><a href="#2、编写配置文件-1" class="headerlink" title="2、编写配置文件"></a>2、编写配置文件</h5><p>注意事项：interceptor type是编写类的全路径，并且内部类使用**$**进行标注</p><pre><code class="properties">a1.sources = r1a1.sinks = s1a1.channels = c1 a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444a1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type = com.caoxufeng.MyCustom.CustomInterceptor$CounterInterceptorBuilder# perserveExisting 是 interceptor获取的值a1.sources.r1.interceptors.i1.perserveExisting = true a1.sinks.s1.type = logger a1.channels.c1.type = memorya1.channels.c1.capacity = 2a1.channels.c1.transactionCapacity = 2 a1.sources.r1.channels = c1a1.sinks.s1.channel = c1</code></pre><h5 id="3、-项目打成jar包，放入flume下的-lib目录下"><a href="#3、-项目打成jar包，放入flume下的-lib目录下" class="headerlink" title="3、 项目打成jar包，放入flume下的 lib目录下"></a>3、 项目打成jar包，放入flume下的 lib目录下</h5><p>(bin目录网上说成功不了)</p><p><img src="/flume/flume/image-20210302190346472.png" alt="配置文件编写"></p><p>在bin目录下编写配置文件执行</p><pre><code>flume-ng agent -c conf -f ../conf/custom-interceptor.conf -n a1</code></pre><h3 id="Flume的失败切换"><a href="#Flume的失败切换" class="headerlink" title="Flume的失败切换"></a>Flume的失败切换</h3><p>Failover Sink Processor失败切换</p><p>将sink分组，一组中只有优先级高的sink工作，另一个在等待。</p><p>如果高优先级的sink发送数据失败，就用优先级低的sink去工作。在配置时间penalty之后，还会尝试高优先级的去发送数据。</p><pre><code class="properties">a1.sinkgroups =g1a1.sinkgroups.g1.sinks = k1 k2a1.sinkgroups.processor.type = failover## 对两个sink分配不同的优先级a1.sinkgroups.g1.processor.priority.k1=200a1.sinkgroups.g1.processor.priority.k2=100##主sink失败后，停用调整时间a1.sinkgroups.g1.processor.maxpenalty=5000</code></pre><h3 id="Flume内存优化"><a href="#Flume内存优化" class="headerlink" title="Flume内存优化"></a>Flume内存优化</h3><p>1 问题描述：如果启动消费Flume抛出如下异常</p><pre><code>ERROR hdfs.HDFSEventSink: process failedjava.lang.OutOfMemoryError: GC overhead limit exceeded</code></pre><p>2 解决方案步骤：</p><p>1）在服务器的&#x2F;opt&#x2F;modules&#x2F;flume&#x2F;conf&#x2F;flume-env.sh文件中增加如下配置</p><pre><code>export JAVA_OPTS=&quot;-Xms100m -Xmx4096m -Dcom.sun.management.jmxremote&quot;</code></pre><p><strong>Xms</strong> 是指设定程序启动时占用内存大小。一般来讲，大点，程序会启动的快一点，但是也可能会导致机器暂时间变慢。</p><p><strong>Xmx</strong> 是指设定程序运行期间最大可占用的内存大小。如果程序运行需要占用更多的内存，超出了这个设置值，就会抛出OutOfMemory异常。</p><p><strong>Xss</strong> 是指设定每个线程的堆栈大小。这个就要依据你的程序，看一个线程大约需要占用多少内存，可能会有多少线程同时运行等。</p><p>2）同步配置到98、97服务器</p><pre><code>[admin@hadoop-yarn conf]$ xsync flume-env.sh</code></pre><p>3 Flume内存参数设置及优化</p><p>JVM heap一般设置为4G或更高，部署在单独的服务器上（4核8线程16G内存）</p><p>-Xmx与-Xms最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导致频繁fullgc。</p><h2 id="Flume的监控"><a href="#Flume的监控" class="headerlink" title="Flume的监控"></a>Flume的监控</h2><h3 id="内置监控"><a href="#内置监控" class="headerlink" title="内置监控"></a>内置监控</h3><p>flume的自带功能，但是功能比较简陋，只会返回json</p><pre><code>-Dflume.monitoring.type=http -Dflume.monitoring.port=34545</code></pre><h3 id="Ganglia"><a href="#Ganglia" class="headerlink" title="Ganglia"></a>Ganglia</h3><p>Ganglia是一个监控工具，不是单独服务flume的工具</p><h4 id="Ganglia的安装与部署"><a href="#Ganglia的安装与部署" class="headerlink" title="Ganglia的安装与部署"></a>Ganglia的安装与部署</h4><p><strong>1)</strong> <strong>安装httpd****服务与php</strong></p><pre><code>[admin@hadoop-yarn flume]$ sudo yum -y install httpd php</code></pre><p><strong>2)</strong> <strong>安装其他依赖</strong></p><pre><code>[admin@hadoop-yarn flume]$ sudo yum -y install rrdtool perl-rrdtool rrdtool-devel[admin@hadoop-yarn flume]$ sudo yum -y install apr-devel</code></pre><p>3）<strong>安装Ganglia</strong></p><pre><code>[admin@hadoop-yarn flume]$ sudo rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm[admin@hadoop-yarn flume]$ sudo yum -y install ganglia-gmetad [admin@hadoop-yarn flume]$ sudo yum -y install ganglia-web[admin@hadoop-yarn flume]$ sudo yum install -y ganglia-gmond</code></pre><p><strong>4)</strong> <strong>修改配置文件&#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F;ganglia.conf</strong></p><pre><code>[admin@hadoop-yarn flume]$ sudo vim /etc/httpd/conf.d/ganglia.conf</code></pre><p><strong>修改为红颜色的配置：</strong></p><pre><code>\# Ganglia monitoring system php web frontendAlias /ganglia /usr/share/ganglia&lt;Location /ganglia&gt; Order deny,allow Deny from all Allow from all  Allow from 127.0.0.1 \# Allow from ::1 \# Allow from .example.com&lt;/Location&gt;</code></pre><p><strong>5)</strong> <strong>修改配置文件&#x2F;etc&#x2F;ganglia&#x2F;gmetad.conf</strong></p><pre><code>[admin@hadoop-yarn flume]$ sudo vim /etc/ganglia/gmetad.conf修改为：data_source &quot;bw61&quot; 192.168.137.66</code></pre><p><strong>6)</strong> <strong>修改配置文件&#x2F;etc&#x2F;ganglia&#x2F;gmond.conf</strong></p><pre><code>[admin@hadoop-yarn flume]$ sudo vim /etc/ganglia/gmond.conf //修改为cluster &#123;  name = &quot;bw66&quot;  owner = &quot;unspecified&quot;  latlong = &quot;unspecified&quot;  url = &quot;unspecified&quot;&#125;udp_send_channel &#123;  #bind_hostname = yes # Highly recommended, soon to be default.                       # This option tells gmond to use a source address                       # that resolves to the machine&#39;s hostname.  Without                       # this, the metrics may appear to come from any                       # interface and the DNS names associated with                       # those IPs will be used to create the RRDs.  # mcast_join = 239.2.11.71  host = bw66  port = 8649  ttl = 1&#125;udp_recv_channel &#123;  # mcast_join = 239.2.11.71  port = 8649  bind = bw66  retry_bind = true  # Size of the UDP buffer. If you are handling lots of metrics you really  # should bump it up to e.g. 10MB or even higher.  # buffer = 10485760&#125;</code></pre><p><strong>7)</strong> <strong>修改配置文件&#x2F;etc&#x2F;selinux&#x2F;config</strong></p><pre><code>[admin@hadoop-yarn flume]$ sudo vim /etc/selinux/config**修改为：**\# This file controls the state of SELinux on the system.\# SELINUX= can take one of these three values:\#   enforcing - SELinux security policy is enforced.\#   permissive - SELinux prints warnings instead of enforcing.\#   disabled - No SELinux policy is loaded.SELINUX=disabled\# SELINUXTYPE= can take one of these two values:\#   targeted - Targeted processes are protected,\#   mls - Multi Level Security protection.SELINUXTYPE=targeted</code></pre><p>尖叫提示：selinux本次生效关闭必须重启，如果此时不想重启，可以临时生效之：</p><pre><code>[admin@hadoop-yarn flume]$ sudo setenforce 0</code></pre><p><strong>8）启动ganglia</strong></p><pre><code>[admin@hadoop-yarn flume]$ sudo service httpd start[admin@hadoop-yarn flume]$ sudo service gmetad start[admin@hadoop-yarn flume]$ sudo service gmond start</code></pre><p><strong>6)</strong> <strong>打开网页浏览ganglia页面</strong></p><pre><code>http://192.168.137.99/ganglia</code></pre><p>尖叫提示：如果完成以上操作依然出现权限不足错误，请修改&#x2F;var&#x2F;lib&#x2F;ganglia目录的权限：</p><pre><code>[admin@hadoop-yarn flume]$ sudo chmod -R 777 /var/lib/ganglia</code></pre><h3 id="Flume面试题"><a href="#Flume面试题" class="headerlink" title="Flume面试题"></a>Flume面试题</h3><p><strong>flume采集断掉，要从上面的进度继续采集，怎么弄？</strong></p><p>思想：记录中间传输的状态</p><ol><li>使用interceptor设置，创建日志文件，在中断连接时按照这个里面的日志文件来进行继续</li><li>taildirsource 可以实现断点续传。</li></ol><p><strong>只有上面方式可以实现</strong></p><p>channel中有<strong>transactionCapacity</strong> 参数设置事务容量大小，channel的transactionCapacity参数不能小于sink和source的batchsize。最好是batchsize的倍数。flume正常运行可能会导致数据的重复，但不会丢失数据！但是flume意外停止就会丢失数据。</p><p><strong>Flume在项目中是如何使用的？</strong></p><p>flume通过taildir source来进监控日志文件，使用了自定义拦截器来判断不同的数据，并添加event head，source发送到channel时，选择器使用了 <strong>multiplexing</strong> 选择器，根据head的不同值将event发送到不同的channel，在进行落地到不同的目录中。</p><p>我负责flume的数据采集，数据分路开发，那边的数据我不管，正常发送就行。</p><p><strong>Flume有事务控制吗？</strong></p><p>Flume拥有事务控制，source到channel和channel到sink，这两个过程都可以实现事务控制。（一起成功一起失败，可以回滚）put事务和take事务</p><p>比如spooling directory source 为文件的每一个<strong>event batch</strong>创建一个事务，一旦事务中所有的事件全部传递到Channel且提交成功，那么Soucrce就将event batch标记为完成。</p><p>同理，事务以类似的方式处理从Channel到Sink的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚，且所有的事件都会保持到Channel中，等待重新传递。</p><p><strong>Flume的权限控制用什么实现？</strong></p><p>kerberos来实现，通常是运维人员来设置</p><p><strong>配置flume的日志文件名称包含时间？</strong></p><ol><li>flume的可以编写interceptor，将数据的时间进行解析添加到event的头部，进行调用</li><li>通过flume自带的 %Y %M 等一系列特殊符号定义，但是可能会出现半夜12点的数据到了第二天，在处理的时候就会少一部分数据。不建议使用</li></ol><p><strong>Flume使用遇到过什么问题？</strong></p><ol><li>Flume的 channel事务容量和 hdfs sink 的批次大小设置不一样，导致数据写入失败。解决方式：设置channel事务大小和 hdfs sink 批次大小一致。</li><li>配置完成后，source和sink报没有连接到channel，报总容量小于事务容量。解决方式：设置channel的总容量大小。</li></ol><pre><code class="properties">a1.channels.c1.capacity=2000</code></pre><p><strong>flume动态加载配置</strong></p><p>运行过程中，如果配置发生改变，需要重新启动flume来使它生效。</p><p>也可以不用重启flume服务，flume会在一段时间后重新检测flume的配置文件，并自动更新。</p><p><strong>flume的运行过程中发生过什么问题？</strong></p><p>flume的内存不够用，内存溢出。解决方法：调大内存</p><p>机器磁盘满了，导致没有空间存储，之前过期数据没有定时处理，写一个脚本定期处理</p><p>事务容量大于容量</p><p><strong>什么是数据的积压？</strong>什么情况下产生积压？积压了怎么解决？</p><p><strong>flume会不会丢数据，会不会产生数据重复？</strong></p><p>看选择的source、channal和sink的类型，选择可靠的类型不会丢失数据，因为他们支持事务。 </p><p><strong>什么情况下会出现采集阻塞，以及，这些情况分别如何应付？</strong></p><h3 id="Failover-Sink-Processor"><a href="#Failover-Sink-Processor" class="headerlink" title="Failover Sink Processor"></a>Failover Sink Processor</h3><p>故障转移接收器处理器维护一个按优先级排序的接收器列表，确保只要有一个可用的事件就会被处理(交付)。</p><p>配置多个sink根据</p><pre><code class="properties">a1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2a1.sinkgroups.g1.processor.type = failovera1.sinkgroups.g1.processor.priority.k1 = 5a1.sinkgroups.g1.processor.priority.k2 = 10a1.sinkgroups.g1.processor.maxpenalty = 10000</code></pre><h3 id="Load-balancing-Sink-Processor"><a href="#Load-balancing-Sink-Processor" class="headerlink" title="Load balancing Sink Processor"></a>Load balancing Sink Processor</h3><p>负载均衡，一组sink中的sink之间轮流发送，策略有：random-robin（轮询）、random（随机）</p><p>负载平衡接收器处理器提供了在多个接收器上负载平衡流的能力。它维护一个活动接收器的索引列表，负载必须分布在该列表上。实现支持通过round_robin或随机选择机制分配负载。选择机制的选择默认为round_robin类型，但可以通过配置重写。通过继承自AbstractSinkSelector的自定义类支持自定义选择机制。</p><pre><code class="properties">a1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2a1.sinkgroups.g1.processor.type = load_balancea1.sinkgroups.g1.processor.backoff = truea1.sinkgroups.g1.processor.selector = random</code></pre><h2 id="Flume的高可用"><a href="#Flume的高可用" class="headerlink" title="Flume的高可用"></a>Flume的高可用</h2><p>agent中多个sink连接同一个chanal，多个sink会形成组的概念，一个组中的sink消费数据策略有两种，一种是负载均衡发送，每个sink都发送一部分数据，一种是复制的方式每个sink都发同样的内容。</p><p>高可用的实现：</p><p><img src="/flume/flume/%E7%BA%A7%E8%81%94%E6%B1%87%E8%81%9A%E7%82%B9%E7%9A%84HA%E9%85%8D%E7%BD%AE%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="级联汇聚点的HA配置示意图"></p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git的使用</title>
      <link href="/git/git-de-shi-yong/"/>
      <url>/git/git-de-shi-yong/</url>
      
        <content type="html"><![CDATA[<h2 id="Git的使用"><a href="#Git的使用" class="headerlink" title="Git的使用"></a>Git的使用</h2><p>1、下载安装过程，此处省略，安装过程中，可以选择是否可以通过第三方软件控制</p><p>2、注册git的账号，github,gitlab都可以，</p><h4 id="3、打开git的命令行窗口-设置账号"><a href="#3、打开git的命令行窗口-设置账号" class="headerlink" title="3、打开git的命令行窗口,设置账号"></a>3、打开git的命令行窗口,设置账号</h4><pre><code>//在这里设置自己的账号和邮箱git config --global user.name &quot;zhanghao&quot;   git config --global user.email &quot;e-mail&quot;</code></pre><h4 id="4、在指定位置初始化你的本地仓库"><a href="#4、在指定位置初始化你的本地仓库" class="headerlink" title="4、在指定位置初始化你的本地仓库"></a>4、在指定位置初始化你的本地仓库</h4><pre><code>git init//该命令将创建一个名为 .git 的子目录，这个子目录含有你初始化的 Git 仓库中所有的必须文件，这些文件是 Git 仓库的骨干。 但是，在这个时候，我们仅仅是做了一个初始化的操作，你的项目里的文件还没有被跟踪。</code></pre><h4 id="5、查看文件状态"><a href="#5、查看文件状态" class="headerlink" title="5、查看文件状态"></a>5、查看文件状态</h4><pre><code>//本地仓库目录下使用命令git status //如果没有提交到本地仓库上，那么就是红色的//提交了就是绿色的git status是用来查看当前工作状态，假如有文件为红色，说明文件修改了，此时在工作区。</code></pre><h4 id="6、添加数据到本地仓库"><a href="#6、添加数据到本地仓库" class="headerlink" title="6、添加数据到本地仓库"></a>6、添加数据到本地仓库</h4><pre><code>git add 文件名称//将当前目录下修改的所有代码从工作区添加到暂存区 . 代表当前目录，之后就是绿色的了</code></pre><h4 id="7、git-提交"><a href="#7、git-提交" class="headerlink" title="7、git 提交"></a>7、git 提交</h4><pre><code>git commit -m &quot;这里面就是message&quot;提交时，会将本地仓库提交上去，需要指定 commit message 数据</code></pre><h4 id="8、git-创建分支并切换到新建分支"><a href="#8、git-创建分支并切换到新建分支" class="headerlink" title="8、git 创建分支并切换到新建分支"></a>8、git 创建分支并切换到新建分支</h4><pre><code>git checkout -b dev//git checkout 命令加上 -b 参数表示创建并切换，相当于以下两条命令：$ git branch dev //创建分支$ git checkout dev  //切换到指定分支</code></pre><h4 id="9、查看所有分支列表"><a href="#9、查看所有分支列表" class="headerlink" title="9、查看所有分支列表"></a>9、查看所有分支列表</h4><pre><code>git branch </code></pre><p><img src="/git/git-de-shi-yong/TyporaImage\image-20210112140653747.png" alt="image-20210112140653747"></p><h4 id="10、合并分支"><a href="#10、合并分支" class="headerlink" title="10、合并分支"></a>10、合并分支</h4><pre><code>git merge dev</code></pre><h4 id="11、删除分支"><a href="#11、删除分支" class="headerlink" title="11、删除分支"></a>11、删除分支</h4><pre><code>$ git branch -d dev//删除dev这个分支克隆远程仓库</code></pre><pre><code>git clone &lt;url&gt; $ git clone //Git 克隆的是该 Git 仓库服务器上的几乎所有数据，而不是仅仅复制完成你的工作所需要文件。 当你执行 git clone 命令的时候，默认配置下远程 Git 仓库中的每一个文件的每一个版本都将被拉取下来//克隆的仓库名称取别名git clone &lt;url&gt;  myurlname$ git clone https://github.com/libgit2/libgit2 mylibgit//这样克隆下来的仓库名称就变成了mylibgit</code></pre><h4 id="12、忽略文件"><a href="#12、忽略文件" class="headerlink" title="12、忽略文件"></a>12、忽略文件</h4><pre><code>在你的项目总创建一个  .gitignore 的文件，列出要忽略的文件的模式。cat .gitignore 案例：.[oa]//第一行告诉 Git 忽略所有以 .o 或 .a 结尾的文件*~//第二行告诉 Git 忽略所有名字以波浪符（~）结尾的文件，许多文本编辑软件（比如 Emacs）都用这样的文件名保存副本。# 忽略所有的 .a 文件*.a# 但跟踪所有的 lib.a，即便你在前面忽略了 .a 文件!lib.a# 只忽略当前目录下的 TODO 文件，而不忽略 subdir/TODO/TODO# 忽略任何目录下名为 build 的文件夹build/# 忽略 doc/notes.txt，但不忽略 doc/server/arch.txtdoc/*.txt# 忽略 doc/ 目录及其所有子目录下的 .pdf 文件doc/**/*.pdf</code></pre><pre><code>文件 .gitignore 的格式规范如下：所有空行或者以 # 开头的行都会被 Git 忽略。可以使用标准的 glob 模式匹配，它会递归地应用在整个工作区中。匹配模式可以以（/）开头防止递归。匹配模式可以以（/）结尾指定目录。要忽略指定模式以外的文件或目录，可以在模式前加上叹号（!）取反。</code></pre><p>13、查看已暂存和未暂存的修改</p>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java8文档笔记</title>
      <link href="/java/java8-wen-dang-xue-xi-bi-ji/"/>
      <url>/java/java8-wen-dang-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>Java 有三个显式关键字来设置类中的访问权限：<code>public</code>（公开），<code>private</code>（私有）和<code>protected</code>（受保护）。这些访问修饰符决定了谁能使用它们修饰的方法、变量或类。</p><h4 id="多态"><a href="#多态" class="headerlink" title="多态"></a><a href="https://lingcoder.github.io/OnJava8/#/book/01-What-is-an-Object?id=%E5%A4%9A%E6%80%81">多态</a></h4><p>在传统意义上，编译器不能进行函数调用。由非 OOP（面向对象程序设计） 编译器产生的函数调用会引起所谓的<strong>早期绑定</strong>，这个术语你可能从未听说过，不会想过其他的函数调用方式。这意味着编译器生成对特定函数名的调用，该调用会被解析为将执行的代码的绝对地址。</p><p>通过继承，程序直到运行时才能确定代码的地址，因此发送消息给对象时，还需要其他一些方案。为了解决这个问题，面向对象语言使用<strong>后期绑定</strong>的概念。当向对象发送信息时，被调用的代码直到运行时才确定。编译器确保方法存在，并对参数和返回值执行类型检查，但是它不知道要执行的确切代码。</p><p>为了执行后期绑定，Java 使用一个特殊的代码位来代替绝对调用。这段代码使用对象中存储的信息来计算方法主体的地址（此过程在多态性章节中有详细介绍）。因此，每个对象的行为根据特定代码位的内容而不同。当你向对象发送消息时，对象知道该如何处理这条消息。在某些语言中，必须显式地授予方法后期绑定属性的灵活性。例如，C++ 使用 <strong>virtual</strong> 关键字。在这些语言中，默认情况下方法不是动态绑定的。在 Java 中，动态绑定是默认行为，不需要额外的关键字来实现多态性。</p><p>这种把子类当成其基类来处理的过程叫做“向上转型”（<strong>upcasting</strong>）。在面向对象的编程里，经常利用这种方法来给程序解耦。</p><p>发送消息给对象时，如果程序不知道接收的具体类型是什么，但最终执行是正确的，这就是对象的“多态性”（Polymorphism）。面向对象的程序设计语言是通过“动态绑定”的方式来实现对象的多态性的。编译器和运行时系统会负责对所有细节的控制；我们只需知道要做什么，以及如何利用多态性来更好地设计程序。</p><h4 id="集合"><a href="#集合" class="headerlink" title="集合"></a><a href="https://lingcoder.github.io/OnJava8/#/book/01-What-is-an-Object?id=%E9%9B%86%E5%90%88">集合</a></h4><p>一般优秀的 OOP 语言都会将“集合”作为其基础包。在 C++ 中，“集合”是其标准库的一部分，通常被称为 STL（Standard Template Library，标准模板库）。SmallTalk 有一套非常完整的集合库。同样，Java 的标准库中也提供许多现成的集合类。</p><p>“集合”这种类型的对象可以存储任意类型、数量的其他对象。它能根据需要自动扩容，我们不用关心过程是如何实现的。</p><p>在一些库中，一两个泛型集合就能满足我们所有的需求了，而在其他一些类库（Java）中，不同类型的集合对应不同的需求：常见的有 List，常用于保存序列；Map，也称为关联数组，常用于将对象与其他对象关联；Set，只能保存非重复的值；其他还包括如队列（Queue）、树（Tree）、栈（Stack）、堆（Heap）等等。从设计的角度来看，我们真正想要的是一个能够解决某个问题的集合。如果一种集合就满足所有需求，那么我们就不需要剩下的了。之所以选择集合有以下两个原因：</p><ol><li>集合可以提供不同类型的接口和外部行为。堆栈、队列的应用场景和集合、列表不同，它们中的一种提供的解决方案可能比其他灵活得多。</li><li>不同的集合对某些操作有不同的效率。例如，List 的两种基本类型：ArrayList 和 LinkedList。虽然两者具有相同接口和外部行为，但是在某些操作中它们的效率差别很大。在 ArrayList 中随机查找元素是很高效的，而 LinkedList 随机查找效率低下。反之，在 LinkedList 中插入元素的效率要比在 ArrayList 中高。由于底层数据结构的不同，每种集合类型在执行相同的操作时会表现出效率上的差异。</li></ol><p>在 Java 5 泛型出来之前，集合中保存的是通用类型 <code>Object</code>。Java 单继承的结构意味着所有元素都基于 <code>Object</code> 类，所以在集合中可以保存任何类型的数据，易于重用。要使用这样的集合，我们先要往集合添加元素。由于 Java 5 版本前的集合只保存 <code>Object</code>，当我们往集合中添加元素时，元素便向上转型成了 <code>Object</code>，从而丢失自己原有的类型特性。这时我们再从集合中取出该元素时，元素的类型变成了 <code>Object</code>。那么我们该怎么将其转回原先具体的类型呢？这里，我们使用了强制类型转换将其转为更具体的类型，这个过程称为对象的“向下转型”。通过“向上转型”，我们知道“圆形”也是一种“形状”，这个过程是安全的。可是我们不能从“Object”看出其就是“圆形”或“形状”，所以除非我们能确定元素的具体类型信息，否则“向下转型”就是不安全的。也不能说这样的错误就是完全危险的，因为一旦我们转化了错误的类型，程序就会运行出错，抛出“运行时异常”（RuntimeException）。（后面的章节会提到） 无论如何，我们要寻找一种在取出集合元素时确定其具体类型的方法。另外，每次取出元素都要做额外的“向下转型”对程序和程序员都是一种开销。以某种方式创建集合，以确认保存元素的具体类型，减少集合元素“向下转型”的开销和可能出现的错误难道不好吗？这种解决方案就是：参数化类型机制（Parameterized Type Mechanism）。</p><p>参数化类型机制可以使得编译器能够自动识别某个 <code>class</code> 的具体类型并正确地执行。举个例子，对集合的参数化类型机制可以让集合仅接受“形状”这种类型的元素，并以“形状”类型取出元素。Java 5 版本支持了参数化类型机制，称之为“泛型”（Generic）。泛型是 Java 5 的主要特性之一。你可以按以下方式向 ArrayList 中添加 Shape（形状）：</p><h4 id="对象创建与生命周期"><a href="#对象创建与生命周期" class="headerlink" title="对象创建与生命周期"></a><a href="https://lingcoder.github.io/OnJava8/#/book/01-What-is-an-Object?id=%E5%AF%B9%E8%B1%A1%E5%88%9B%E5%BB%BA%E4%B8%8E%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F">对象创建与生命周期</a></h4><p>我们在使用对象时要注意的一个关键问题就是对象的创建和销毁方式。每个对象的生存都需要资源，尤其是内存。为了资源的重复利用，当对象不再被使用时，我们应该及时释放资源，清理内存。</p><p>在简单的编程场景下，对象的清理并不是问题。我们创建对象，按需使用，最后销毁它。然而，情况往往要比这更复杂：</p><p>假设，我们正在为机场设计一个空中交通管制的系统（该例也适用于仓库货柜管理、影带出租或者宠物寄养仓库系统）。第一步比较简单：创建一个用来保存飞机的集合，每当有飞机进入交通管制区域时，我们就创建一个“飞机”对象并将其加入到集合中，等到飞机离开时将其从这个集合中清除。与此同时，我们还需要一个记录飞机信息的系统，也许这些数据不像主要控制功能那样引人注意。比如，我们要记录所有飞机中的小型飞机的的信息（比如飞行计划）。此时，我们又创建了第二个集合来记录所有小型飞机。 每当创建一个“飞机”对象的时候，将其放入第一个集合；若它属于小型飞机，也必须同时将其放入第二个集合里。</p><p>现在问题开始棘手了：我们怎么知道何时该清理这些对象呢？当某一个系统处理完成，而其他系统可能还没有处理完成。这样的问题在其他的场景下也可能发生。在 C++ 程序设计中，当使用完一个对象后，必须明确将其删除，这就让问题变复杂了。</p><p>对象的数据在哪？它的生命周期是怎么被控制的？ 在 C++ 设计中采用的观点是效率第一，因此它将选择权交给了程序员。为了获得最大的运行时速度，程序员可以在编写程序时，通过将对象放在栈（Stack，有时称为自动变量或作用域变量）或静态存储区域（static storage area）中来确定内存占用和生存时间。这些区域的对象会被优先分配内存和释放。这种控制在某些情况下非常有用。</p><p>然而相对的，我们也牺牲了程序的灵活性。因为在编写代码时，我们必须要弄清楚对象的数量、生存时间还有类型。如果我们要用它来解决一个相当普遍的问题时（如计算机辅助设计、仓库管理或空中交通管制等），限制就太大了。</p><p>第二种方法是在堆内存（Heap）中动态地创建对象。在这种方式下，直到程序运行我们才能确定需要创建的对象数量、生存时间和类型。什么时候需要，什么时候在堆内存中创建。 因为内存的占用是动态管理的，所以在运行时，在堆内存上开辟空间所需的时间可能比在栈内存上要长（但也不一定）。在栈内存开辟和释放空间通常是一条将栈指针向下移动和一条将栈指针向上移动的汇编指令。开辟堆内存空间的时间取决于内存机制的设计。</p><p>动态方法有这样一个合理假设：对象通常是复杂的，相比于对象创建的整体开销，寻找和释放内存空间的开销微不足道。（原文：*The dynamic approach makes the generally logical assumption that objects tend to be complicated, so the extra overhead of finding storage and releasing that storage will not have an important impact on the creation of an object.*）此外，更好的灵活性对于问题的解决至关重要。</p><p>Java 使用动态内存分配。每次创建对象时，使用 <code>new</code> 关键字构建该对象的动态实例。这又带来另一个问题：对象的生命周期。较之堆内存，在栈内存中创建对象，编译器能够确定该对象的生命周期并自动销毁它；然而如果你在堆内存创建对象的话，编译器是不知道它的生命周期的。在 C++ 中你必须以编程方式确定何时销毁对象，否则可能导致内存泄漏。Java 的内存管理是建立在垃圾收集器上的，它能自动发现对象不再被使用并释放内存。垃圾收集器的存在带来了极大的便利，它减少了我们之前必须要跟踪的问题和编写相关代码的数量。因此，垃圾收集器提供了更高级别的保险，以防止潜在的内存泄漏问题，这个问题使得许多 C++ 项目没落。</p><p>Java 的垃圾收集器被设计用来解决内存释放的问题（虽然这不包括对象清理的其他方面）。垃圾收集器知道对象什么时候不再被使用并且自动释放内存。结合单继承和仅可在堆中创建对象的机制，Java 的编码过程比用 C++ 要简单得多。我们所要做的决定和要克服的障碍也会少很多！</p><h4 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a><a href="https://lingcoder.github.io/OnJava8/#/book/03-Objects-Everywhere?id=%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8">数据存储</a></h4><p>那么，程序在运行时是如何存储的呢？尤其是内存是怎么分配的。有5个不同的地方可以存储数据：</p><ol><li><strong>寄存器</strong>（Registers）最快的存储区域，位于 CPU 内部 <a href="https://lingcoder.github.io/OnJava8/#/%E5%A4%A7%E5%A4%9A%E6%95%B0%E5%BE%AE%E5%A4%84%E7%90%86%E5%99%A8%E8%8A%AF%E7%89%87%E9%83%BD%E6%9C%89%E9%A2%9D%E5%A4%96%E7%9A%84%E9%AB%98%E9%80%9F%E7%BC%93%E5%86%B2%E5%AD%98%E5%82%A8%E5%99%A8%EF%BC%8C%E4%BD%86%E8%BF%99%E6%98%AF%E6%8C%89%E7%85%A7%E4%BC%A0%E7%BB%9F%E5%AD%98%E5%82%A8%E5%99%A8%E8%80%8C%E4%B8%8D%E6%98%AF%E5%AF%84%E5%AD%98%E5%99%A8%E3%80%82">^2</a>。然而，寄存器的数量十分有限，所以寄存器根据需求进行分配。我们对其没有直接的控制权，也无法在自己的程序里找到寄存器存在的踪迹（另一方面，C&#x2F;C++ 允许开发者向编译器建议寄存器的分配）。</li><li><strong>栈内存</strong>（Stack）存在于常规内存 RAM（随机访问存储器，Random Access Memory）区域中，可通过栈指针获得处理器的直接支持。栈指针下移分配内存，上移释放内存。这是一种仅次于寄存器的非常快速有效的分配存储方式。创建程序时，Java 系统必须知道栈内保存的所有项的生命周期。这种约束限制了程序的灵活性。因此，虽然在栈内存上存在一些 Java 数据（如对象引用），但 Java 对象本身的数据却是保存在堆内存的。</li><li><strong>堆内存</strong>（Heap）这是一种通用的内存池（也在 RAM 区域），所有 Java 对象都存在于其中。与栈内存不同，编译器不需要知道对象必须在堆内存上停留多长时间。因此，用堆内存保存数据更具灵活性。创建一个对象时，只需用 <code>new</code> 命令实例化对象即可，当执行代码时，会自动在堆中进行内存分配。这种灵活性是有代价的：分配和清理堆内存要比栈内存需要更多的时间（如果可以用 Java 在栈内存上创建对象，就像在 C++ 中那样的话）。随着时间的推移，Java 的堆内存分配机制现在已经非常快，因此这不是一个值得关心的问题了。</li><li><strong>常量存储</strong>（Constant storage）常量值通常直接放在程序代码中，因为它们永远不会改变。如需严格保护，可考虑将它们置于只读存储器 ROM （只读存储器，Read Only Memory）中 <a href="https://lingcoder.github.io/OnJava8/#/%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%E6%98%AF%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%B8%B8%E9%87%8F%E6%B1%A0%E3%80%82%E6%89%80%E6%9C%89%E6%96%87%E5%AD%97%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%80%BC%E5%B8%B8%E9%87%8F%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%83%BD%E4%BC%9A%E8%87%AA%E5%8A%A8%E6%94%BE%E5%85%A5%E7%89%B9%E6%AE%8A%E7%9A%84%E9%9D%99%E6%80%81%E5%AD%98%E5%82%A8%E4%B8%AD%E3%80%82">^3</a>。</li><li><strong>非 RAM 存储</strong>（Non-RAM storage）数据完全存在于程序之外，在程序未运行以及脱离程序控制后依然存在。两个主要的例子：（1）序列化对象：对象被转换为字节流，通常被发送到另一台机器；（2）持久化对象：对象被放置在磁盘上，即使程序终止，数据依然存在。这些存储的方式都是将对象转存于另一个介质中，并在需要时恢复成常规的、基于 RAM 的对象。Java 为轻量级持久化提供了支持。而诸如 JDBC 和 Hibernate 这些类库为使用数据库存储和检索对象信息提供了更复杂的支持。</li></ol><h5 id="基本类型的存储"><a href="#基本类型的存储" class="headerlink" title="基本类型的存储"></a><a href="https://lingcoder.github.io/OnJava8/#/book/03-Objects-Everywhere?id=%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%AD%98%E5%82%A8">基本类型的存储</a></h5><p>有一组类型在 Java 中使用频率很高，它们需要特殊对待，这就是 Java 的基本类型。之所以这么说，是因为它们的创建并不是通过 <code>new</code> 关键字来产生。通常 <code>new</code> 出来的对象都是保存在堆内存中的，以此方式创建小而简单的变量往往是不划算的。所以对于这些基本类型的创建方法，Java 使用了和 C&#x2F;C++ 一样的策略。也就是说，不是使用 <code>new</code> 创建变量，而是使用一个“自动”变量。 这个变量直接存储”值”，并置于栈内存中，因此更加高效。</p><p>Java 确定了每种基本类型的内存占用大小。 这些大小不会像其他一些语言那样随着机器环境的变化而变化。这种不变性也是 Java 更具可移植性的一个原因。</p><h5 id="数组的存储"><a href="#数组的存储" class="headerlink" title="数组的存储"></a><a href="https://lingcoder.github.io/OnJava8/#/book/03-Objects-Everywhere?id=%E6%95%B0%E7%BB%84%E7%9A%84%E5%AD%98%E5%82%A8">数组的存储</a></h5><p>许多编程语言都支持数组类型。在 C 和 C++ 中使用数组是危险的，因为那些数组只是内存块。如果程序访问了内存块之外的数组或在初始化之前使用该段内存（常见编程错误），则结果是不可预测的。</p><p>Java 的设计主要目标之一是安全性，因此许多困扰 C 和 C++ 程序员的问题不会在 Java 中再现。在 Java 中，数组使用前需要被初始化，并且不能访问数组长度以外的数据。这种范围检查，是以每个数组上少量的内存开销及运行时检查下标的额外时间为代价的，但由此换来的安全性和效率的提高是值得的。（并且 Java 经常可以优化这些操作）。</p><p>当我们创建对象数组时，实际上是创建了一个引用数组，并且每个引用的初始值都为 <strong>null</strong> 。在使用该数组之前，我们必须为每个引用指定一个对象 。如果我们尝试使用为 <strong>null</strong> 的引用，则会在运行时报错。因此，在 Java 中就防止了数组操作的常规错误。</p><p>我们还可创建基本类型的数组。编译器通过将该数组的内存全部置零来保证初始化。本书稍后将详细介绍数组，特别是在数组章节中。</p><h4 id="对象清理"><a href="#对象清理" class="headerlink" title="对象清理"></a><a href="https://lingcoder.github.io/OnJava8/#/book/03-Objects-Everywhere?id=%E5%AF%B9%E8%B1%A1%E6%B8%85%E7%90%86">对象清理</a></h4><p>我们在 Java 中并没有主动清理这些对象，那么它是如何避免 C++ 中出现的内存被填满从而阻塞程序的问题呢？答案是：Java 的垃圾收集器会检查所有 <code>new</code> 出来的对象并判断哪些不再可达，继而释放那些被占用的内存，供其他新的对象使用。也就是说，我们不必担心内存回收的问题了。你只需简单创建对象即可。当其不再被需要时，能自行被垃圾收集器释放。垃圾回收机制有效防止了因程序员忘记释放内存而造成的“内存泄漏”问题。</p><h4 id="static关键字"><a href="#static关键字" class="headerlink" title="static关键字"></a><a href="https://lingcoder.github.io/OnJava8/#/book/03-Objects-Everywhere?id=static%E5%85%B3%E9%94%AE%E5%AD%97">static关键字</a></h4><p>类是对象的外观及行为方式的描述。通常只有在使用 <code>new</code> 创建那个类的对象后，数据存储空间才被分配，对象的方法才能供外界调用。这种方式在两种情况下是不足的。</p><ol><li>有时你只想为特定字段（注：也称为属性、域）分配一个共享存储空间，而不去考虑究竟要创建多少对象，甚至根本就不创建对象。</li><li>创建一个与此类的任何对象无关的方法。也就是说，即使没有创建对象，也能调用该方法。</li></ol><p><strong>static</strong> 关键字（从 C++ 采用）就符合上述两点要求。当我们说某个事物是静态时，就意味着该字段或方法不依赖于任何特定的对象实例 。 即使我们从未创建过该类的对象，也可以调用其静态方法或访问其静态字段。相反，对于普通的非静态字段和方法，我们必须要先创建一个对象并使用该对象来访问字段或方法，因为非静态字段和方法必须与特定对象关联 <a href="https://lingcoder.github.io/OnJava8/#/%E9%9D%99%E6%80%81%E6%96%B9%E6%B3%95%E5%9C%A8%E4%BD%BF%E7%94%A8%E4%B9%8B%E5%89%8D%E4%B8%8D%E9%9C%80%E8%A6%81%E5%88%9B%E5%BB%BA%E5%AF%B9%E8%B1%A1%EF%BC%8C%E5%9B%A0%E6%AD%A4%E5%AE%83%E4%BB%AC%E4%B8%8D%E8%83%BD%E7%9B%B4%E6%8E%A5%E8%B0%83%E7%94%A8%E9%9D%9E%E9%9D%99%E6%80%81%E7%9A%84%E6%88%90%E5%91%98%E6%88%96%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%A0%E4%B8%BA%E9%9D%9E%E9%9D%99%E6%80%81%E6%88%90%E5%91%98%E5%92%8C%E6%96%B9%E6%B3%95%E5%BF%85%E9%A1%BB%E8%A6%81%E5%85%88%E5%AE%9E%E4%BE%8B%E5%8C%96%E4%B8%BA%E5%AF%B9%E8%B1%A1%E6%89%8D%E5%8F%AF%E4%BB%A5%E8%A2%AB%E4%BD%BF%E7%94%A8%EF%BC%89%E3%80%82">^6</a> 。</p><p><strong>static</strong> 关键字不能应用于局部变量，所以只能作用于属性（字段、域）。</p><h4 id="this关键字"><a href="#this关键字" class="headerlink" title="this关键字"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=this%E5%85%B3%E9%94%AE%E5%AD%97">this关键字</a></h4><p><strong>this</strong> 关键字只能在非静态方法内部使用。当你调用一个对象的方法时，<strong>this</strong> 生成了一个对象引用。你可以像对待其他引用一样对待这个引用。如果你在一个类的方法里调用该类的其他方法，不要使用 <strong>this</strong>，直接调用即可，<strong>this</strong> 自动地应用于其他方法上了。</p><p><strong>this</strong> 关键字只用在一些必须显式使用当前对象引用的特殊场合。</p><h4 id="垃圾回收器"><a href="#垃圾回收器" class="headerlink" title="垃圾回收器"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8">垃圾回收器</a></h4><p>使用完一个对象就不管它并非总是安全的。Java 中有垃圾回收器回收无用对象占用的内存。但现在考虑一种特殊情况：你创建的对象不是通过 <strong>new</strong> 来分配内存的，而垃圾回收器只知道如何释放用 <strong>new</strong> 创建的对象的内存，所以它不知道如何回收不是 <strong>new</strong> 分配的内存。为了处理这种情况，Java 允许在类中定义一个名为 <code>finalize()</code> 的方法。</p><p>在 Java 中，对象并非总是被垃圾回收，或者换句话说：</p><ol><li>对象可能不被垃圾回收。</li><li>垃圾回收不等同于析构。</li></ol><p>也许你会发现，只要程序没有濒临内存用完的那一刻，对象占用的空间就总也得不到释放。如果程序执行结束，而垃圾回收器一直没有释放你创建的任何对象的内存，则当程序退出时，那些资源会全部交还给操作系统。这个策略是恰当的，因为垃圾回收本身也有开销，要是不使用它，那就不用支付这部分开销了。</p><h5 id="finalize-的用途"><a href="#finalize-的用途" class="headerlink" title="finalize() 的用途"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=finalize-%E7%9A%84%E7%94%A8%E9%80%94"><code>finalize()</code> 的用途</a></h5><p>如果你不能将 <code>finalize()</code> 作为通用的清理方法，那么这个方法有什么用呢？</p><p>这引入了要记住的第3点：</p><ol><li>垃圾回收只与内存有关。</li></ol><h5 id="终结条件"><a href="#终结条件" class="headerlink" title="终结条件"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E7%BB%88%E7%BB%93%E6%9D%A1%E4%BB%B6">终结条件</a></h5><p>通常，不能指望 <code>finalize()</code> ，你必须创建其他的”清理”方法，并明确地调用它们。所以看起来，<code>finalize()</code> 只对大部分程序员很难用到的一些晦涩内存清理里有用了。但是，<code>finalize()</code> 还有一个有趣的用法，它不依赖于每次都要对 <code>finalize()</code> 进行调用，这就是对象终结条件的验证。</p><p>当对某个对象不感兴趣时——也就是它将被清理了，这个对象应该处于某种状态，这种状态下它占用的内存可以被安全地释放掉。例如，如果对象代表了一个打开的文件，在对象被垃圾回收之前程序员应该关闭这个文件。只要对象中存在没有被适当清理的部分，程序就存在很隐晦的 bug。<code>finalize()</code> 可以用来最终发现这个情况，尽管它并不总是被调用。如果某次 <code>finalize()</code> 的动作使得 bug 被发现，那么就可以据此找出问题所在——这才是人们真正关心的。以下是个简单的例子，示范了 <code>finalize()</code> 的可能使用方式：</p><pre><code>// housekeeping/TerminationCondition.java// Using finalize() to detect a object that// hasn&#39;t been properly cleaned upimport onjava.*;class Book &#123;    boolean checkedOut = false;    Book(boolean checkOut) &#123;        checkedOut = checkOut;    &#125;    void checkIn() &#123;        checkedOut = false;    &#125;    @Override    protected void finalize() throws Throwable &#123;        if (checkedOut) &#123;            System.out.println(&quot;Error: checked out&quot;);        &#125;        // Normally, you&#39;ll also do this:        // super.finalize(); // Call the base-class version    &#125;&#125;public class TerminationCondition &#123;    public static void main(String[] args) &#123;        Book novel = new Book(true);        // Proper cleanup:        novel.checkIn();        // Drop the reference, forget to clean up:        new Book(true);        // Force garbage collection &amp; finalization:        System.gc();        new Nap(1); // One second delay    &#125;&#125;</code></pre><p>本例的终结条件是：所有的 <strong>Book</strong> 对象在被垃圾回收之前必须被登记。但在 <code>main()</code> 方法中，有一本书没有登记。要是没有 <code>finalize()</code> 方法来验证终结条件，将会很难发现这个 bug。</p><p>你可能注意到使用了 <code>@Override</code>。<code>@</code> 意味着这是一个注解，注解是关于代码的额外信息。在这里，该注解告诉编译器这不是偶然地重定义在每个对象中都存在的 <code>finalize()</code> 方法——程序员知道自己在做什么。编译器确保你没有拼错方法名，而且确保那个方法存在于基类中。注解也是对读者的提醒，<code>@Override</code> 在 Java 5 引入，在 Java 7 中改善，本书通篇会出现。</p><p>注意，<code>System.gc()</code> 用于强制进行终结动作。但是即使不这么做，只要重复地执行程序（假设程序将分配大量的存储空间而导致垃圾回收动作的执行），最终也能找出错误的 <strong>Book</strong> 对象。</p><p>你应该总是假设基类版本的 <code>finalize()</code> 也要做一些重要的事情，使用 <strong>super</strong> 调用它，就像在 <code>Book.finalize()</code> 中看到的那样。本例中，它被注释掉了，因为它需要进行异常处理，而我们到现在还没有涉及到。</p><h5 id="垃圾回收器如何工作"><a href="#垃圾回收器如何工作" class="headerlink" title="垃圾回收器如何工作"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C">垃圾回收器如何工作</a></h5><p>重点</p><h4 id="初始化的顺序"><a href="#初始化的顺序" class="headerlink" title="初始化的顺序"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E9%A1%BA%E5%BA%8F">初始化的顺序</a></h4><p>class初始化会先初始化全部变量值，然后再调用构造器</p><h4 id="静态数据的初始化"><a href="#静态数据的初始化" class="headerlink" title="静态数据的初始化"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E9%9D%99%E6%80%81%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96">静态数据的初始化</a></h4><p>静态数据在class初始化变量之前初始化数据，静态初始化只有在必要时刻才会进行</p><p>无论创建多少个对象，静态数据都只占用一份存储区域。<strong>static</strong> 关键字不能应用于局部变量，所以只能作用于属性（字段、域）。如果一个字段是静态的基本类型，你没有初始化它，那么它就会获得基本类型的标准初值。如果它是对象引用，那么它的默认初值就是 <strong>null</strong>。</p><p>如果在定义时进行初始化，那么静态变量看起来就跟非静态变量一样。</p><h5 id="创建对象的过程"><a href="#创建对象的过程" class="headerlink" title="创建对象的过程"></a>创建对象的过程</h5><p>创建对象的过程，假设有个名为 <strong>Dog</strong> 的类：</p><ol><li>即使没有显式地使用 <strong>static</strong> 关键字，构造器实际上也是静态方法。所以，当首次创建 <strong>Dog</strong> 类型的对象或是首次访问 <strong>Dog</strong> 类的静态方法或属性时，Java 解释器必须在类路径中查找，以定位 <strong>Dog.class</strong>。</li><li>当加载完 <strong>Dog.class</strong> 后（后面会学到，这将创建一个 <strong>Class</strong> 对象），有关静态初始化的所有动作都会执行。因此，静态初始化只会在首次加载 <strong>Class</strong> 对象时初始化一次。</li><li>当用 <code>new Dog()</code> 创建对象时，首先会在堆上为 <strong>Dog</strong> 对象分配足够的存储空间。</li><li>分配的存储空间首先会被清零，即会将 <strong>Dog</strong> 对象中的所有基本类型数据设置为默认值（数字会被置为 0，布尔型和字符型也相同），引用被置为 <strong>null</strong>。</li><li>执行所有出现在字段定义处的初始化动作。</li><li>执行构造器</li></ol><h5 id="显式的静态初始化"><a href="#显式的静态初始化" class="headerlink" title="显式的静态初始化"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E6%98%BE%E5%BC%8F%E7%9A%84%E9%9D%99%E6%80%81%E5%88%9D%E5%A7%8B%E5%8C%96">显式的静态初始化</a></h5><p>你可以将一组静态初始化动作放在类里面一个特殊的”静态子句”（有时叫做静态块）中</p><p>与其他静态初始化动作一样，这段代码仅执行一次：当首次创建这个类的对象或首次访问这个类的静态成员（甚至不需要创建该类的对象）时。</p><h4 id="非静态实例初始化"><a href="#非静态实例初始化" class="headerlink" title="非静态实例初始化"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E9%9D%9E%E9%9D%99%E6%80%81%E5%AE%9E%E4%BE%8B%E5%88%9D%E5%A7%8B%E5%8C%96">非静态实例初始化</a></h4><p>Java 提供了被称为<em>实例初始化</em>的类似语法，用来初始化每个对象的非静态变量，例如：</p><pre><code>class Mug &#123;    Mug(int marker) &#123;        System.out.println(&quot;Mug(&quot; + marker + &quot;)&quot;);    &#125;&#125;public class Mugs &#123;    Mug mug1;    Mug mug2;    &#123; // [1]        mug1 = new Mug(1);        mug2 = new Mug(2);        System.out.println(&quot;mug1 &amp; mug2 initialized&quot;);    &#125;    Mugs() &#123;        System.out.println(&quot;Mugs()&quot;);    &#125;    Mugs(int i) &#123;        System.out.println(&quot;Mugs(int)&quot;);    &#125;     public static void main(String[] args) &#123;        System.out.println(&quot;Inside main()&quot;);        new Mugs();        System.out.println(&quot;new Mugs() completed&quot;);        new Mugs(1);        System.out.println(&quot;new Mugs(1) completed&quot;);    &#125;</code></pre><p>输出：</p><pre><code>Inside mainMug(1)Mug(2)mug1 &amp; mug2 initializedMugs()new Mugs() completedMug(1)Mug(2)mug1 &amp; mug2 initializedMugs(int)new Mugs(1) completed</code></pre><p>看起来它很像静态代码块，只不过少了 <strong>static</strong> 关键字。</p><p>他会随着类的初始化执行，会执行多次，而static只会执行一次，并保存在存储区域中</p><h4 id="枚举类型"><a href="#枚举类型" class="headerlink" title="枚举类型"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E6%9E%9A%E4%B8%BE%E7%B1%BB%E5%9E%8B">枚举类型</a></h4><p>Java 5 中添加了一个看似很小的特性 <strong>enum</strong> 关键字，它使得我们在需要群组并使用枚举类型集时，可以很方便地处理。</p><pre><code class="java">// housekeeping/Spiciness.javapublic enum Spiciness &#123;    NOT, MILD, MEDIUM, HOT, FLAMING&#125;</code></pre><p>这里创建了一个名为 <strong>Spiciness</strong> 的枚举类型，它有5个值。由于枚举类型的实例是常量，因此按照命名惯例，它们都用大写字母表示（如果名称中含有多个单词，使用下划线分隔）。</p><p>要使用 <strong>enum</strong>，需要创建一个该类型的引用，然后将其赋值给某个实例：</p><pre><code class="java">// housekeeping/SimpleEnumUse.javapublic class SimpleEnumUse &#123;    public static void main(String[] args) &#123;        Spiciness howHot = Spiciness.MEDIUM;        System.out.println(howHot);    &#125;&#125;</code></pre><p>在你创建 <strong>enum</strong> 时，编译器会自动添加一些有用的特性。例如，它会创建 <code>toString()</code> 方法，以便你方便地显示某个 <strong>enum</strong> 实例的名称，这从上面例子中的输出可以看出。编译器还会创建 <code>ordinal()</code> 方法表示某个特定 <strong>enum</strong> 常量的声明顺序，<code>static values()</code> 方法按照 enum 常量的声明顺序，生成这些常量值构成的数组：</p><pre><code class="java">public class EnumOrder &#123;    public static void main(String[] args) &#123;        for (Spiciness s: Spiciness.values()) &#123;            System.out.println(s + &quot;, ordinal &quot; + s.ordinal());        &#125;    &#125;&#125;</code></pre><p>输出：</p><pre><code>NOT, ordinal 0MILD, ordinal 1MEDIUM, ordinal 2HOT, ordinal 3FLAMING, ordinal 4</code></pre><p><strong>enum</strong> 有一个很实用的特性，就是在 <strong>switch</strong> 语句中使用：</p><pre><code class="java">// housekeeping/Burrito.javapublic class Burrito &#123;    Spiciness degree;    public Burrito(Spiciness degree) &#123;        this.degree = degree;    &#125;    public void describe() &#123;        System.out.print(&quot;This burrito is &quot;);        switch(degree) &#123;            case NOT:                System.out.println(&quot;not spicy at all.&quot;);                break;            case MILD:            case MEDIUM:                System.out.println(&quot;a little hot.&quot;);                break;            case HOT:            case FLAMING:            default:                System.out.println(&quot;maybe too hot&quot;);        &#125;    &#125;    public static void main(String[] args) &#123;        Burrito plain = new Burrito(Spiciness.NOT),        greenChile = new Burrito(Spiciness.MEDIUM),        jalapeno = new Burrito(Spiciness.HOT);        plain.describe();        greenChile.describe();        jalapeno.describe();    &#125;&#125;复制ErrorOK!</code></pre><p>输出：</p><pre><code>This burrito is not spicy at all.This burrito is a little hot.This burrito is maybe too hot.</code></pre><p>将数据和方法包装进类中并把具体实现隐藏被称作是<em>封装</em>（encapsulation）</p><p>访问控制通常被称为<em>隐藏实现</em>（implementation hiding）。</p><h1 id="第六章-初始化和清理"><a href="#第六章-初始化和清理" class="headerlink" title="第六章 初始化和清理"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E7%AC%AC%E5%85%AD%E7%AB%A0-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E6%B8%85%E7%90%86">第六章 初始化和清理</a></h1><p>C++ 引入了构造器的概念，这是一个特殊的方法，每创建一个对象，这个方法就会被自动调用。Java 采用了构造器的概念，另外还使用了垃圾收集器（Garbage Collector, GC）去自动回收不再被使用的对象所占的资源。这一章将讨论初始化和清理的问题，以及在 Java 中对它们的支持。</p><h2 id="利用构造器保证初始化"><a href="#利用构造器保证初始化" class="headerlink" title="利用构造器保证初始化"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E5%88%A9%E7%94%A8%E6%9E%84%E9%80%A0%E5%99%A8%E4%BF%9D%E8%AF%81%E5%88%9D%E5%A7%8B%E5%8C%96">利用构造器保证初始化</a></h2><p>注意点：java中初始化如果类中有多个构造器只会调用一个构造器，然后调用父类构造器，中间不会去调用自己的无参构造器。而scala多个构造器会一级一级的调用自己的无参构造器，然后再调用父类的构造器</p><h2 id="方法重载"><a href="#方法重载" class="headerlink" title="方法重载"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E6%96%B9%E6%B3%95%E9%87%8D%E8%BD%BD">方法重载</a></h2><p>重载就是相同方法名称不同参数值，调用时根据不同的参数来进行区分</p><h3 id="返回值的重载"><a href="#返回值的重载" class="headerlink" title="返回值的重载"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E8%BF%94%E5%9B%9E%E5%80%BC%E7%9A%84%E9%87%8D%E8%BD%BD">返回值的重载</a></h3><p>java8支持返回值的重载，但是并不建议使用。</p><h2 id="无参构造器"><a href="#无参构造器" class="headerlink" title="无参构造器"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E6%97%A0%E5%8F%82%E6%9E%84%E9%80%A0%E5%99%A8">无参构造器</a></h2><p>如果你创建一个类，类中没有构造器，那么编译器就会自动为你创建一个无参构造器</p><p>但是,一旦你显式地定义了构造器（无论有参还是无参），编译器就不会自动为你创建无参构造器。</p><pre><code class="java">// housekeeping/DefaultConstructor.javaclass Bird &#123;&#125;public class DefaultConstructor &#123;    public static void main(String[] args) &#123;        Bird bird = new Bird(); // 默认的    &#125;&#125;</code></pre><h2 id="this关键字-1"><a href="#this关键字-1" class="headerlink" title="this关键字"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=this%E5%85%B3%E9%94%AE%E5%AD%97">this关键字</a></h2><p>对于两个相同类型的对象 <strong>a</strong> 和 <strong>b</strong>，你可能在想如何调用这两个对象的 <code>peel()</code> 方法：</p><pre><code class="java">// housekeeping/BananaPeel.javaclass Banana &#123;    void peel(int i) &#123;        /*...*/    &#125;&#125;public class BananaPeel &#123;    public static void main(String[] args) &#123;        Banana a = new Banana(), b = new Banana();        a.peel(1);        b.peel(2);    &#125;&#125;</code></pre><p>如果只有一个方法 <code>peel()</code> ，那么怎么知道调用的是对象 <strong>a</strong> 的 <code>peel()</code>方法还是对象 <strong>b</strong> 的 <code>peel()</code> 方法呢？</p><p><strong>this</strong> 关键字只能在非静态方法内部使用。当你调用一个对象的方法时，<strong>this</strong> 生成了一个对象引用。你可以像对待其他引用一样对待这个引用。如果你在一个类的方法里调用该类的其他方法，不要使用 <strong>this</strong>，直接调用即可，<strong>this</strong> 自动地应用于其他方法上了。因此你可以像这样：</p><pre><code class="java">// housekeeping/Apricot.javapublic class Apricot &#123;    void pick() &#123;        /* ... */    &#125;    void pit() &#123;        pick();        /* ... */    &#125;&#125;</code></pre><p>在 <code>pit()</code> 方法中，你可以使用 <code>this.pick()</code>，但是没有必要。编译器自动为你做了这些。<strong>this</strong> 关键字只用在一些必须显式使用当前对象引用的特殊场合。例如，用在 <strong>return</strong> 语句中返回对当前对象的引用。</p><pre><code class="java">// housekeeping/Leaf.java// Simple use of the &quot;this&quot; keywordpublic class Leaf &#123;    int i = 0;    Leaf increment() &#123;        i++;        return this;    &#125;    void print() &#123;        System.out.println(&quot;i = &quot; + i);    &#125;    public static void main(String[] args) &#123;        Leaf x = new Leaf();        x.increment().increment().increment().print();    &#125;&#125;</code></pre><h3 id="在构造器中调用构造器"><a href="#在构造器中调用构造器" class="headerlink" title="在构造器中调用构造器"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E5%9C%A8%E6%9E%84%E9%80%A0%E5%99%A8%E4%B8%AD%E8%B0%83%E7%94%A8%E6%9E%84%E9%80%A0%E5%99%A8">在构造器中调用构造器</a></h3><p>当你在一个类中写了多个构造器，有时你想在一个构造器中调用另一个构造器来避免代码重复。你通过 <strong>this</strong> 关键字实现这样的调用。</p><p>通常当你说 <strong>this</strong>，意味着”这个对象”或”当前对象”，它本身生成对当前对象的引用。在一个构造器中，当你给 <strong>this</strong> 一个参数列表时，它是另一层意思。它通过最直接的方式显式地调用匹配参数列表的构造器：</p><pre><code class="java">// housekeeping/Flower.java// Calling constructors with &quot;this&quot;public class Flower &#123;    int petalCount = 0;    String s = &quot;initial value&quot;;    Flower(int petals) &#123;        petalCount = petals;        System.out.println(&quot;Constructor w/ int arg only, petalCount = &quot; + petalCount);    &#125;    Flower(String ss) &#123;        System.out.println(&quot;Constructor w/ string arg only, s = &quot; + ss);        s = ss;    &#125;    Flower(String s, int petals) &#123;        this(petals);        //- this(s); // Can&#39;t call two!        this.s = s; // Another use of &quot;this&quot;        System.out.println(&quot;String &amp; int args&quot;);    &#125;    Flower() &#123;        this(&quot;hi&quot;, 47);        System.out.println(&quot;no-arg constructor&quot;);    &#125;    void printPetalCount() &#123;        //- this(11); // Not inside constructor!        System.out.println(&quot;petalCount = &quot; + petalCount + &quot; s = &quot; + s);    &#125;    public static void main(String[] args) &#123;        Flower x = new Flower();        x.printPetalCount();    &#125;&#125;</code></pre><h3 id="static-的含义"><a href="#static-的含义" class="headerlink" title="static 的含义"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=static-%E7%9A%84%E5%90%AB%E4%B9%89">static 的含义</a></h3><p><strong>static</strong> 方法中不会存在 <strong>this</strong>。你不能在静态方法中调用非静态方法（反之可以）。静态方法是为类而创建的，不需要任何对象。</p><p>事实上，这就是静态方法的主要目的，静态方法看起来就像全局方法一样，但是 Java 中不允许全局方法，一个类中的静态方法可以访问其他静态方法和静态属性。一些人认为静态方法不是面向对象的，因为它们的确具有全局方法的语义。使用静态方法，因为不存在 <strong>this</strong>，所以你没有向一个对象发送消息。的确，如果你发现代码中出现了大量的 <strong>static</strong> 方法，就该重新考虑自己的设计了</p><h2 id="垃圾回收器-1"><a href="#垃圾回收器-1" class="headerlink" title="垃圾回收器"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8">垃圾回收器</a></h2><p>使用完一个对象就不管它并非总是安全的。Java 中有垃圾回收器回收无用对象占用的内存。但现在考虑一种特殊情况：你创建的对象不是通过 <strong>new</strong> 来分配内存的，而垃圾回收器只知道如何释放用 <strong>new</strong> 创建的对象的内存，所以它不知道如何回收不是 <strong>new</strong> 分配的内存。为了处理这种情况，Java 允许在类中定义一个名为 <code>finalize()</code> 的方法。</p><p>它的工作原理”假定”是这样的：当垃圾回收器准备回收对象的内存时，首先会调用其 <code>finalize()</code> 方法，并在下一轮的垃圾回收动作发生时，才会真正回收对象占用的内存。所以如果你打算使用 <code>finalize()</code> ，就能在垃圾回收时做一些重要的清理工作。</p><p>所以有必要明确区分一下：在 C++ 中，对象总是被销毁的（在一个 bug-free 的程序中），而在 Java 中，对象并非总是被垃圾回收，或者换句话说：</p><ol><li>对象可能不被垃圾回收。</li><li>垃圾回收不等同于析构。</li></ol><p>这意味着在你不再需要某个对象之前，如果必须执行某些动作，你得自己去做。Java 没有析构器或类似的概念，所以你必须得自己创建一个普通的方法完成这项清理工作。例如，对象在创建的过程中会将自己绘制到屏幕上。如果不是明确地从屏幕上将其擦除，它可能永远得不到清理。如果在 <code>finalize()</code> 方法中加入某种擦除功能，那么当垃圾回收发生时，<code>finalize()</code> 方法被调用（不保证一定会发生），图像就会被擦除，要是”垃圾回收”没有发生，图像则仍会保留下来。</p><p>也许你会发现，只要程序没有濒临内存用完的那一刻，对象占用的空间就总也得不到释放。如果程序执行结束，而垃圾回收器一直没有释放你创建的任何对象的内存，则当程序退出时，那些资源会全部交还给操作系统。这个策略是恰当的，因为垃圾回收本身也有开销，要是不使用它，那就不用支付这部分开销了。</p><h3 id="终结条件-1"><a href="#终结条件-1" class="headerlink" title="终结条件"></a><a href="https://lingcoder.github.io/OnJava8/#/book/06-Housekeeping?id=%E7%BB%88%E7%BB%93%E6%9D%A1%E4%BB%B6">终结条件</a></h3><p>通常，不能指望 <code>finalize()</code> ，你必须创建其他的”清理”方法，并明确地调用它们。所以看起来，<code>finalize()</code> 只对大部分程序员很难用到的一些晦涩内存清理里有用了。但是，<code>finalize()</code> 还有一个有趣的用法，它不依赖于每次都要对 <code>finalize()</code> 进行调用，这就是对象终结条件的验证。）。</p><h2 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h2><p>那什么是日志？日志就是Logging，它的目的是为了取代<code>System.out.println()</code>。</p><p>JDK的Logging定义了7个日志级别，从严重到普通</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NTP时间服务器配置</title>
      <link href="/linux/ntp-shi-jian-fu-wu-qi-pei-zhi/"/>
      <url>/linux/ntp-shi-jian-fu-wu-qi-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h1 id="NTP时间服务器配置"><a href="#NTP时间服务器配置" class="headerlink" title="NTP时间服务器配置"></a>NTP时间服务器配置</h1><p>最近服务器中时间和现实时间不对应，需要修改服务器时间，这时候需要安装时间同步的服务器。</p><p>ntp配置有两种:</p><p>第一种是自己定制一台服务器为主节点，其他节点进行同步.</p><p>第二种是连接别人已经配好的时间服务器进行时间同步。这里NTP配置使用的是第二种，连接的是<code>ntp.neu.edu.cn iburst</code> 东北大学的时间服务器。</p><h2 id="一、-东北大学NTP服务"><a href="#一、-东北大学NTP服务" class="headerlink" title="一、 东北大学NTP服务"></a>一、 东北大学NTP服务</h2><p>1、查看centos是否已经安装过ntp服务器</p><pre><code>rpm -qa|grep ntp</code></pre><p>如果安装过，可以先进行卸载在重新安装。</p><p>2、安装ntp服务</p><pre><code>yum -y install ntp ntpdate</code></pre><p>3、开放防火墙，打开端口</p><p>这次没有涉及到访问失败等错误，暂时没有涉及这一过程</p><p>这里再尝试访问 ntp.neu.edu.cn iburst 东北大学服务器，看是否能返回时间，确认可用性</p><p><img src="/linux/ntp-shi-jian-fu-wu-qi-pei-zhi/image-20210118132401581-1611060385064.png" alt="尝试连接访问"></p><p>4、ntp配置文件配置</p><pre><code>vim /etc/ntp.conf</code></pre><p>配置文件内容：只用添加一行内容</p><pre><code>// 注释配置文件中原有的NTP服务器，并在其中添加自己所需的NTP服务器地址# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).# server 0.centos.pool.ntp.org iburst# server 1.centos.pool.ntp.org iburst# server 2.centos.pool.ntp.org iburst# server 3.centos.pool.ntp.org iburstserver ntp.neu.edu.cn iburst</code></pre><p>5、NTP配置为自启动并启动</p><pre><code>systemctl enable ntpd.service//安装为服务systemctl start ntpd  //启动运行</code></pre><p>6、查看客户端运行状态</p><pre><code>systemctl status ntpd</code></pre><p>7、查看同步状态信息</p><pre><code>ntpstat[root@localhost jars]# ntpstatsynchronised to NTP server (202.118.1.46) at stratum 2   time correct to within 35 ms   polling server every 64 s</code></pre><h2 id="二、自定义主节点配置NTP"><a href="#二、自定义主节点配置NTP" class="headerlink" title="二、自定义主节点配置NTP"></a>二、自定义主节点配置NTP</h2><p>需要多台节点才能够使用此方式,如果只有一台节点,你配置为主节点,他会以自身时间为基准和手动修改时间没有区别</p><p>三台服务器:10.130.210.245  10.130.210.246  10.130.210.247</p><p>这里选择了245为主节点</p><p>1、查看centos是否已经安装过ntp服务器</p><pre><code>rpm -qa|grep ntp</code></pre><p>如果安装过，可以先进行卸载在重新安装。</p><p>2、安装ntp服务</p><pre><code>yum -y install ntp ntpdate</code></pre><p>3、开放防火墙，打开端口</p><p>这次没有涉及到访问失败等错误，暂时没有涉及这一过程</p><p>4、ntp配置文件配置</p><p>这里主节点从节点配置是不一样的</p><pre><code>vim /etc/ntp.conf</code></pre><p>主节点配置文件内容：</p><pre><code>restrict 10.130.210.245 nomodify notrap nopeer noquery //当前节点IP地址# 授权172.22.10.0网段上所有机器可以从这台机器上查询和时间同步restrict 10.130.210.0 mask 255.255.255.0 nomodify notrap //集群所在网段、掩码、权限# 新增本地ntp服务器172.22.10.6，注释掉原有的server 0-n// 注释配置文件中原有的NTP服务器# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).# server 0.centos.pool.ntp.org iburst# server 1.centos.pool.ntp.org iburst# server 2.centos.pool.ntp.org iburst# server 3.centos.pool.ntp.org iburstserver 127.127.1.0#使用本地时间fudge 127.127.1.0 stratum 10#ntp stratum层#这两行代码设置时间服务器是自己</code></pre><pre><code>/etc/ntp.conf# 参数详解：#ignore  ：关闭所有的 NTP 联机服务#nomodify：客户端不能更改服务端的时间参数，但是客户端可以通过服务端进行网络校时。#notrust ：客户端除非通过认证，否则该客户端来源将被视为不信任子网#noquery ：不提供客户端的时间查询：用户端不能使用ntpq，ntpc等命令来查询ntp服务器#notrap ：不提供trap远端登陆：拒绝为匹配的主机提供模式 6 控制消息陷阱服务。          陷阱服务是 ntpdq 控制消息协议的子系统，用于远程事件日志记录程序。#nopeer ：用于阻止主机尝试与服务器对等，并允许欺诈性服务器控制时钟#kod ： 访问违规时发送 KoD 包。#restrict -6 表示IPV6地址的权限设置。          #stratum说明:# 正常情况下stratum值范围“0~15”# Stratum-2则从Stratum-1获取时间，Stratum-3从Stratum-2获取时间，以此类推，# 但Stratum层的总数限制在15以内。所有这些服务器在逻辑上形成阶梯式的架构相互连接，# 而Stratum-1的时间服务器是整个系统的基础</code></pre><p>客户端配置:</p><pre><code class="conf">/etc/ntp.conf// 注释配置文件中原有的NTP服务器，并在其中添加自己刚刚定义的NTP服务器地址# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).# server 0.centos.pool.ntp.org iburst# server 1.centos.pool.ntp.org iburst# server 2.centos.pool.ntp.org iburst# server 3.centos.pool.ntp.org iburstserver 10.130.210.245  fudge 10.130.210.245 stratum 8</code></pre><p>5、NTP配置为自启动并启动</p><pre><code>systemctl enable ntpd.service//安装为服务systemctl start ntpd  //启动运行</code></pre><p>6、查看客户端运行状态</p><pre><code>systemctl status ntpd</code></pre><p>7、查看同步状态信息</p><pre><code>ntpstat[root@localhost jars]# ntpstatsynchronised to NTP server (202.118.1.46) at stratum 2   time correct to within 35 ms   polling server every 64 s</code></pre><h2 id="三、配置NTP（集群时间的定时同步）"><a href="#三、配置NTP（集群时间的定时同步）" class="headerlink" title="三、配置NTP（集群时间的定时同步）"></a>三、配置NTP（集群时间的定时同步）</h2><pre><code>rpm -qa |grep ntp//查看是否安装，查不出来要先安装</code></pre><p>1.vim &#x2F;etc&#x2F;ntp.conf （修改配置文件）</p><pre><code class="xml">restrict default ignore&lt;!--解开注释，修改网段改成自己的--&gt;restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap&lt;!--注释掉---&gt;不使用北京时间了#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst&lt;!--去掉以下注释--&gt;没有就添加上&lt;!--失去网络后，使用本地的时间配置--&gt;server 127.127.1.0fudge 127.127.1.0 stratum 10&lt;!--这里的参数说明在第二个类型配置中有说明--&gt;</code></pre><p>更改主节点的net.conf</p><p><img src="/linux/ntp-shi-jian-fu-wu-qi-pei-zhi/1567673183759-1611060370035.png" alt="第一张图"></p><p><img src="/linux/ntp-shi-jian-fu-wu-qi-pei-zhi/1567673197384-1611060399818.png" alt="第二张图"></p><p>2、vim &#x2F;etc&#x2F;sysconfig&#x2F;ntpd</p><pre><code class="sh">&lt;!--增加内容--&gt;SYNC_HWCLOCK=yes</code></pre><p>3、service ntpd status</p><p>查看运行状态，进行重启ntp服务</p><p>4、service ntpd start</p><p>5、设置开机启动</p><pre><code>chkconfig ntpd on</code></pre><p>6、在其他节点上进行设置定时任务crontab -e</p><p>&#x2F;&#x2F;代表了分 时 日  月  周 命令  主机名 </p><pre><code>10 * * * * /usr/sbin/ntpdate bw77</code></pre><p>每隔10分钟执行一次</p><p>7、定时任务完成<img src="/linux/ntp-shi-jian-fu-wu-qi-pei-zhi/1567673332501-1611060410677.png" alt="1567673332501">后可以crontab -l 来查看定时任务</p><pre><code>crontab -l查看任务</code></pre><h2 id="Ntp常用命令"><a href="#Ntp常用命令" class="headerlink" title="Ntp常用命令"></a>Ntp常用命令</h2><pre><code>#查询已安装的ntp版本信息等rpm -qa | grep ntp      #启动,停止, 重启, 查询ntp服务状态service ntpd status     检查ntp是否启动 service ntpd start      启动NTP(开一台就好)  service ntpd stop       关闭ntpservice ntpd restart    重新启动ntpwhich ntpdate       ntpdate这个系统命令在哪里 which renoot  renoot这个命令在哪里 which(查看可执行文件的位置)chkconfig ntpd on       设置ntpd为开机启动 #查看ntp服务器与上层ntp的状态ntpq -p  # 命令查看时间同步状态ntpstat  #下面返回的数据则表示成功连接synchronised to NTP server (202.118.1.46) at stratum 2   time correct to within 35 ms   polling server every 64 s</code></pre>]]></content>
      
      
      <categories>
          
          <category> NTP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NTP配置 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rabbit的安装</title>
      <link href="/rabbit/rabbit/"/>
      <url>/rabbit/rabbit/</url>
      
        <content type="html"><![CDATA[<p>Window下Rabbit的安装</p><ol><li>安装Erlang环境，这是Rabbit必须的。</li></ol><p><a href="https://www.erlang.org/downloads">https://www.erlang.org/downloads</a></p><p>下载完毕直接按照默认配置安装就可以了</p><ol start="2"><li>下载rabbit</li></ol><p><a href="https://www.rabbitmq.com/download.html">https://www.rabbitmq.com/download.html</a></p><p>下载完毕选择安装路径其他默认配置就可以</p><ol start="3"><li>安装插件</li></ol><pre><code>rabbitmq-plugins enable rabbitmq_management</code></pre><p><img src="/rabbit/rabbit/image-20210204095728484.png" alt="image-20210204095728484"></p><ol start="4"><li>启动rabbitMQ，查看监控页面</li></ol><p><img src="/rabbit/rabbit/image-20210204095809400.png" alt="image-20210204095809400"></p><ol start="5"><li>输入账号密码</li></ol><p>guest guest</p><p>Rabbit端口</p><table><thead><tr><th>端口</th><th>说明</th></tr></thead><tbody><tr><td>15672</td><td>rabbit的web端口</td></tr><tr><td>5672</td><td>rabbit的通信端口</td></tr><tr><td></td><td></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Rabbit </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rabbit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据部分知识总结</title>
      <link href="/da-shu-ju/da-shu-ju-zhi-shi-zong-jie/"/>
      <url>/da-shu-ju/da-shu-ju-zhi-shi-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><p>HttpFs</p><p>NFS Gateway工具可以将HDFS上面的空间映射到linux本地磁盘上，然后再进行操作</p><p>只要在一个hdfs客户端上装上，启动NFS Gateway服务，并mount上这个NFS文件夹，其他主机即可访问（编辑文件有限制，读写没问题）。</p><p>在尝试启动 NFS Gateway 角色实例之前，请先启动该主机上的portmap或者rpcbind服务。</p><pre><code>yum install rpcbind -y//安装rpcbind</code></pre><h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><p>hive Metastore 服务器端口  9083</p><h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><h2 id="zk仲裁机制"><a href="#zk仲裁机制" class="headerlink" title="zk仲裁机制"></a>zk仲裁机制</h2><p>zk服务器运行方式有两种</p><ul><li>独立模式(standlone)</li><li>仲裁模式</li></ul><p>zk的状态无法进行覆盖，生产环境中会有一定危险</p><p>仲裁模式集群中，具备高可用的覆写功能。如果zk信息全部同步完成后，在进行下一条数据的同步，延时会比较突出</p><p>为了规避这个问题，zk使用了法定人数的思想。在zk信息同步时，保证若干个指定的节点同步完成，就继续下一次操作，而不是等全部的节点都同步完成。</p><p>法定人数至少是3，为了正常工作，集群中至少有3台服务器是正常运行的。</p><h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><h2 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h2><h2 id="RDD五大特性"><a href="#RDD五大特性" class="headerlink" title="RDD五大特性"></a>RDD五大特性</h2><p>A list of partitions<br>a function for computing each split<br>a list of dependencies on other RDDs<br>Optionally,a partitioner for key-value RDDs<br>Optionally,a list of preferred locations to compute each splip</p><h3 id="SparkStreaming背压机制"><a href="#SparkStreaming背压机制" class="headerlink" title="SparkStreaming背压机制"></a>SparkStreaming背压机制</h3><p>当Spark消费数据时，batch processing time&gt;batch interval时，也就是批次数据的处理时间比批次数据产生的时间长的时候，越来越多的数据被接收，但是数据的处理速度没有跟上，导致数据开始积压，可能进一步导致OOM异常</p><p>Spark1.5之前，使用Receiver-based数据接收器，可以通过配置<code>spark。streaming.receiver.maxRate</code>参数来限制每个receiver每秒最大可以接受的记录的数据。对于 Direct Approach 的数据接收，我们可以通过配置<code>spark.streaming.kafka.maxRatePerPartition</code> 参数来限制每次作业中每个 Kafka 分区最多读取的记录条数。</p><p>这种方法虽然可以通过限制接收速率，来适配当前的处理能力，但这种方式存在以下几个问题：</p><ul><li>我们需要事先估计好集群的处理速度以及消息数据的产生速度；</li><li>这两种方式需要人工参与，修改完相关参数之后，我们需要手动重启 Spark Streaming 应用程序；</li><li>如果当前集群的处理能力高于我们配置的 maxRate，而且 producer 产生的数据高于 maxRate，这会导致集群资源利用率低下，而且也会导致数据不能够及时处理。</li></ul><p><strong>反压机制</strong></p><p>Spark 1.5 引入了反压（Back Pressure）机制，其通过动态收集系统的一些数据来自动地适配集群数据处理能力。</p><h2 id="Spark-shuffle优化设置"><a href="#Spark-shuffle优化设置" class="headerlink" title="Spark shuffle优化设置"></a>Spark shuffle优化设置</h2><pre><code>set hive.exec.dynamic.partition=true;  set hive.exec.dynamic.partition.mode=nonstrict;set spark.speculation=true;set spark.sql.shuffle.partitions=1000;set spark.sql.adaptive.enabled=true;set spark.sql.adaptive.shuffle.targetPostShuffleInputSize=128000000;set spark.sql.adaptiveBroadcastJoinThreshold=10485760; set spark.sql.adaptive.allowAdditionalShuffle=true;set spark.sql.adaptive.join.enabled=true;set spark.sql.adaptive.skewedJoin.enabled=true;set spark.sql.adaptive.minNumPostShufflePartitions=1;set spark.sql.adaptive.maxNumPostShufflePartitions=1000;set spark.sql.planner.skewJoin=true;set spark.sql.planner.skewJoin.threshold=100000;set spark.sql.cbo.enabled=true;set spark.sql.cbo.joinReorder.card.weight=0.8;set spark.executor.extraJavaOptions=-XX:ParallelGCThreads=4 -XX:+UseParallelGC;set spark.sql.autoBroadcastJoinThreshold=10485760;set spark.sql.broadcastTimeout=600s;set spark.sql.bigdata.useExecutorBroadcast=true;set spark.hadoopRDD.ignoreEmptySplits=true;</code></pre><h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><h2 id="kafka数据同步-x2F-镜像工具-kafka-mirror-maker"><a href="#kafka数据同步-x2F-镜像工具-kafka-mirror-maker" class="headerlink" title="kafka数据同步&#x2F;镜像工具 kafka mirror maker"></a>kafka数据同步&#x2F;镜像工具 kafka mirror maker</h2><p>MirrorMaker为kafka的镜像工具，如果没有这个需求就可以不启动这个服务。</p><p>可以使用kafkamaker来进行kafka之间的数据迁移&#x2F;发送</p><h2 id="Kafka为什么快"><a href="#Kafka为什么快" class="headerlink" title="Kafka为什么快"></a>Kafka为什么快</h2><h3 id="磁盘读写"><a href="#磁盘读写" class="headerlink" title="磁盘读写"></a>磁盘读写</h3><p><a href="https://mp.weixin.qq.com/s/b5d1zNec-RcFwgEtNxNRUg">原文地址</a></p><p><img src="/da-shu-ju/da-shu-ju-zhi-shi-zong-jie/ddd70f0e1e858cc507a23112e6d05010.png" alt="磁盘原理"></p><p>磁盘读写过程</p><ol><li>先进行寻址（找到对应的柱面，磁头要瞄准相应的磁道）</li><li>旋转（等待扇区从磁头地下旋转经过）</li><li>数据传输（从内存和磁盘中进行实际传输）</li></ol><p>kafka读取数据是采用的是顺序读写的方式，省去了磁盘寻址的时间，比随机读写要快约1000倍</p><h3 id="页缓存pagecache-buffcache"><a href="#页缓存pagecache-buffcache" class="headerlink" title="页缓存pagecache+buffcache"></a>页缓存pagecache+buffcache</h3><p><code>pagecache</code>（以page为单位，进行缓存文件内容）</p><p>缓存在pagecache中的文件数据能够更快的被用户读取</p><p>同时，带有buffer的写入操作，数据写入到page cache中就立即返回，不需要等待持久化到磁盘中，提高了上层应用读写文件的整体性能。cached这列的数值表示的是当前的页缓存（page cache）的占用量，page cache文件的页数据，页是逻辑上的概念，因此page cache是与文件系统同级的</p><p><strong>好处是</strong>：避免了brock的内存开销，避免了GC问题，应用程序重启数据不会丢失。操作系统层面的缓存利用率会更高，服务重启不会消失，避免了缓存重建的过程</p><p><code>buffer cache</code>：磁盘等设备缓冲</p><p>buffers列 表示当前的块缓存（buffer cache）占用量，buffer cache用于缓存块设备（如磁盘）的块数据。块是物理上的概念，因此buffer cache是与块设备驱动程序同级的。</p><h3 id="mmap（内存文件映射）"><a href="#mmap（内存文件映射）" class="headerlink" title="mmap（内存文件映射）"></a>mmap（内存文件映射）</h3><p>把物理磁盘文件和page cache进行映射，可以向读写硬盘一样读写内存</p><h3 id="零拷贝（zero-copy）"><a href="#零拷贝（zero-copy）" class="headerlink" title="零拷贝（zero-copy）"></a>零拷贝（zero-copy）</h3><p> 操作系统非零拷贝过程：</p><ol><li>从磁盘copy到page cache</li><li>从page cache copy到用户缓存区中</li><li>从用户缓存区中copy到socket缓存中</li><li>socket缓存中copy到网卡接口中</li></ol><p>零拷贝的过程：</p><ol><li>磁盘文件copy到page cache中</li><li>从page cachecopy到网卡接口中</li></ol><h3 id="存储设计"><a href="#存储设计" class="headerlink" title="存储设计"></a>存储设计</h3><p>topic分为了多个partition</p><p>partition存储时分成了多个sgement</p><p>sgement中存储了.index和.log文件</p><p>sgement只允许追加的形式</p><p>offset支持连续的预读和批量写</p><h3 id="批量发送"><a href="#批量发送" class="headerlink" title="批量发送"></a>批量发送</h3><p>为了减少网络io的开销，kafka支持batch.size和linger.ms</p><p><code>batch.szie</code>:消息条数达到个数就立刻发送</p><p><code>linger.ms</code>:消息不够，但是超过一定时间就发送</p><h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><p>节省网络io</p><p>如果每个消息都压缩，但是压缩率相对很低，所以Kafka使用了批量压缩，即将多个消息一起压缩而不是单个消息压缩</p><p>Kafka允许使用递归的消息集合，批量的消息可以通过压缩的形式传输并且在日志中也可以保持压缩格式，直到被消费者解压缩</p><p>Kafka支持多种压缩协议，包括Gzip和Snappy压缩协议</p><h3 id="消息写入过程"><a href="#消息写入过程" class="headerlink" title="消息写入过程"></a>消息写入过程</h3><p><img src="/da-shu-ju/da-shu-ju-zhi-shi-zong-jie/640" alt="img"></p><p>producer需要从用户空间到网卡(zero-copy)</p><p>生产者发送批量压缩的数据到broker，broker通过MappedByteBuffer的map()函数映射其地址到你的虚拟内存地址。</p><p>接着就可以对这个MappedByteBuffer执行写入操作了，写入的时候他会直接进入PageCache中，然后过一段时间之后，由os的线程异步刷入磁盘中，可以看上面的示意图。</p><p>上图中似乎只有一次数据拷贝的过程，他就是从PageCache里拷贝到磁盘文件里而已！这个就是你使用mmap技术之后，相比于传统磁盘IO的一个性能优化</p><h3 id="消息读的过程"><a href="#消息读的过程" class="headerlink" title="消息读的过程"></a>消息读的过程</h3><p>读取数据会先判断消息是否在page cache存在，存在就可以直接从page cache消费数据，所以消费实时数据会很快。</p><p>但是消费历史数据，就得将之前的历史数据和邻近的数据块，加载到page cache中。</p><p>这里加载邻近的数据就是一个预读的过程，是一个优化的过程</p>]]></content>
      
      
      <categories>
          
          <category> 复习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 复习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>缓慢变化维解决的几种方式</title>
      <link href="/shu-ju-cang-ku/huan-man-bian-hua-wei/"/>
      <url>/shu-ju-cang-ku/huan-man-bian-hua-wei/</url>
      
        <content type="html"><![CDATA[<h2 id="缓慢变化纬"><a href="#缓慢变化纬" class="headerlink" title="缓慢变化纬"></a>缓慢变化纬</h2><p>Slowly Changing Dimensions are dimensions that have data that slowly changes. 意思就是说数据会发生缓慢变化的维度就叫”缓慢变化维”。</p><p>处理缓慢变化维度是Kimball数仓体系中永恒的话题，因为数据仓库的本质，以及维度表在维度建模中的基础作用，我们几乎总是要跟踪维度的变更（change tracking），以保留历史，并提供准确的查询和分析结果。在《The Data Warehouse Toolkit, 3rd Edition》一书的第5章，Kimball提出了多种缓慢变化维度的类型和处理方法，其中前五种是原生的，后面的方法都是混合方法（hybrid techniques），因此下面来看看前五种，即Type 0~Type 4。</p><h4 id="Tpye0-保留原始值"><a href="#Tpye0-保留原始值" class="headerlink" title="Tpye0 保留原始值"></a>Tpye0 保留原始值</h4><p>某一个属性值绝不会变化。事实表始终按照该原始值进行分组。比如在用户维度表中，用户注册时使用的原始用户名（original_user_name）。如果它发生变化，那么变化后的值是无效的，会被抛弃，始终按照用户第一次填写的数据为准。很明显这种方式是不推荐的。</p><p>说白话就是只要第一次的数据，不要变化后的数据</p><h4 id="Type1-覆盖更新"><a href="#Type1-覆盖更新" class="headerlink" title="Type1 覆盖更新"></a>Type1 覆盖更新</h4><p>与业务数据保持一致，同样为直接update。这样就难以记录历史变化，例如如果周杰伦于15年7月调入北京，那么我们想要知道北京销售员在15年的销售数据时，就会将周杰伦的业绩算入北京分公司下，实际上周杰伦7月份以前的销售数据均应算在台北，所以为了避免这样的问题就有了TYPE2的处理方式。</p><p>这样的方式就是Mysql中一般表的处理方式，如果变中的值发生了变化就直接update，不保留历史数据，这样显然是不符合我们需求的。</p><h4 id="Type2-增加新的列"><a href="#Type2-增加新的列" class="headerlink" title="Type2 增加新的列"></a>Type2 增加新的列</h4><p>数据仓库系统的目标之一是正确地表示历史记录。我们在生产环境中的基于Hive的数仓建设过程中，拉链表就是直接的体现。</p><p>这种类型在维度表中<strong>添加两个辅助列</strong>：该行的有效日期（effective date）和过期日期（expiration date），分别指示该行从哪个时间点开始生效，以及在哪个时间点过后会变为无效。<strong>每当一个或多个维度发生更改时，就创建一个新的行</strong>，新行包含有修改后的维度值，而旧行包含有修改前的维度值，且旧行的过期日期也会同步修改。书中的例子如下：</p><p><img src="/shu-ju-cang-ku/huan-man-bian-hua-wei/640" alt="图片"></p><p>在上图中，当前有效列（current列）的过期日期会被记录为9999-12-31。当Department Name维度变化时，旧有的Product Key为12345的行的过期日期被更新为修改日期，并且新建了一个Key为25984的行，包含新的数据。</p><p>需要注意的是，这里的Product Key是所谓代理键（surrogate key），即不表示具体业务含义，而只是代表表内数据行的唯一ID。在处理SCD时，代理键可以直接用来区分同一自然键（natural key）的数据的新旧版本。上图中的SKU就是自然键。</p><p>这种类型的SCD处理方式能够非常有效且精确地保留历史与反映变更，但缺点是会造成数据的膨胀，因为即使只有一个维度变化，也要创建新行。</p><h4 id="Type3-新增属性列"><a href="#Type3-新增属性列" class="headerlink" title="Type3 新增属性列"></a>Type3 新增属性列</h4><p>用不同的字段来保存不同的值，就是在表中增加一个字段，这个字段用来保存变化后的当前值，而原来的值则被称为变化前的值。我们举个很简单的例子，例如我们在用户表中的用户住址这一列会变化，那么我们可以通过新增一个列来表示曾经的地址：</p><p>这么做虽然解决上面的数据膨胀的问题，但是如果很多个列都会变化，那么我们要新增很多系列，显然这是不合理的。另外，这种做法只能保留上一次的数据，那么更久远的变化就丢失了。</p><p>这种方式是我第一次了解拉链表时，最快想到的方式，但是这样并不理想，如果某一列持续update 那么会一直新增column，如果column没有了那么就会覆盖上次的值，有这上面两种类型的缺点</p><h4 id="Type4-新增维度表"><a href="#Type4-新增维度表" class="headerlink" title="Type4 新增维度表"></a>Type4 新增维度表</h4><p>如果我们的表规模非常大，数据量千万以上，大量的列变化非常频繁，那么这时候就不能用上面的办法来支撑了，我们需要将那些快速变化的维度从原来的大维度表中拆分出来单独处理，是为微维度（mini-dimension）。</p><p>我们用书中的内容举例，如果顾客维度中有一部分人口统计学（demographic）维度是RCD，就将它们拆成单独的维度表：</p><p><img src="/shu-ju-cang-ku/huan-man-bian-hua-wei/640" alt="图片"></p><p>最后给出一张《The Data Warehouse Toolkit, 3rd Edition》中的这几种方式的比较图：</p><p><img src="/shu-ju-cang-ku/huan-man-bian-hua-wei/640" alt="比较图"></p><h3 id="最常用的三种"><a href="#最常用的三种" class="headerlink" title="最常用的三种"></a>最常用的三种</h3><p>1.直接覆盖。缺点：历史变化无法分析。优点：节省空间。</p><p>2.增加新的字段，标识它的修改日期。缺点：不管修改了几个字段，都会新增一行，造成数据的膨胀。优点：完全反应历史变化和标识时间。</p><p>3.增加属性值，新增相同的一个字段同时<strong>储存新旧值</strong>。缺点：只能反应一次变化，如果数据二次变化，那么原始数据就会被覆盖。优点：节省空间</p>]]></content>
      
      
      <categories>
          
          <category> 数据仓库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
            <tag> 缓慢变化维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch的学习使用</title>
      <link href="/cdh/elasticsearch/elasticsearch/"/>
      <url>/cdh/elasticsearch/elasticsearch/</url>
      
        <content type="html"><![CDATA[<h1 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h1><p>当阅读本书时，将会遇到有关 Elasticsearch 分布式特性的补充章节。这些章节将介绍有关集群扩容、故障转移(<a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/distributed-cluster.html"><em>集群内的原理</em></a>) 、应对文档存储(<a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/distributed-docs.html"><em>分布式文档存储</em></a>) 、执行分布式搜索(<a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/distributed-search.html"><em>执行分布式检索</em></a>) ，以及分区（shard）及其工作原理(<a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/inside-a-shard.html"><em>分片内部原理</em></a>) 。</p><p>这些章节并非必读，完全可以无需了解内部机制就使用 Elasticsearch，但是它们将从另一个角度帮助你了解更完整的 Elasticsearch 知识。可以根据需要跳过它们，或者想更完整地理解时再回头阅读也无妨。</p><p>一般存储文档库和索引库。</p><h2 id="Elasticsearch是什么"><a href="#Elasticsearch是什么" class="headerlink" title="Elasticsearch是什么"></a>Elasticsearch是什么</h2><p>Elasticsearch 是一个开源的搜索引擎，建立在一个全文搜索引擎库 <a href="https://lucene.apache.org/core/">Apache Lucene™</a> 基础之上。 Lucene 可以说是当下最先进、高性能、全功能的搜索引擎库—无论是开源还是私有。</p><p>但是 Lucene 仅仅只是一个库。为了充分发挥其功能，你需要使用 Java 并将 Lucene 直接集成到应用程序中。 更糟糕的是，您可能需要获得信息检索学位才能了解其工作原理。Lucene <em>非常</em> 复杂。</p><p>Elasticsearch 不仅仅是 Lucene，并且也不仅仅只是一个全文搜索引擎。 它可以被下面这样准确的形容：</p><ul><li>一个分布式的实时文档存储，<em>每个字段</em> 可以被索引与搜索</li><li>一个分布式实时分析搜索引擎</li><li>能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据</li></ul><p>Elasticsearch 天生就是分布式的，并且在设计时屏蔽了分布式的复杂性。</p><p>Elasticsearch 在分布式方面几乎是透明的。教程中并不要求了解分布式系统、分片、集群发现或其他的各种分布式概念。</p><p>Elasticsearch 尽可能地屏蔽了分布式系统的复杂性。这里列举了一些在后台自动执行的操作：</p><ul><li>分配文档到不同的容器 或 <em>分片</em> 中，文档可以储存在一个或多个节点中</li><li>按集群节点来均衡分配这些分片，从而对索引和搜索过程进行负载均衡</li><li>复制每个分片以支持数据冗余，从而防止硬件故障导致的数据丢失</li><li>将集群中任一节点的请求路由到存有相关数据的节点</li><li>集群扩容时无缝整合新节点，重新分配分片以便从离群节点恢复</li></ul><h2 id="Elasticsearch-安装"><a href="#Elasticsearch-安装" class="headerlink" title="Elasticsearch 安装"></a>Elasticsearch 安装</h2><p>1、在安装好jdk8.0以上版本的情况下，上传elasticSearch的jar包，进行解压，<strong>解压到普通目录下，除了root目录</strong>。</p><p>2、修改配置文件config下面的elasticsearch.yml配置文件</p><pre><code class="yml"># ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:#集群名称cluster.name: my-application## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:#指定当前节点名称node.name: node-1## Add custom attributes to the node:##node.attr.rack: r1# ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):#当前节点地址network.host: 192.168.247.20## Set a custom port for HTTP:# 指定http访问端口(默认9200)http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]#配置为集群中所有节点的ip地址discovery.zen.ping.unicast.hosts: [&quot;Linux01&quot;, &quot;Linux02&quot;,&quot;Linux03&quot;]## Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):#配置集群节点数量(nodes/2+1)discovery.zen.minimum_master_nodes: 2## For more information, consult the zen discovery module documentation.</code></pre><p>3、创建新用户</p><pre><code>//因为es不能用root来开启，得新建立一个用户来开启esuseradd elkpasswd  elk</code></pre><p>4、开放权限</p><pre><code>chmod -R 777 elasticsearch-6.2.4</code></pre><p>5、所有机器切换用户elk到bin目录开启elasticSearch</p><pre><code>[root@Linux01 elasticsearch-6.2.4]# su elk[elk@Linux01 elasticsearch-6.2.4]#  bin/elasticsearch</code></pre><p><strong>注意：搭建集群如出现问题，请先删除data文件夹中内容，再启动集群</strong></p><p>6、查看结果</p><p>查看节点状态</p><pre><code class="http">http:192.168.247.20:9200</code></pre><p>查到以下内容则正常</p><pre><code>&#123;  &quot;name&quot;: &quot;node-1&quot;,  &quot;cluster_name&quot;: &quot;my-application&quot;,  &quot;cluster_uuid&quot;: &quot;P6bOcKBgQ_61HKKgh5lNgA&quot;,  &quot;version&quot;: &#123;    &quot;number&quot;: &quot;6.2.4&quot;,    &quot;build_hash&quot;: &quot;ccec39f&quot;,    &quot;build_date&quot;: &quot;2018-04-12T20:37:28.497551Z&quot;,    &quot;build_snapshot&quot;: false,    &quot;lucene_version&quot;: &quot;7.2.1&quot;,    &quot;minimum_wire_compatibility_version&quot;: &quot;5.6.0&quot;,    &quot;minimum_index_compatibility_version&quot;: &quot;5.0.0&quot;  &#125;,  &quot;tagline&quot;: &quot;You Know, for Search&quot;&#125;</code></pre><p>查看集群的状态</p><pre><code class="http">http://192.168.247.20:9200/_cluster/health</code></pre><p>查到以下内容则正常</p><pre><code>&#123;    &quot;cluster_name&quot;: &quot;my-application&quot;,    &quot;status&quot;: &quot;green&quot;,    &quot;timed_out&quot;: false,    &quot;number_of_nodes&quot;: 3,    &quot;number_of_data_nodes&quot;: 3,    &quot;active_primary_shards&quot;: 10,    &quot;active_shards&quot;: 20,    &quot;relocating_shards&quot;: 0,    &quot;initializing_shards&quot;: 0,    &quot;unassigned_shards&quot;: 0,    &quot;delayed_unassigned_shards&quot;: 0,    &quot;number_of_pending_tasks&quot;: 0,    &quot;number_of_in_flight_fetch&quot;: 0,    &quot;task_max_waiting_in_queue_millis&quot;: 0,    &quot;active_shards_percent_as_number&quot;: 100.0&#125;//green：正常//  yellow: 集群正常 数据正常，部分副本不正常//  red: 集群部分正常，数据可能丢失，需要紧急修复</code></pre><h3 id="安装可能出现的问题"><a href="#安装可能出现的问题" class="headerlink" title="安装可能出现的问题"></a>安装可能出现的问题</h3><pre><code class="http">http://www.dajiangtai.com/community/18136.do?origin=csdn-geek&amp;dt=1214</code></pre><h2 id="Elasticsearch-请求语句的组成"><a href="#Elasticsearch-请求语句的组成" class="headerlink" title="Elasticsearch 请求语句的组成"></a>Elasticsearch 请求语句的组成</h2><p>一个 Elasticsearch 请求和任何 HTTP 请求一样由若干相同的部件组成：</p><pre><code>curl -X&lt;VERB&gt; &#39;&lt;PROTOCOL&gt;://&lt;HOST&gt;:&lt;PORT&gt;/&lt;PATH&gt;?&lt;QUERY_STRING&gt;&#39; -d &#39;&lt;BODY&gt;&#39;curl -XGET &#39;http://localhost:9200/_count?pretty&#39; -d &#39;&#123;    &quot;query&quot;: &#123;        &quot;match_all&quot;: &#123;&#125;    &#125;&#125;&#39;Elasticsearch 返回一个 HTTP 状态码（例如：200 OK）和（除`HEAD`请求）一个 JSON 格式的返回值。前面的 curl 请求将返回一个像下面一样的 JSON 体：&#123;    &quot;count&quot; : 0,    &quot;_shards&quot; : &#123;        &quot;total&quot; : 5,        &quot;successful&quot; : 5,        &quot;failed&quot; : 0    &#125;&#125;在返回结果中没有看到 HTTP 头信息是因为我们没有要求`curl`显示它们。想要看到头信息，需要结合 -i 参数来使用 curl 命令：curl -i -XGET &#39;localhost:9200/&#39;缩写格式:省略请求中所有相同的部分，例如主机名、端口号以及 curl 命令本身。而不是像下面显示的那样用一个完整的请求：curl -XGET &#39;localhost:9200/_count?pretty&#39; -d &#39;&#123;    &quot;query&quot;: &#123;        &quot;match_all&quot;: &#123;&#125;    &#125;&#125;&#39;变成了GET /_count&#123;    &quot;query&quot;: &#123;        &quot;match_all&quot;: &#123;&#125;    &#125;&#125;</code></pre><h2 id="Elasticsearch-请求语句的参数"><a href="#Elasticsearch-请求语句的参数" class="headerlink" title="Elasticsearch 请求语句的参数"></a>Elasticsearch 请求语句的参数</h2><p>被 <code>&lt; &gt;</code> 标记的部件：</p><table><thead><tr><th><code>PROTOCOL</code></th><th><code>http</code> 或者 <code>https</code>（如果你在 Elasticsearch 前面有一个 <code>https</code> 代理）</th></tr></thead><tbody><tr><td><code>VERB</code></td><td>适当的 HTTP <em>方法</em> 或 <em>谓词</em> : <code>GET</code>、 <code>POST</code>、 <code>PUT</code>、 <code>HEAD</code> 或者 <code>DELETE</code>。</td></tr><tr><td><code>HOST</code></td><td>Elasticsearch 集群中任意节点的主机名，或者用 <code>localhost</code> 代表本地机器上的节点。</td></tr><tr><td><code>PORT</code></td><td>运行 Elasticsearch HTTP 服务的端口号，默认是 <code>9200</code> 。</td></tr><tr><td><code>PATH</code></td><td>API 的终端路径（例如 <code>_count</code> 将返回集群中文档数量）。Path 可能包含多个组件，例如：<code>_cluster/stats</code> 和 <code>_nodes/stats/jvm</code> 。</td></tr><tr><td><code>QUERY_STRING</code></td><td>任意可选的查询字符串参数 (例如 <code>?pretty</code> 将格式化地输出 JSON 返回值，使其更容易阅读)</td></tr><tr><td><code>BODY</code></td><td>一个 JSON 格式的请求体 (如果请求需要的话)</td></tr></tbody></table><h2 id="ES-基本概念"><a href="#ES-基本概念" class="headerlink" title="ES 基本概念"></a>ES 基本概念</h2><p><strong>1）节点（Node）</strong></p><p>运行了<strong>单个实例的ES主机称为节点</strong>，它是集群的一个成员，可以存储数据、参与集群索引及搜索操作。节点通过为其配置的ES集群名称确定其所要加入的集群。</p><p><strong>2）集群（cluster）</strong></p><p>ES可以作为一个独立的单个搜索服务器。不过，一般为了处理大型数据集，实现容错和高可用性，ES可以运行在许多互相合作的服务器上。这些服务器的集合称为集群。</p><p><strong>3）分片（Shard）</strong></p><p>ES的“分片(shard)”机制可将一个索引内部的数据分布地存储于多个节点，它通过<strong>将一个索引切分为多个</strong>底层物理的Lucene索引完成<strong>索引数据的分割存储</strong>功能，这每一个物理的Lucene索引称为一个分片(shard)。</p><p>这样的好处是可以<strong>把一个大的索引拆分成多个，分布到不同的节点上</strong>。降低单服务器的压力，构成分布式搜索，<strong>提高整体检索的效率（分片数的最优值与硬件参数和数据量大小有关）。</strong>分片的数量<strong>只能在索引创建前指定，并且索引创建后不能更改。</strong></p><p><strong>4）副本（Replica）</strong></p><p>副本是一个分片的<strong>精确复制</strong>，每个分片可以有零个或多个副本。副本的作用一是<strong>提高系统的容错性</strong>，当某个节点某个分片损坏或丢失时可以从副本中恢复。二是<strong>提高es的查询效率</strong>，es会自动对搜索请求进行负载均衡。</p><p>Elasticsearch中的每个索引默认分配5个主分片和1个副本</p><h2 id="面向文档"><a href="#面向文档" class="headerlink" title="面向文档"></a>面向文档</h2><p>在应用程序中对象很少只是一个简单的键和值的列表。通常，它们拥有更复杂的数据结构，可能包括日期、地理信息、其他对象或者数组等。</p><p>也许有一天你想把这些对象存储在数据库中。使用关系型数据库的行和列存储，这相当于是把一个表现力丰富的对象塞到一个非常大的电子表格中：为了适应表结构，你必须设法将这个对象扁平化—通常一个字段对应一列—而且每次查询时又需要将其重新构造为对象。</p><p>Elasticsearch 是 <em>面向文档</em> 的，意味着它存储整个对象或 <em>文档</em>。<strong>Elasticsearch 不仅存储文档，而且 <em>索引</em> 每个文档的内容，使之可以被检索。在 Elasticsearch 中，我们对文档进行索引、检索、排序和过滤—而不是对行列数据</strong>。这是一种完全不同的思考数据的方式，也是 Elasticsearch 能支持复杂全文检索的原因。</p><h2 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h2><p>Elasticsearch 使用 JavaScript Object Notation（或者 <a href="http://en.wikipedia.org/wiki/Json"><em>JSON</em></a>）作为文档的序列化格式。JSON 序列化为大多数编程语言所支持，并且已经成为 NoSQL 领域的标准格式</p><p>官方 <a href="https://www.elastic.co/guide/en/elasticsearch/client/index.html">Elasticsearch 客户端</a> 自动为您提供 JSON 转化。</p><p>一个 Elasticsearch 集群可以 包含多个 <em>索引</em> ，相应的每个索引可以包含多个 <em>类型</em> 。 这些不同的类型存储着多个 <em>文档</em> ，每个文档又有 多个 <em>属性</em> 。</p><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><pre><code>核心数据类型字符串型：text、keyword数值型：long、integer、short、byte、double、float、half_float、scaled_float日期类型：date布尔类型：boolean二进制类型：binary范围类型：integer_range、float_range、long_range、double_range、date_range复杂数据类型数组类型：array对象类型：object嵌套类型：nested object地理位置数据类型geo_point(点)、geo_shape(形状)专用类型记录IP地址ip实现自动补全completion记录分词数：token_count记录字符串hash值 murmur3多字段特性multi-fields允许对同一个字段采用不同的配置，比如分词，例如对人名实现拼音搜索，只需要在人名中新增一个子字段为pinyin即可</code></pre><h3 id="dynamic-严格性设置"><a href="#dynamic-严格性设置" class="headerlink" title="dynamic  严格性设置"></a>dynamic  严格性设置</h3><p>a.true：允许自动新增字段（默认的配置）<br>b.False：不允许自动新增字段，但是文档可以正常写入，无法对字段进行查询操作(可写不能查询)<br>c.strict：文档不能写入（如果写入会报错）</p><h3 id="数据类型text和keyword的区别"><a href="#数据类型text和keyword的区别" class="headerlink" title="数据类型text和keyword的区别"></a>数据类型text和keyword的区别</h3><p>text:text类型的字段值 保存入的数据会被进行分词处理<br>keyword:保存入的数据不会被分词处理</p><h2 id="倒排索引"><a href="#倒排索引" class="headerlink" title="倒排索引"></a>倒排索引</h2><p>Elasticsearch 使用一种称为 <em>倒排索引</em> 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。内存空间不够冷热交替，热点key放内存，冷的放磁盘，为了避免这个问题，设定好es的内存大小。</p><p>例如，假设我们有两个文档，每个文档的 <code>content</code> 域包含如下内容：</p><ol><li>The quick brown fox jumped over the lazy dog</li><li>Quick brown foxes leap over lazy dogs in summer</li></ol><p>为了创建倒排索引，我们首先将每个文档的 <code>content</code> 域拆分成单独的 词（我们称它为 <code>词条</code> 或 <code>tokens</code> ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示：</p><pre><code>Term      Doc_1  Doc_2-------------------------Quick   |       |  XThe     |   X   |brown   |   X   |  Xdog     |   X   |dogs    |       |  Xfox     |   X   |foxes   |       |  Xin      |       |  Xjumped  |   X   |lazy    |   X   |  Xleap    |       |  Xover    |   X   |  Xquick   |   X   |summer  |       |  Xthe     |   X   |------------------------</code></pre><p>如果我们想搜索 <code>quick brown</code> ，我们只需要查找包含每个词条的文档：</p><pre><code>Term      Doc_1  Doc_2-------------------------brown   |   X   |  Xquick   |   X   |------------------------Total   |   2   |  1</code></pre><h2 id="分析-的过程："><a href="#分析-的过程：" class="headerlink" title="分析 的过程："></a><em>分析</em> 的过程：</h2><ul><li>首先，将一块文本分成适合于倒排索引的独立的 <em>词条</em> ，</li><li>之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 <em>recall</em></li></ul><p>分析器执行上面的工作。 <em>分析器</em> 实际上是将三个功能封装到了一个包里：</p><ul><li><p><strong>字符过滤器</strong></p><p>首先，字符串按顺序通过每个 <em>字符过滤器</em> 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 <code>&amp;</code> 转化成 <code>and</code>。</p></li><li><p><strong>分词器</strong></p><p>其次，字符串被 <em>分词器</em> 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。</p></li><li><p><strong>Token 过滤器</strong></p><p>最后，词条按顺序通过每个 <em>token 过滤器</em> 。这个过程可能会改变词条（例如，小写化 <code>Quick</code> ），删除词条（例如， 像 <code>a</code>， <code>and</code>， <code>the</code> 等无用词），或者增加词条（例如，像 <code>jump</code> 和 <code>leap</code> 这种同义词）。</p></li></ul><p>Elasticsearch提供了开箱即用的字符过滤器、分词器和token 过滤器。 这些可以组合起来形成自定义的分析器以用于不同的目的。我们会在 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/custom-analyzers.html">自定义分析器</a> 章节详细讨论。</p><h2 id="分词器"><a href="#分词器" class="headerlink" title="分词器"></a>分词器</h2><p>Elasticsearch还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条：</p><pre><code>&quot;Set the shape to semi-transparent by calling set_trans(5)&quot;</code></pre><p>Standard（es默认）支持多语言，按词切分并做小写处理<br>Simple按照非字母切分，小写处理<br>Whitespace按照空格来切分<br>Stop去除语气助词，如the、an、的、这等<br>Keyword不分词<br>Pattern正则分词，默认\w+,即非字词符号做分割符<br>Language常见语言的分词器（30+）</p><ul><li><p><strong>标准分析器</strong></p><p>标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 <a href="http://www.unicode.org/reports/tr29/">Unicode 联盟</a> 定义的 <em>单词边界</em> 划分文本。删除绝大部分标点。最后，将词条小写。它会产生<code>set, the, shape, to, semi, transparent, by, calling, set_trans, 5</code></p></li><li><p><strong>简单分析器</strong></p><p>简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生<code>set, the, shape, to, semi, transparent, by, calling, set, trans</code></p></li><li><p><strong>空格分析器</strong></p><p>空格分析器在空格的地方划分文本。它会产生<code>Set, the, shape, to, semi-transparent, by, calling, set_trans(5)</code></p></li><li><p><strong>语言分析器</strong></p><p>特定语言分析器可用于 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-lang-analyzer.html">很多语言</a>。它们可以考虑指定语言的特点。例如， <code>英语</code> 分析器附带了一组英语无用词（常用单词，例如 <code>and</code> 或者 <code>the</code> ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 <em>词干</em> 。<code>英语</code> 分词器会产生下面的词条：<code>set, shape, semi, transpar, call, set_tran, 5</code>注意看 <code>transparent</code>、 <code>calling</code> 和 <code>set_trans</code> 已经变为词根格式。</p></li></ul><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><h3 id="添加内容"><a href="#添加内容" class="headerlink" title="添加内容"></a>添加内容</h3><pre><code class="sense">PUT /megacorp/employee/1&#123;    &quot;first_name&quot; : &quot;John&quot;,    &quot;last_name&quot; :  &quot;Smith&quot;,    &quot;age&quot; :        25,    &quot;about&quot; :      &quot;I love to go rock climbing&quot;,    &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125;PUT /megacorp/employee/2&#123;    &quot;first_name&quot; :  &quot;Jane&quot;,    &quot;last_name&quot; :   &quot;Smith&quot;,    &quot;age&quot; :         32,    &quot;about&quot; :       &quot;I like to collect rock albums&quot;,    &quot;interests&quot;:  [ &quot;music&quot; ]&#125;PUT /megacorp/employee/3&#123;    &quot;first_name&quot; :  &quot;Douglas&quot;,    &quot;last_name&quot; :   &quot;Fir&quot;,    &quot;age&quot; :         35,    &quot;about&quot;:        &quot;I like to build cabinets&quot;,    &quot;interests&quot;:  [ &quot;forestry&quot; ]&#125;</code></pre><p>注意，路径 <code>/megacorp/employee/1</code> 包含了三部分的信息：</p><ul><li><p><strong><code>megacorp</code></strong></p><p>索引名称</p></li><li><p><strong><code>employee</code></strong></p><p>类型名称</p></li><li><p><strong><code>1</code></strong></p><p>特定雇员的ID</p></li></ul><p>请求体 —— JSON 文档 —— 包含了这位员工的所有详细信息，他的名字叫 John Smith ，今年 25 岁，喜欢攀岩。</p><h3 id="获取单条数据"><a href="#获取单条数据" class="headerlink" title="获取单条数据"></a>获取单条数据</h3><p>es查询方式都可以使用postman来进行访问</p><pre><code class="sense">GET /megacorp/employee/1获取的值：&#123;  &quot;_index&quot; :   &quot;megacorp&quot;,  &quot;_type&quot; :    &quot;employee&quot;,  &quot;_id&quot; :      &quot;1&quot;,  &quot;_version&quot; : 1,  &quot;found&quot; :    true,  &quot;_source&quot; :  &#123;      &quot;first_name&quot; :  &quot;John&quot;,      &quot;last_name&quot; :   &quot;Smith&quot;,      &quot;age&quot; :         25,      &quot;about&quot; :       &quot;I love to go rock climbing&quot;,      &quot;interests&quot;:  [ &quot;sports&quot;, &quot;music&quot; ]  &#125;&#125;</code></pre><h3 id="查询所有数据"><a href="#查询所有数据" class="headerlink" title="查询所有数据"></a>查询所有数据</h3><pre><code class="sense">GET /megacorp/employee/_search</code></pre><p>可以看到，我们仍然使用索引库 megacorp 以及类型 employee，但与指定一个文档 ID 不同，这次使用 _search 。返回结果包括了所有三个文档，放在数组 hits 中。一个搜索默认返回十条结果。</p><pre><code>&#123;   &quot;took&quot;:      6,   &quot;timed_out&quot;: false,   &quot;_shards&quot;: &#123; ... &#125;,   &quot;hits&quot;: &#123;      &quot;total&quot;:      3,      &quot;max_score&quot;:  1,      &quot;hits&quot;: [         &#123;            &quot;_index&quot;:         &quot;megacorp&quot;,            &quot;_type&quot;:          &quot;employee&quot;,            &quot;_id&quot;:            &quot;3&quot;,            &quot;_score&quot;:         1,            &quot;_source&quot;: &#123;               &quot;first_name&quot;:  &quot;Douglas&quot;,               &quot;last_name&quot;:   &quot;Fir&quot;,               &quot;age&quot;:         35,               &quot;about&quot;:       &quot;I like to build cabinets&quot;,               &quot;interests&quot;: [ &quot;forestry&quot; ]            &#125;         &#125;,         &#123;            &quot;_index&quot;:         &quot;megacorp&quot;,            &quot;_type&quot;:          &quot;employee&quot;,            &quot;_id&quot;:            &quot;1&quot;,            &quot;_score&quot;:         1,            &quot;_source&quot;: &#123;               &quot;first_name&quot;:  &quot;John&quot;,               &quot;last_name&quot;:   &quot;Smith&quot;,               &quot;age&quot;:         25,               &quot;about&quot;:       &quot;I love to go rock climbing&quot;,               &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]            &#125;         &#125;,         &#123;            &quot;_index&quot;:         &quot;megacorp&quot;,            &quot;_type&quot;:          &quot;employee&quot;,            &quot;_id&quot;:            &quot;2&quot;,            &quot;_score&quot;:         1,            &quot;_source&quot;: &#123;               &quot;first_name&quot;:  &quot;Jane&quot;,               &quot;last_name&quot;:   &quot;Smith&quot;,               &quot;age&quot;:         32,               &quot;about&quot;:       &quot;I like to collect rock albums&quot;,               &quot;interests&quot;: [ &quot;music&quot; ]            &#125;         &#125;      ]   &#125;&#125;</code></pre><h3 id="轻量搜索-条件查询"><a href="#轻量搜索-条件查询" class="headerlink" title="轻量搜索-条件查询"></a>轻量搜索-条件查询</h3><p>这个方法一般涉及到一个 <em>查询字符串</em> （<em>query-string</em>） 搜索，因为我们通过一个URL参数来传递查询信息给搜索接口：</p><p>尝试下搜索姓氏为 <code>Smith</code> 的雇员：</p><p>就是查询后面加上?q&#x3D;条件</p><pre><code>GET /megacorp/employee/_search?q=last_name:Smith</code></pre><pre><code>&#123;   ...   &quot;hits&quot;: &#123;      &quot;total&quot;:      2,      &quot;max_score&quot;:  0.30685282,      &quot;hits&quot;: [         &#123;            ...            &quot;_source&quot;: &#123;               &quot;first_name&quot;:  &quot;John&quot;,               &quot;last_name&quot;:   &quot;Smith&quot;,               &quot;age&quot;:         25,               &quot;about&quot;:       &quot;I love to go rock climbing&quot;,               &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]            &#125;         &#125;,         &#123;            ...            &quot;_source&quot;: &#123;               &quot;first_name&quot;:  &quot;Jane&quot;,               &quot;last_name&quot;:   &quot;Smith&quot;,               &quot;age&quot;:         32,               &quot;about&quot;:       &quot;I like to collect rock albums&quot;,               &quot;interests&quot;: [ &quot;music&quot; ]            &#125;         &#125;      ]   &#125;&#125;</code></pre><h3 id="查询表达式-条件查询"><a href="#查询表达式-条件查询" class="headerlink" title="查询表达式-条件查询"></a>查询表达式-条件查询</h3><p>Query-string 搜索通过命令非常方便地进行临时性的即席搜索 ，但它有自身的局限性（参见 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/search-lite.html"><em>轻量</em> 搜索</a> ）。Elasticsearch 提供一个丰富灵活的查询语言叫做 <em>查询表达式</em> ， 它支持构建更加复杂和健壮的查询。</p><p><em>领域特定语言</em> （DSL）， 使用 JSON 构造了一个请求。我们可以像这样重写之前的查询所有名为 Smith 的搜索 ：</p><pre><code>GET /megacorp/employee/_search&#123;    &quot;query&quot; : &#123;        &quot;match&quot; : &#123;            &quot;last_name&quot; : &quot;Smith&quot;        &#125;    &#125;&#125;</code></pre><p>返回结果与之前的查询一样，但还是可以看到有一些变化。其中之一是，不再使用 <em>query-string</em> 参数，而是一个请求体替代。这个请求使用 JSON 构造，并使用了一个 <code>match</code> 查询（属于查询类型之一，后面将继续介绍）。</p><h3 id="查询表达式-复杂查询"><a href="#查询表达式-复杂查询" class="headerlink" title="查询表达式-复杂查询"></a>查询表达式-复杂查询</h3><p>同样搜索姓氏为 Smith 的员工，但这次我们只需要年龄大于 30 的。查询需要稍作调整，使用过滤器 <em>filter</em> ，它支持高效地执行一个结构化查询。</p><pre><code>GET /megacorp/employee/_search&#123;    &quot;query&quot; : &#123;        &quot;bool&quot;: &#123;            &quot;must&quot;: &#123;                &quot;match&quot; : &#123;                    &quot;last_name&quot; : &quot;smith&quot;                 &#125;            &#125;,            &quot;filter&quot;: &#123;                &quot;range&quot; : &#123;                    &quot;age&quot; : &#123; &quot;gt&quot; : 30 &#125;                 &#125;            &#125;        &#125;    &#125;&#125;</code></pre><p>这里使用了bool只要对应的，并且锁定age  的 range 是 gt 大于 30的 </p><pre><code>结果：&#123;   ...   &quot;hits&quot;: &#123;      &quot;total&quot;:      1,      &quot;max_score&quot;:  0.30685282,      &quot;hits&quot;: [         &#123;            ...            &quot;_source&quot;: &#123;               &quot;first_name&quot;:  &quot;Jane&quot;,               &quot;last_name&quot;:   &quot;Smith&quot;,               &quot;age&quot;:         32,               &quot;about&quot;:       &quot;I like to collect rock albums&quot;,               &quot;interests&quot;: [ &quot;music&quot; ]            &#125;         &#125;      ]   &#125;&#125;</code></pre><h3 id="查询表达式-全文搜索"><a href="#查询表达式-全文搜索" class="headerlink" title="查询表达式-全文搜索"></a>查询表达式-全文搜索</h3><p>搜索下所有喜欢攀岩（rock climbing）的员工：</p><pre><code>GET /megacorp/employee/_search&#123;    &quot;query&quot; : &#123;        &quot;match&quot; : &#123;            &quot;about&quot; : &quot;rock climbing&quot;        &#125;    &#125;&#125;</code></pre><p>显然我们依旧使用之前的 <code>match</code> 查询在<code>about</code> 属性上搜索 “rock climbing” 。得到两个匹配的文档：</p><pre><code>&#123;   ...   &quot;hits&quot;: &#123;      &quot;total&quot;:      2,      &quot;max_score&quot;:  0.16273327,      &quot;hits&quot;: [         &#123;            ...            &quot;_score&quot;:         0.16273327,             &quot;_source&quot;: &#123;               &quot;first_name&quot;:  &quot;John&quot;,               &quot;last_name&quot;:   &quot;Smith&quot;,               &quot;age&quot;:         25,               &quot;about&quot;:       &quot;I love to go rock climbing&quot;,               &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]            &#125;         &#125;,         &#123;            ...            &quot;_score&quot;:         0.016878016,             &quot;_source&quot;: &#123;               &quot;first_name&quot;:  &quot;Jane&quot;,               &quot;last_name&quot;:   &quot;Smith&quot;,               &quot;age&quot;:         32,               &quot;about&quot;:       &quot;I like to collect rock albums&quot;,               &quot;interests&quot;: [ &quot;music&quot; ]            &#125;         &#125;      ]   &#125;&#125;</code></pre><p>找出一个属性中的独立单词是没有问题的，但有时候想要精确匹配一系列单词或者_短语_ 。 比如， 我们想执行这样一个查询，仅匹配同时包含 “rock” <em>和</em> “climbing” ，<em>并且</em> 二者以短语 “rock climbing” 的形式紧挨着的雇员记录。</p><h3 id="查询表达式-短语搜索"><a href="#查询表达式-短语搜索" class="headerlink" title="查询表达式-短语搜索"></a>查询表达式-短语搜索</h3><p>短语就是指把查询的多个词看成一个整体来查询</p><p>找出一个属性中的独立单词是没有问题的，但有时候想要精确匹配一系列单词或者_短语_ 。 比如， 我们想执行这样一个查询，仅匹配同时包含 “rock” <em>和</em> “climbing” ，<em>并且</em> 二者以短语 “rock climbing” 的形式紧挨着的雇员记录。</p><p>为此对 <code>match</code> 查询稍作调整，使用一个叫做 <code>match_phrase</code> 的查询：</p><pre><code class="sense">GET /megacorp/employee/_search&#123;    &quot;query&quot; : &#123;        &quot;match_phrase&quot; : &#123;            &quot;about&quot; : &quot;rock climbing&quot;        &#125;    &#125;&#125;</code></pre><pre><code>结果：&#123;   ...   &quot;hits&quot;: &#123;      &quot;total&quot;:      1,      &quot;max_score&quot;:  0.23013961,      &quot;hits&quot;: [         &#123;            ...            &quot;_score&quot;:         0.23013961,            &quot;_source&quot;: &#123;               &quot;first_name&quot;:  &quot;John&quot;,               &quot;last_name&quot;:   &quot;Smith&quot;,               &quot;age&quot;:         25,               &quot;about&quot;:       &quot;I love to go rock climbing&quot;,               &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]            &#125;         &#125;      ]   &#125;&#125;</code></pre><h3 id="查询表达式-高亮搜索"><a href="#查询表达式-高亮搜索" class="headerlink" title="查询表达式-高亮搜索"></a>查询表达式-高亮搜索</h3><p>许多应用都倾向于在每个搜索结果中 <em>高亮</em> 部分文本片段，以便让用户知道为何该文档符合查询条件。在 Elasticsearch 中检索出高亮片段也很容易。</p><p>再次执行前面的查询，并增加一个新的 <code>highlight</code> 参数：</p><pre><code>GET /megacorp/employee/_search&#123;    &quot;query&quot; : &#123;        &quot;match_phrase&quot; : &#123;            &quot;about&quot; : &quot;rock climbing&quot;        &#125;    &#125;,    &quot;highlight&quot;: &#123;        &quot;fields&quot; : &#123;            &quot;about&quot; : &#123;&#125;        &#125;    &#125;&#125;</code></pre><p>当执行该查询时，返回结果与之前一样，与此同时结果中还多了一个叫做 <code>highlight</code> 的部分。这个部分包含了 <code>about</code> 属性匹配的文本片段，并以 HTML 标签 <code>&lt;em&gt;&lt;/em&gt;</code> 封装：</p><pre><code>结果：&#123;   ...   &quot;hits&quot;: &#123;      &quot;total&quot;:      1,      &quot;max_score&quot;:  0.23013961,      &quot;hits&quot;: [         &#123;            ...            &quot;_score&quot;:         0.23013961,            &quot;_source&quot;: &#123;               &quot;first_name&quot;:  &quot;John&quot;,               &quot;last_name&quot;:   &quot;Smith&quot;,               &quot;age&quot;:         25,               &quot;about&quot;:       &quot;I love to go rock climbing&quot;,               &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]            &#125;,            &quot;highlight&quot;: &#123;               &quot;about&quot;: [                  &quot;I love to go &lt;em&gt;rock&lt;/em&gt; &lt;em&gt;climbing&lt;/em&gt;&quot;                ]            &#125;         &#125;      ]   &#125;&#125;</code></pre><h3 id="查询表达式-聚合"><a href="#查询表达式-聚合" class="headerlink" title="查询表达式-聚合"></a>查询表达式-聚合</h3><p>支持管理者对员工目录做分析。 Elasticsearch 有一个功能叫聚合（aggregations），允许我们基于数据生成一些精细的分析结果。聚合与 SQL 中的 <code>GROUP BY</code> 类似但更强大。</p><pre><code>GET /megacorp/employee/_search&#123;  &quot;aggs&quot;: &#123;    &quot;all_interests&quot;: &#123;      &quot;terms&quot;: &#123; &quot;field&quot;: &quot;interests&quot; &#125;    &#125;  &#125;&#125;</code></pre><pre><code>结果：&#123;   ...   &quot;hits&quot;: &#123; ... &#125;,   &quot;aggregations&quot;: &#123;      &quot;all_interests&quot;: &#123;         &quot;buckets&quot;: [            &#123;               &quot;key&quot;:       &quot;music&quot;,               &quot;doc_count&quot;: 2            &#125;,            &#123;               &quot;key&quot;:       &quot;forestry&quot;,               &quot;doc_count&quot;: 1            &#125;,            &#123;               &quot;key&quot;:       &quot;sports&quot;,               &quot;doc_count&quot;: 1            &#125;         ]      &#125;   &#125;&#125;</code></pre><p>可以看到，两位员工对音乐感兴趣，一位对林业感兴趣，一位对运动感兴趣。这些聚合的结果数据并非预先统计，而是根据匹配当前查询的文档即时生成的。</p><h3 id="查询表达式-组合查询"><a href="#查询表达式-组合查询" class="headerlink" title="查询表达式-组合查询"></a>查询表达式-组合查询</h3><p>如果想知道叫 Smith 的员工中最受欢迎的兴趣爱好，可以直接构造一个组合查询：</p><pre><code>GET /megacorp/employee/_search&#123;  &quot;query&quot;: &#123;    &quot;match&quot;: &#123;      &quot;last_name&quot;: &quot;smith&quot;    &#125;  &#125;,  &quot;aggs&quot;: &#123;    &quot;all_interests&quot;: &#123;      &quot;terms&quot;: &#123;        &quot;field&quot;: &quot;interests&quot;      &#125;     &#125;  &#125;&#125;</code></pre><p><code>all_interests</code> 聚合已经变为只包含匹配查询的文档：</p><pre><code>结果：...  &quot;all_interests&quot;: &#123;     &quot;buckets&quot;: [        &#123;           &quot;key&quot;: &quot;music&quot;,           &quot;doc_count&quot;: 2        &#125;,        &#123;           &quot;key&quot;: &quot;sports&quot;,           &quot;doc_count&quot;: 1        &#125;     ]  &#125;</code></pre><h3 id="新增索引"><a href="#新增索引" class="headerlink" title="新增索引"></a>新增索引</h3><pre><code>PUT index_name/?prettycurl -XPUT &#39;192.168.247.20:9200/index_name?pretty&#39;//例子://在创建索引时还可以设置默认的分片数量、备份数量和刷新的秒数curl -XPUT &#39;http://192.168..247.20:9200/index_name?pretty&#39; -H &#39;Content-Type: application/json&#39; -d&#39;    &#123;    &quot;settings&quot; : &#123;    &quot;number_of_shards&quot;:8,    &quot;number_of_replicas&quot; : 1,    &quot;refresh_interval&quot;:&quot;30s&quot;        &#125;       &#125;</code></pre><h3 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h3><pre><code>curl -XDELETE &#39;192.168.247.20:9200/index_name?pretty&#39;DELETE index_name</code></pre><h5 id><a href="#" class="headerlink" title></a></h5><h2 id="相关性得分"><a href="#相关性得分" class="headerlink" title="相关性得分"></a>相关性得分</h2><p>Elasticsearch 默认按照相关性得分排序，即每个文档跟查询的匹配程度。第一个最高得分的结果很明显：John Smith 的 <code>about</code> 属性清楚地写着 “rock climbing” 。</p><p>但为什么 Jane Smith 也作为结果返回了呢？原因是她的 <code>about</code> 属性里提到了 “rock” 。因为只有 “rock” 而没有 “climbing” ，所以她的相关性得分低于 John 的。</p><p>这是一个很好的案例，阐明了 Elasticsearch 如何 <em>在</em> 全文属性上搜索并返回相关性最强的结果。Elasticsearch中的 <em>相关性</em> 概念非常重要，也是完全区别于传统关系型数据库的一个概念，数据库中的一条记录要么匹配要么不匹配。</p><h1 id="常用搜索引擎框架"><a href="#常用搜索引擎框架" class="headerlink" title="常用搜索引擎框架"></a>常用搜索引擎框架</h1><p>1、<strong>Lucene</strong></p><p>Apache下面的一个开源项目，高性能的、可扩展的工具库，提供搜索的基本架构，如果开发人员需用使用的话，需用自己进行开发,成本比较大，但是性能高</p><p>2、<strong>Solr</strong></p><p>Solr基于Lucene的全文搜索框架，提供了比Lucene更为丰富的功能，同时实现了可配置、可扩展并对查询性能进行了优化<br>建立索引时，搜索效率下降，实时索引搜索效率不高<br>数据量的增加，Solr的搜索效率会变得更低,适合小的搜索应用，对应java客户端的是solrj</p><p>3、<strong>ElasticSearch</strong></p><p>基于Lucene的搜索框架, 它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口<br>上手容易，拓展节点方便，可用于存储和检索海量数据，接近实时搜索，海量数据量增加，搜索响应性能几乎不受影响，分布式搜索框架，自动发现节点，副本机制，保障可用性</p>]]></content>
      
      
      <categories>
          
          <category> Elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop Federation</title>
      <link href="/cdh/hadoop/hadoop-federation-lian-bang/"/>
      <url>/cdh/hadoop/hadoop-federation-lian-bang/</url>
      
        <content type="html"><![CDATA[<h1 id="Hadoop-Federation联邦"><a href="#Hadoop-Federation联邦" class="headerlink" title="Hadoop Federation联邦"></a>Hadoop Federation联邦</h1><p>Hadoopnamenode 水平扩展</p><p><a href="https://www.cnblogs.com/jifengblog/p/9307702.html">https://www.cnblogs.com/jifengblog/p/9307702.html</a></p><p>单 NameNode 的架构使得 HDFS 在集群扩展性和性能上都有潜在的问题，当集群大到一定程度后，NameNode 进程使用的内存可能会达到上百 G，NameNode 成为了性能的瓶颈。因而提出了 namenode 水平扩展方案– Federation。</p><p>NameNode 的 Federation,也就是会有多个NameNode。多个 NameNode 的情况意味着有多个 namespace(命名空间)，区别于 HA 模式下的多 NameNode，它们是拥有着同一个 namespace。</p><p>所有关于存储数据的信息和管理是放在 NameNode 这边,而真实数据的存储则是在各个 DataNode 下。而这些隶属于同一个 NameNode 所管理的数据都是在同一个命名空间下的。而一个 namespace 对应一个 block pool。Block Pool 是同一个 namespace 下的 block 的集合.当然这是我们最常见的单个 namespace 的情况,也就是一个 NameNode 管理集群中所有元数据信息的时候.如果我们遇到了之前提到的 NameNode 内存使用过高的问题,这时候怎么办?元数据空间依然还是在不断增大,一味调高 NameNode 的 jvm 大小绝对不是一个持久的办法.这时候就诞生了 HDFS Federation 的机制.</p><h2 id="Federation-架构设计"><a href="#Federation-架构设计" class="headerlink" title="Federation 架构设计"></a>Federation 架构设计</h2><p>HDFS Federation 是解决 namenode 内存瓶颈问题的水平横向扩展方案。</p><p>Federation 意味着在集群中将会有多个 namenode&#x2F;namespace。这些 namenode 之间是联合的，也就是说，他们之间相互独立且不需要互相协调，各自分工，管理自己的区域。分布式的 datanode 被用作通用的数据块存储存储设备。每个 datanode 要向集群中所有的namenode 注册，且周期性地向所有 namenode 发送心跳和块报告，并执行来自所有 namenode的命令。</p><p><img src="https://images2018.cnblogs.com/blog/1385779/201807/1385779-20180713221451764-694934100.png" alt="img"></p><p><img src="https://images2018.cnblogs.com/blog/1385779/201807/1385779-20180713221458715-863634950.png" alt="img"></p><p>Federation 一个典型的例子就是上面提到的 NameNode 内存过高问题,我们完全可以将上面部分大的文件目录移到另外一个NameNode上做管理. 更重要的一点在于,, 这些 NameNode是共享集群中所有的 e DataNode 的 , 它们还是在同一个集群内的 。</p><p>这时候在DataNode上就不仅仅存储一个Block Pool下的数据了,而是多个(在DataNode的 datadir 所在目录里面查看 BP-xx.xx.xx.xx 打头的目录)。</p><h3 id="概括起来："><a href="#概括起来：" class="headerlink" title="概括起来："></a>概括起来：</h3><p>　　　　多个 NN 共用一个集群里的存储资源，每个 NN 都可以单独对外提供服务。</p><p>　　　　每个 NN 都会定义一个存储池，有单独的 id，每个 DN 都为所有存储池提供存储。</p><p>　　　　DN 会按照存储池 id 向其对应的 NN 汇报块信息，同时，DN 会向所有 NN 汇报本地存储可用资源情况。</p><p><img src="https://images2018.cnblogs.com/blog/1385779/201807/1385779-20180713221819834-409214850.png" alt="img"></p><h3 id="HDFS-Federation不足"><a href="#HDFS-Federation不足" class="headerlink" title="HDFS Federation不足"></a>HDFS Federation不足</h3><p>　　HDFS Federation 并没有完全解决单点故障问题。虽然 namenode&#x2F;namespace 存在多个，但是从单个 namenode&#x2F;namespace 看，仍然存在单点故障：如果某个 namenode 挂掉了，其管理的相应的文件便不可以访问。Federation中每个namenode仍然像之前HDFS上实现一样，配有一个 secondary namenode，以便主 namenode 挂掉一下，用于还原元数据信息。</p><p>　　所以一般集群规模真的很大的时候，会采用 HA+Federation 的部署方案。也就是每个联合的 namenodes 都是 ha 的。</p><h1 id="HA方案之QJM"><a href="#HA方案之QJM" class="headerlink" title="HA方案之QJM"></a>HA方案之QJM</h1>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadopp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop学习笔记</title>
      <link href="/cdh/hadoop/hadoop/"/>
      <url>/cdh/hadoop/hadoop/</url>
      
        <content type="html"><![CDATA[<h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><h3 id="Hadoop是什么？"><a href="#Hadoop是什么？" class="headerlink" title="Hadoop是什么？"></a>Hadoop是什么？</h3><p>分布式系统基础架构。</p><h3 id="配置jdk和hadoop的环境"><a href="#配置jdk和hadoop的环境" class="headerlink" title="配置jdk和hadoop的环境"></a>配置jdk和hadoop的环境</h3><p>1.使用 SSH Secure Shell 上传文件<br>2.上传到 ~&#x2F;Downloads&#x2F;下并解压<br>3.配置&#x2F;etc下面的profile文件的JAVA_HOME和HADOOP_HOME</p><pre><code></code></pre><p>4.重启服务  Source profile</p><h3 id="Hadoop-1"><a href="#Hadoop-1" class="headerlink" title="Hadoop"></a>Hadoop</h3><h4 id="Namenode和Datanode的关系图"><a href="#Namenode和Datanode的关系图" class="headerlink" title="Namenode和Datanode的关系图"></a>Namenode和Datanode的关系图</h4><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565334786917.png" alt="1565334786917"></p><h4 id="Hadoop1-0和Hadoop2-0-系统的区别"><a href="#Hadoop1-0和Hadoop2-0-系统的区别" class="headerlink" title="Hadoop1.0和Hadoop2.0 系统的区别"></a>Hadoop1.0和Hadoop2.0 系统的区别</h4><p> HADOOP1.0主要是由MapReduce和HDFS构成的，其中MapReduce是一个离线处理框架，由编程模型（新旧API）、运行时环境（JobTracker和TaskTracker）和数据处理引擎（MapTask和ReduceTask）三部分组成。</p><p>HADOOP2.0主要由HDFS、MapReduce和YARN三个系统组成，其中YARN是一个资源管理系统，负责集群资源管理和调度，MapReduce则是运行在YARN上的离线处理框架，它与Hadoop 1.0中的MapReduce在编程模型（新旧API）和数据处理引擎（MapTask和ReduceTask）两个方面是相同的。</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565432697598.png" alt="1565432697598"></p><h4 id="Hadoop2的三大核心"><a href="#Hadoop2的三大核心" class="headerlink" title="Hadoop2的三大核心"></a>Hadoop2的三大核心</h4><p>HDFS、MapReduce、YARN</p><p>MapReduce、YARN</p><p>YARN（通用资源管理系统）</p><h4 id="Hadoop的四个模块"><a href="#Hadoop的四个模块" class="headerlink" title="Hadoop的四个模块"></a>Hadoop的四个模块</h4><p>1.Hadoop Common 为其他 Hadoop模块提供基础设施</p><p>2.Hadoop HDFS 一个高可靠、高吞吐量的分布式文件系统 </p><p>3.Hadoop MapReduce 一个分布式的离线并行计算框架</p><p>4.Hadoop YARN  新的MapReduce框架，资源管理 任务调度 </p><h4 id="HDFS三个服务"><a href="#HDFS三个服务" class="headerlink" title="HDFS三个服务"></a>HDFS三个服务</h4><p>NameNode</p><p>Secondary NameNode  秘书</p><p>DataNode 数据 （多个）</p><p>三个进程 相互合作产生结果实现大量数据存储 </p><h4 id="安全模式-SafeMode"><a href="#安全模式-SafeMode" class="headerlink" title="安全模式 (SafeMode)"></a>安全模式 (SafeMode)</h4><p>1.查询当前是否是安全模式 </p><p>hdfs dfsadmin -safemode get </p><p>2.退出安全模式 </p><p>hdfs dfsadmin -safemode leave</p><p>3.进入安全模式</p><p>hdfs dfsadmin -safemode enter </p><p>4.在执行某条命令之前退出安全模式(在进行一些集群维护操作时很有用，可以保持数据依然可读。也可以在集群已是安全模式使执行，确保进入安全模式。)</p><p>hdfs dfsadmin -safemode wait </p><h4 id="HDFS架构-——NameNode"><a href="#HDFS架构-——NameNode" class="headerlink" title="HDFS架构 ——NameNode"></a>HDFS架构 ——NameNode</h4><p> 1.Namenode 是一个中心服务器，单一节点，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问，NameNode  是主节点，存储文件的元数据<br> 2.NameNode 负责文件元数据的操作，DataNode负责处理文件内容的读写请求<br> 3.读取文件时NameNode尽量让用户先读取最近的副本，降低带块消耗和读取时延（机架感知）<br> 4.Namenode 全权管理数据块的复制（namenode发送指令让datanode 来复制文件块），<br> 它周期性地从集群中的每个Datanode接收心跳信号和块状态报告(Blockreport)(元数据)。<br> 接收到心跳信号意味着该Datanode节点工作正常。<br> 块状态报告包含了一个该Datanode上所有数据块的列表</p><h6 id="NameNode启动过程"><a href="#NameNode启动过程" class="headerlink" title="NameNode启动过程"></a>NameNode启动过程</h6><p>1.NameNode启动时首先将fsimage（镜像）载入内存，并执行编译日志editlog的各项操作，（secondaryNamenode和namenode不要在同一台机器上）</p><p>2.一旦在内存中建立文件系统元数据映射，则创建一个新的fsimage文件（这个过程不需SecondaryNameNode）和一个空的editlog</p><p>3.在安全模式下各个datanode回向namenode发送块列表的最新情况（向namenode注册时，文件是否大量损坏不能用）</p><p>4.namenode在安全模式中，namenode的文件系统对于客户端来说是只读的。</p><p>5.NameNode开始监听RPC和HTTP请求（namenode开始接受心跳）</p><p>6.系统中的数据块是以块的形式存储在datanode中的</p><p>7.正常操作期间，namenode在内存中保留所有块的映射信息</p><p> 1.加载fsimage和edits文件<br> 2.生成新的fsimage和edits_new文件<br> 3.等待DataNode注册与发送Block Report（块报告）</p><h6 id="NameNode的两个重要文件"><a href="#NameNode的两个重要文件" class="headerlink" title="NameNode的两个重要文件"></a>NameNode的两个重要文件</h6><p>fsimage 元数据镜像文件（保存文件系统的目录树）</p><p>edits 元数据操作日志 （针对目录树的修改操作）</p><h6 id="元数据镜像"><a href="#元数据镜像" class="headerlink" title="元数据镜像"></a>元数据镜像</h6><p>内存中保持最新的一份 （实时更新）</p><p>内存中的镜像 &#x3D;fsiamge+edits</p><h4 id="HDFS架构-——DataNode"><a href="#HDFS架构-——DataNode" class="headerlink" title="HDFS架构 ——DataNode"></a>HDFS架构 ——DataNode</h4><p> 1.一个数据块在DataNode以文件存储在磁盘上，包括两个文件，一个是数据本身，<br> 一个是元数据包括数据块的长度，块数据的校验和（自检块信息）以及时间戳<br> 2.DataNode启动后向NameNode注册，（谁先启动）通过后，周期性的向NameNode上报所有的块信息（namenode被动的）<br> 3.心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令，如删除某个数据块。（namenode被动发送信息，发送指令给block块删除备份）如果超过10分钟没有收到某个DataNode 的心跳，则认为该节点不可用。<br> 4.集群运行中可以安全加入和退出一些机器</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565342966100.png" alt="1565342966100"></p><h6 id="DataNode启动过程"><a href="#DataNode启动过程" class="headerlink" title="DataNode启动过程"></a>DataNode启动过程</h6><p>先NameNode注册、发送Block Report </p><h4 id="HDFS架构-——文件"><a href="#HDFS架构-——文件" class="headerlink" title="HDFS架构 ——文件"></a>HDFS架构 ——文件</h4><p> 1.文件切分成块（默认大小128M ，文件切分成块（默认大小128M），以块为单位，每个块有多个副本存储在不同的机器上，副本数可在文件生成时指定（默认3）<br> 2.可以创建、删除、移动或重命名文件，当文件创建、写入和关闭之后不能修改文件内容。（易读不易写）</p><h4 id="HDFS架构——Client"><a href="#HDFS架构——Client" class="headerlink" title="HDFS架构——Client"></a>HDFS架构——Client</h4><h6 id="Client职责"><a href="#Client职责" class="headerlink" title="Client职责"></a>Client职责</h6><p>1.文件切分成块（128M）</p><p>2.与NameNode交互，获取文件位置信息</p><p>3.与DataNode交互，读取或者写入数据</p><p>4.管理HDFS</p><p>5.访问HDFS</p><h4 id="HDFS架构——SNN"><a href="#HDFS架构——SNN" class="headerlink" title="HDFS架构——SNN"></a>HDFS架构——SNN</h4><h6 id="Secondary-NameNode职责"><a href="#Secondary-NameNode职责" class="headerlink" title="Secondary NameNode职责"></a>Secondary NameNode职责</h6><p>SecondaryNameNode有两个作用：</p><p><strong>一是镜像备份，二是日志与镜像的定期合并</strong></p><p>辅助NameNode分担工作量</p><p>edits文件过大将导致NameNode重启速度慢，定期合并fsimage和fsedits推送给namenode</p><p>在紧急情况下，辅助namenode恢复</p><h6 id="secondary-NameNode-工作流程"><a href="#secondary-NameNode-工作流程" class="headerlink" title="secondary NameNode 工作流程"></a>secondary NameNode 工作流程</h6><p><img src="https://img-blog.csdn.net/20150104202517828" alt="img"></p><p>1.SecondaryNameNode通知NameNode准备提交edits文件，此时主节点产生edits.new<br>SecondaryNameNode通过http get方式获取NameNode的fsimage与edits文件（在SecondaryNameNode的current同级目录下可见到 temp.check-point或者previous-checkpoint目录，这些目录中存储着从namenode拷贝来的镜像文件）<br>SecondaryNameNode开始合并获取的上述两个文件，产生一个新的fsimage文件fsimage.ckpt<br>SecondaryNameNode用http post方式发送fsimage.ckpt至NameNode<br>NameNode将fsimage.ckpt与edits.new文件分别重命名为fsimage与edits，然后更新fstime，整个checkpoint过程到此结束。 在新版本的hadoop中（hadoop0.21.0）,SecondaryNameNode两个作用被两个节点替换， checkpoint node与backup node. SecondaryNameNode备份由三个参数控制fs.checkpoint.period控制周期，fs.checkpoint.size控制日志文件超过多少大小时合并， dfs.http.address表示http地址，这个参数在SecondaryNameNode为单独节点时需要设置。</p><h6 id="fsimage和edits文件查看位置"><a href="#fsimage和edits文件查看位置" class="headerlink" title="fsimage和edits文件查看位置"></a>fsimage和edits文件查看位置</h6><p>&#x2F;home&#x2F;hadoopdata&#x2F;dfs&#x2F;name&#x2F;current  目录下查看fsimage和edits文件</p><h6 id="block块文件查看位置"><a href="#block块文件查看位置" class="headerlink" title="block块文件查看位置"></a>block块文件查看位置</h6><p>&#x2F;home&#x2F;hadoopdata&#x2F;dfs&#x2F;data&#x2F;current&#x2F;BP-747889207-192.168.10.11-1456891004770&#x2F;current&#x2F;finalized&#x2F;subdir0&#x2F;subdir0   目录下查看block文件</p><h3 id="HDFS写流程"><a href="#HDFS写流程" class="headerlink" title="HDFS写流程"></a>HDFS写流程</h3><p>block<br>文件上传前需要分块，这个块就是block，一般为128MB，当然你可以去改，不顾不推荐。因为块太小：寻址时间占比过高。块太大：Map任务数太少，作业执行速度变慢。它是最大的一个单位。</p><p>packet<br>packet是第二大的单位，它是client端向DataNode，或DataNode的PipLine之间传数据的基本单位，默认64KB。</p><p>chunk<br>chunk是最小的单位，它是client向DataNode，或DataNode的PipLine之间进行数据校验的基本单位，默认512Byte，因为用作校验，故每个chunk需要带有4Byte的校验位。所以实际每个chunk写入packet的大小为516Byte。由此可见真实数据与校验值数据的比值约为128 : 1。（即64*1024 &#x2F; 512）<br>————————————————<br>版权声明：本文为CSDN博主「bw_233」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/whdxjbw/article/details/81072207">https://blog.csdn.net/whdxjbw/article/details/81072207</a></p><p><img src="https://img2018.cnblogs.com/blog/699090/201906/699090-20190626155745864-1227676006.png" alt="img"></p><p>1、客户端向NameNode发出写文件请求。</p><p>2、查是否已存在文件、检查权限。若通过检查，<strong>直接先将操作写入EditLog</strong>，并返回输出流对象。</p><p>3、client端<strong>按128MB的块切分文件</strong></p><p>4、client将NameNode返回的分配的可写的<strong>DataNode列表</strong>和<strong>Data数据</strong>一同发送给最近的第一个DataNode节点，此后client端和NameNode分配的多个DataNode构成pipeline管道，client端向输出流对象中写数据。</p><p>5、每个DataNode写完一个块后，会返回<strong>确认信息</strong>。</p><p>6、写完数据，关闭输出流。</p><p>7、发送完后发送完成信号给NameNode。</p><h3 id="HDFS读流程"><a href="#HDFS读流程" class="headerlink" title="HDFS读流程"></a>HDFS读流程</h3><p><img src="https://images2018.cnblogs.com/blog/1228818/201803/1228818-20180312131601322-859729566.png" alt="img"></p><p>1、client访问NameNode，查询元数据信息，获得这个文件的数据块位置列表，返回输入流对象。</p><p>2、就近挑选一台datanode服务器，请求建立输入流 。</p><p>3、DataNode向输入流中中写数据，以packet为单位来校验。</p><p>4、关闭输入流</p><h3 id="MR流程"><a href="#MR流程" class="headerlink" title="MR流程"></a>MR流程</h3><p>1、一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程</p><p>2、maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为：<br>1）利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对<br>2）将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存<br>3）将缓存中的KV对按照K分区排序后不断溢写到磁盘文件</p><p>3、MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分区）</p><p>4、Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储</p><h3 id="MR的shuffle流程"><a href="#MR的shuffle流程" class="headerlink" title="MR的shuffle流程"></a>MR的shuffle流程</h3><p>一个block文件就是一个mr任务</p><p>map阶段</p><p>通过行读取器按行读取(k,v)，在内存中执行自定义的map()方法，因为频繁的磁盘I&#x2F;O操作会严重的降低效率，因此“中间结果”不会立马写入磁盘，存储到map节点的环形缓冲区，环形缓冲区默认100M，当写入数据到预先设置好的阀值后，默认0.8，便会溢写文件到磁盘上生成一个小的临时文件。溢写之前在写出的过程中进行分区(按照k的索引根据字典顺序排序 [ 快排 ])排序合并(combiner)(可选)，整个map全部溢写完毕后，进行merge合并(捏合默认一次拉取10)和排序，合并成一个已分区且已排序大文件，这里的捏合是将分区相同的数据进行捏合。溢出写文件归并完毕后，Map将删除所有的临时溢出写文件</p><p>reduce阶段</p><p>reducetask来copy数据，先放入到缓冲区中，这个缓冲区不同Map端的，大小是jvm的堆内存大小。</p><p>在内存中每个Map对应一块数据，当内存缓存区中存储的Map数据占用空间达到一定程度的时候，开始启动内存中merge，把内存中的数据merge输出到磁盘上一个文件中，即<strong>内存到磁盘merge</strong>。默认是JVM的heap size的70%。（如果拖取的所有map数据总量都没有内存缓冲区，则数据就只存在于内存中）</p><p>reduce从map磁盘中拉取数据（默认拉取5个），当属于该reducer的map输出全部拷贝完成，则会在reducer上生成多个文件这时开始执行合并操作，即<strong>磁盘到磁盘merge</strong>。最终Reduce shuffle过程会输出一个整体有序的数据块。采用归并排序将内存和磁盘中的数据都<strong>进行排序</strong></p><p>然后进行分组，只要这个比较器比较的两个Key相同，它们就属于同一组，它们的 Value就会放在一个Value迭代器</p><p>然后执行reduce方法。</p><h3 id="MRshuffle的优化"><a href="#MRshuffle的优化" class="headerlink" title="MRshuffle的优化"></a>MRshuffle的优化</h3><p>1、在map端的环形缓冲区可以调大，调成200M，溢写的阈值可以改成90%，这样做为的是把溢写的文件数量减少。</p><p>2、map的压缩，可以在三个地方对数据进行压缩。</p><p>1）map之前</p><p>2）map之后</p><p>3）reduce之后</p><p>map之前讲究切片，可以使用lzo和bzip2，他们都支持切片</p><p>map之后讲究快，可以使用snappy和lzo，他们都挺快</p><p>reduce之后看用途，如果是下一个使用的，就使用一个可以压缩的，如果是最终的结果了，那么选一个压缩比最高的。</p><p>3、reduce缓冲区调大，提高拉取的个数，默认5个，改成一分钟10个。</p><p>4、调整maptask和reducetask内存大小，正常默认1G，把他们调成4-5个G</p><p>5、增大他们失败重试的次数</p><p>6、增大他们cpu的核数</p><h3 id="MR数据倾斜"><a href="#MR数据倾斜" class="headerlink" title="MR数据倾斜"></a>MR数据倾斜</h3><p>1、自定义分区器，按照自己的规则将不同的key归类到不同的partition。</p><pre><code>//每个分区对应一个ReduceTask//通过自定义分区控制最后结果输出在哪个文件public class MyPartition extends Partitioner&lt;Text, NullWritable&gt; &#123;@Override    public int getPartition(Text text, NullWritable nullWritable, int i) &#123;        //获取数据        String Phnum = text.toString();        //截取号码前三位，判断运营商        String s=Phnum.substring(0,3);        //移动0区，联通1区，电信2区        if(Arrays.asList(YD).contains(s)) return 0;        else if(Arrays.asList(LT).contains(s)) return 1;        else return 2;    &#125;&#125;</code></pre><p>2、第一次在 map 阶段对那些导致了数据倾斜的 key 加上 1 到 n 的随机前缀，这样本来相</p><p>同的 key 也会被分到多个 Reducer 中进行局部聚合，数量就会大大降低。</p><p>第二次 mapreduce，去掉 key 的随机前缀，进行全局聚合。</p><p>思想：二次 mr，第一次将 key 随机散列到不同 reducer 进行处理达到负载均衡目的。第</p><p>二次再根据去掉 key 的随机前缀，按原 key 进行 reduce 处理。</p><p>这个方法进行两次 mapreduce，性能稍差。</p><p>3、增加reduce数量，提高并行度</p><h4 id="HDFS特性"><a href="#HDFS特性" class="headerlink" title="HDFS特性"></a>HDFS特性</h4><p>1.自动快速检测应对硬件错误 </p><p>2.额定备份</p><p>3.流式访问数据 （易读不易写）</p><p>4.移动数据不如移动计算 </p><ol start="4"><li>1减少io损耗4.2分布式处理 并行计算 </li><li>简单一致性模型</li><li>异构平台可移植</li></ol><h5 id="HDFS心跳机制"><a href="#HDFS心跳机制" class="headerlink" title="HDFS心跳机制"></a>HDFS心跳机制</h5><p>1.报活3秒一次<br>2.发送 namenode–&gt;datanode（namenode从不主动，namenode被动发送指令给datanode）<br>3.datanode发送的自身情况 </p><h5 id="HDFS-优点"><a href="#HDFS-优点" class="headerlink" title="HDFS 优点"></a>HDFS 优点</h5><p>1.高可靠性 （自动备份数据，在block块中 ）<br>2.高容错性 （自动保存多个副本，失败任务重新分配）<br>3.高效性   （能够在节点中动态的移动数据，保证节点平衡，处理速度快）<br>4.高扩展性 （可以动态的增删节点）</p><h5 id="HDFS-缺点"><a href="#HDFS-缺点" class="headerlink" title="HDFS  缺点"></a>HDFS  缺点</h5><p>一 不适合快速的访问数据<br>二 无法高效存储大量小文件<br>三 不支持多用户写入及修改文件</p><h5 id="HDFS-块放置策略"><a href="#HDFS-块放置策略" class="headerlink" title="HDFS 块放置策略"></a>HDFS 块放置策略</h5><p>两个机架分开放，相同的备份不能放在同一台机器上 </p><h5 id="HDFS-的读写流程"><a href="#HDFS-的读写流程" class="headerlink" title="HDFS 的读写流程"></a>HDFS 的读写流程</h5><p>写：clien请求上传文件，切块（128M），</p><p>读：client发送请求，拉取块数据，获取块地址，namenode控制datanode中的数据（自己的话） </p><h3 id="伪分布式的搭建"><a href="#伪分布式的搭建" class="headerlink" title="伪分布式的搭建"></a>伪分布式的搭建</h3><p>准备：</p><p>关防火墙</p><p>按jdk、hadoop和配置hadoop</p><p>ip地址和主机名设置</p><p>映射文件配置</p><p>ssh免密登陆 </p><h6 id="1-配置文件-5个"><a href="#1-配置文件-5个" class="headerlink" title="1.配置文件(5个)"></a>1.配置文件(5个)</h6><p>hadoop-env.sh   修改JAVA_HOME和HADOOP_HOME的路径<br>core-site.xml       node主机名    tmpdir 放feimage和edits  format时必须将tmpdir删除 （没删起冲突）<br>hdfs-site.xml       文件备份数量<br>yarn-site.xml  yarn    给mr提供资源<br>mapread-site.xml.template  因为识别不了得改个名字 mv mapred-site.xml.template mapred-site.xml<br>mapread-site.xml                   提供work工作 的名字 具体给他分配资源的是yarn</p><p>在配置文件完成后 通过 hadoop namenode -format 进行格式化 </p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565333152187.png" alt="1565333152187"></p><h6 id="2-格式化成功后-启动进程"><a href="#2-格式化成功后-启动进程" class="headerlink" title="2.格式化成功后 启动进程"></a>2.格式化成功后 启动进程</h6><p>start-dfs.sh　启动namenode 和datanode</p><p>start-yarn.sh启动yarn进程 </p><p>start-all.sh            还包括yarn的resourcemanager 和nodemanager</p><p>查看当前运行进程   jps</p><p>杀死进程      kill -9 进程编号 </p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565333901204.png" alt="1565333901204"></p><p> stop-all.sh   关闭所有进程 </p><p>hadoop jar jar包名称   描述名称    </p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565334501352.png" alt="1565334501352"></p><h2 id="Hadoop全分布式环境部署"><a href="#Hadoop全分布式环境部署" class="headerlink" title="Hadoop全分布式环境部署"></a>Hadoop全分布式环境部署</h2><p>克隆好的虚拟机</p><p>1.安装jdk</p><p>2.安装hadoop</p><p>3.配置&#x2F;root&#x2F;Downloads&#x2F;hadoop-2.6.5&#x2F;etc&#x2F;hadoop的文件（一共有8个，如果配置过伪分布要删除根目录下的hadoop下的tmpdir文件夹，和重新解压hadoop）</p><h5 id="3-1-hadoop-env-sh"><a href="#3-1-hadoop-env-sh" class="headerlink" title="3.1  hadoop-env.sh"></a>3.1  hadoop-env.sh</h5><pre><code class="sh"># The java implementation to use.export JAVA_HOME=/opt/modules/jdk# The jsvc implementation to use. Jsvc is required to run secure datanodes# that bind to privileged ports to provide authentication of data transfer# protocol.  Jsvc is not required if SASL is configured for authentication of# data transfer protocol using non-privileged ports.#export JSVC_HOME=$&#123;JSVC_HOME&#125;export HADOOP_CONF_DIR=$&#123;HADOOP_CONF_DIR:-&quot;/etc/hadoop&quot;&#125;export HADOOP_CLASSPATH=&quot;&lt;extra_entries&gt;:$HADOOP_CLASSPATH:$&#123;HADOOP_HOME&#125;/share/hadoop/common&quot;export JAVA_LIBRARY_PATH=$&#123;JAVA_LIBRARY_PATH&#125;:/opt/modules/hadoop/lib/native# Extra Java CLASSPATH elements.  Automatically insert capacity-scheduler.for f in $HADOOP_HOME/contrib/capacity-scheduler/*.jar; do  if [ &quot;$HADOOP_CLASSPATH&quot; ]; then    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f  else    export HADOOP_CLASSPATH=$f  fidone# The maximum amount of heap to use, in MB. Default is 1000.#export HADOOP_HEAPSIZE=#export HADOOP_NAMENODE_INIT_HEAPSIZE=&quot;&quot;# Extra Java runtime options.  Empty by default.export HADOOP_OPTS=&quot;$HADOOP_OPTS -Djava.net.preferIPv4Stack=true&quot;# Command specific options appended to HADOOP_OPTS when specifiedexport HADOOP_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; -Dhdfs.audit.logger=$&#123;HDFS_AUDIT_LOGGER:-INFO,NullAppender&#125; $HADOOP_NAMENODE_OPTS&quot;export HADOOP_DATANODE_OPTS=&quot;-Dhadoop.security.logger=ERROR,RFAS $HADOOP_DATANODE_OPTS&quot;export HADOOP_SECONDARYNAMENODE_OPTS=&quot;-Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; -Dhdfs.audit.logger=$&#123;HDFS_AUDIT_LOGGER:-INFO,NullAppender&#125; $HADOOP_SECONDARYNAMENODE_OPTS&quot;export HADOOP_NFS3_OPTS=&quot;$HADOOP_NFS3_OPTS&quot;export HADOOP_PORTMAP_OPTS=&quot;-Xmx512m $HADOOP_PORTMAP_OPTS&quot;# The following applies to multiple commands (fs, dfs, fsck, distcp etc)export HADOOP_CLIENT_OPTS=&quot;-Xmx512m $HADOOP_CLIENT_OPTS&quot;#HADOOP_JAVA_PLATFORM_OPTS=&quot;-XX:-UsePerfData $HADOOP_JAVA_PLATFORM_OPTS&quot;# On secure datanodes, user to run the datanode as after dropping privileges.# This **MUST** be uncommented to enable secure HDFS if using privileged ports# to provide authentication of data transfer protocol.  This **MUST NOT** be# defined if SASL is configured for authentication of data transfer protocol# using non-privileged ports.export HADOOP_SECURE_DN_USER=$&#123;HADOOP_SECURE_DN_USER&#125;# Where log files are stored.  $HADOOP_HOME/logs by default.#export HADOOP_LOG_DIR=$&#123;HADOOP_LOG_DIR&#125;/$USER# Where log files are stored in the secure data environment.export HADOOP_SECURE_DN_LOG_DIR=$&#123;HADOOP_LOG_DIR&#125;/$&#123;HADOOP_HDFS_USER&#125;#### HDFS Mover specific parameters#### Specify the JVM options to be used when starting the HDFS Mover.# These options will be appended to the options specified as HADOOP_OPTS# and therefore may override any similar flags set in HADOOP_OPTS## export HADOOP_MOVER_OPTS=&quot;&quot;#### Advanced Users Only!#### The directory where pid files are stored. /tmp by default.# NOTE: this should be set to a directory that can only be written to by #       the user that will run the hadoop daemons.  Otherwise there is the#       potential for a symlink attack.export HADOOP_PID_DIR=$&#123;HADOOP_PID_DIR&#125;export HADOOP_SECURE_DN_PID_DIR=$&#123;HADOOP_PID_DIR&#125;# A string representing this instance of hadoop. $USER by default.export HADOOP_IDENT_STRING=$USER</code></pre><p>集群规划：</p><table><thead><tr><th></th><th>服务器61</th><th>服务器62</th><th>服务器63</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode  DataNode</td><td>DataNode</td><td>DataNode  SecondaryNameNode</td></tr><tr><td>Yarn</td><td>NodeManager</td><td>Resourcemanager  NodeManager</td><td>NodeManager</td></tr></tbody></table><h5 id="3-2-core-site-xml"><a href="#3-2-core-site-xml" class="headerlink" title="3.2  core-site.xml"></a>3.2  core-site.xml</h5><pre><code class="xml">&lt;configuration&gt;&lt;property&gt;&lt;!--配置hadoop的压缩方式，用哪个--&gt;&lt;name&gt;io.compression.codecs&lt;/name&gt;&lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;        &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;        &lt;/property&gt; &lt;property&gt;  &lt;name&gt;fs.defaultFS&lt;/name&gt;  &lt;value&gt;hdfs://bw61:8020&lt;/value&gt;&lt;/property&gt;  &lt;property&gt;  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;  &lt;value&gt;/opt/modules/hadoop/data/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre><h5 id="3-3-hdfs-site-xml"><a href="#3-3-hdfs-site-xml" class="headerlink" title="3.3  hdfs-site.xml"></a>3.3  hdfs-site.xml</h5><pre><code class="xml">&lt;configuration&gt;&lt;property&gt;  &lt;name&gt;dfs.replication&lt;/name&gt;  &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;!--打开权限hdfs--&gt; &lt;property&gt;  &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;  &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;    &lt;!-- 查看secondarynamenode的访问页面 --&gt; &lt;property&gt;  &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;  &lt;value&gt;bw62:50090&lt;/value&gt;&lt;/property&gt;&lt;!--hdfs通过web查看时的端口号--&gt; &lt;property&gt;  &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;  &lt;value&gt;bw61:50070&lt;/value&gt;&lt;/property&gt;    &lt;!-- block块在每台机器上的地址 --&gt; &lt;property&gt;  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;  &lt;value&gt;/opt/modules/hadoop/data/dfs/dn&lt;/value&gt;&lt;/property&gt;&lt;!-- 放置的是namenode中的fsimage --&gt;&lt;property&gt;  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;  &lt;value&gt;/opt/modules/hadoop/data/dfs/nn/name&lt;/value&gt;&lt;/property&gt;&lt;!-- 放置的是namenode中的edits --&gt;&lt;property&gt;  &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt;  &lt;value&gt;/opt/modules/hadoop/data/dfs/nn/edits&lt;/value&gt;&lt;/property&gt;&lt;!-- snn的fsimage日志文件的地址 --&gt;&lt;property&gt;  &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;  &lt;value&gt;/opt/modules/hadoop/data/dfs/snn/name&lt;/value&gt;&lt;/property&gt;    &lt;!-- snn的edits日志文件的地址 --&gt;&lt;property&gt;  &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;/name&gt;  &lt;value&gt;/opt/modules/hadoop/data/dfs/snn/edits&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre><h5 id="3-4-yarn-site-xml"><a href="#3-4-yarn-site-xml" class="headerlink" title="3.4  yarn-site.xml"></a>3.4  yarn-site.xml</h5><pre><code class="xml">&lt;configuration&gt;    &lt;property&gt;        &lt;!-- Site specific YARN configuration properties --&gt;        &lt;!-- 是否需要mr shuffle--&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    &lt;/property&gt;        &lt;!-- 指定resourcemanager在那一台机器上 --&gt;        &lt;!--resourcemanager分配的端口地址 --&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;        &lt;value&gt;bw62&lt;/value&gt;    &lt;/property&gt;                &lt;!-- 日志聚合功能 --&gt;        &lt;property&gt;        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;                &lt;!-- 日志的保存时长 --&gt;        &lt;property&gt;        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;        &lt;value&gt;604800&lt;/value&gt;    &lt;/property&gt;        &lt;property&gt;        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;        &lt;value&gt;false&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h5 id="3-5-mapred-site-xml"><a href="#3-5-mapred-site-xml" class="headerlink" title="3.5  mapred-site.xml"></a>3.5  mapred-site.xml</h5><p>(注意修改名称)</p><pre><code class="xml">&lt;configuration&gt;    &lt;!-- mr由yarn来分配资源 --&gt;     &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;local&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 历史服务器web访问地址 --&gt;    &lt;property&gt;          &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;          &lt;value&gt;bw61:19888&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 启动历史服务器 --&gt;    &lt;property&gt;          &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;          &lt;value&gt;bw61:10020&lt;/value&gt;    &lt;/property&gt;    &lt;!--user模式--&gt;     &lt;property&gt;      &lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt;          &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h5 id="3-6-yarn-env-sh"><a href="#3-6-yarn-env-sh" class="headerlink" title="3.6  yarn-env.sh"></a>3.6  yarn-env.sh</h5><pre><code class="sh">export JAVA_HOME=/opt/modules/jdk</code></pre><h5 id="3-7-mapred-env-sh"><a href="#3-7-mapred-env-sh" class="headerlink" title="3.7  mapred_env.sh"></a>3.7  mapred_env.sh</h5><pre><code class="sh">export JAVA_HOME=/opt/modules/jdk</code></pre><h5 id="3-8-配置slave文件"><a href="#3-8-配置slave文件" class="headerlink" title="3.8  配置slave文件"></a>3.8  配置slave文件</h5><p>(指定节点，在此文件下编写所有节点的名称)</p><p>vim slaves </p><pre><code class="sh">linux01linux02linux03</code></pre><p>4.发送hadoop文件夹到所有的机器上</p><pre><code class="linux">scp -r hadoop root@linux02:/opt/modules/hadoop</code></pre><p>5、格式化</p><pre><code>hdfs namenode -format</code></pre><p>6、启动集群</p><pre><code>//第一台启动start-dfs.sh//第二台启动start-yarn.sh</code></pre><h3 id="Hadoop启动命令列表"><a href="#Hadoop启动命令列表" class="headerlink" title="Hadoop启动命令列表"></a>Hadoop启动命令列表</h3><p>start-all.sh启动所有</p><p>stop-all.sh关闭所有</p><p>start-dfs.sh启动5个</p><p>start-yarn.sh启动2个</p><p>hadoop namenode -format 格式化进程</p><h3 id="单独启动进程命令"><a href="#单独启动进程命令" class="headerlink" title="单独启动进程命令"></a>单独启动进程命令</h3><p>hadoop-daemon.sh start datanode只启动datanode</p><p>hadoop-daemon.sh start namenode只启动namenode</p><p>yarn-daemon.sh start nodemanager只启动yarn</p><p>yarn-daemon.sh start resourcemanager只启动resourcemanager</p><p>yarn-daemon.sh start proxyserver</p><p>mr-jobhistory-daemon.sh start historyserver 启动jobhistory服务</p><h3 id="HDFS命令总结"><a href="#HDFS命令总结" class="headerlink" title="HDFS命令总结"></a>HDFS命令总结</h3><p>(1)启动Hdfs<br>start-dfs.sh</p><p>(2)关闭HDFS<br>Stop-dfs.sh</p><p>(3)查看帮助<br>hdfs dfs -help </p><p>(4) 查看当前目录信息<br>hdfs dfs -ls &#x2F;</p><p>(5)上传文件<br>hdfs dfs -put &#x2F;本地路径 &#x2F;hdfs路径</p><p>(6)剪切文件<br>hdfs dfs -moveFromLocal a.txt &#x2F;aa.txt<br>(7)下载文件到本地<br>hdfs dfs -get &#x2F;hdfs路径 &#x2F;本地路径<br>(8)合并下载<br>hdfs dfs -getmerge &#x2F;hdfs路径文件夹 &#x2F;合并后的文件<br>(9)创建文件夹<br>hdfs dfs -mkdir &#x2F;hello<br>(10)创建多级文件夹<br>hdfs dfs -mkdir -p &#x2F;hello&#x2F;world</p><p>(11)移动hdfs文件<br>hdfs dfs -mv &#x2F;hdfs路径 &#x2F;hdfs路径<br>(12)复制hdfs文件<br>hdfs dfs -cp &#x2F;hdfs路径 &#x2F;hdfs路径<br>(13)删除hdfs文件<br>hdfs dfs -rm &#x2F;aa.txt<br>(14)删除hdfs文件夹<br>hdfs dfs -rm -r &#x2F;hello<br>(15)查看hdfs中的文件<br>hdfs dfs -cat &#x2F;文件<br>hdfs dfs -tail -f &#x2F;文件<br>(16)查看文件夹中有多少个文件<br>hdfs dfs -count &#x2F;文件夹<br>(17)查看hdfs的总空间<br>hdfs dfs -df &#x2F;<br>hdfs dfs -df -h &#x2F;<br>(18)修改副本数<br>hdfs dfs -setrep 1 &#x2F;a.txt</p><p>(19)显示hadoop目录结构<br>hdfs dfs -ls -R &#x2F;</p><p>(20)将正在运行的hadoop作业kill掉<br>hadoop job -kill [job-id]</p><h3 id="端口号功能总结"><a href="#端口号功能总结" class="headerlink" title="端口号功能总结"></a>端口号功能总结</h3><p><a href="https://blog.csdn.net/qq_40757296/article/details/82490831">https://blog.csdn.net/qq_40757296/article/details/82490831</a></p><p>8020namenode的RPC调用端口（接收client连接的RPC端口 ，获取文件系统的metadata信息）</p><p>&lt;8030-8033resourcemanage组件&gt;</p><p>2181zookeeper端口号</p><p>2888    zk节点通信端口号</p><p>3306mysql端口号</p><p>3888      选举的端口号</p><p>4040Spark当前执行的任务页面查看端口</p><p>7077spark端口号</p><p>8030RPC的分配资源端口号</p><p>8031心跳机制的端口 </p><p>8032resourcemanager的通信端口号</p><p>8033admin模块的地址 </p><p>8088yarn的web监控端口</p><p>8089web application proxy web代理 </p><p>9000namenode常用端口 给机子   hdfs的默认(host)</p><p>10020历史服务器端口号 （host）</p><p>19888历史服务器web的监控端口号（webUI）</p><p>50070hdfs 的web服务器的监控端口号(web)</p><p>50010datanode　控制端口</p><p>50090secondaryNamenode web管理端口</p><h4 id="静态增加删除节点"><a href="#静态增加删除节点" class="headerlink" title="静态增加删除节点"></a>静态增加删除节点</h4><p>静态添加节点时需要关闭hadoop集群，配置相应配置，重启集群</p><p>考虑hdfs和yarn这两个平台的问题</p><p>1.修改slave文件 重启hadoop集群，添加的是从节点</p><p>优点：改动少 </p><p>缺点：暴力  需要停止服务 </p><p>检查：50070 和 8088 端口检查  进行页面访问 </p><h4 id="动态增加删除节点"><a href="#动态增加删除节点" class="headerlink" title="动态增加删除节点"></a>动态增加删除节点</h4><p>在不重启集群的情况下添加节点</p><p>1.设置新节点的免密登陆</p><p>2.子节点 hosts添加主机名称</p><p>3.修改主节点上的slaves文件</p><p>4.在新从节点中启动进程，可以单独起，不需要一起服务了（需要配置配置文件）</p><pre><code class="shell">hadoop-daemon.sh start datanode#启动linux04自身datanode</code></pre><p>优点：非暴力，不需要停止服务 </p><p>缺点：改动多，一次上很多会乱</p><p>5.主节点修改<code>hdfs-site</code>配置文件添加<code>dfs.hosts</code>标签</p><p><img src="/cdh/hadoop/hadoop/image-20220601180009026.png" alt="image-20220601180009026"></p><p>6.主节点：在etc&#x2F;hadoop下新建dfs-hosts.conf文件，添加主机名</p><pre><code>Hadoop01Hadoop02Hadoop03Hadoop04</code></pre><p>7.主节点：hadoop dfsadmin -refreshNodes  刷新节点</p><pre><code class="shell">hadoop dfsadmin -refreshNodes  #刷新节点</code></pre><p>8.hadoop dfsadmin -report </p><pre><code class="shell">hadoop dfsadmin -report  #列举所有的从节点(slaves)</code></pre><p>检查：50070 和 8088 端口检查  进行页面访问 </p><h5 id="集群上线节点"><a href="#集群上线节点" class="headerlink" title="集群上线节点"></a>集群上线节点</h5><p>启动datanode </p><p>hadoop-daemon.sh start datanode   </p><p>(只是自身启动, 其它主机并不识别)</p><p>1.在&#x2F;root&#x2F;Downloads&#x2F;hadoop-2.6.5&#x2F;etc&#x2F;hadoop新建dfs-hosts.conf文件，添加所有从节点的主机名</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565741909177.png" alt="1565741909177"></p><p>2.在hdfs-site添加dfs.hosts标签，将文件引入</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565708859953.png" alt="1565708859953"></p><p>3.hadoop dfsadmin  -refreshNodes  刷新节点(在linux01中)</p><p>4.hdfs dfsadmin  -report列举所有的从节点(slaves) </p><p>5.从新节点中启动：start-balancer.sh 均衡当前的hdfs块</p><h5 id="动态添加yarn节点"><a href="#动态添加yarn节点" class="headerlink" title="动态添加yarn节点"></a>动态添加yarn节点</h5><p>1.在&#x2F;root&#x2F;Downloads&#x2F;hadoop-2.6.5&#x2F;etc&#x2F;hadoop下新建include文件</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565761607025.png" alt="1565761607025"></p><p>2.在yarn-site文件中添加yarn.resourcemanager.nodes.include-path标签</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565709061576.png" alt="1565709061576"></p><p>3.Yarn rmadmin -refreshNodes刷新节点</p><h5 id="集群下线节点"><a href="#集群下线节点" class="headerlink" title="集群下线节点"></a>集群下线节点</h5><p>1.在hdfs-site.xml配置文件中配置</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565709227787.png" alt="1565709227787"></p><p>2.exclude文件中添加需要删除的节点</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565742284106.png" alt="1565742284106"></p><p>3.hadoop dfsadmin  -refreshNodes  刷新节点</p><p>4.hdfs dfsadmin -report 列举所有的从节点(slaves)</p><p>5.在节点中关闭进程</p><p>sbin&#x2F;hadoop-daemon.sh stop datanode<br>sbin&#x2F;yarn-daemon.sh stop nodemanager</p><h5 id="动态卸载yarn节点"><a href="#动态卸载yarn节点" class="headerlink" title="动态卸载yarn节点"></a>动态卸载yarn节点</h5><p>1.在&#x2F;root&#x2F;Downloads&#x2F;hadoop-2.6.5&#x2F;etc&#x2F;hadoop新建include文件</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565761658204.png" alt="1565761658204"></p><p>2.在yarn-site.xml文件中添加yarn.resourcemanager.nodes.exclude-path标签</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565761905492.png" alt="1565761905492"></p><p>3.yarn rmadmin -refreshNodes刷新节点 </p><h3 id="NN故障-SNN解决流程"><a href="#NN故障-SNN解决流程" class="headerlink" title="NN故障 SNN解决流程"></a>NN故障 SNN解决流程</h3><p>演示</p><p>1.删除主机的元数据(name nameedits中的<br>current)</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565745283334.png" alt="1565745283334"></p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565745292245.png" alt="1565745292245"></p><p>2.start-dfs.sh启动元数据丢失(namenode进程丢失)</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565745547739.png" alt="1565745547739"></p><p>3.恢复namenode(启动只会读取name和nameedits所以需要恢复)</p><p><img src="/cdh/hadoop/hadoop/Users\Administrator\AppData\Roaming\Typora\typora-user-images\1565745507670.png" alt="1565745507670"></p><p>4.恢复namenode（启动会读取name和nameedits，所有得恢复）</p><p>hadoop namennode -importCheckpoint</p><h2 id="HA（高可用集群）双机集群系统简称"><a href="#HA（高可用集群）双机集群系统简称" class="headerlink" title="HA（高可用集群）双机集群系统简称"></a>HA（高可用集群）双机集群系统简称</h2><p>HDFS通常由两个NameNode组成，一个处于active状态，另一个处于standby状态。Active NameNode对外提供服务，比如处理来自客户端的RPC请求，而Standby NameNode则不对外提供服务，仅同步Active NameNode的状态，以便能够在它失败时快速进行切换。</p><p>一个典型的HA集群，NameNode会被配置在两台独立的机器上，在任何时间上，一个NameNode处于活动状态，而另一个NameNode处于备份状态，活动状态的NameNode会响应集群中所有的客户端，备份状态的NameNode只是作为一个副本，保证在必要的时候提供一个快速的转移。</p><p><a href="https://www.cnblogs.com/luhaojie/p/9236839.html">https://www.cnblogs.com/luhaojie/p/9236839.html</a></p><h3 id="HDFS自动故障转移"><a href="#HDFS自动故障转移" class="headerlink" title="HDFS自动故障转移"></a>HDFS自动故障转移</h3><p>HDFS的自动故障转移主要由Zookeeper（Zookeeper很重要）和ZKFC两个组件组成。</p><h3 id="zookeeper在ha中的作用"><a href="#zookeeper在ha中的作用" class="headerlink" title="zookeeper在ha中的作用"></a>zookeeper在ha中的作用</h3><p>Zookeeper集群作用主要有：</p><p>一：是故障监控。每个NameNode将会和Zookeeper建立一个持久session，如果NameNode失效，那么此session将会过期失效，此后Zookeeper将会通知另一个Namenode，然后触发Failover；</p><p>二：是NameNode选举。ZooKeeper提供了简单的机制来实现Acitve Node选举，如果当前Active失效，Standby将会获取一个特定的排他锁，那么获取锁的Node接下来将会成为Active。</p><p>ZKFC是一个Zookeeper的客户端，，它主要用来监测和管理NameNodes的状态，每个NameNode机器上都会运行一个ZKFC程序，它的职责主要有：一是健康监控。ZKFC间歇性的ping NameNode，得到NameNode返回状态，如果NameNode失效或者不健康，那么ZKFS将会标记其为不健康；二是Zookeeper会话管理。当本地NaneNode运行良好时，ZKFC将会持有一个Zookeeper session，如果本地NameNode为Active，它同时也持有一个“排他锁”znode，如果session过期，那么次lock所对应的znode也将被删除；三是选举。当集群中其中一个NameNode宕机，Zookeeper会自动将另一个激活。</p><h3 id="三台机器的HA环境搭建"><a href="#三台机器的HA环境搭建" class="headerlink" title="三台机器的HA环境搭建"></a>三台机器的HA环境搭建</h3><p>删除hadoop解压包，删除临时位置存放的tmpdir</p><pre><code>//搭建准备映射关系修改 ssh免密登陆 java环境zookeeper安装 环境变量 proflie</code></pre><p>正式配置</p><p>1、hadoop-env.sh</p><pre><code>//解压hadoop安装包后修改配置文件 1、hadoop-env.shexport JAVA_HOME=/root/Downloads/jdk1.8.0_161</code></pre><p>2、core-site.xml</p><pre><code class="xml">&lt;configuration&gt;    &lt;!-- 指定hdfs的nameservice为ns(任取) --&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://ns&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定hadoop临时目录 --&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;/hadoop/tmpdir&lt;/value&gt;    &lt;/property&gt;    &lt;!--流文件的缓冲区单位KB&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt;         &lt;/property&gt; &lt;!-- 指定zookeeper集群的地址 --&gt;    &lt;property&gt;        &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;        &lt;value&gt;linux01:2181,linux02:2181,linux03:2181&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>3、hdfs-site.xml</p><pre><code class="xml">&lt;configuration&gt;    &lt;!--指定hdfs的nameservice为ns，需要和core-site.xml中的保持一致 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.nameservices&lt;/name&gt;        &lt;value&gt;ns&lt;/value&gt;    &lt;/property&gt;    &lt;!-- ns下面有两个NameNode，分别是nn1，nn2 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.namenodes.ns&lt;/name&gt;        &lt;value&gt;nn1, nn2&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn1的RPC通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.ns.nn1&lt;/name&gt;        &lt;value&gt;linux01:9000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn1的http通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.ns.nn1&lt;/name&gt;        &lt;value&gt;linux01:50070&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn2的RPC通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.ns.nn2&lt;/name&gt;        &lt;value&gt;linux02:9000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn2的http通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.ns.nn2&lt;/name&gt;        &lt;value&gt;linux02:50070&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;        &lt;value&gt;qjournal://linux01:8485;linux02:8485;linux03:8485/ns&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;        &lt;value&gt;/root/hadoop/journal&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 开启NameNode失败自动切换 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置失败自动切换实现方式 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider        &lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;        &lt;value&gt;            sshfence            shell(/bin/true)        &lt;/value&gt;    &lt;/property&gt;    &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;        &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置sshfence隔离机制超时时间 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;        &lt;value&gt;30000&lt;/value&gt;    &lt;/property&gt;    &lt;!--设置副本数为2 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;2&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt; </code></pre><p>4、mapred-site.xml</p><pre><code class="xml">&lt;configuration&gt;    &lt;!-- 指定mr框架为yarn方式 --&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;yarn&lt;/value&gt;    &lt;/property&gt;    &lt;!--map任务内存大小，默认1G --&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;        &lt;value&gt;230&lt;/value&gt;    &lt;/property&gt;    &lt;!--reduce任务内存大小，默认1G --&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;        &lt;value&gt;460&lt;/value&gt;    &lt;/property&gt;    &lt;!--map任务运行的JVM进程内存大小,默认-Xmx200M --&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;        &lt;value&gt;-Xmx184m&lt;/value&gt;    &lt;/property&gt;    &lt;!--reduce任务运行的JVM进程内存,默认-Xmx200M --&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;        &lt;value&gt;-Xmx368m&lt;/value&gt;    &lt;/property&gt;    &lt;!--MR AppMaster运行需要内存，默认1536M --&gt;    &lt;property&gt;        &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt;        &lt;value&gt;460&lt;/value&gt;    &lt;/property&gt;    &lt;!--MR AppMaster运行的JVM进程内存，默认-Xmx1024m --&gt;    &lt;property&gt;        &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt;        &lt;value&gt;-Xmx368m&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>5.yarn-site.xml</p><pre><code class="xml">&lt;configuration&gt;    &lt;!-- 分别指定RM的地址 --&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;        &lt;value&gt;linux03&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定zk集群地址 --&gt;    &lt;property&gt;        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;        &lt;value&gt;linux01:2181,linux02:2181,linux03:2181&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    &lt;/property&gt;    &lt;!--RM中分配容器的内存最小值，默认1G --&gt;    &lt;property&gt;        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;        &lt;value&gt;230&lt;/value&gt;    &lt;/property&gt;    &lt;!--RM中分配容器的内存最大值，默认8G --&gt;    &lt;property&gt;        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;        &lt;value&gt;700&lt;/value&gt;    &lt;/property&gt;    &lt;!--可用物理内存大小,默认8G --&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;        &lt;value&gt;700&lt;/value&gt;    &lt;/property&gt;    &lt;!--虚拟内存检查是否开始 --&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;        &lt;value&gt;false&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt; </code></pre><p>6、slaves</p><pre><code>linux01linux02linux03</code></pre><p>7、更改zoo.cfg和myid</p><pre><code>//重命名/root/Downloads/zookeeper/conf/zoo_sample.cfg文件为zoo.cfg#修改dataDir=/root/zookeeper-3.4.12/datadataLogDir=/root/zookeeper-3.4.12/datalog#末尾添加(内网IP)server.1=hadoop1:2888:3888       server.2=hadoop2:2888:3888server.3=hadoop3:2888:3888</code></pre><h3 id="搭建完成启动集群"><a href="#搭建完成启动集群" class="headerlink" title="搭建完成启动集群"></a>搭建完成启动集群</h3><p>在三台节点上全部启动zookeeper</p><pre><code>//启动zookeeperzkServer.sh start//查看当前状态zkServer.sh status</code></pre><p>在linux01上启动journalnode进程</p><pre><code>//三个节点全部多了journalNode进程hadoop-daemons.sh start journalnode</code></pre><p>在linux01上格式化hdfs，根据core-site.xml中的位置来生成tmpdir文件夹</p><pre><code class="oz">//格式化hdfshdfs namenode -format//将生成的tmpdir文件夹发送给其他节点scp -r tmpdir root@主机名:发送的位置</code></pre><p>在linux1格式化ZK</p><pre><code>hdfs zkfc -formatZK</code></pre><p>linux01上启动hdfs</p><pre><code>//启动HDFSstart-dfs.sh</code></pre><p>linux03启动yarn</p><pre><code>start-yarn.sh</code></pre><h3 id="启动完成后的结果"><a href="#启动完成后的结果" class="headerlink" title="启动完成后的结果"></a>启动完成后的结果</h3><pre><code>#hadoop1和hadoop2root@hadoop1:~/hadoop-2.8.4# jpsNodeManagerJournalNodeJpsDataNodeDFSZKFailoverControllerNameNodeQuorumPeerMain#hadoop3root@hadoop3:~# jpsQuorumPeerMainResourceManagerDataNodeJpsJournalNodeNodeManager</code></pre><h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><p>安装Hadoop需要修改多少个文件： 8个 4个xml 以及对应的env + slaves </p><p>修改8个配置文件</p><p>hdfs-site.xml  yarn-site.xml hadoop-env.sh core-site.xml </p><p>yarn-env.sh mapred-site.xml  slaves  mapred-env.sh </p><p>1）集群规划：</p><table><thead><tr><th></th><th>服务器61</th><th>服务器62</th><th>服务器63</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode  DataNode</td><td>DataNode</td><td>DataNode  SecondaryNameNode</td></tr><tr><td>Yarn</td><td>NodeManager</td><td>Resourcemanager  NodeManager</td><td>NodeManager</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadopp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>别人的知识点整理</title>
      <link href="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/"/>
      <url>/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/</url>
      
        <content type="html"><![CDATA[<p>Hive on Spark参数调优姿势小结</p><p>本文简单列举一些调优项。为了符合实际情况，Spark也采用on YARN部署方式来说明。</p><pre><code class="python">Driver参数spark.driver.cores该参数表示每个Executor可利用的CPU核心数。其值不宜设定过大，因为Hive的底层以HDFS存储，而HDFS有时对高并发写入处理不太好，容易造成race condition。根据我们的实践，设定在3~6之间比较合理。假设我们使用的服务器单节点有32个CPU核心可供使用。考虑到系统基础服务和HDFS等组件的余量，一般会将YARN NodeManager的yarn.nodemanager.resource.cpu-vcores参数设为28，也就是YARN能够利用其中的28核，此时将spark.executor.cores设为4最合适，最多可以正好分配给7个Executor而不造成浪费。又假设yarn.nodemanager.resource.cpu-vcores为26，那么将spark.executor.cores设为5最合适，只会剩余1个核。由于一个Executor需要一个YARN Container来运行，所以还需保证spark.executor.cores的值不能大于单个Container能申请到的最大核心数，即yarn.scheduler.maximum-allocation-vcores的值。</code></pre><pre><code class="python">spark.executor.memory/spark.yarn.executor.memoryOverhead这两个参数分别表示每个Executor可利用的堆内内存量和堆外内存量。堆内内存越大，Executor就能缓存更多的数据，在做诸如map join之类的操作时就会更快，但同时也会使得GC变得更麻烦。Hive官方提供了一个计算Executor总内存量的经验公式，如下：yarn.nodemanager.resource.memory-mb * (spark.executor.cores / yarn.nodemanager.resource.cpu-vcores)其实就是按核心数的比例分配。在计算出来的总内存量中，80%~85%划分给堆内内存，剩余的划分给堆外内存。假设集群中单节点有128G物理内存，yarn.nodemanager.resource.memory-mb（即单个NodeManager能够利用的主机内存量）设为120G，那么总内存量就是：120 * 1024 * (4 / 28) ≈ 17554MB。再按8:2比例划分的话，最终spark.executor.memory设为约13166MB，spark.yarn.executor.memoryOverhead设为约4389MB。与上一节同理，这两个内存参数相加的总量也不能超过单个Container最多能申请到的内存量，即yarn.scheduler.maximum-allocation-mb。</code></pre><pre><code class="python">spark.executor.instances该参数表示执行查询时一共启动多少个Executor实例，这取决于每个节点的资源分配情况以及集群的节点数。若我们一共有10台32C/128G的节点，并按照上述配置（即每个节点承载7个Executor），那么理论上讲我们可以将spark.executor.instances设为70，以使集群资源最大化利用。但是实际上一般都会适当设小一些（推荐是理论值的一半左右），因为Driver也要占用资源，并且一个YARN集群往往还要承载除了Hive on Spark之外的其他业务。spark.dynamicAllocation.enabled上面所说的固定分配Executor数量的方式可能不太灵活，尤其是在Hive集群面向很多用户提供分析服务的情况下。所以更推荐将spark.dynamicAllocation.enabled参数设为true，以启用Executor动态分配。</code></pre><pre><code class="python">Driver参数spark.driver.cores该参数表示每个Driver可利用的CPU核心数。绝大多数情况下设为1都够用。spark.driver.memory/spark.driver.memoryOverhead这两个参数分别表示每个Driver可利用的堆内内存量和堆外内存量。根据资源富余程度和作业的大小，一般是将总量控制在512MB~4GB之间，并且沿用Executor内存的“二八分配方式”。例如，spark.driver.memory可以设为约819MB，spark.driver.memoryOverhead设为约205MB，加起来正好1G。</code></pre><pre><code class="python">Hive参数绝大部分Hive参数的含义和调优方法都与on MR时相同，但仍有两个需要注意。hive.auto.convert.join.noconditionaltask.size我们知道，当Hive中做join操作的表有一方是小表时，如果hive.auto.convert.join和hive.auto.convert.join.noconditionaltask开关都为true（默认即如此），就会自动转换成比较高效的map-side join。而hive.auto.convert.join.noconditionaltask.size这个参数就是map join转化的阈值，在Hive on MR下默认为10MB。但是Hive on MR下统计表的大小时，使用的是数据在磁盘上存储的近似大小，而Hive on Spark下则改用在内存中存储的近似大小。由于HDFS上的数据很有可能被压缩或序列化，使得大小减小，所以由MR迁移到Spark时要适当调高这个参数，以保证map join正常转换。一般会设为100~200MB左右，如果内存充裕，可以更大点。hive.merge.sparkfiles小文件是HDFS的天敌，所以Hive原生提供了合并小文件的选项，在on  MR时是hive.merge.mapredfiles，但是on Spark时会改成hive.merge.sparkfiles，注意要把这个参数设为true。至于小文件合并的阈值参数，即hive.merge.smallfiles.avgsize与hive.merge.size.per.task都没有变化。</code></pre><h2 id="Hive性能优化"><a href="#Hive性能优化" class="headerlink" title="Hive性能优化"></a><strong>Hive性能优化</strong></h2><h4 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a><strong>1.介绍</strong></h4><p>首先，我们来看看Hadoop的计算框架特性，在此特性下会衍生哪些问题？</p><ul><li>数据量大不是问题，数据倾斜是个问题。</li><li>jobs数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次汇总，产生十几个jobs，耗时很长。原因是map reduce作业初始化的时间是比较长的。</li><li>sum,count,max,min等UDAF，不怕数据倾斜问题,hadoop在map端的汇总合并优化，使数据倾斜不成问题。</li><li>count(distinct ),在数据量大的情况下，效率较低，如果是多count(distinct )效率更低，因为count(distinct)是按group by 字段分组，按distinct字段排序，一般这种分布方式是很倾斜的。举个例子：比如男uv,女uv，像淘宝一天30亿的pv，如果按性别分组，分配2个reduce,每个reduce处理15亿数据。</li></ul><p>面对这些问题，我们能有哪些有效的优化手段呢？下面列出一些在工作有效可行的优化手段：</p><ul><li>好的模型设计事半功倍。</li><li>解决数据倾斜问题。</li><li>减少job数。</li><li>设置合理的map reduce的task数，能有效提升性能。(比如，10w+级别的计算，用160个reduce，那是相当的浪费，1个足够)。</li><li>了解数据分布，自己动手解决数据倾斜问题是个不错的选择。set hive.groupby.skewindata&#x3D;true;这是通用的算法优化，但算法优化有时不能适应特定业务背景，开发人员了解业务，了解数据，可以通过业务逻辑精确有效的解决数据倾斜问题。</li><li>数据量较大的情况下，慎用count(distinct)，count(distinct)容易产生倾斜问题。</li><li>对小文件进行合并，是行至有效的提高调度效率的方法，假如所有的作业设置合理的文件数，对云梯的整体调度效率也会产生积极的正向影响。</li><li>优化时把握整体，单个作业最优不如整体最优。</li></ul><p>而接下来，我们心中应该会有一些疑问，影响性能的根源是什么？</p><h4 id="2-性能低下的根源"><a href="#2-性能低下的根源" class="headerlink" title="2.性能低下的根源"></a><strong>2.性能低下的根源</strong></h4><p>hive性能优化时，把HiveQL当做M&#x2F;R程序来读，即从M&#x2F;R的运行角度来考虑优化性能，从更底层思考如何优化运算性能，而不仅仅局限于逻辑代码的替换层面。</p><p>RAC（Real Application Cluster）真正应用集群就像一辆机动灵活的小货车，响应快；Hadoop就像吞吐量巨大的轮船，启动开销大，如果每次只做小数量的输入输出，利用率将会很低。所以用好Hadoop的首要任务是增大每次任务所搭载的数据量。</p><p>Hadoop的核心能力是parition和sort，因而这也是优化的根本。</p><p>观察Hadoop处理数据的过程，有几个显著的特征：</p><ul><li>数据的大规模并不是负载重点，造成运行压力过大是因为运行数据的倾斜。</li><li>jobs数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联对此汇总，产生几十个jobs，将会需要30分钟以上的时间且大部分时间被用于作业分配，初始化和数据输出。M&#x2F;R作业初始化的时间是比较耗时间资源的一个部分。</li><li>在使用SUM，COUNT，MAX，MIN等UDAF函数时，不怕数据倾斜问题，Hadoop在Map端的汇总合并优化过，使数据倾斜不成问题。</li><li>COUNT(DISTINCT)在数据量大的情况下，效率较低，如果多COUNT(DISTINCT)效率更低，因为COUNT(DISTINCT)是按GROUP BY字段分组，按DISTINCT字段排序，一般这种分布式方式是很倾斜的；比如：男UV，女UV，淘宝一天30亿的PV，如果按性别分组，分配2个reduce,每个reduce处理15亿数据。</li><li>数据倾斜是导致效率大幅降低的主要原因，可以采用多一次 Map&#x2F;Reduce 的方法， 避免倾斜。</li></ul><p>最后得出的结论是：避实就虚，用 job 数的增加，输入量的增加，占用更多存储空间，充分利用空闲 CPU 等各种方法，分解数据倾斜造成的负担。</p><h4 id="3-配置角度优化"><a href="#3-配置角度优化" class="headerlink" title="3.配置角度优化"></a><strong>3.配置角度优化</strong></h4><p>我们知道了性能低下的根源，同样，我们也可以从Hive的配置解读去优化。Hive系统内部已针对不同的查询预设定了优化方法，用户可以通过调整配置进行控制， 以下举例介绍部分优化的策略以及优化控制选项。</p><pre><code class="python">#### 3.1列裁剪**Hive 在读数据的时候，可以只读取查询中所需要用到的列，而忽略其它列。例如，若有以下查询：SELECT a,b FROM q WHERE e&lt;10;在实施此项查询中，Q 表有 5 列（a，b，c，d，e），Hive 只读取查询逻辑中真实需要 的 3 列 a、b、e，而忽略列 c，d；这样做节省了读取开销，中间表存储开销和数据整合开销。裁剪所对应的参数项为：hive.optimize.cp=true（默认值为真）#### **3.2分区裁剪**可以在查询的过程中减少不必要的分区。例如，若有以下查询：SELECT * FROM (SELECTT a1,COUNT(1) FROM T GROUP BY a1) subq WHERE subq.prtn=100; #（多余分区）SELECT * FROM T1 JOIN (SELECT * FROM T2) subq ON (T1.a1=subq.a2) WHERE subq.prtn=100;查询语句若将“subq.prtn=100”条件放入子查询中更为高效，可以减少读入的分区 数目。Hive 自动执行这种裁剪优化。分区参数为：hive.optimize.pruner=true（默认值为真）#### **3.3JOIN操作**在编写带有 join 操作的代码语句时，应该将条目少的表/子查询放在 Join 操作符的左边。因为在 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，载入条目较少的表 可以有效减少 OOM（out of memory）即内存溢出。所以对于同一个 key 来说，对应的 value 值小的放前，大的放后，这便是“小表放前”原则。若一条语句中有多个 Join，依据 Join 的条件相同与否，有不同的处理方法。#### **3.3.1JOIN原则**在使用写有 Join 操作的查询语句时有一条原则：应该将条目少的表/子查询放在 Join 操作符的左边。原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率。对于一条语句中有多个 Join 的情况，如果 Join 的条件相同，比如查询：INSERT OVERWRITE TABLE pv_usersSELECT pv.pageid, u.age FROM page_view pJOIN user u ON (pv.userid = u.userid)JOIN newuser x ON (u.userid = x.userid);- 如果 Join 的 key 相同，不管有多少个表，都会则会合并为一个 Map-Reduce- 一个 Map-Reduce 任务，而不是 ‘n’ 个- 在做 OUTER JOIN 的时候也是一样如果 Join 的条件不相同，比如：INSERT OVERWRITE TABLE pv_usersSELECT pv.pageid, u.age FROM page_view pJOIN user u ON (pv.userid = u.userid)JOIN newuser x on (u.age = x.age);Map-Reduce 的任务数目和 Join 操作的数目是对应的，上述查询和以下查询是等价的：INSERT OVERWRITE TABLE tmptableSELECT * FROM page_view p JOIN user uON (pv.userid = u.userid);INSERT OVERWRITE TABLE pv_usersSELECT x.pageid, x.age FROM tmptable xJOIN newuser y ON (x.age = y.age);#### **3.4MAP JOIN操作**Join 操作在 Map 阶段完成，不再需要Reduce，前提条件是需要的数据在 Map 的过程中可以访问到。比如查询：INSERT OVERWRITE TABLE pv_usersSELECT /*+ MAPJOIN(pv) */ pv.pageid, u.ageFROM page_view pvJOIN user u ON (pv.userid = u.userid);可以在 Map 阶段完成 Join.相关的参数为：- **hive.join.emit.interval = 1000**- **hive.mapjoin.size.key = 10000**- **hive.mapjoin.cache.numrows = 10000**#### **3.5GROUP BY操作**进行GROUP BY操作时需要注意一下几点：- **Map端部分聚合**事实上并不是所有的聚合操作都需要在reduce部分进行，很多聚合操作都可以先在Map端进行部分聚合，然后reduce端得出最终结果。这里需要修改的参数为：hive.map.aggr=true（用于设定是否在 map 端进行聚合，默认值为真） hive.groupby.mapaggr.checkinterval=100000（用于设定 map 端进行聚合操作的条目数）- **有数据倾斜时进行负载均衡**此处需要设定 hive.groupby.skewindata，当选项设定为 true 是，生成的查询计划有两 个 MapReduce 任务。在第一个 MapReduce 中，map 的输出结果集合会随机分布到 reduce 中， 每个 reduce 做部分聚合操作，并输出结果。这样处理的结果是，相同的 Group By Key 有可 能分发到不同的 reduce 中，从而达到负载均衡的目的；第二个 MapReduce 任务再根据预处 理的数据结果按照 Group By Key 分布到 reduce 中（这个过程可以保证相同的 Group By Key 分布到同一个 reduce 中），最后完成最终的聚合操作。#### **3.6合并小文件**我们知道文件数目小，容易在文件存储端造成瓶颈，给 HDFS 带来压力，影响处理效率。对此，可以通过合并Map和Reduce的结果文件来消除这样的影响。用于设置合并属性的参数有：- 是否合并Map输出文件：hive.merge.mapfiles=true（默认值为真）- 是否合并Reduce 端输出文件：hive.merge.mapredfiles=false（默认值为假）- 合并文件的大小：hive.merge.size.per.task=256*1000*1000（默认值为 256000000）</code></pre><h4 id="4-程序角度优化"><a href="#4-程序角度优化" class="headerlink" title="4.程序角度优化"></a><strong>4.程序角度优化</strong></h4><pre><code class="python">#### 4.1熟练使用SQL提高查询**熟练地使用 SQL，能写出高效率的查询语句。场景：有一张 user 表，为卖家每天收到表，user_id，ds（日期）为 key，属性有主营类目，指标有交易金额，交易笔数。每天要取前10天的总收入，总笔数，和最近一天的主营类目。 **解决方法 1**如下所示：常用方法INSERT OVERWRITE TABLE t1SELECT user_id,substr(MAX(CONCAT(ds,cat),9) AS main_cat) FROM usersWHERE ds=20120329 // 20120329 为日期列的值，实际代码中可以用函数表示出当天日期 GROUP BY user_id;INSERT OVERWRITE TABLE t2SELECT user_id,sum(qty) AS qty,SUM(amt) AS amt FROM usersWHERE ds BETWEEN 20120301 AND 20120329GROUP BY user_idSELECT t1.user_id,t1.main_cat,t2.qty,t2.amt FROM t1JOIN t2 ON t1.user_id=t2.user_id下面给出方法1的思路，实现步骤如下：第一步：利用分析函数，取每个 user_id 最近一天的主营类目，存入临时表 t1。第二步：汇总 10 天的总交易金额，交易笔数，存入临时表 t2。第三步：关联 t1，t2，得到最终的结果。**解决方法 2**如下所示：优化方法SELECT user_id,substr(MAX(CONCAT(ds,cat)),9) AS main_cat,SUM(qty),SUM(amt) FROM usersWHERE ds BETWEEN 20120301 AND 20120329GROUP BY user_id在工作中我们总结出：方案 2 的开销等于方案 1 的第二步的开销，性能提升，由原有的 25 分钟完成，缩短为 10 分钟以内完成。节省了两个临时表的读写是一个关键原因，这种方式也适用于 Oracle 中的数据查找工作。SQL 具有普适性，很多 SQL 通用的优化方案在 Hadoop 分布式计算方式中也可以达到效果。#### **4.2无效ID在关联时的数据倾斜问题**问题：日志中常会出现信息丢失，比如每日约为 20 亿的全网日志，其中的 user_id 为主 键，在日志收集过程中会丢失，出现主键为 null 的情况，如果取其中的 user_id 和 bmw_users 关联，就会碰到数据倾斜的问题。原因是 Hive 中，主键为 null 值的项会被当做相同的 Key 而分配进同一个计算 Map。**解决方法 1：user_id 为空的不参与关联，子查询过滤 null**SELECT * FROM log aJOIN bmw_users b ON a.user_id IS NOT NULL AND a.user_id=b.user_idUNION All SELECT * FROM log a WHERE a.user_id IS NULL**解决方法 2 如下所示：函数过滤 null**SELECT * FROM log a LEFT OUTERJOIN bmw_users b ONCASE WHEN a.user_id IS NULL THEN CONCAT(‘dp_hive’,RAND()) ELSE a.user_id END =b.user_id;调优结果：原先由于数据倾斜导致运行时长超过 1 小时，解决方法 1 运行每日平均时长 25 分钟，解决方法 2 运行的每日平均时长在 20 分钟左右。优化效果很明显。我们在工作中总结出：解决方法2比解决方法1效果更好，不但IO少了，而且作业数也少了。解决方法1中log读取两次，job 数为2。解决方法2中 job 数是1。这个优化适合无效 id（比如-99、 ‘’，null 等）产生的倾斜问题。把空值的 key 变成一个字符串加上随机数，就能把倾斜的 数据分到不同的Reduce上，从而解决数据倾斜问题。因为空值不参与关联，即使分到不同 的 Reduce 上，也不会影响最终的结果。附上 Hadoop 通用关联的实现方法是：关联通过二次排序实现的，关联的列为 partion key，关联的列和表的 tag 组成排序的 group key，根据 pariton key分配Reduce。同一Reduce内根据group key排序。#### **4.3不同数据类型关联产生的倾斜问题**问题：不同数据类型 id 的关联会产生数据倾斜问题。一张表 s8 的日志，每个商品一条记录，要和商品表关联。但关联却碰到倾斜的问题。s8 的日志中有 32 为字符串商品 id，也有数值商品 id，日志中类型是 string 的，但商品中的 数值 id 是 bigint 的。猜想问题的原因是把 s8 的商品 id 转成数值 id 做 hash 来分配 Reduce， 所以字符串 id 的 s8 日志，都到一个 Reduce 上了，解决的方法验证了这个猜测。**解决方法：把数据类型转换成字符串类型**SELECT * FROM s8_log a LEFT OUTERJOIN r_auction_auctions b ON a.auction_id=CASE(b.auction_id AS STRING)调优结果显示：数据表处理由 1 小时 30 分钟经代码调整后可以在 20 分钟内完成。#### **4.4利用Hive对UNION ALL优化的特性**多表 union all 会优化成一个 job。问题：比如推广效果表要和商品表关联，效果表中的 auction_id 列既有 32 为字符串商 品 id，也有数字 id，和商品表关联得到商品的信息。解决方法：Hive SQL 性能会比较好SELECT * FROM effect aJOIN(SELECT auction_id AS auction_id FROM auctionsUNION AllSELECT auction_string_id AS auction_id FROM auctions) bON a.auction_id=b.auction_id比分别过滤数字 id，字符串 id 然后分别和商品表关联性能要好。这样写的好处：1 个 MapReduce 作业，商品表只读一次，推广效果表只读取一次。把 这个 SQL 换成 Map/Reduce 代码的话，Map 的时候，把 a 表的记录打上标签 a，商品表记录 每读取一条，打上标签 b，变成两个&lt;key,value&gt;对，&lt;(b,数字 id),value&gt;，&lt;(b,字符串 id),value&gt;。所以商品表的 HDFS 读取只会是一次。#### **4.5解决Hive对UNION ALL优化的短板**Hive 对 union all 的优化的特性：对 union all 优化只局限于非嵌套查询。-**消灭子查询内的 group by**示例 1：子查询内有 group bySELECT * FROM(SELECT * FROM t1 GROUP BY c1,c2,c3 UNION ALL SELECT * FROM t2 GROUP BY c1,c2,c3)t3GROUP BY c1,c2,c3从业务逻辑上说，子查询内的 GROUP BY 怎么都看显得多余（功能上的多余，除非有 COUNT(DISTINCT)），如果不是因为 Hive Bug 或者性能上的考量（曾经出现如果不执行子查询 GROUP BY，数据得不到正确的结果的 Hive Bug）。所以这个 Hive 按经验转换成如下所示：SELECT * FROM (SELECT * FROM t1 UNION ALL SELECT * FROM t2)t3 GROUP BY c1,c2,c3调优结果：经过测试，并未出现 union all 的 Hive Bug，数据是一致的。MapReduce 的 作业数由 3 减少到 1。t1 相当于一个目录，t2 相当于一个目录，对 Map/Reduce 程序来说，t1，t2 可以作为 Map/Reduce 作业的 mutli inputs。这可以通过一个 Map/Reduce 来解决这个问题。Hadoop 的 计算框架，不怕数据多，就怕作业数多。但如果换成是其他计算平台如 Oracle，那就不一定了，因为把大的输入拆成两个输入， 分别排序汇总后 merge（假如两个子排序是并行的话），是有可能性能更优的（比如希尔排 序比冒泡排序的性能更优）。- **消灭子查询内的 COUNT(DISTINCT)，MAX，MIN。**SELECT * FROM(SELECT * FROM t1UNION ALL SELECT c1,c2,c3 COUNT(DISTINCT c4) FROM t2 GROUP BY c1,c2,c3) t3GROUP BY c1,c2,c3;由于子查询里头有 COUNT(DISTINCT)操作，直接去 GROUP BY 将达不到业务目标。这时采用 临时表消灭 COUNT(DISTINCT)作业不但能解决倾斜问题，还能有效减少 jobs。INSERT t4 SELECT c1,c2,c3,c4 FROM t2 GROUP BY c1,c2,c3;SELECT c1,c2,c3,SUM(income),SUM(uv) FROM(SELECT c1,c2,c3,income,0 AS uv FROM t1UNION ALLSELECT c1,c2,c3,0 AS income,1 AS uv FROM t2) t3GROUP BY c1,c2,c3;job 数是 2，减少一半，而且两次 Map/Reduce 比 COUNT(DISTINCT)效率更高。调优结果：千万级别的类目表，member 表，与 10 亿级得商品表关联。原先 1963s 的任务经过调整，1152s 即完成。- **消灭子查询内的 JOIN**SELECT * FROM(SELECT * FROM t1 UNION ALL SELECT * FROM t4 UNION ALL SELECT * FROM t2 JOIN t3 ON t2.id=t3.id) xGROUP BY c1,c2;上面代码运行会有 5 个 jobs。加入先 JOIN 生存临时表的话 t5，然后 UNION ALL，会变成 2 个 jobs。INSERT OVERWRITE TABLE t5SELECT * FROM t2 JOIN t3 ON t2.id=t3.id;SELECT * FROM (t1 UNION ALL t4 UNION ALL t5);调优结果显示：针对千万级别的广告位表，由原先 5 个 Job 共 15 分钟，分解为 2 个 job 一个 8-10 分钟，一个3分钟。**4.6GROUP BY替代COUNT(DISTINCT)达到优化效果**计算 uv 的时候，经常会用到 COUNT(DISTINCT)，但在数据比较倾斜的时候 COUNT(DISTINCT) 会比较慢。这时可以尝试用 GROUP BY 改写代码计算 uv。- **原有代码**INSERT OVERWRITE TABLE s_dw_tanx_adzone_uv PARTITION (ds=20120329)SELECT 20120329 AS thedate,adzoneid,COUNT(DISTINCT acookie) AS uv FROM s_ods_log_tanx_pv t WHERE t.ds=20120329 GROUP BY adzoneid关于COUNT(DISTINCT)的数据倾斜问题不能一概而论，要依情况而定，下面是我测试的一组数据：测试数据：169857条**\#统计每日IP**CREATE TABLE ip_2014_12_29 AS SELECT COUNT(DISTINCT ip) AS IP FROM logdfs WHERE logdate=’2014_12_29′;耗时：24.805 seconds**\#统计每日IP（改造）**CREATE TABLE ip_2014_12_29 AS SELECT COUNT(1) AS IP FROM (SELECT DISTINCT ip from logdfs WHERE logdate=’2014_12_29′) tmp;耗时：46.833 seconds测试结果表名：明显改造后的语句比之前耗时，这是因为改造后的语句有2个SELECT，多了一个job，这样在数据量小的时候，数据不会存在倾斜问题。**5.优化总结**</code></pre><h4 id="5-优化总结"><a href="#5-优化总结" class="headerlink" title="5.优化总结"></a><strong>5.优化总结</strong></h4><pre><code class="python">优化时，把hive sql当做mapreduce程序来读，会有意想不到的惊喜。理解hadoop的核心能力，是hive优化的根本。这是这一年来，项目组所有成员宝贵的经验总结。长期观察hadoop处理数据的过程，有几个显著的特征:不怕数据多，就怕数据倾斜。对jobs数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次汇总，产生十几个jobs，没半小时是跑不完的。map reduce作业初始化的时间是比较长的。对sum，count来说，不存在数据倾斜问题。对count(distinct ),效率较低，数据量一多，准出问题，如果是多count(distinct )效率更低。优化可以从几个方面着手：好的模型设计事半功倍。解决数据倾斜问题。减少job数。设置合理的map reduce的task数，能有效提升性能。(比如，10w+级别的计算，用160个reduce，那是相当的浪费，1个足够)。自己动手写sql解决数据倾斜问题是个不错的选择。set hive.groupby.skewindata=true;这是通用的算法优化，但算法优化总是漠视业务，习惯性提供通用的解决方法。Etl开发人员更了解业务，更了解数据，所以通过业务逻辑解决倾斜的方法往往更精确，更有效。对count(distinct)采取漠视的方法，尤其数据大的时候很容易产生倾斜问题，不抱侥幸心理。自己动手，丰衣足食。对小文件进行合并，是行至有效的提高调度效率的方法，假如我们的作业设置合理的文件数，对云梯的整体调度效率也会产生积极的影响。优化时把握整体，单个作业最优不如整体最优。</code></pre><h4 id="6-优化的常用手段"><a href="#6-优化的常用手段" class="headerlink" title="6.优化的常用手段"></a><strong>6.优化的常用手段</strong></h4><pre><code class="python">主要由三个属性来决定：hive.exec.reducers.bytes.per.reducer   ＃这个参数控制一个job会有多少个reducer来处理，依据的是输入文件的总大小。默认1GB。hive.exec.reducers.max    ＃这个参数控制最大的reducer的数量， 如果 input / bytes per reduce &gt; max  则会启动这个参数所指定的reduce个数。  这个并不会影响mapre.reduce.tasks参数的设置。默认的max是999。mapred.reduce.tasks ＃这个参数如果指定了，hive就不会用它的estimation函数来自动计算reduce的个数，而是用这个参数来启动reducer。默认是-1。6.1参数设置的影响如果reduce太少：如果数据量很大，会导致这个reduce异常的慢，从而导致这个任务不能结束，也有可能会OOM 2、如果reduce太多：  产生的小文件太多，合并起来代价太高，namenode的内存占用也会增大。如果我们不指定mapred.reduce.tasks， hive会自动计算需要多少个reducer。</code></pre><h4 id="7-常用函数"><a href="#7-常用函数" class="headerlink" title="7.常用函数"></a><strong>7.常用函数</strong></h4><pre><code class="python">## 1、LIKE比较: LIKE语法: A LIKE B操作类型: strings描述: 如果字符串A或者字符串B为NULL，则返回NULL；如果字符串A符合表达式B 的正则语法，则为TRUE；否则为FALSE。B中字符”_”表示任意单个字符，而字符”%”表示任意数量的字符。hive&gt; select 1 from iteblog where &#39;football&#39; like &#39;foot%&#39;;1hive&gt; select 1 from iteblog where &#39;football&#39; like &#39;foot____&#39;;1&lt;strong&gt;注意：否定比较时候用NOT A LIKE B&lt;/strong&gt;hive&gt; select 1 from iteblog where NOT &#39;football&#39; like &#39;fff%&#39;;1## 2、REGEXP操作: REGEXP语法: A REGEXP B操作类型: strings描述: 功能与RLIKE相同hive&gt; select 1 from iteblog where &#39;footbar&#39; REGEXP &#39;^f.*r$&#39;;1## 3.INSTR 操作语法：INSTR(str1,str2)操作类型strings描述： 返回匹配的字母的索引值。hive&gt; SELECT INSTR(&#39;MySQL INSTR&#39;, &#39;SQL&#39;); 包含数据默认从1 开始3hive&gt; SELECT INSTR(&#39;MySQL INSTR&#39;, &#39;sql&#39;); 等于零证明里面没有数据0hive&gt; hive&gt; SELECT INSTR(&#39;MySQL INSTR&#39;, &#39;sql&#39;); 匹配到首位从1 开始1## 数据运算：# 1、加法操作: +语法: A + B操作类型：所有数值类型说明：返回A与B相加的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。比如，int + int 一般结果为int类型，而 int + double 一般结果为double类型hive&gt; select 1 + 9 from iteblog;10hive&gt; create table iteblog as select 1 + 1.2 from iteblog;hive&gt; describe iteblog;_c0     double# 2、减法操作: -语法: A – B操作类型：所有数值类型说明：返回A与B相减的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。比如，int – int 一般结果为int类型，而 int – double 一般结果为double类型hive&gt; select 10 – 5 from iteblog;5hive&gt; create table iteblog as select 5.6 – 4 from iteblog;hive&gt; describe iteblog;_c0     double# 3、乘法操作: *语法: A * B操作类型：所有数值类型说明：返回A与B相乘的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。注意，如果A乘以B的结果超过默认结果类型的数值范围，则需要通过cast将结果转换成范围更大的数值类型hive&gt; select 40 * 5 from iteblog;200# 4、除法操作: /语法: A / B操作类型：所有数值类型说明：返回A除以B的结果。结果的数值类型为doublehive&gt; select 40 / 5 from iteblog;8.0注意：hive中最高精度的数据类型是double,只精确到小数点后16位，在做除法运算的时候要特别注意hive&gt;select ceil(28.0/6.999999999999999999999) from iteblog limit 1;   结果为4hive&gt;select ceil(28.0/6.99999999999999) from iteblog limit 1;          结果为5# 5、取余操作: %语法: A % B操作类型：所有数值类型说明：返回A除以B的余数。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。hive&gt; select 41 % 5 from iteblog;1hive&gt; select 8.4 % 4 from iteblog;0.40000000000000036&lt;strong&gt;注意&lt;/strong&gt;：精度在hive中是个很大的问题，类似这样的操作最好通过round指定精度hive&gt; select round(8.4 % 4 , 2) from iteblog;0.4# 6、位与操作: &amp;语法: A &amp; B操作类型：所有数值类型说明：返回A和B按位进行与操作的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。hive&gt; select 4 &amp; 8 from iteblog;0hive&gt; select 6 &amp; 4 from iteblog;4# 7、位或操作: |语法: A | B操作类型：所有数值类型说明：返回A和B按位进行或操作的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。hive&gt; select 4 | 8 from iteblog;12hive&gt; select 6 | 8 from iteblog;14# 8、位异或操作: ^语法: A ^ B操作类型：所有数值类型说明：返回A和B按位进行异或操作的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。hive&gt; select 4 ^ 8 from iteblog;12hive&gt; select 6 ^ 4 from iteblog;2# 9．位取反操作: ~语法: ~A操作类型：所有数值类型说明：返回A按位取反操作的结果。结果的数值类型等于A的类型。hive&gt; select ~6 from iteblog;-7hive&gt; select ~4 from iteblog;-5# 逻辑运算：## 1、逻辑与操作: AND语法: A AND B操作类型：boolean说明：如果A和B均为TRUE，则为TRUE；否则为FALSE。如果A为NULL或B为NULL，则为NULLhive&gt; select 1 from iteblog where 1=1 and 2=2;1## 2、逻辑或操作: OR语法: A OR B操作类型：boolean说明：如果A为TRUE，或者B为TRUE，或者A和B均为TRUE，则为TRUE；否则为FALSEhive&gt; select 1 from iteblog where 1=2 or 2=2;1## 3、逻辑非操作: NOT语法: NOT A操作类型：boolean说明：如果A为FALSE，或者A为NULL，则为TRUE；否则为FALSEhive&gt; select 1 from iteblog where not 1=2;1## 数值计算：## 1、取整函数: round语法: round(double a)返回值: BIGINT说明: 返回double类型的整数值部分 （遵循四舍五入）hive&gt; select round(3.1415926) from iteblog;3hive&gt; select round(3.5) from iteblog;4hive&gt; create table iteblog as select round(9542.158) from iteblog;hive&gt; describe iteblog;_c0     bigint## 2、指定精度取整函数: round语法: round(double a, int d)返回值: DOUBLE说明: 返回指定精度d的double类型hive&gt; select round(3.1415926,4) from iteblog;3.1416## 3、向下取整函数: floor语法: floor(double a)返回值: BIGINT说明: 返回等于或者小于该double变量的最大的整数hive&gt; select floor(3.1415926) from iteblog;3hive&gt; select floor(25) from iteblog;25## 4、向上取整函数: ceil语法: ceil(double a)返回值: BIGINT说明: 返回等于或者大于该double变量的最小的整数hive&gt; select ceil(3.1415926) from iteblog;4hive&gt; select ceil(46) from iteblog;46## 5、向上取整函数: ceiling语法: ceiling(double a)返回值: BIGINT说明: 与ceil功能相同hive&gt; select ceiling(3.1415926) from iteblog;4hive&gt; select ceiling(46) from iteblog;46## 6、取随机数函数: rand语法: rand(),rand(int seed)返回值: double说明: 返回一个0到1范围内的随机数。如果指定种子seed，则会等到一个稳定的随机数序列hive&gt; select rand() from iteblog;0.5577432776034763hive&gt; select rand() from iteblog;0.6638336467363424hive&gt; select rand(100) from iteblog;0.7220096548596434hive&gt; select rand(100) from iteblog;0.7220096548596434## 7、自然指数函数: exp语法: exp(double a)返回值: double说明: 返回自然对数e的a次方hive&gt; select exp(2) from iteblog;7.38905609893065&lt;strong&gt;自然对数函数&lt;/strong&gt;: ln&lt;strong&gt;语法&lt;/strong&gt;: ln(double a)&lt;strong&gt;返回值&lt;/strong&gt;: double&lt;strong&gt;说明&lt;/strong&gt;: 返回a的自然对数1hive&gt; select ln(7.38905609893065) from iteblog;2.0## 8、以10为底对数函数: log10语法: log10(double a)返回值: double说明: 返回以10为底的a的对数hive&gt; select log10(100) from iteblog;2.0## 9、以2为底对数函数: log2语法: log2(double a)返回值: double说明: 返回以2为底的a的对数hive&gt; select log2(8) from iteblog;3.0## 10、对数函数: log语法: log(double base, double a)返回值: double说明: 返回以base为底的a的对数hive&gt; select log(4,256) from iteblog;4.0## 11、幂运算函数: pow语法: pow(double a, double p)返回值: double说明: 返回a的p次幂hive&gt; select pow(2,4) from iteblog;16.0## 12、幂运算函数: power语法: power(double a, double p)返回值: double说明: 返回a的p次幂,与pow功能相同hive&gt; select power(2,4) from iteblog;16.0## 13、开平方函数: sqrt语法: sqrt(double a)返回值: double说明: 返回a的平方根hive&gt; select sqrt(16) from iteblog;4.0## 14、二进制函数: bin语法: bin(BIGINT a)返回值: string说明: 返回a的二进制代码表示hive&gt; select bin(7) from iteblog;111## 15、十六进制函数: hex语法: hex(BIGINT a)返回值: string说明: 如果变量是int类型，那么返回a的十六进制表示；如果变量是string类型，则返回该字符串的十六进制表示hive&gt; select hex(17) from iteblog;11hive&gt; select hex(‘abc’) from iteblog;616263## 16、反转十六进制函数: unhex语法: unhex(string a)返回值: string说明: 返回该十六进制字符串所代码的字符串hive&gt; select unhex(‘616263’) from iteblog;abchive&gt; select unhex(‘11’) from iteblog;-hive&gt; select unhex(616263) from iteblog;abc## 17、进制转换函数: conv语法: conv(BIGINT num, int from_base, int to_base)返回值: string说明: 将数值num从from_base进制转化到to_base进制hive&gt; select conv(17,10,16) from iteblog;11hive&gt; select conv(17,10,2) from iteblog;10001## 18、绝对值函数: abs语法: abs(double a) abs(int a)返回值: double int说明: 返回数值a的绝对值hive&gt; select abs(-3.9) from iteblog;3.9hive&gt; select abs(10.9) from iteblog;10.9## 19、正取余函数: pmod语法: pmod(int a, int b),pmod(double a, double b)返回值: int double说明: 返回正的a除以b的余数hive&gt; select pmod(9,4) from iteblog;1hive&gt; select pmod(-9,4) from iteblog;3## 20、正弦函数: sin语法: sin(double a)返回值: double说明: 返回a的正弦值hive&gt; select sin(0.8) from iteblog;0.7173560908995228## 21、反正弦函数: asin语法: asin(double a)返回值: double说明: 返回a的反正弦值hive&gt; select asin(0.7173560908995228) from iteblog;0.8## 22、余弦函数: cos语法: cos(double a)返回值: double说明: 返回a的余弦值hive&gt; select cos(0.9) from iteblog;0.6216099682706644## 23、反余弦函数: acos语法: acos(double a)返回值: double说明: 返回a的反余弦值hive&gt; select acos(0.6216099682706644) from iteblog;0.9## 24、positive函数: positive语法: positive(int a), positive(double a)返回值: int double说明: 返回ahive&gt; select positive(-10) from iteblog;-10hive&gt; select positive(12) from iteblog;12## 25、negative函数: negative语法: negative(int a), negative(double a)返回值: int double说明: 返回-ahive&gt; select negative(-5) from iteblog;5hive&gt; select negative(8) from iteblog;-8## 日期函数：## 1、UNIX时间戳转日期函数: from_unixtime语法: from_unixtime(bigint unixtime[, string format])返回值: string说明: 转化UNIX时间戳（从1970-01-01 00:00:00 UTC到指定时间的秒数）到当前时区的时间格式hive&gt; select from_unixtime(1323308943,&#39;yyyyMMdd&#39;) from iteblog;20111208## 2、获取当前UNIX时间戳函数: unix_timestamp语法: unix_timestamp()返回值: bigint说明: 获得当前时区的UNIX时间戳hive&gt; select unix_timestamp() from iteblog;1323309615## 3、日期转UNIX时间戳函数: unix_timestamp语法: unix_timestamp(string date)返回值: bigint说明: 转换格式为&quot;yyyy-MM-dd HH:mm:ss&quot;的日期到UNIX时间戳。如果转化失败，则返回0。hive&gt; select unix_timestamp(&#39;2011-12-07 13:01:03&#39;) from iteblog;1323234063## 4、指定格式日期转UNIX时间戳函数: unix_timestamp语法: unix_timestamp(string date, string pattern)返回值: bigint说明: 转换pattern格式的日期到UNIX时间戳。如果转化失败，则返回0。hive&gt; select unix_timestamp(&#39;20111207 13:01:03&#39;,&#39;yyyyMMdd HH:mm:ss&#39;) from iteblog;1323234063## 5、日期时间转日期函数: to_date语法: to_date(string timestamp)返回值: string说明: 返回日期时间字段中的日期部分。hive&gt; select to_date(&#39;2011-12-08 10:03:01&#39;) from iteblog;2011-12-08## 6、日期转年函数: year语法: year(string date)返回值: int说明: 返回日期中的年。hive&gt; select year(&#39;2011-12-08 10:03:01&#39;) from iteblog;2011hive&gt; select year(&#39;2012-12-08&#39;) from iteblog;2012## 7、日期转月函数: month语法: month (string date)返回值: int说明: 返回日期中的月份。hive&gt; select month(&#39;2011-12-08 10:03:01&#39;) from iteblog;12hive&gt; select month(&#39;2011-08-08&#39;) from iteblog;8## 8、日期转天函数: day语法: day (string date)返回值: int说明: 返回日期中的天。hive&gt; select day(&#39;2011-12-08 10:03:01&#39;) from iteblog;8hive&gt; select day(&#39;2011-12-24&#39;) from iteblog;24## 9、日期转小时函数: hour语法: hour (string date)返回值: int说明: 返回日期中的小时。hive&gt; select hour(&#39;2011-12-08 10:03:01&#39;) from iteblog;10## 10、日期转分钟函数: minute语法: minute (string date)返回值: int说明: 返回日期中的分钟。hive&gt; select minute(&#39;2011-12-08 10:03:01&#39;) from iteblog;3## 11、日期转秒函数: second语法: second (string date)返回值: int说明: 返回日期中的秒。hive&gt; select second(&#39;2011-12-08 10:03:01&#39;) from iteblog;1## 12、日期转周函数: weekofyear语法: weekofyear (string date)返回值: int说明: 返回日期在当前的周数。hive&gt; select weekofyear(&#39;2011-12-08 10:03:01&#39;) from iteblog;49## 13、日期比较函数: datediff语法: datediff(string enddate, string startdate)返回值: int说明: 返回结束日期减去开始日期的天数。hive&gt; select datediff(&#39;2012-12-08&#39;,&#39;2012-05-09&#39;) from iteblog;213## 14、日期增加函数: date_add语法: date_add(string startdate, int days)返回值: string说明: 返回开始日期startdate增加days天后的日期。hive&gt; select date_add(&#39;2012-12-08&#39;,10) from iteblog;2012-12-18## 15、日期减少函数: date_sub语法: date_sub (string startdate, int days)返回值: string说明: 返回开始日期startdate减少days天后的日期。hive&gt; select date_sub(&#39;2012-12-08&#39;,10) from iteblog;2012-11-28## 条件函数# 1、If函数: if语法: if(boolean testCondition, T valueTrue, T valueFalseOrNull)返回值: T说明: 当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNullhive&gt; select if(1=2,100,200) from iteblog;200hive&gt; select if(1=1,100,200) from iteblog;100# 2、非空查找函数: COALESCE语法: COALESCE(T v1, T v2, …)返回值: T说明: 返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULLhive&gt; select COALESCE(null,&#39;100&#39;,&#39;50′) from iteblog;100## 3. 非空查找函数： NVLNVLnvl(COMMISSION_PCT,0)如果第一个参数为null，则返回第二个参数如果第一个参数为非null，则返回第一个参数 -- nvl 和 coalesce的区别    COALESCE(EXPR1,EXPR2,EXPR3...EXPRn)从左往右数，遇到第一个非null值，则返回该非null值。多层判断第一点区别：从上面可以知道，nvl只适合于两个参数的，COALESCE适合于多个参数。第二点区别：COALESCE里的所有参数类型必须保持一致，nvl可以不一致。## 4.1、条件判断函数：CASE语法: CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END返回值: T说明：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回fhive&gt; Select case 100 when 50 then &#39;tom&#39; when 100 then &#39;mary&#39; else &#39;tim&#39; end from iteblog;maryhive&gt; Select case 200 when 50 then &#39;tom&#39; when 100 then &#39;mary&#39; else &#39;tim&#39; end from iteblog;tim## 4.2、条件判断函数：CASE语法: CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END返回值: T说明：如果a为TRUE,则返回b；如果c为TRUE，则返回d；否则返回ehive&gt; select case when 1=2 then &#39;tom&#39; when 2=2 then &#39;mary&#39; else &#39;tim&#39; end from iteblog;maryhive&gt; select case when 1=1 then &#39;tom&#39; when 2=2 then &#39;mary&#39; else &#39;tim&#39; end from iteblog;tom## 字符串函数:#1、字符串长度函数：length语法: length(string A)返回值: int说明：返回字符串A的长度hive&gt; select length(&#39;abcedfg&#39;) from iteblog;7#2、字符串反转函数：reverse语法: reverse(string A)返回值: string说明：返回字符串A的反转结果hive&gt; select reverse(abcedfg’) from iteblog;gfdecba#3、字符串连接函数：concat语法: concat(string A, string B…)返回值: string说明：返回输入字符串连接后的结果，支持任意个输入字符串#注意： select concat(&#39;abc&#39;,NULL,&#39;asd&#39;); 最后返回的是null,如果三个字符串中有一个是null 的 则最后返回null值。hive&gt; select concat(‘abc’,&#39;def’,&#39;gh’) from iteblog;abcdefgh#4、带分隔符字符串连接函数：concat_ws#注意里面传递的参数如果有一个是nu l l值 则最后返回的是nu l l语法: concat_ws(string SEP, string A, string B…),concat_ws(string SEP,Array(str1,str2))返回值: string说明：返回输入字符串连接后的结果，SEP表示各个字符串间的分隔符hive&gt; select concat_ws(&#39;,&#39;,&#39;abc&#39;,&#39;def&#39;,&#39;gh&#39;) from iteblog;abc,def,gh#5、字符串截取函数：substr,substring语法: substr(string A, int start),substring(string A, int start)返回值: string说明：返回字符串A从start位置到结尾的字符串hive&gt; select substr(&#39;abcde&#39;,3) from iteblog;cdehive&gt; select substring(&#39;abcde&#39;,3) from iteblog;cdehive&gt;  select substr(&#39;abcde&#39;,-1) from iteblog;  （和ORACLE相同）e#6、字符串截取函数：substr,substring#包头不包未，索引从1开始语法: substr(string A, int start, int len),substring(string A, int start, int len)返回值: string说明：返回字符串A从start位置开始，长度为len的字符串hive&gt; select substr(&#39;abcde&#39;,3,2) from iteblog;cdhive&gt; select substring(&#39;abcde&#39;,3,2) from iteblog;cdhive&gt;select substring(&#39;abcde&#39;,-2,2) from iteblog;de#7、字符串转大写函数：upper,ucase语法: upper(string A) ucase(string A)返回值: string说明：返回字符串A的大写格式hive&gt; select upper(&#39;abSEd&#39;) from iteblog;ABSEDhive&gt; select ucase(&#39;abSEd&#39;) from iteblog;ABSED#8、字符串转小写函数：lower,lcase语法: lower(string A) lcase(string A)返回值: string说明：返回字符串A的小写格式hive&gt; select lower(&#39;abSEd&#39;) from iteblog;absedhive&gt; select lcase(&#39;abSEd&#39;) from iteblog;absed#9、去空格函数：trim语法: trim(string A)返回值: string说明：去除字符串两边的空格hive&gt; select trim(&#39; abc &#39;) from iteblog;abc#10、左边去空格函数：ltrim语法: ltrim(string A)返回值: string说明：去除字符串左边的空格hive&gt; select ltrim(&#39; abc &#39;) from iteblog;abc#11、右边去空格函数：rtrim语法: rtrim(string A)返回值: string说明：去除字符串右边的空格hive&gt; select rtrim(&#39; abc &#39;) from iteblog;abc#12、正则表达式替换函数：regexp_replace语法: regexp_replace(string A, string B, string C)返回值: string说明：将字符串A中的符合java正则表达式B的部分替换为C。注意，在有些情况下要使用转义字符,类似oracle中的regexp_replace函数。hive&gt; select regexp_replace(&#39;foobar&#39;, &#39;oo|ar&#39;, &#39;&#39;) from iteblog;fb#13、正则表达式解析函数：regexp_extract语法: regexp_extract(string subject, string pattern, int index)返回值: string说明：将字符串subject按照pattern正则表达式的规则拆分，返回index指定的字符。hive&gt; select regexp_extract(&#39;foothebar&#39;, &#39;foo(.*?)(bar)&#39;, 1) from iteblog;thehive&gt; select regexp_extract(&#39;foothebar&#39;, &#39;foo(.*?)(bar)&#39;, 2) from iteblog;barhive&gt; select regexp_extract(&#39;foothebar&#39;, &#39;foo(.*?)(bar)&#39;, 0) from iteblog;foothebarstrong&gt;注意，在有些情况下要使用转义字符，下面的等号要用双竖线转义，这是java正则表达式的规则。select data_field,  regexp_extract(data_field,&#39;.*?bgStart\\=([^&amp;]+)&#39;,1) as aaa,  regexp_extract(data_field,&#39;.*?contentLoaded_headStart\\=([^&amp;]+)&#39;,1) as bbb,  regexp_extract(data_field,&#39;.*?AppLoad2Req\\=([^&amp;]+)&#39;,1) as ccc  from pt_nginx_loginlog_st  where pt = &#39;2012-03-26&#39; limit 2;#14.1、URL解析函数：parse_url语法: parse_url(string urlString, string partToExtract [, string keyToExtract])返回值: string说明：返回URL中指定的部分。partToExtract的有效值为：HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO.hive&gt; select parse_url(&#39;https://www.iteblog.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;, &#39;HOST&#39;) from iteblog;facebook.comhive&gt; select parse_url(&#39;https://www.iteblog.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;, &#39;QUERY&#39;, &#39;k1&#39;) from iteblog;v1#14.2、URL解析函数: parse_url_tuple语法: parse_url(string urlString, string partToExtract [, string keyToExtract])，parse_url_tuple功能类似parse_url()，但它可以同时提取多个部分并返回返回值: string　　　　说明：返回URL中指定的部分。partToExtract的有效值为：HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO.                        hive&gt; select parse_url_tuple(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1’, ‘QUERY:k1’, ‘QUERY:k2’);v1 v2                                  #15.1、json解析函数：get_json_object语法: get_json_object(string json_string, string path)返回值: string说明：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL。hive&gt; select  get_json_object(&#39;&#123;&quot;store&quot;:&gt;   &#123;&quot;fruit&quot;:\[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;,&#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],&gt;    &quot;bicycle&quot;:&#123;&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;&#125;&gt;   &#125;,&gt;  &quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;,&gt;  &quot;owner&quot;:&quot;amy&quot;&gt; &#125;&gt; &#39;,&#39;$.owner&#39;) from iteblog;amy#15.2、json解析函数：json_tuplejson_tuple 相对于 get_json_object 的优势就是一次可以解析多个 Json 字段。但是如果我们有个 Json 数组，这两个函数都无法处理.json_tuple(jsonStr, k1, k2, ...)参数为一组键k1，k2……和JSON字符串，返回值的元组.set hivevar:msg=&#123;&quot;message&quot;:&quot;2015/12/08 09:14:4&quot;, &quot;client&quot;: &quot;10.108.24.253&quot;, &quot;server&quot;: &quot;passport.suning.com&quot;, &quot;request&quot;: &quot;POST /ids/needVerifyCode HTTP/1.1&quot;,&quot;server&quot;: &quot;passport.sing.co&quot;,&quot;version&quot;:&quot;1&quot;,&quot;timestamp&quot;:&quot;2015-12-08T01:14:43.273Z&quot;,&quot;type&quot;:&quot;B2C&quot;,&quot;center&quot;:&quot;JSZC&quot;,&quot;system&quot;:&quot;WAF&quot;,&quot;clientip&quot;:&quot;192.168.61.4&quot;,&quot;host&quot;:&quot;wafprdweb03&quot;,&quot;path&quot;:&quot;/usr/local/logs/waf.error.log&quot;,&quot;redis&quot;:&quot;192.168.24.46&quot;&#125;                              hive&gt; select a.* from test lateral view json_tuple(‘$&#123;hivevar:msg&#125;’,’server’,’host’) a as f1,f2;     passport.sing.com wafprdweb03                                                                                   #16、空格字符串函数：space语法: space(int n)返回值: string说明：返回长度为n的字符串hive&gt; select space(10) from iteblog;hive&gt; select length(space(10)) from iteblog;10#17、重复字符串函数：repeat语法: repeat(string str, int n)返回值: string说明：返回重复n次后的str字符串hive&gt; select repeat(&#39;abc&#39;,5) from iteblog;abcabcabcabcabc#18、首字符ascii函数：ascii语法: ascii(string str)返回值: int说明：返回字符串str第一个字符的ascii码hive&gt; select ascii(&#39;abcde&#39;) from iteblog;97#19、左补足函数：lpad语法: lpad(string str, int len, string pad)返回值: string说明：将str进行用pad进行左补足到len位hive&gt; select lpad(&#39;abc&#39;,10,&#39;td&#39;) from iteblog;tdtdtdtabc注意：与GP，ORACLE不同，pad 不能默认#20、右补足函数：rpad语法: rpad(string str, int len, string pad)返回值: string说明：将str进行用pad进行右补足到len位hive&gt; select rpad(&#39;abc&#39;,10,&#39;td&#39;) from iteblog;abctdtdtdt#21、分割字符串函数: split语法: split(string str, string pat)返回值: array说明: 按照pat字符串分割str，会返回分割后的字符串数组hive&gt; select split(&#39;abtcdtef&#39;,&#39;t&#39;) from iteblog;[&quot;ab&quot;,&quot;cd&quot;,&quot;ef&quot;]#22、集合查找函数: find_in_set语法: find_in_set(string str, string strList)返回值: int说明: 返回str在strlist第一次出现的位置，strlist是用逗号分割的字符串。如果没有找该str字符，则返回0hive&gt; select find_in_set(&#39;ab&#39;,&#39;ef,ab,de&#39;) from iteblog;2hive&gt; select find_in_set(&#39;at&#39;,&#39;ef,ab,de&#39;) from iteblog;0                              #23、greatest函数greatest(col_a, col_b, ..., col_n)比较n个column的大小，过滤掉null，但是当某个column中是string，而其他是int/double/float等时，返回nullselect greatest(-1, 0, 5, 8) 返回：8select greatest(-1, 0, 5, 8, null) 返回：NULLselect greatest(-1, 0, 5, 8, &quot;dfsf&quot;) 返回：NULLselect greatest(&quot;2020-02-26&quot;,&quot;2020-02-23&quot;,&quot;2020-02-22&quot;) 返回：2020-02-26  #24、least函数select least(-1, 0, 5, 8)结果：-1select least(-1, 0, 5, 8, null)结果：NULLselect least(-1, 0, 5, 8, &quot;dfsf&quot;)结果：NULLselect least(&quot;2020-02-26&quot;,&quot;2020-02-23&quot;,&quot;2020-02-22&quot;)结果：2020-02-22                                                           ##复合类型构建操作#1、Map类型构建: map语法: map (key1, value1, key2, value2, …)说明：根据输入的key和value对构建map类型hive&gt; Create table iteblog as select map(&#39;100&#39;,&#39;tom&#39;,&#39;200&#39;,&#39;mary&#39;) as t from iteblog;hive&gt; describe iteblog;t       map&lt;string ,string&gt;hive&gt; select t from iteblog;&#123;&quot;100&quot;:&quot;tom&quot;,&quot;200&quot;:&quot;mary&quot;&#125;#2、Struct类型构建: struct语法: struct(val1, val2, val3, …)说明：根据输入的参数构建结构体struct类型hive&gt; create table iteblog as select struct(&#39;tom&#39;,&#39;mary&#39;,&#39;tim&#39;) as t from iteblog;hive&gt; describe iteblog;t       struct&lt;col1:string ,col2:string,col3:string&gt;hive&gt; select t from iteblog;&#123;&quot;col1&quot;:&quot;tom&quot;,&quot;col2&quot;:&quot;mary&quot;,&quot;col3&quot;:&quot;tim&quot;&#125;#3、array类型构建: array语法: array(val1, val2, …)说明：根据输入的参数构建数组array类型hive&gt; create table iteblog as select array(&quot;tom&quot;,&quot;mary&quot;,&quot;tim&quot;) as t from iteblog;hive&gt; describe iteblog;t       array&lt;string&gt;hive&gt; select t from iteblog;[&quot;tom&quot;,&quot;mary&quot;,&quot;tim&quot;]##4. 窗口函数：窗口函数row_number()over(partition by order by )：从1开始按照partition by 字段进行排序rank()over(partition by order by )：从1开始按照partition by 字段排序，遇到并列取相同数值，后面的跳跃计数dense_rank()over(partition by order by )：从1开始按照partition by 字段排序，遇到并列取相同数值，后面的顺序计数sum()over(partition by order by #rows between unbounded preceding and current row)：根据某列进行汇总，常用在算累计销售额或者累计订单量，#之后的rows between unbounded preceding and current row）代表累计增加之前的行，可加可不加lead(col,n,default)over(partition by order by )：col字段向上n行，default为向上为NULL时的默认值lag(col,n,default)over(partition by order by)：col字段向下n行，default为向下为NULL时的默认值first_value(col)over(partition by order by)：分组排序后，组内的第一个col的值last_value(col)over(partition by order by)：分组排序后，组内的最后一个col的值排序：1.order by：全局排序，缺点是会把分组的数据放到一个Reduce中2.sort by：局部排序，把每个Reduce中的数据进行排序，Reduce数量在2个以上有用，缺点是输出排序结果会有重叠3.distribute by：提前排序，在Map端先进行排序，再结合sort by，把排好序的数据放到同一个Reduce中再进行局部排序，如同group by，放在sort by之前，例如：select * from A group by col distribute by time sort by id4.cluster by：如果distribute by和sort by 的字段相同，就可以用cluster by代替他们的组合用法，缺点是只能升序，不能asc和desc，而且只能保证一个分区内的排序，例如：select * from A cluster by id (同select * from A 5.distribute by id sort by id)列转行：1.concat_ws(&#39;,&#39;,collect_set(col))：collect_list 不去重，collect_set 去重。col的数据类型要求是stringunion all/union行转列：2.lateral view explode(split(col,&#39;,&#39;)) 虚拟表别名 as 列别名：用法很特殊，放在from tablename和where中间，原理是lateral view(侧视图)与UDTF函数将一行拆分为多行，创建一个虚拟表，再与原表进行笛卡尔积关联。explode()是专门处理map/array结构数据的，解析json数据要配合get_json_object()case when...endhive去重方式：group bydistinctrow_number()时间区段的提取：Extract-- field可以是day、hour、minute, month, quarter等等-- source可以是date、timestamp类型extract(field FROM source)SELECT extract(year FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 2020SELECT extract(quarter FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 3SELECT extract(month FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 8SELECT extract(week FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 31,一年中的第几周SELECT extract(day FROM &#39;2020-08-05 09:30:08&#39;);  -- 结果为 5SELECT extract(hour FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 9SELECT extract(minute FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 30SELECT extract(second FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 8next_day(STRING start_date, STRING day_of_week)-- 返回当前日期对应的下一个周几对应的日期-- 2020-08-05为周三SELECT next_day(&#39;2020-08-05&#39;,&#39;MO&#39;) -- 下一个周一对应的日期：2020-08-10SELECT next_day(&#39;2020-08-05&#39;,&#39;TU&#39;) -- 下一个周二对应的日期：2020-08-11SELECT next_day(&#39;2020-08-05&#39;,&#39;WE&#39;) -- 下一个周三对应的日期：2020-08-12SELECT next_day(&#39;2020-08-05&#39;,&#39;TH&#39;) -- 下一个周四对应的日期：2020-08-06，即为本周四SELECT next_day(&#39;2020-08-05&#39;,&#39;FR&#39;) -- 下一个周五对应的日期：2020-08-07，即为本周五SELECT next_day(&#39;2020-08-05&#39;,&#39;SA&#39;) -- 下一个周六对应的日期：2020-08-08，即为本周六SELECT next_day(&#39;2020-08-05&#39;,&#39;SU&#39;) -- 下一个周日对应的日期：2020-08-09，即为本周日-- 星期一到星期日的英文（Monday，Tuesday、Wednesday、Thursday、Friday、Saturday、Sunday）使用那么该如何获取当前日期所在周的周一对应的日期呢？只需要先获取当前日期的下周一对应的日期，然后减去7天，即可获得：SELECT date_add(next_day(&#39;2020-08-05&#39;,&#39;MO&#39;),-7);同理，获取当前日期所在周的周日对应的日期，只需要先获取当前日期的下周一对应的日期，然后减去1天，即可获得：select date_add(next_day(&#39;2020-08-05&#39;,&#39;MO&#39;),-1) -- 2020-08-09月的提取语法至于怎么将月份从单一日期提取出来呢，LAST_DAY这个函数可以将每个月中的日期变成该月的最后一天(28号，29号，30号或31号)，如下：last_day(STRING date)*使用SELECT last_day(&#39;2020-08-05&#39;); -- 2020-08-31除了上面的方式，也可以使用date_format函数，比如：SELECT date_format(&#39;2020-08-05&#39;,&#39;yyyy-MM&#39;);-- 2020-08日期的范围上面可供提取的字段，不同的数据库存在些许的差异。以Hive为例，支持day, dayofweek, hour, minute, month, quarter, second, week 和 year。其中周、月、年使用最为广泛，因为无论是公司内部产品，还是商用的产品所提供的数据后台统计，周报和月报(比如近7天、近30天)最注重表现的周期。SELECT date_add(next_day(&#39;2020-08-05&#39;,&#39;MO&#39;),-7);月的Window：使用add_months加上trunc()的应用-- 2020-07-05select add_months(&#39;2020-08-05&#39;, -1)-- 返回当前日期的月初日期-- 2020-08-01select trunc(&quot;2020-08-05&quot;,&#39;MM&#39;)由上面范例可见，单纯使用add_months，减N个月的用法，可以刚好取到整数月的数据，但如果加上trunc()函数，则会从前N个月的一号开始取值。-- 选取2020-07-05到2020-08-05所有数据BETWEEN add_months(&#39;2020-08-05&#39;, -1) AND &#39;2020-08-05&#39; -- 选取2020-07-01到2020-08-05之间所有数据BETWEEN add_months(trunc(&quot;2020-08-05&quot;,&#39;MM&#39;),-1) AND &#39;2020-08-05&#39; 第二：临时表与Common Table Expression (WITH)这两种方法是日常工作中经常被使用到，对于一些比较复杂的计算任务，为了避免过多的JOIN，通常会先把一些需要提取的部分数据使用临时表或是CTE的形式在主要查询区块前进行提取。第二：临时表与Common Table Expression (WITH)临时表的作法：CREATE TEMPORARY TABLE table_1 AS      SELECT         columns    FROM table A;CREATE TEMPORARY table_2 AS     SELECT        columns    FROM table B;SELECT    table_1.columns,    table_2.columns,     c.columns FROM table C JOIN table_1     JOIN table_2;CTE的作法：-- 注意Hive、Impala支持这种语法，低版本的MySQL不支持(高版本支持)WITH employee_by_title_count AS (    SELECT        t.name as job_title        , COUNT(e.id) as amount_of_employees    FROM employees e        JOIN job_titles t on e.job_title_id = t.id    GROUP BY 1),salaries_by_title AS (     SELECT         name as job_title         , salary     FROM job_titles)SELECT *FROM employee_by_title_count e    JOIN salaries_by_title s ON s.job_title = e.job_title  </code></pre><h4 id="8-Aggregation-与CASE-WHEN的结合使用"><a href="#8-Aggregation-与CASE-WHEN的结合使用" class="headerlink" title="8.Aggregation 与CASE WHEN的结合使用**"></a><strong>8.Aggregation 与CASE WHEN的结合使用</strong>**</h4><pre><code class="sql">将Aggregation function (SUM/COUNT/COUNT DISTINCT/MIN/MAX) 结合CASE WHEN是最强大且最有趣的使用方式。这样的使用创造出一种类似EXCEL中SUMIF/COUNTIF的效果，可以用这个方式做出很多高效的分析。数据准备CREATE TABLE order(    register_date string,    order_date string,    user_id string,    country string,    order_sales decimal(10,2),    order_id string);INSERT INTO TABLE order VALUES(&quot;2020-06-07&quot;,&quot;2020-06-09&quot;,&quot;001&quot;,&#39;c0&#39;,210,&quot;o1&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-08&quot;,&quot;2020-06-09&quot;,&quot;002&quot;,&#39;c1&#39;,220,&quot;o2&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-07&quot;,&quot;2020-06-10&quot;,&quot;003&quot;,&#39;c2&#39;,230,&quot;o3&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-09&quot;,&quot;2020-06-10&quot;,&quot;004&quot;,&#39;c3&#39;,200,&quot;o4&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-07&quot;,&quot;2020-06-20&quot;,&quot;005&quot;,&#39;c4&#39;,300,&quot;o5&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-10&quot;,&quot;2020-06-23&quot;,&quot;006&quot;,&#39;c5&#39;,400,&quot;o6&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-07&quot;,&quot;2020-06-19&quot;,&quot;007&quot;,&#39;c6&#39;,600,&quot;o7&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-12&quot;,&quot;2020-06-18&quot;,&quot;008&quot;,&#39;c7&#39;,700,&quot;o8&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-07&quot;,&quot;2020-06-09&quot;,&quot;009&quot;,&#39;c8&#39;,100,&quot;o9&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-15&quot;,&quot;2020-06-18&quot;,&quot;0010&quot;,&#39;c9&#39;,200,&quot;o10&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-15&quot;,&quot;2020-06-19&quot;,&quot;0011&quot;,&#39;c10&#39;,250,&quot;o11&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-12&quot;,&quot;2020-06-29&quot;,&quot;0012&quot;,&#39;c11&#39;,270,&quot;o12&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-16&quot;,&quot;2020-06-19&quot;,&quot;0013&quot;,&#39;c12&#39;,230,&quot;o13&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-17&quot;,&quot;2020-06-20&quot;,&quot;0014&quot;,&#39;c13&#39;,290,&quot;o14&quot;);INSERT INTO TABLE order VALUES(&quot;2020-06-20&quot;,&quot;2020-06-29&quot;,&quot;0015&quot;,&#39;c14&#39;,203,&quot;o15&quot;);</code></pre><p>CASE WHEN 时间，进行留存率&#x2F;使用率的分析</p><pre><code class="sql">-- 允许多列去重set hive.groupby.skewindata = false-- 允许使用位置编号分组或排序set hive.groupby.orderby.position.alias = trueSELECT    date_add(Next_day(register_date, &#39;MO&#39;),-1) AS week_end,    COUNT(DISTINCT CASE WHEN order_date BETWEEN register_date AND date_add(register_date,6) THEN user_id END) AS first_week_order,    COUNT(DISTINCT CASE WHEN order_date BETWEEN date_add(register_date ,7) AND date_add(register_date,13) THEN user_id END) AS sencod_week_order,    COUNT(DISTINCT CASE WHEN order_date BETWEEN date_add(register_date ,14) AND date_add(register_date,20) THEN user_id END) as third_week_orderFROM orderGROUP BY 1上面的示例可以得知到用户在注册之后，有没有创建订单的行为。比如注册后的第一周，第二周，第三周分别有多少下单用户，这样可以分析出用户的使用情况和留存情况。CASE WHEN 时间，进行每个用户消费金额的分析SELECT    user_id,    SUM (CASE WHEN order_date BETWEEN register_date AND date_add(register_date,6) THEN order_sales END) AS first_week_amount,    SUM (CASE WHEN order_date BETWEEN date_add(register_date ,7) AND date_add(register_date,13) THEN order_sales END) AS second_week_amount    FROM orderGROUP BY 1通过筛选出注册与消费的日期，并且进行消费金额统计，每个用户在每段时间段(注册后第一周、第二周…以此类推)的消费金额，可以观察用户是否有持续维持消费习惯或是消费金额变低等分析。CASE WHEN数量，消费金额超过某一定额的数量分析SELECT    user_id,    COUNT(DISTINCT CASE WHEN order_sales &gt;= 100 THEN order_id END) AS count_of_order_greateer_than_100FROM orderGROUP BY 1上面的示例就是类似countif的用法，针对每个用户，统计其订单金额大于某个值的订单数量，分析去筛选出高价值的顾客。CASE WHEN数量，加上时间的用法SELECT    user_id,    MIN(CASE WHEN order_sales &gt; 100 THEN order_date END) AS first_order_date_over1000,    MAX(CASE WHEN order_sales &gt; 100 THEN order_date END) AS recent_order_date_over100FROM orderGROUP BY 1CASE WHEN加上MIN/MAX时间，可以得出该用户在其整个使用过程中，首次购买超过一定金额的订单日期，以及最近一次购买超过一定金额的订单日期。用户访问session分析数据准备CREATE TABLE user_visit_action(     user_id string,    session_id string,    page_url string,    action_time string);    INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss001&quot;,&quot;http://a.com&quot;,&quot;2020-08-06 13:34:11.478&quot;);INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss001&quot;,&quot;http://b.com&quot;,&quot;2020-08-06 13:35:11.478&quot;);INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss001&quot;,&quot;http://c.com&quot;,&quot;2020-08-06 13:36:11.478&quot;);INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss002&quot;,&quot;http://a.com&quot;,&quot;2020-08-06 14:30:11.478&quot;);INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss002&quot;,&quot;http://b.com&quot;,&quot;2020-08-06 14:31:11.478&quot;);INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss002&quot;,&quot;http://e.com&quot;,&quot;2020-08-06 14:33:11.478&quot;);INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss002&quot;,&quot;http://f.com&quot;,&quot;2020-08-06 14:35:11.478&quot;);INSERT INTO TABLE user_visit_action VALUES(&quot;002&quot;,&quot;ss003&quot;,&quot;http://u.com&quot;,&quot;2020-08-06 18:34:11.478&quot;);INSERT INTO TABLE user_visit_action VALUES(&quot;002&quot;,&quot;ss003&quot;,&quot;http://k.com&quot;,&quot;2020-08-06 18:38:11.478&quot;);用户访问session分析SELECT    user_id,    session_id,    page_url,    DENSE_RANK() OVER (PARTITION BY user_id, session_id ORDER BY action_time ASC) AS page_order,    MIN(action_time) OVER (PARTITION BY user_id, session_id) AS session_start_time,    MAX(action_time) OVER (PARTITION BY user_id, session_id) AS session_finisht_timeFROM user_visit_actionuser_idsession_idpage_urlpage_ordersession_start_timesession_finisht_time001ss001http://a.com12020-08-06 13:34:11.4782020-08-06 13:36:11.478001ss001http://b.com22020-08-06 13:34:11.4782020-08-06 13:36:11.478001ss001http://c.com32020-08-06 13:34:11.4782020-08-06 13:36:11.478001ss002http://a.com12020-08-06 14:30:11.4782020-08-06 14:35:11.478001ss002http://b.com22020-08-06 14:30:11.4782020-08-06 14:35:11.478001ss002http://e.com32020-08-06 14:30:11.4782020-08-06 14:35:11.478001ss002http://f.com42020-08-06 14:30:11.4782020-08-06 14:35:11.478002ss003http://u.com12020-08-06 18:34:11.4782020-08-06 18:38:11.478002ss003http://k.com22020-08-06 18:34:11.4782020-08-06 18:38:11.478</code></pre><h4 id="9-自定义UDF解析JSON"><a href="#9-自定义UDF解析JSON" class="headerlink" title="9. 自定义UDF解析JSON"></a>9. <strong>自定义UDF解析JSON</strong></h4><pre><code class="python">## JSONObject解析JSON对象import org.apache.commons.lang.StringUtils;import org.apache.hadoop.hive.ql.exec.UDF;import org.json.JSONException;import org.json.JSONObject;import org.json.JSONTokener;/** * JSON对象解析UDF * date:2017-04-20 */public class GetJsonObject extends UDF &#123;    /** * 解析json并返回对应的值。例如 add jar jar/bdp_udf_demo-1.0.0.jar; create temporary function getJsonObject as &#39;com.jd.bdp.util.udf.GetJsonObject&#39;; select getJsonObject(json字符串,key值) * @param jsonStr * @param objName * @return */    public String evaluate(String jsonStr,String objName) throws JSONException &#123;        if(StringUtils.isBlank(jsonStr)|| StringUtils.isBlank(objName))&#123;            return null;        &#125;        JSONObject jsonObject = new JSONObject(new JSONTokener(jsonStr));        Object objValue = jsonObject.get(objName);        if(objValue==null)&#123;            return null;        &#125;        return objValue.toString();    &#125;&#125;## JSONArray解析JSON数组对象 import org.apache.commons.lang.StringUtils;import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.hive.ql.metadata.HiveException;import org.apache.hadoop.io.Text;import org.json.JSONArray;import org.json.JSONException;import org.json.JSONTokener;import java.util.ArrayList;/** * JSON数组对象解析UDF * date:2017-04-20 */public class GetJsonArray extends UDF &#123;    /** * 解析json并返回对应子json字符串数组,例如 add jar jar/bdp_udf_demo-1.0.0.jar;    create temporary function getJsonArray as &#39;com.jd.bdp.util.udf.GetJsonArray&#39;;     select getJsonArray(json字符串) * @param jsonArrayStr * @return * @throws HiveException */    public ArrayList&lt;Text&gt; evaluate(String jsonArrayStr) throws JSONException &#123;        if(StringUtils.isBlank(jsonArrayStr)||StringUtils.isBlank(jsonArrayStr))&#123;            return null;        &#125;        ArrayList&lt;Text&gt; textList = new ArrayList&lt;Text&gt;();        if(!jsonArrayStr.trim().startsWith(&quot;[&quot;))&#123;            textList.add(new Text(jsonArrayStr));        &#125;else&#123;            JSONArray jsonArray = new JSONArray(new JSONTokener(jsonArrayStr));            Text[] jsonTexts = new Text[jsonArray.length()];            for(int i=0;i&lt;jsonArray.length();i++)&#123;                String json = jsonArray.getJSONObject(i).toString();                textList.add(new Text(json));            &#125;        &#125;        return textList;    &#125;&#125;</code></pre><h2 id="Git命令整理"><a href="#Git命令整理" class="headerlink" title="Git命令整理**"></a>Git命令整理**</h2><pre><code class="python">git push -f origin 分支名 #强制推送  当出现[reject] master -&gt; master(non-fast-forward)时git config –list     #获取Git配置信息git init # 初始化仓库git add .(文件name) # 添加文件到暂存区git commit -m &quot;first commit&quot; # 添加文件到本地仓库并提交描述信息git remote add origin 远程仓库地址 # 链接远程仓库，创建主分支git pull origin master --allow-unrelated-histories # 把本地仓库的变化连接到远程仓库主分支git clone -b 分支名 url  #下载某个分支下的代码git push -u origin master # 把本地仓库的文件推送到远程仓库git branch      #显示所有本地分支git checkout [branch-name]    #切换到指定分支，并更新工作区git merge [branch]    #合并指定分支到当前分支git branch -d [branch-name]    #删除分支git checkout -b [branch] #新建一个分支，并切换到该分支git rm [file1] [file2] ...# 删除工作区文件，并且将这次删除放入暂存区git rm --cached [file]# 停止追踪指定文件，但该文件会保留在工作区git commit -a # 提交工作区自上次commit之后的变化，直接到仓库区git tag [tag] # 新建一个tag在当前commitgit tag [tag] [commit] #新建一个tag在指定commitgit tag -d [tag] #删除本地taggit push [remote] [tag] #提交指定taggit status  #查看仓库状态，显示有变更的文件git pull [remote] [branch] #取回远程仓库的变化，并与本地分支合并git stash #暂时将未提交的变化移除，稍后再移入git rebase [branch]git reset HEAD [file]   #用版本库里的版本替换工作去的版本，无论工作区是修改还是删除可以把暂存区的修改撤销掉git reset --hard HEAD^或git reset --hard 3628164     # 版本回退git fetch origin master   # 取回origin主机的master分支git diff HEAD # 显示工作区与当前分支最新commit之间的差异git reflog # 可以很好地帮助你恢复你误操作的数据，</code></pre><h2 id="java重复造轮子"><a href="#java重复造轮子" class="headerlink" title="java重复造轮子"></a><strong>java重复造轮子</strong></h2><pre><code class="java">package org.jeecgframework.core.util;import java.io.IOException;import java.util.ArrayList;import java.util.HashMap;import java.util.Iterator;import java.util.List;import java.util.Map;import net.sf.json.JSONArray;import net.sf.json.JSONObject;import org.apache.commons.beanutils.BeanUtils;import org.codehaus.jackson.JsonParseException;import org.codehaus.jackson.map.JsonMappingException;import org.codehaus.jackson.map.ObjectMapper;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import net.sf.json.JSONArray;import net.sf.json.JSONObject;/** * JSON和JAVA的POJO的相互转换 * @author  张代浩 * JSONHelper.java */public final class JSONHelper &#123;    private static final Logger logger = LoggerFactory.getLogger(JSONHelper.class);    // 将数组转换成JSON    public static String array2json(Object object) &#123;        JSONArray jsonArray = JSONArray.fromObject(object);        return jsonArray.toString();    &#125;    // 将JSON转换成数组,其中valueClz为数组中存放的对象的Class    public static Object json2Array(String json, Class valueClz) &#123;        JSONArray jsonArray = JSONArray.fromObject(json);        return JSONArray.toArray(jsonArray, valueClz);    &#125;    // 将Collection转换成JSON    public static String collection2json(Object object) &#123;        JSONArray jsonArray = JSONArray.fromObject(object);        return jsonArray.toString();    &#125;    // 将Map转换成JSON    public static String map2json(Object object) &#123;        JSONObject jsonObject = JSONObject.fromObject(object);        return jsonObject.toString();    &#125;    // 将JSON转换成Map,其中valueClz为Map中value的Class,keyArray为Map的key    public static Map json2Map(Object[] keyArray, String json, Class valueClz) &#123;        JSONObject jsonObject = JSONObject.fromObject(json);        Map classMap = new HashMap();        for (int i = 0; i &lt; keyArray.length; i++) &#123;            classMap.put(keyArray[i], valueClz);        &#125;        return (Map) JSONObject.toBean(jsonObject, Map.class, classMap);    &#125;    // 将POJO转换成JSON    public static String bean2json(Object object) &#123;        JSONObject jsonObject = JSONObject.fromObject(object);        return jsonObject.toString();    &#125;    // 将JSON转换成POJO,其中beanClz为POJO的Class    public static Object json2Object(String json, Class beanClz) &#123;        return JSONObject.toBean(JSONObject.fromObject(json), beanClz);    &#125;    /**     * json转换为java对象     *      * &lt;pre&gt;     * return JackJson.fromJsonToObject(this.answersJson, JackJson.class);     * &lt;/pre&gt;     *      * @param &lt;T&gt;     *            要转换的对象     * @param json     *            字符串     * @param valueType     *            对象的class     * @return 返回对象     */    public static &lt;T&gt; T fromJsonToObject(String json, Class&lt;T&gt; valueType) &#123;        ObjectMapper mapper = new ObjectMapper();        try &#123;            return mapper.readValue(json, valueType);        &#125; catch (JsonParseException e) &#123;            logger.error(&quot;JsonParseException: &quot;, e);        &#125; catch (JsonMappingException e) &#123;            logger.error(&quot;JsonMappingException: &quot;, e);        &#125; catch (IOException e) &#123;            logger.error(&quot;IOException: &quot;, e);        &#125;        return null;    &#125;    // 将String转换成JSON    public static String string2json(String key, String value) &#123;        JSONObject object = new JSONObject();        object.put(key, value);        return object.toString();    &#125;    // 将JSON转换成String    public static String json2String(String json, String key) &#123;        JSONObject jsonObject = JSONObject.fromObject(json);        return jsonObject.get(key).toString();    &#125;    /***     * 将List对象序列化为JSON文本     */    public static &lt;T&gt; String toJSONString(List&lt;T&gt; list) &#123;        JSONArray jsonArray = JSONArray.fromObject(list);        return jsonArray.toString();    &#125;    /***     * 将对象序列化为JSON文本     *      * @param object     * @return     */    public static String toJSONString(Object object) &#123;        JSONArray jsonArray = JSONArray.fromObject(object);        return jsonArray.toString();    &#125;    /***     * 将JSON对象数组序列化为JSON文本     *      * @param jsonArray     * @return     */    public static String toJSONString(JSONArray jsonArray) &#123;        return jsonArray.toString();    &#125;    /***     * 将JSON对象序列化为JSON文本     *      * @param jsonObject     * @return     */    public static String toJSONString(JSONObject jsonObject) &#123;        return jsonObject.toString();    &#125;    /***     * 将对象转换为List对象     *      * @param object     * @return     */    public static List toArrayList(Object object) &#123;        List arrayList = new ArrayList();        JSONArray jsonArray = JSONArray.fromObject(object);        Iterator it = jsonArray.iterator();        while (it.hasNext()) &#123;            JSONObject jsonObject = (JSONObject) it.next();            Iterator keys = jsonObject.keys();            while (keys.hasNext()) &#123;                Object key = keys.next();                Object value = jsonObject.get(key);                arrayList.add(value);            &#125;        &#125;        return arrayList;    &#125;    /* *//***     * 将对象转换为Collection对象     *      * @param object     * @return     */    /*     * public static Collection toCollection(Object object) &#123; JSONArray     * jsonArray = JSONArray.fromObject(object);     *      * return JSONArray.toCollection(jsonArray); &#125;     */    /***     * 将对象转换为JSON对象数组     *      * @param object     * @return     */    public static JSONArray toJSONArray(Object object) &#123;        return JSONArray.fromObject(object);    &#125;    /***     * 将对象转换为JSON对象     *      * @param object     * @return     */    public static JSONObject toJSONObject(Object object) &#123;        return JSONObject.fromObject(object);    &#125;    /***     * 将对象转换为HashMap     *      * @param object     * @return     */    public static HashMap toHashMap(Object object) &#123;        HashMap&lt;String, Object&gt; data = new HashMap&lt;String, Object&gt;();        JSONObject jsonObject = JSONHelper.toJSONObject(object);        Iterator it = jsonObject.keys();        while (it.hasNext()) &#123;            String key = String.valueOf(it.next());            Object value = jsonObject.get(key);            data.put(key, value);        &#125;        return data;    &#125;    /***     * 将对象转换为List&lt;Map&lt;String,Object&gt;&gt;     *      * @param object     * @return     */    // 返回非实体类型(Map&lt;String,Object&gt;)的List    public static List&lt;Map&lt;String, Object&gt;&gt; toList(Object object) &#123;        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;Map&lt;String, Object&gt;&gt;();        JSONArray jsonArray = JSONArray.fromObject(object);        for (Object obj : jsonArray) &#123;            JSONObject jsonObject = (JSONObject) obj;            Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;();            Iterator it = jsonObject.keys();            while (it.hasNext()) &#123;                String key = (String) it.next();                Object value = jsonObject.get(key);                map.put((String) key, value);            &#125;            list.add(map);        &#125;        return list;    &#125;        // 返回非实体类型(Map&lt;String,Object&gt;)的List    public static List&lt;Map&lt;String, Object&gt;&gt; toList(JSONArray jsonArray) &#123;        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;Map&lt;String, Object&gt;&gt;();        for (Object obj : jsonArray) &#123;            JSONObject jsonObject = (JSONObject) obj;            Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;();            Iterator it = jsonObject.keys();            while (it.hasNext()) &#123;                String key = (String) it.next();                Object value = jsonObject.get(key);                map.put((String) key, value);            &#125;            list.add(map);        &#125;        return list;    &#125;    /***     * 将JSON对象数组转换为传入类型的List     *      * @param &lt;T&gt;     * @param jsonArray     * @param objectClass     * @return     */    public static &lt;T&gt; List&lt;T&gt; toList(JSONArray jsonArray, Class&lt;T&gt; objectClass) &#123;        return JSONArray.toList(jsonArray, objectClass);    &#125;    /***     * 将对象转换为传入类型的List     *      * @param &lt;T&gt;     * @param jsonArray     * @param objectClass     * @return     */    public static &lt;T&gt; List&lt;T&gt; toList(Object object, Class&lt;T&gt; objectClass) &#123;        JSONArray jsonArray = JSONArray.fromObject(object);        return JSONArray.toList(jsonArray, objectClass);    &#125;    /***     * 将JSON对象转换为传入类型的对象     *      * @param &lt;T&gt;     * @param jsonObject     * @param beanClass     * @return     */    public static &lt;T&gt; T toBean(JSONObject jsonObject, Class&lt;T&gt; beanClass) &#123;        return (T) JSONObject.toBean(jsonObject, beanClass);    &#125;    /***     * 将将对象转换为传入类型的对象     *      * @param &lt;T&gt;     * @param object     * @param beanClass     * @return     */    public static &lt;T&gt; T toBean(Object object, Class&lt;T&gt; beanClass) &#123;        JSONObject jsonObject = JSONObject.fromObject(object);        return (T) JSONObject.toBean(jsonObject, beanClass);    &#125;    /***     * 将JSON文本反序列化为主从关系的实体     *      * @param &lt;T&gt;     *            泛型T 代表主实体类型     * @param &lt;D&gt;     *            泛型D 代表从实体类型     * @param jsonString     *            JSON文本     * @param mainClass     *            主实体类型     * @param detailName     *            从实体类在主实体类中的属性名称     * @param detailClass     *            从实体类型     * @return     */    public static &lt;T, D&gt; T toBean(String jsonString, Class&lt;T&gt; mainClass,            String detailName, Class&lt;D&gt; detailClass) &#123;        JSONObject jsonObject = JSONObject.fromObject(jsonString);        JSONArray jsonArray = (JSONArray) jsonObject.get(detailName);        T mainEntity = JSONHelper.toBean(jsonObject, mainClass);        List&lt;D&gt; detailList = JSONHelper.toList(jsonArray, detailClass);        try &#123;            BeanUtils.setProperty(mainEntity, detailName, detailList);        &#125; catch (Exception ex) &#123;            throw new RuntimeException(&quot;主从关系JSON反序列化实体失败！&quot;);        &#125;        return mainEntity;    &#125;    /***     * 将JSON文本反序列化为主从关系的实体     *      * @param &lt;T&gt;泛型T 代表主实体类型     * @param &lt;D1&gt;泛型D1 代表从实体类型     * @param &lt;D2&gt;泛型D2 代表从实体类型     * @param jsonString     *            JSON文本     * @param mainClass     *            主实体类型     * @param detailName1     *            从实体类在主实体类中的属性     * @param detailClass1     *            从实体类型     * @param detailName2     *            从实体类在主实体类中的属性     * @param detailClass2     *            从实体类型     * @return     */    public static &lt;T, D1, D2&gt; T toBean(String jsonString, Class&lt;T&gt; mainClass,            String detailName1, Class&lt;D1&gt; detailClass1, String detailName2,            Class&lt;D2&gt; detailClass2) &#123;        JSONObject jsonObject = JSONObject.fromObject(jsonString);        JSONArray jsonArray1 = (JSONArray) jsonObject.get(detailName1);        JSONArray jsonArray2 = (JSONArray) jsonObject.get(detailName2);        T mainEntity = JSONHelper.toBean(jsonObject, mainClass);        List&lt;D1&gt; detailList1 = JSONHelper.toList(jsonArray1, detailClass1);        List&lt;D2&gt; detailList2 = JSONHelper.toList(jsonArray2, detailClass2);        try &#123;            BeanUtils.setProperty(mainEntity, detailName1, detailList1);            BeanUtils.setProperty(mainEntity, detailName2, detailList2);        &#125; catch (Exception ex) &#123;            throw new RuntimeException(&quot;主从关系JSON反序列化实体失败！&quot;);        &#125;        return mainEntity;    &#125;    /***     * 将JSON文本反序列化为主从关系的实体     *      * @param &lt;T&gt;泛型T 代表主实体类型     * @param &lt;D1&gt;泛型D1 代表从实体类型     * @param &lt;D2&gt;泛型D2 代表从实体类型     * @param jsonString     *            JSON文本     * @param mainClass     *            主实体类型     * @param detailName1     *            从实体类在主实体类中的属性     * @param detailClass1     *            从实体类型     * @param detailName2     *            从实体类在主实体类中的属性     * @param detailClass2     *            从实体类型     * @param detailName3     *            从实体类在主实体类中的属性     * @param detailClass3     *            从实体类型     * @return     */    public static &lt;T, D1, D2, D3&gt; T toBean(String jsonString,            Class&lt;T&gt; mainClass, String detailName1, Class&lt;D1&gt; detailClass1,            String detailName2, Class&lt;D2&gt; detailClass2, String detailName3,            Class&lt;D3&gt; detailClass3) &#123;        JSONObject jsonObject = JSONObject.fromObject(jsonString);        JSONArray jsonArray1 = (JSONArray) jsonObject.get(detailName1);        JSONArray jsonArray2 = (JSONArray) jsonObject.get(detailName2);        JSONArray jsonArray3 = (JSONArray) jsonObject.get(detailName3);        T mainEntity = JSONHelper.toBean(jsonObject, mainClass);        List&lt;D1&gt; detailList1 = JSONHelper.toList(jsonArray1, detailClass1);        List&lt;D2&gt; detailList2 = JSONHelper.toList(jsonArray2, detailClass2);        List&lt;D3&gt; detailList3 = JSONHelper.toList(jsonArray3, detailClass3);        try &#123;            BeanUtils.setProperty(mainEntity, detailName1, detailList1);            BeanUtils.setProperty(mainEntity, detailName2, detailList2);            BeanUtils.setProperty(mainEntity, detailName3, detailList3);        &#125; catch (Exception ex) &#123;            throw new RuntimeException(&quot;主从关系JSON反序列化实体失败！&quot;);        &#125;        return mainEntity;    &#125;    /***     * 将JSON文本反序列化为主从关系的实体     *      * @param &lt;T&gt;     *            主实体类型     * @param jsonString     *            JSON文本     * @param mainClass     *            主实体类型     * @param detailClass     *            存放了多个从实体在主实体中属性名称和类型     * @return     */    public static &lt;T&gt; T toBean(String jsonString, Class&lt;T&gt; mainClass,            HashMap&lt;String, Class&gt; detailClass) &#123;        JSONObject jsonObject = JSONObject.fromObject(jsonString);        T mainEntity = JSONHelper.toBean(jsonObject, mainClass);        for (Object key : detailClass.keySet()) &#123;            try &#123;                Class value = (Class) detailClass.get(key);                BeanUtils.setProperty(mainEntity, key.toString(), value);            &#125; catch (Exception ex) &#123;                throw new RuntimeException(&quot;主从关系JSON反序列化实体失败！&quot;);            &#125;        &#125;        return mainEntity;    &#125;        public static String listtojson(String[] fields, int total, List list) throws Exception &#123;        Object[] values = new Object[fields.length];        String jsonTemp = &quot;&#123;\&quot;total\&quot;:&quot; + total + &quot;,\&quot;rows\&quot;:[&quot;;        for (int j = 0; j &lt; list.size(); j++) &#123;            jsonTemp = jsonTemp + &quot;&#123;\&quot;state\&quot;:\&quot;closed\&quot;,&quot;;            for (int i = 0; i &lt; fields.length; i++) &#123;                String fieldName = fields[i].toString();                values[i] = org.jeecgframework.tag.core.easyui.TagUtil.fieldNametoValues(fieldName, list.get(j));                jsonTemp = jsonTemp + &quot;\&quot;&quot; + fieldName + &quot;\&quot;&quot; + &quot;:\&quot;&quot; + values[i] + &quot;\&quot;&quot;;                if (i != fields.length - 1) &#123;                    jsonTemp = jsonTemp + &quot;,&quot;;                &#125;            &#125;            if (j != list.size() - 1) &#123;                jsonTemp = jsonTemp + &quot;&#125;,&quot;;            &#125; else &#123;                jsonTemp = jsonTemp + &quot;&#125;&quot;;            &#125;        &#125;        jsonTemp = jsonTemp + &quot;]&#125;&quot;;        return jsonTemp;    &#125;&#125;</code></pre><h2 id="kudu性能调优"><a href="#kudu性能调优" class="headerlink" title="kudu性能调优"></a><strong>kudu性能调优</strong></h2><pre><code class="python">#### 1、Kudu Tablet Server Maintenance Threads**解释：Kudu后台对数据进行维护操作，如写入数据时的并发线程数，一般设置为4，官网建议的是数据目录的3倍参数：maintenance_manager_num_threads#### 2、Kudu Tablet Server Block Cache Capacity Tablet**解释：分配给Kudu Tablet Server块缓存的最大内存量，建议是2-4G参数：block_cache_capacity_mb#### 3、Kudu Tablet Server Hard Memory Limit Kudu**解释：Tablet Server能使用的最大内存量，有多大，设置多大，tablet Server在批量写入数据时并非实时写入磁盘，而是先Cache在内存中，在flush到磁盘。这个值设置过小时，会造成Kudu数据写入性能显著下降。对于写入性能要求比较高的集群，建议设置更大的值（一般是机器内存的百分之80）参数：memory_limit_hard_bytesCgroup 内存软限制，这个限制并不会阻止进程使用超过限额的内存，只是在系统内存不足时，会优先回收超过限额的进程占用的内存，使之向限定值靠拢,当进程试图占用的内存超过了cgroups的限制，会触发out of memory，导致进程被kill掉memory.soft_limit_in_bytes：Cgroup 内存硬限制，限制该组中的进程使用的物理内存总量不超过设定值memory.limit_in_bytes：报错：Service unavailable: Soft memory limit exceeded (at 96.35% of capacity)#### 4、建议每个表50columns左右，不能超过300个**#### **5、hash分区数量\*range分区数量不能超过60个（1.7.0版本之后没限制了）**#### 6、设置block的管理器为文件管理器（默认是日志服务器）**解释：并非所有文件系统格式都需要设置该选项。ext4、xfs格式支持hole punching（打孔），所以不需要设置block_manager=file，但是ext3 格式需要。可以通过df -Th命令来查看文件系统的格式。参数：--block_manager=file#### **7、设置ntp服务器的时间误差不超过20s（默认是10s）**参数：max_clock_sync_error_usec=20000000#### **8 、设置rpc的连接时长（默认是3s，建议不要设置）**参数：--rpc_negotiation_timeout_ms=300000#### **9、设置rpc一致性选择的连接时长（默认为1s，建议不要设置）**参数：--consensus_rpc_timeout_ms=1000#### 10、记录kudu的crash的信息**解释：Kudu在Kudu遇到崩溃时，使用GoogleBreakpad库来生成minidump。这些minidumps的大小通常只有几MB，即使禁用了核心转储生成，也会生成，生成minidumps只能在Linux上建立。minidump文件包含有关崩溃的进程的重要调试信息，包括加载的共享库及其版本，崩溃时运行的线程列表，处理器寄存器的状态和每个线程的堆栈内存副本，以及CPU和操作系统版本信息。Minitump可以通过电子邮件发送给Kudu开发人员或附加到JIRA，以帮助Kudu开发人员调试崩溃。为了使其有用，开发人员将需要知道Kudu的确切版本和发生崩溃的操作系统。请注意，虽然minidump不包含堆内存转储，但它确实包含堆栈内存，因此可以将应用程序数据显示在minidump中。如果机密或个人信息存储在群集上，请不要共享minidump文件。参数： --minidump_path=minidumps                         --max_minidumps=9 （默认是在设置的log目录下生成minidumps目录，里边包含最多9个以dmp结尾的文件，无法设置为空值，需要注意的是如果自定义minidump文件，在master不能启动的情况下，需要将该目录中的文件删除）#### 11、Stack WatchLog**解释：每个Kudu服务器进程都有一个称为Stack Watchdog的后台线程，它监视服务器中的其他线程，以防它们被阻塞超过预期的时间段。这些跟踪可以指示操作系统问题或瓶颈存储。通过WARN日志信息的跟踪（Trace）可以用于诊断由于Kudu以下的系统（如磁盘控制器或文件系统）引起的根本原因延迟问题。### 12、kudu表如果不新建的情况下，在表中增加字段，对数据是没有影响的**kudu对表字段进行操作 String tableName = &quot;wyh_main&quot;;    KuduClient client = new KuduClient.KuduClientBuilder(&quot;hadoop4,hadoop5,hadoop6&quot;).defaultAdminOperationTimeoutMs(600000).build();    try &#123;        Object o = 0L;        // 创建非空的列        client.alterTable(tableName, new AlterTableOptions().addColumn(&quot;device_id&quot;, Type.INT64, o));        // 创建列为空        client.alterTable(tableName, new AlterTableOptions().addNullableColumn(&quot;site_id&quot;, Type.INT64));### 13、cdh设置多master**        参数：--master_addresses=hadoop4:7051,hadoop5:7051,hadoop6:7051### 14、kudu出现启动速度特别慢**解决办法：        1、取消所有配置参数（除了资源、时间同步）        2、升级版本到kudu1.6.0        3、client必须停止（client不占用io的情况，3台机器，每台机器60G，127分区数量，启动速度3分钟）        4、查看io使用情况 iostat -d -x -k 1 200### 15、单hash分区最大是60**### 16、安装kudu过程中，会要求CPU支持ssc4.2指令集，但是我们的虚拟机cpu没有这个执行集，所以无法安装**### 17、设置client长连接过期时间**        参数：--authn_token_validity_seconds=12960000（150天）        注意：设置到tserver的配置文件中        ### 18、tserver和master的wal和data目录要分隔（或者是目录设置为lvm卷轴）**原因：wal目录只能设置为1个参数：--fs_wal_dir_reserved_bytes用于非kudu都使用的日志目录文件系统的字节数，默认情况下是-1，每个磁盘上的磁盘空间的1%将被保留，指定的任何其他值表示保留的字节数，必须大于或等于0。         ### 19、设置用户权限，能移动tablet**参数：--superuser_acl=*        20、说明文档          http://kudu.apache.org/releases/1.5.0/docs/known_issues.html        ### 21、tserver宕掉后，5分钟后没有恢复的情况下，该机器上的tablet会移动到其他机器**参数：--follower_unavailable_considered_failed_sec=300        ### 22、超过参数时间的历史数据会被清理，如果是base数据不会被清理。而真实运行时数据大小持续累加，没有被清理。**参数：--tablet_history_max_age_sec=900</code></pre><h2 id="Hive运行参数优化"><a href="#Hive运行参数优化" class="headerlink" title="Hive运行参数优化"></a><strong>Hive运行参数优化</strong></h2><p>下边介绍一些运行HQL时的设置参数，以此达到一定程度上的优化。</p><p><strong>小文件处理的参数</strong></p><pre><code class="sql">set mapred.max.split.size=1024000000;  // 每个Map最大输入大小set mapred.min.split.size=256000000// 每个Map最小输入大小set mapred.min.split.size.per.node=256000000; //每个节点处理的最小splitset mapred.min.split.size.per.rack=256000000;//每个机架处理的最小slit//1.注意一般来说这四个参数的配置结果大小要满足如下关系。max.split.size &gt;= min.split.size &gt;= min.size.per.node &gt;= min.size.per.nodeset hive.hadoop.supports.splittable.combineinputformat=true;     set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; hive.merge.mapredfiles=false;   //reduce输出是否合并 hive.merge.mapfiles=true;     //map输出是否合并hive.merge.smallfiles.avgsize： //合并后文件的基本阈值，设置大点可以减少小文件个数，需要mapfiles和mapredfiles为true，默认值是16MB；set hive.merge.size.per.task = 256*1000*1000  //设置合并文件的大小1.修改参数  hive.merge.mapredfiles=true2.通过mapreduece的办法生成一张新的分区表,此时生成的文件变成了每个分区一个文件小结：正确处理hive小文件 是 控制map数的一个重要环节.处理的不好 会大大影响任务的执行效率.使用hadoop的archive归档#用来控制归档是否可用set hive.archive.enabled=true;#通知Hive在创建归档时是否可以设置父目录set hive.archive.har.parentdir.settable=true;#控制需要归档文件的大小set har.partfile.size=1099511627776;#使用以下命令进行归档ALTER TABLE srcpart ARCHIVE PARTITION(ds=&#39;2008-04-08&#39;, hr=&#39;12&#39;);#对已归档的分区恢复为原文件ALTER TABLE srcpart UNARCHIVE PARTITION(ds=&#39;2008-04-08&#39;, hr=&#39;12&#39;);#::注意，归档的分区不能够INSERT OVERWRITE，必须先unarchive</code></pre><p><strong>数据倾斜参数</strong></p><pre><code class="sql">hive.map.aggr=true //map端是否聚合hive.map.aggr.hash.force.flush.memory.threshold=0.9hive.map.aggr.hash.min.reduction=0.5 hive.map.aggr.hash.percentmemory=0.5 hive.groupby.skewindata=false; //是否开启倾斜优化set hive.exec.reducers.max=200;set mapred.reduce.tasks= 200;---增大Reduce个数 set hive.groupby.mapaggr.checkinterval=100000 ;--这个是group的键对应的记录条数超过这个值则会进行分拆,--值根据具体数据量设置 set hive.groupby.skewindata=true; --如果是group by过程出现倾斜 应该设置为true set hive.skewjoin.key=100000; --这个是join的键对应的记录条数超过这个值则会进行分拆,--值根据具体数据量设置 set hive.optimize.skewjoin=true;--如果是join 过程出现倾斜 -- 应该设置为true 1.通过修改参数hive.map.aggr=true // map端聚合,相当于combinerhive.groupby.skewindata=true //数据倾斜优化,为true时,查询计划生产两个mapreduce,第一个mr随机处理,第二个按照业务主键聚合,</code></pre><p><strong>分区表参数</strong></p><pre><code class="sql">hive.exec.dynamic.partition=true; -- 是否允许动态分区 hive.exec.dynamic.partition.mode=strict; -- strict是避免-- 全分区字段是动态的，必须有至少一个分区字段是指定有值的.另一个值-- 为 nonstrict //以下是配置阀值 hive.exec.max.created.files=100000; -- 一个DML操作可以创建的-- 文件数 hive.exec.max.dynamic.partitions=1000; -- 一个DML操作可以-- 创建的最大动态分区数 hive.exec.max.dynamic.partitions.pernode=100;  -- each mapper or reducer 可以创建的最大动态分区数  </code></pre><p><strong>并行执行参数</strong></p><pre><code class="sql">hive.exec.parallel=false; -- 打开任务并行执行 set hive.exec.parallel.thread.number=16; -- 同一个sql允许最大并行度，默认为8。</code></pre><p><strong>hive提供的文件合并功能</strong></p><table><thead><tr><th align="center"><strong>参数名</strong></th><th align="center"><strong>作用</strong></th></tr></thead><tbody><tr><td align="center">hive.merge.mapfiles</td><td align="center">是否在纯Map的任务（没有reduce task）后开启小文件合并</td></tr><tr><td align="center">hive.merge.mapredfiles</td><td align="center">是否在mapreduce任务后开启小文件合并</td></tr><tr><td align="center">hive.merge.sparkfiles</td><td align="center">是否在hive on spark任务后开启小文件合并</td></tr><tr><td align="center">hive.merge.smallfiles.avgsize</td><td align="center">如果原先输出的文件平均大小小于这个值，则开启小文件合并。比如输出原本有100个文件，总大小1G，那平均每个文件大小只有10M，如果我们这个参数设置为16M，这时就会开启文件合并</td></tr><tr><td align="center">hive.merge.size.per.task</td><td align="center">开启小文件合并后，预期的一个合并文件的大小。比如原先的总大小有1G，我们预期一个文件256M的话，那么最终经过合并会生成4个文件。</td></tr></tbody></table><p><strong>其它一些参数说明</strong></p><pre><code class="sql">set hive.auto.convert.join=true;解释: hive是否会根据输入文件大小将普通的join转为mapjoin，默认是true。set hive.auto.convert.join.noconditionaltask=true;set hive.vectorized.execution.enabled=false;   //向量化配置,针对orc文件set hive.vectorized.execution.reduce.enabled=false; //向量化配置,针对orc文件</code></pre><h2 id="Redis开发规范总结"><a href="#Redis开发规范总结" class="headerlink" title="Redis开发规范总结**"></a>Redis开发规范总结**</h2><h2 id="Hive开发规范总结"><a href="#Hive开发规范总结" class="headerlink" title="Hive开发规范总结"></a><strong>Hive开发规范总结</strong></h2><h2 id="Spark最佳实践"><a href="#Spark最佳实践" class="headerlink" title="Spark最佳实践"></a><strong>Spark最佳实践</strong></h2><h2 id="Docker-es-部署-spark-写入ES"><a href="#Docker-es-部署-spark-写入ES" class="headerlink" title="Docker es 部署 spark 写入ES"></a>Docker es 部署 spark 写入ES</h2><pre><code class="shell">docker search elasticsearchdocker pull elasticsearch:6.5.4docker imageselasticsearch        6.5.4               93109ce1d590        20 months ago       774MB#创建容器，并将9200端口和9300端口进行映射到本机docker run -d -p 9200:9200 -p 9300:9300 --name elasticsearch elasticsearch:6.5.4docker pshttp://localhost:9200/&#123;  &quot;name&quot; : &quot;lY2RmAj&quot;,  &quot;cluster_name&quot; : &quot;docker-cluster&quot;,  &quot;cluster_uuid&quot; : &quot;kroqIJSUTICoNJAXHD3tmA&quot;,  &quot;version&quot; : &#123;    &quot;number&quot; : &quot;6.5.4&quot;,    &quot;build_flavor&quot; : &quot;default&quot;,    &quot;build_type&quot; : &quot;tar&quot;,    &quot;build_hash&quot; : &quot;d2ef93d&quot;,    &quot;build_date&quot; : &quot;2018-12-17T21:17:40.758843Z&quot;,    &quot;build_snapshot&quot; : false,    &quot;lucene_version&quot; : &quot;7.5.0&quot;,    &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,    &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;  &#125;,  &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125;#安装elasticsearch‐head#因为我们需要修改Elasticsearch下的elasticsearch.yml文件，而容器并没有vi命令，所以我们使用文件挂载的方式来进行操作docker stop 启动docker时的镜像的ID#先查看所有的容器 docker ps -a#移除镜像中e s的容器docker rm a059e6a2bb51docker run -d -p 9200:9200 -p 9300:9300 --name elasticsearch -e &quot;ES_JAVA_OPTS=-Xms256 -Xmx512m&quot; elasticsearch:6.5.4#首先进入到容器中，然后进入到指定目录修改elasticsearch.yml文件。docker exec -it elasticsearch /bin/bashcd /usr/share/elasticsearch/config/vi elasticsearch.yml#在elasticsearch.yml的文件末尾加上:http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;#修改配置后重启容器即可。exit #退出容器docker restart elasticsearchDocker 部署 ElasticSearch-Headdocker pull mobz/elasticsearch-head:5docker search elasticsearch-headyanliangzhong/elasticsearch-head         支持es6.X的elasticsearch-head  docker pull yanliangzhong/elasticsearch-headdocker run -d --name es_admin -p 9100:9100 yanliangzhong/elasticsearch-head:latest</code></pre><pre><code class="scala">val spark: SparkSession = SparkSession.builder()      .appName(&quot;test&quot;)      .master(&quot;local[3]&quot;)      .config(&quot;es.index.auto.create&quot;, &quot;true&quot;)      .config(&quot;spark.es.batch.size.entries&quot;,&quot;5000&quot;)      .config(&quot;spark.es.batch.write.refresh&quot;,false)      .config(&quot;es.nodes&quot;, &quot;127.0.0.1:9200&quot;)      .config(&quot;es.nodes.wan.only&quot;,&quot;true&quot;)      .getOrCreate()    val rdd = spark.sparkContext.makeRDD(Array((1, &quot;闵得志&quot;, &quot;male&quot;), (2, &quot;刘德华&quot;, &quot;femal&quot;), (3, &quot;周润发&quot;, &quot;document&quot;)))      .map(tp=&gt;&#123;Person(tp._1,tp._2,tp._3)&#125;)    //数据写入es    import org.elasticsearch.spark._    rdd.saveToEs(&quot;sql_command/sql_info&quot;, Map(&quot;es.mapping.id&quot; -&gt; &quot;id&quot;))    spark.stop()  &#125;  case class Person(                   var id:Int,                   var name:String,                   var sex:String                   )</code></pre><h2 id="java多线程的四种实现方式"><a href="#java多线程的四种实现方式" class="headerlink" title="java多线程的四种实现方式"></a>java多线程的四种实现方式</h2><pre><code class="java">**第一种：**1: 继承Thread类****2: 重写run方法（线程要做的事，定义在方法中）****3：创建一个子类的对象****4：使用该对象，调用start方法，开启新线程。**public class ThreadDemo &#123;    public static void main(String[] args) &#123;        Demo d1 = new Demo(&quot;Jack&quot;);        Demo d2 = new Demo(&quot;Rose&quot;);        System.out.println(Thread.currentThread().getName());//获取当前线程名字,当前线程是主线程   主线程的名字是main        d1.setName(&quot;Jack线程&quot;);//修改        d2.setName(&quot;Rose线程&quot;);        System.out.println(d1.getName());//获取线程的名称        System.out.println(d2.getName());        d2.start();//新线程        d1.start();    &#125;&#125;class Demo extends Thread&#123;    String nickName;    public Demo(String nickName) &#123;        this.nickName = nickName;    &#125;    String name;     public void run() &#123;        for(int i=0;i&lt;30;i++) &#123;            System.out.println(Thread.currentThread().getName()+&quot;---&quot;+nickName+&quot;------&quot;+i);        &#125;    &#125;&#125;</code></pre><pre><code class="java">**第二种方式**:**1. 实现Runnable接口****2. 重写run方法****3. 创建Runnable接口的子类对象** **4. 创建Thread类的对象,把第三步的对象传到构造方法中****5. 使用Thread类的对象,调用start方法**public class ThreadTest &#123;    public static void main(String[] args) &#123;        new Thread(new PrintUpperCase()).start();        new Thread(new PrintLowerCase()).start();    &#125;&#125;class PrintUpperCase implements Runnable&#123;    @Override    public void run() &#123;        for(char i=&#39;A&#39;;i&lt;=&#39;Z&#39;;i++) &#123;            System.out.println(i);            Thread.yield();        &#125;    &#125;&#125;class PrintLowerCase implements Runnable&#123;    @Override    public void run() &#123;        for(char i=&#39;a&#39;;i&lt;=&#39;z&#39;;i++) &#123;            System.out.println(i);            Thread.yield();        &#125;    &#125;&#125;</code></pre><pre><code class="java">**第三种方式**:**1：创建线程池对象****2：线程池调用execute()方法****3:  execute()方法中传入一个Runable接口的子类的对象（匿名内部类）**例：ExecutorService pool = Executors.newCachedThreadPool();           pool.execute(new Runnable() &#123;            @Override            public void run() &#123;              该方法体中定义了线程任务                &#125;            &#125;        &#125;);</code></pre><pre><code class="java">**第四种方式:通过Callable和FutureTask创建线程****a:创建Callable接口的实现类 ，并实现Call方法** **b:创建Callable实现类的实现，使用FutureTask类包装Callable对象，该FutureTask对象封装了Callable对象的Call方法的返回值****c:使用FutureTask对象作为Thread对象的target创建并启动线程 (start方法）****d:调用FutureTask对象的get()来获取子线程执行结束的返回值**public class Test2 &#123;    public static void main(String[] args) throws InterruptedException, ExecutionException &#123;        // 创建线程任务对象        Demo demo = new Demo();        //创建FutureTask        FutureTask&lt;String&gt; f = new FutureTask&lt;String&gt;(demo);        //开启线程        new Thread(f).start();        //获取线程返回的数据        String res = f.get();        System.out.println(res);    &#125;&#125;</code></pre><h2 id="KUDU-API的操作"><a href="#KUDU-API的操作" class="headerlink" title="KUDU-API的操作"></a><strong>KUDU-API的操作</strong></h2><pre><code class="java">引入pom.xml依赖&lt;dependency&gt;    &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt;    &lt;artifactId&gt;kudu-client&lt;/artifactId&gt;    &lt;version&gt;1.6.0&lt;/version&gt;&lt;/dependency&gt;import org.apache.kudu.ColumnSchem;import org.apache.kudu.Schema;import org.apache.kudu.Type;import org.apache.kudu.client.CreateTableOptions;import org.apache.kudu.client.KuduClient;import org.apache.kudu.client.KuduException;import java.util.LinkedList;import java.util.List;public class CreateTable&#123;    private static ColumnSchema newColumn(String name,Type type,boolean iskey)&#123;    ColumnScheama.ColumnSchemaBuilder column=new ColumnSchema.ColumnSchemaBuilder(name,type);    column.key(iskey);    return column.build();&#125;    public static void main(String[] args)throws KuduException&#123;    // master 地址    String masteraddr = &quot;hdp-101,hdp-102,hdp-103&quot;    //创建kudu的数据库连接    KuduClient client = new KuduClient.KuduClientBuilder(masteraddr).defaultSocketReadTimeoutMS(6000).build();    //设置表的schema    List&lt;ColumnSchema&gt; columns = new LinkedList&lt;&gt;();    /**        与RDBMS不同，kudu不提供自动自增功能，因此应用程序必须始终在插入期间提供完整的主键 */    colums.add(newColumn(&quot;id&quot;,Type.INT32，true));    colums.add(newColumn(&quot;name&quot;,Type.String，true));     Schema  schema = new Schema(columns);       // 创建表时提供的所有选项    CreateTableOptions options = new CreateTableOptions();    // 设置表的replication备份和分区规则    List&lt;String&gt; parcols = new LinkedList&lt;~&gt;();    parcols.add(&quot;id&quot;);    // 设置表的备份数    options.setNumReplicas(1);    // 设置range分区和数量    options.setRangePartitionColumns(parcols,3);   // 设置hash分区和数量   // options.addHashPartitions(parcols,3);    try&#123;        client.createTable(&quot;student&quot;,schema,options)        &#125;catch(KuduException e)&#123;            e.printStackTrace();        &#125;finally&#123;            client.close()        &#125;&#125;&#125;</code></pre><pre><code class="java">/*** 向表中加载数据* @throws KuduException*/@Testpublic void loadData() throws KuduException &#123;//打开表KuduTable kuduTable = kuduClient.openTable(tableName);   //创建KuduSession对象 kudu必须通过KuduSession写入数据KuduSession kuduSession = kuduClient.newSession();   //采用flush方式 手动刷新kuduSession.setFlushMode(SessionConfiguration.FlushMode.MANUAL_FLUSH);kuduSession.setMutationBufferSpace(3000);   //准备数据for(int i=1; i&lt;=10; i++)&#123;Insert insert = kuduTable.newInsert();//设置字段的内容insert.getRow().addInt(&quot;CompanyId&quot;,i);insert.getRow().addInt(&quot;WorkId&quot;,i);insert.getRow().addString(&quot;Name&quot;,&quot;lisi&quot;+i);insert.getRow().addString(&quot;Gender&quot;,&quot;male&quot;);insert.getRow().addString(&quot;Photo&quot;,&quot;person&quot;+i);  kuduSession.flush();kuduSession.apply(insert);&#125;  kuduSession.close();kuduClient.close();&#125;</code></pre><pre><code class="scala">package org.kududb.spark.demo.basicimport org.kududb.client.KuduClientobject ScanTable &#123;  def main(args:Array[String]): Unit = &#123;    if (args.length == 0) &#123;      println(&quot;&lt;kuduMaster&gt; &lt;tableName&gt; &lt;limit&gt;&quot;)      return    &#125;    val kuduMaster = args(0)    val tableName = args(1)    val limit = args(2).toInt    val kuduClient = new KuduClient.KuduClientBuilder(kuduMaster).build()    val table = kuduClient.openTable(tableName)    println(&quot;starting scan&quot;)    val scannerX = kuduClient.newScannerBuilder(table).build()    while (scannerX.hasMoreRows) &#123;      val rows = scannerX.nextRows()      while (rows.hasNext) &#123;        val row = rows.next()        println(&quot; - &quot; + row.rowToString())      &#125;    &#125;    println(&quot;finished scan&quot;)    scannerX.close();    kuduClient.close()  &#125;&#125;</code></pre><h2 id="最受欢迎的JVM参数"><a href="#最受欢迎的JVM参数" class="headerlink" title="最受欢迎的JVM参数"></a><strong>最受欢迎的JVM参数</strong></h2><pre><code class="java">1.-Xms：初始堆大小。只要启动，就占用的堆大小。2.-Xmx：最大堆大小。java.lang.OutOfMemoryError：Java heap这个错误可以通过配置-Xms和-Xmx参数来设置。3.-Xss：栈大小分配。栈是每个线程私有的区域，通常只有几百K大小，决定了函数调用的深度，而局部变量、参数都分配到栈上。当出现大量局部变量，递归时，会发生栈空间OOM（java.lang.StackOverflowError）之类的错误。4.XX:NewSize：设置新生代大小的绝对值。5.-XX:NewRatio：设置年轻代和年老代的比值。比如设置为3，则新生代：老年代=1:3，新生代占总heap的1/4。6.-XX:MaxPermSize：设置持久代大小。java.lang.OutOfMemoryError:PermGenspace这个OOM错误需要合理调大PermSize和MaxPermSize大小。7.-XX:SurvivorRatio：年轻代中Eden区与两个Survivor区的比值。注意，Survivor区有form和to两个。比如设置为8时，那么eden:form:to=8:1:1。8.-XX:HeapDumpOnOutOfMemoryError：发生OOM时转储堆到文件，这是一个非常好的诊断方法。9.-XX:HeapDumpPath：导出堆的转储文件路径。10.-XX:OnOutOfMemoryError：OOM时，执行一个脚本，比如发送邮件报警，重启程序。后面跟着一个脚本的路径。</code></pre><h2 id="Spark提交参数说明和常见优化"><a href="#Spark提交参数说明和常见优化" class="headerlink" title="Spark提交参数说明和常见优化"></a><strong>Spark提交参数说明和常见优化</strong></h2><pre><code class="python">./bin/spark-submit --class org.apache.spark.examples.SparkPi \    --master yarn \    --deploy-mode cluster \    --driver-memory 4g \    --executor-memory 2g \    --executor-cores 1 \    --queue thequeue \    --jars my-other-jar.jar,my-other-other-jar.jar \    examples/jars/spark-examples*.jar \    app_arg1 app_arg2    在提交任务时的几个重要参数:executor-cores —— 每个executor使用的内核数，默认为1num-executors —— 启动executors的数量，默认为2executor-memory —— executor内存大小，默认1Gdriver-cores —— driver使用内核数，默认为1driver-memory —— driver内存大小，默认512Mexecutor_cores*num_executors 表示的是能够并行执行Task的数目 不宜太小或太大！一般不超过总队列 cores 的 25%，比如队列总 cores 400，最大不要超过100，最小不建议低于 40，除非日志量很小。executor_cores 不宜为1！否则 work 进程中线程数过少，一般 2~4 为宜。executor_memory 一般 6~10g 为宜，最大不超过20G，否则会导致GC代价过高，或资源浪费严重。driver-memory driver 不做任何计算和存储，只是下发任务与yarn资源管理器和task交互，除非你是 spark-shell，否则一般 1-2g增加每个executor的内存量，增加了内存量以后，对性能的提升，有三点：1、如果需要对RDD进行cache，那么更多的内存，就可以缓存更多的数据，将更少的数据写入磁盘， 甚至不写入磁盘。减少了磁盘IO。2、对于shuffle操作，reduce端，会需要内存来存放拉取的数据并进行聚合。如果内存不够，也会写入磁盘。如果给executor分配更多内存以后，就有更少的数据，需要写入磁盘，甚至不需要写入磁盘。减少了磁盘IO，提升了性能。3、对于task的执行，可能会创建很多对象。如果内存比较小，可能会频繁导致JVM堆内存满了，然后频繁GC，垃圾回收，minor GC和full GC。（速度很慢）。内存加大以后，带来更少的GC，垃圾回收，避免了速度变慢，性能提升。常规注意事项:预处理数据，丢掉一些不必要的数据增加Task的数量过滤掉一些容易导致发生倾斜的key避免创建重复的RDD尽可能复用一个RDD对多次使用的RDD进行持久化尽量避免使用shuffle算子在要使用groupByKey算子的时候,尽量用reduceByKey或者aggregateByKey算子替代.因为调用groupByKey时候,按照相同的key进行分组,形成RDD[key,Iterable[value]]的形式,此时所有的键值对都将被重新洗牌,移动,对网络数据传输造成理论上的最大影响.使用高性能的算子## spark资源配置spark on Yarn资源参数配置方式代码配置：conf = SparkConf()conf.set(&quot;spark.executor.instances&quot;, &quot;50&quot;)conf.set(&quot;spark.executor.memory&quot;, &quot;8g&quot;)conf.set(&quot;spark.executor.cores&quot;, &quot;2&quot;)conf.set(&quot;spark.yarn.executor.memoryOverhead&quot;, &quot;2g&quot;)SparkSession.builder.master(&quot;yarn&quot;).appName(&quot;aml&quot;).config(conf=conf) \  .enableHiveSupport().getOrCreate() 脚本配置：spark-submit \ --name aml --master yarn \ --deploy-mode cluster \ --executor-memory 20G \  --num-executors 50 \  --driver-memory 4g \  --executor-cores 2 \ /path/to/examples.jar 1000 spark.executor.instances参数说明：设置Spark作业的Executor总个数。推荐设置：50~100参数详解：Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。spark.executor.memory参数说明：设置每个Executor的内存。推荐设置：4~8g参数详解：Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。另外这个参数最大不能超过32G。spark.executor.cores参数说明：设置每个Executor占用的CPU core数量。推荐设置：2~4参数详解：这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么executors个数 * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。核数与内存需满足一定比例，通常在1：4左右，如果任务gc时间较长，可以考虑增大内存，资源不够则减少核数。spark.driver.memory参数说明：设置Driver的内存。推荐设置：默认1G参数详解：Driver端的内存是本地的，通常是存放spark job的执行结果以及运行本地代码使用的。如果需要使用collect、collectAsMap等算子将RDD的大量数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。spark.default.parallelism参数说明：设置shuffle后的stage的默认task数量。（仅rdd算子有效）推荐设置：500~1000参数详解：这个参数很重要，如果不设置可能会直接影响Spark作业性能。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。spark.sql.shuffle.partitions参数说明：设置shuffle后stage的默认task数量。（仅spark SQL有效）推荐设置：500~1000参数详解：与spark.default.parallelism一样。不同的是这个参数是对spark SQL的shuffle有效，对rdd的shuffle无效。spark.yarn.executor.memoryOverhead参数说明：设置每个executor的对外内存。推荐设置：默认是executorMemory的10%，但不会低于384M。参数详解：对于pyspark开发的程序，对executor的对外内存要求会更高。当算子里的自定义函数比较复杂时，有可能使job失败，建议适当调大这个参数。如果无效，需减少executor-cores，减少出错stage的task数量。</code></pre><h2 id="JVM常用调优-x2F-调试参数"><a href="#JVM常用调优-x2F-调试参数" class="headerlink" title="JVM常用调优&#x2F;调试参数:"></a>JVM常用调优&#x2F;调试参数:</h2><table><thead><tr><th><strong>-Xms<size></size></strong></th><th>例如:-Xms2G,设置Heap的初始大小</th></tr></thead><tbody><tr><td>-Xmx<size></size></td><td>设置Heap的最大尺寸,建议为最大物理内存的1&#x2F;3(建议此值不超过12G)</td></tr><tr><td>-Xss<size></size></td><td>方法栈大小</td></tr><tr><td>-Xmn<size></size></td><td>新生代大小</td></tr><tr><td>-XX:NewSize&#x3D;<value>-XX:MaxNewSize&#x3D;<value>-XX:SurvivorRatio&#x3D;<value></value></value></value></td><td>新生代设置.单位为byteSurvivorRatio &#x3D; Eden&#x2F;survivor;例如此值为3,则表示,Eden : from : to &#x3D; 3:1:1;默认值为8</td></tr><tr><td>-XX:PermSize&#x3D;<value>-XX:MaxPermSize&#x3D;<value></value></value></td><td>方法区大小,max值默认为64M</td></tr><tr><td>-XX:[+ | -]UseConcMarkSweepGC</td><td>打开或关闭基于ParNew + CMS + SerialOld的收集器组合.(-XX:+UseParNewGC)</td></tr><tr><td>-XX:[+ | -]UseParallelGC</td><td>在server模式下的默认值(+),表示使用Parallel Scavenge + Serial Old收集器组合</td></tr><tr><td>-XX:[+ | -]UseParallelOldGC</td><td>默认关闭,+表示打开&#x2F;使用Parallel Scavenge + Parallel Old收集器组合</td></tr><tr><td>-XX:PretenureSizeThreshold&#x3D;<value></value></td><td>直接晋升为旧生代的对象大小.大于此值的将会直接被分配到旧生代,单位byte</td></tr><tr><td>-XX:MaxTenuringThreshold&#x3D;<value></value></td><td>晋升到旧生代的对象年龄(已经或者即将被回收的次数);每个对象被minor GC一次,它的年龄+1,如果对象的年龄达到此值,将会进入旧生代.</td></tr><tr><td>-XX:[+ | -]UseAdaptiveSizePolicy</td><td>默认开启;是否动态调整java中堆中各个区域大小以及进入旧生代的年龄;此参数可以方便我们对参数调优,找到最终适合配置的参数.</td></tr><tr><td>-XX:[+ | -]HandlePromotionFailure</td><td>JDK1.6默认开启,是否支持内存分配失败担保策略;在发生Minor GC时，虚拟机会检测之前每次晋升到老年代的平均大小是否大于老年代的剩余空间大小，如果大于，则改为直接进行一次Full GC。如果小于，则查看HandlePromotionFailure设置是否允许担保失败；如果允许，那只会进行Minor GC；如果不允许，则也要改为进行一次Full GC。</td></tr><tr><td>-XX:ParallelGCThreads&#x3D;<value></value></td><td>并行GC时所使用的线程个数.建议保持默认值(和CPU个数有换算关系).</td></tr><tr><td>-XX:GCTimeRatio&#x3D;<value></value></td><td>GC占JVM服务总时间比.默认为99,即允许1%的GC时间消耗.此参数只在Parallel Scavenge收集器下生效</td></tr><tr><td>-XX:CMSInitiatingOccupancyFraction&#x3D;<value></value></td><td>设置CMS收集器在旧生代空间使用占比达到此值时,触发GC.</td></tr><tr><td>-XX:[+ | -]UseCMSCompactAtFullCollection</td><td>默认开启,表示在CMS收集器进行一次Full gc后是否进行一次内存碎片整理,[原因:CMS回收器会带来内存碎片]</td></tr><tr><td>-XX:CMSFullGCSBeforeCompaction&#x3D;<value></value></td><td>进行多少次FullGC之后,进行一次内存碎片整理.[原因:CMS回收器会带来内存碎片]</td></tr></tbody></table><h2 id="Elasticsearch性能优化实战指南"><a href="#Elasticsearch性能优化实战指南" class="headerlink" title="Elasticsearch性能优化实战指南"></a><strong>Elasticsearch性能优化实战指南</strong></h2><pre><code class="python">#### **1、索引层面优化配置**默认情况下，6.x及之前的版本中Elasticsearch索引有5个主分片和1个副本，7.X及之后版本1主1副。 这种配置并不适用于所有业务场景。 需要正确设置分片配置，以便维持索引的稳定性和有效性。**1.1、分片大小**分片大小对于搜索查询非常重要。一方面， 如果分配给索引的分片太多，则Lucene分段会很小，从而导致开销增加。当同时进行多个查询时，许多小分片也会降低查询吞吐量。另一方面，太大的分片会导致搜索性能下降和故障恢复时间更长。Elasticsearch官方建议一个分片的大小应该在20到40 GB左右。例如，如果您计算出索引将存储300 GB的数据，则可以为该索引分配9到15个主分片。 根据集群大小，假设群集中有10个节点，您可以选择为此索引分配10个主分片，以便在集群节点之间均匀分配分片。深入原理推荐：Elasticsearch之如何合理分配索引分片？https://www.elastic.co/cn/blog/how-many-shards-should-i-have-in-my-elasticsearch-clusterElasticsearch究竟要设置多少分片数？https://qbox.io/blog/optimizing-elasticsearch-how-many-shards-per-index**1.2、数据动态持续写入场景**如果存在连续写入到Elasticsearch集群的数据流，如：实时爬虫互联网数据写入ES集群。则应使用基于时间的索引以便更轻松地维护索引。如果写入数据流的吞吐量随时间而变化，则需要适当地改变下一个索引的配置才能实现数据的动态扩展。那么，如何查询分散到不同的基于时间索引的所有文档？答案是别名。可以将多个索引放入别名中，并且对该别名进行搜索会使查询就像在单个索引上一样。当然，需要保持好平衡。注意思考：将多少数据写入别名？别名上写入太多小索引会对性能产生负面影响。例如，是以周还是以月为单位为单位建立索引是需要结合业务场景平衡考虑的问题？如果以月为单位建议索引性能最优，那么相同数据以周为单位建立索引势必会因为索引太多导致负面的性能问题。**1.3、Index Sorting**注意：索引排序机制是6.X版本才有的特性。在Elasticsearch中创建新索引时，可以配置每个分片中的分段的排序方式。 默认情况下，Lucene不会应用任何排序。 index.sort.* 定义应使用哪些字段对每个Segment内的文档进行排序。PUT /twitter &#123;     &quot;settings&quot; : &#123;         &quot;index&quot; : &#123;             &quot;sort.field&quot; : &quot;date&quot;,              &quot;sort.order&quot; : &quot;desc&quot;          &#125;     &#125;,     &quot;mappings&quot;: &#123;        &quot;properties&quot;: &#123;            &quot;date&quot;: &#123;                &quot;type&quot;: &quot;date&quot;            &#125;        &#125;    &#125;&#125;目的：index sorting是优化Elasticsearch检索性能的非常重要的方式之一。大白话：index sorting机制通过写入的时候指定了某一个或者多个字段的排序方式，会极大提升检索的性能。更深原理：推荐阅读：https://www.elastic.co/cn/blog/index-sorting-elasticsearch-6-0    #### **2、分片层面优化配置**分片是底层基本的读写单元，分片的目的是分割巨大索引，让读写并行执行。写入过程先写入主分片，主分片写入成功后再写入副本分片。副本分片的出现，提升了集群的高可用性和读取吞吐率。在优化分片时，分片的大小、节点中有多少分片是主要考虑因素。副本分片对于扩展搜索吞吐量很重要，如果硬件条件允许，则可以小心增加副本分片的数量。容量规划的一个很好的启动点是分配分片，“《深入理解Elasticsearch》强调：最理想的分片数量应该依赖于节点的数量。”其数量是节点数量的1.5到3倍。分配副本分片数的公式：max（max_failures，ceil（num_nodes /） num_primaries） -  1）。原理：如果您的群集具有num_nodes节点，总共有num_primaries主分片，如果您希望最多能够同时处理max_failures节点故障，那么适合您的副本数量为如上公式值。公式来源：https://www.elastic.co/guide/en/elasticsearch/reference/master/tune-for-search-speed.html总的来说：节点数和分片数、副本数的简单计算公式如下：所需做大节点数=分片数*（副本数+1）。#### **3、Elasticsearch整体层面配置**配置Elasticsearch集群时，最主要的考虑因素之一是确保至少有一半的可用内存进入文件系统缓存，以便Elasticsearch可以将索引的hot regions保留在物理内存中。在设计集群时还应考虑物理可用堆空间。 Elasticsearch建议基于可用堆空间的分片分配最多应为20个分片/ GB，这是一个很好的经验法则。例如，具有30 GB堆的节点最多应有600个分片，以保持集群的良好状态。一个节点上的存储可以表述如下：节点可以支持的磁盘空间= 20 （堆大小单位：GB）（以GB为单位的分片大小），由于在高效集群中通常会看到大小在20到40 GB之间的分片，因此最大存储空间可以支持16 GB可用堆空间的节点，最多可达12 TB的磁盘空间（20*16*40=12.8TB）。边界意识有助于为更好的设计和未来的扩展操作做好准备。可以在运行时以及初始阶段进行许多配置设置。在构建Elasticsearch索引和集群本身以获得更好的搜索性能时，了解在运行时哪些配置可以修改以及哪些配不可以修改是至关重要的。**3.1 动态设置****1、设置历史数据索引为只读状态。**基于时间的动态索引的执行阶段，如果存放历史数据的索引没有写操作，可以将月度索引设置为只读模式，以提高对这些索引的搜索性能。6.X之后的只读索引实战设置方式：PUT /twitter/_settings&#123;  &quot;index.blocks.read_only_allow_delete&quot;: null&#125;**2、对只读状态索引，进行段合并。**当索引设置为只读时，可以通过强制段合并操作以减少段的数量。优化段合并将导致更好的搜索性能，因为每个分片的开销取决于段的计数和大小。注意1：不要将段合并用于读写索引，因为它将导致产生非常大的段（每段&gt; 5Gb）。注意2：此操作应在非高峰时间进行，因为这是一项非常耗资源的操作。段合并操作实战方式：curl -X POST &quot;localhost:9200/kimchy/_forcemerge only_expunge_deletes=false&amp;max_num_segments=100&amp;flush=true&quot;    **3、使用preference优化缓存利用率**有多个缓存可以帮助提高搜索性能，例如文件系统缓存，请求缓存或查询缓存。然而，所有这些缓存都维护在节点级别，这意味着如果您在拥有1个或更多副本且基于默认路由算法集群上连续两次运行相同的请求，这两个请求将转到不同的分片副本上 ，阻止节点级缓存帮助。由于搜索应用程序的用户一个接一个地运行类似的请求是常见的，例如为了检索分析索引的部分较窄子集，使用preference标识当前用户或会话的偏好值可以帮助优化高速缓存的使用。preference实战举例：GET /_search?preference=xyzabc123&#123;    &quot;query&quot;: &#123;        &quot;match&quot;: &#123;            &quot;title&quot;: &quot;elasticsearch&quot;        &#125;    &#125;&#125;**4、禁止交换**可以在每个节点上禁用交换以确保稳定性，并且应该不惜一切代价避免交换。它可能导致垃圾收集持续数分钟而不是毫秒，并且可能导致节点响应缓慢甚至断开与集群的连接。在Elasticsearch分布式系统中，让操作系统终止节点更有效。可以通过将bootstrap.memory_lock设置为True来禁用它。Linux系统级配置：sudo swapoff -aElasticsearch配置文件elasticsearch.yml配置：bootstrap.memory_lock: true**5、增加刷新间隔 refresh_interval**默认刷新间隔为1秒。这迫使Elasticsearch每秒创建一个分段。实际业务中，应该根据使用情况增加刷新间隔，举例：增加到30秒。这样之后，30s产生一个大的段，较每秒刷新大大减少未来的段合并压力。最终会提升写入性能并使搜索查询更加稳定。更新刷新间隔实战：    PUT /twitter/_settings&#123;    &quot;index&quot; : &#123;        &quot;refresh_interval&quot; : &quot;1s&quot;    &#125;&#125;**6、设置max_thread_count**index.merge.scheduler.max_thread_count默认设置为Math.max(1, Math.min(4, Runtime.getRuntime().availableProcessors() / 2))但这适用于SSD配置。对于HDD，应将其设置为1。实战：curl -XPUT &#39;localhost:9200/_settings&#39; -d &#39;&#123;      &quot;index.merge.scheduler.max_thread_count&quot; : 1&#125;**7、禁止动态分配分片**有时，Elasticsearch将重新平衡集群中的分片。此操作可能会降低检索的性能。在生产模式下，需要时，可以通过cluster.routing.rebalance.enable设置将重新平衡设置为none。PUT /_cluster/settings &#123;   &quot;transient&quot; : &#123;    &quot;cluster.routing.allocation.enable&quot; : &quot;none&quot;  &#125;&#125;其中典型的应用场景之包括：1:集群中临时重启、剔除一个节点；2:集群逐个升级节点；当您关闭节点时，分配过程将立即尝试将该节点上的分片复制到集群中的其他节点，从而导致大量浪费的IO. 在关闭节点之前禁用分配可以避免这种情况。更多实践，推荐阅读：https://www.elastic.co/guide/en/elasticsearch/reference/5.5/restart-upgrade.html**8、充分利用近似日期缓存效果**现在使用的日期字段上的查询通常不可缓存，因为匹配的范围一直在变化。然而，就用户体验而言，切换到近似日期通常是可接受的，并且能更好地使用查询高速缓存带来的益处。实战如下:GET index/_search &#123;  &quot;query&quot;: &#123;     &quot;constant_score&quot;: &#123;       &quot;filter&quot;: &#123;         &quot;range&quot;: &#123;           &quot;my_date&quot;: &#123;             &quot;gte&quot;: &quot;now-1h/m&quot;,             &quot;lte&quot;: &quot;now/m&quot;          &#125;        &#125;      &#125;    &#125;  &#125;&#125;这里可能不大好理解，推荐深入阅读：https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-search-speed.html    #### **3.2 初始设置****1、合并多字段提升检索性能**query_string或multi_match查询所针对的字段越多，检索越慢。提高多个字段的搜索速度的常用技术是在索引时将其值复制到单个字段中。对于经常查询的某些字段，请使用Elasticsearch的copy-to功能。例如，汽车的品牌名称，发动机版本，型号名称和颜色字段可以与复制到指令合并。它将改善在这些字段上进行的搜索查询性能。PUT movies&#123;  &quot;mappings&quot;: &#123;    &quot;properties&quot;: &#123;      &quot;cars_infos&quot;: &#123;        &quot;type&quot;: &quot;text&quot;      &#125;,      &quot;brand_name&quot;: &#123;        &quot;type&quot;: &quot;text&quot;,        &quot;copy_to&quot;: &quot;cars_infos&quot;      &#125;,      &quot;engine_version&quot;: &#123;        &quot;type&quot;: &quot;text&quot;,        &quot;copy_to&quot;: &quot;cars_infos&quot;      &#125;,   &quot;model &quot;: &#123;        &quot;type&quot;: &quot;text&quot;,        &quot;copy_to&quot;: &quot;cars_infos&quot;      &#125;,   &quot;color&quot;: &#123;        &quot;type&quot;: &quot;text&quot;,        &quot;copy_to&quot;: &quot;cars_infos&quot;      &#125;    &#125;  &#125;&#125;**2、设置分片分配到指定节点**实战业务中经常遇到的业务场景问题：如何将分片设置非均衡分配，有新节点配置极高，能否多分片点过去？某个 shard 分配在哪个节点上，一般来说，是由 ES 自动决定的。以下几种情况会触发分配动作：1）新索引生成2）索引的删除3）新增副本分片4）节点增减引发的数据均衡ES 提供了一系列参数详细控制这部分逻辑，其中之一是：在异构集群的情为具有更好硬件的节点的分片分配分配权重。为了分配权重，需要设置cluster.routing.allocation.balance.shard值，默认值为0.45f。数值越大越倾向于在节点层面均衡分片。实战：PUT _cluster/settings&#123;“transient” : &#123;“cluster.routing.allocation.balance.shard” : 0.60&#125;&#125;**3、调整熔断内存比例大小**查询本身也会对响应的延迟产生重大影响。为了在查询时不触发熔断并导致Elasticsearch集群处于不稳定状态，可以根据查询的复杂性将indices.breaker.total.limit设置为适合您的JVM堆大小。此设置的默认值是JVM堆的70％。PUT /_cluster/settings&#123;  &quot;persistent&quot; : &#123;    &quot;indices.breaker.fielddata.limit&quot; : &quot;60%&quot;   &#125;&#125;最好为断路器设置一个相对保守点的值。更深原理推荐阅读：https://www.elastic.co/guide/cn/elasticsearch/guide/current/_limiting_memory_usage.html。《Elastic源码分析》作者张超指出：“Elasticsearch 7.0 增加了 indices.breaker.total.use_real_memory 配置项，可以更加精准的分析当前的内存情况，及时防止 OOM 出现。虽然该配置会增加一点性能损耗，但是可以提高 JVM 的内存使用率，增强了节点的保护机制。”**4、特定搜索场景，增加搜索线程池配置**默认情况下，Elasticsearch将主要用例是搜索。在需要增加检索并发性的情况下，可以增加用于搜索设置的线程池，与此同时，可以根据节点上的CPU中的核心数量多少斟酌减少用于索引的线程池。举例：更改配置文件elasticsearch.yml增加如下内容：1thread_pool.search.queue_size: 5002#queue_size允许控制没有线程执行它们的挂起请求队列的初始大小。官方：https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html**5、打开自适应副本选择**应打开自适应副本选择。该请求将被重定向到响应最快的节点。当存在多个数据副本时，elasticsearch可以使用一组称为自适应副本选择的标准，根据包含每个分片副本的节点的响应时间，服务时间和队列大小来选择数据的最佳副本。这样可以提高查询吞吐量并减少搜索量大的应用程序的延迟。这个配置默认是关闭的，实战打开方法：PUT /_cluster/settings&#123;    &quot;transient&quot;: &#123;        &quot;cluster.routing.use_adaptive_replica_selection&quot;: true    &#125;&#125;#### **4、小结**Elasticsearch集群有许多配置设置可以减少响应延迟，提升检索性能。 以上只是冰山一角。更多实践配置推荐阅读官方文档之鼻祖级优化指南：https://www.elastic.co/guide/en/elasticsearch/reference/6.7/tune-for-search-speed.html</code></pre><h2 id="Elasticsearch性能优化实践"><a href="#Elasticsearch性能优化实践" class="headerlink" title="Elasticsearch性能优化实践"></a><strong>Elasticsearch性能优化实践</strong></h2><pre><code class="python">### **1、集群规划优化实践**#### **1.1 基于目标数据量规划集群**在业务初期，经常被问到的问题，要几个节点的集群，内存、CPU要多大，要不要SSD？最主要的考虑点是：你的`目标存储数据量`是多大？可以针对目标数据量反推节点多少。#### **1.2 要留出容量Buffer**注意：Elasticsearch有三个警戒水位线，磁盘使用率达到85%、90%、95%。不同警戒水位线会有不同的应急处理策略。这点，磁盘容量选型中要规划在内。控制在`85%之下`是合理的。当然，也可以通过配置做调整。#### **1.3 ES集群各节点尽量不要和其他业务功能复用一台机器。**除非内存非常大。举例：普通服务器，安装了ES+Mysql+redis，业务数据量大了之后，势必会出现内存不足等问题。#### **1.4 磁盘尽量选择SSD**Elasticsearch官方文档肯定`推荐SSD`，考虑到成本的原因。需要结合业务场景，如果业务对写入、检索速率有较高的速率要求，建议使用SSD磁盘。阿里的业务场景，SSD磁盘比机械硬盘的速率提升了5倍。但要因业务场景而异。#### **1.5 内存配置要合理**官方建议：堆内存的大小是官方建议是：Min（32GB，机器内存大小/2）。Medcl和wood大叔都有明确说过，不必要设置32/31GB那么大，建议：`热数据设置：26GB，冷数据：31GB`。总体内存大小没有具体要求，但肯定是内容越大，检索性能越好。经验值供参考：每天200GB+增量数据的业务场景，服务器至少要64GB内存。除了JVM之外的预留内存要充足，否则也会经常OOM。#### **1.6 CPU核数不要太小**CPU核数是和ESThread pool关联的。和写入、检索性能都有关联。建议：`16核+`。#### **1.7 超大量级的业务场景，可以考虑跨集群检索**除非业务量级非常大，例如：滴滴、携程的PB+的业务场景，否则基本不太需要跨集群检索。#### **1.8 集群节点个数无需奇数**ES内部维护集群通信，不是基于zookeeper的分发部署机制，所以，`无需奇数`。但是discovery.zen.minimum_master_nodes的值要设置为：候选主节点的个数/2+1，才能有效避免脑裂。#### **1.9 节点类型优化分配**集群节点数：&lt;=3，建议：所有节点的master：true， data：true。既是主节点也是路由节点。 集群节点数：&gt;3, 根据业务场景需要，建议：逐步独立出Master节点和协调/路由节点。#### **1.10 建议冷热数据分离**`热数据存储SSD`和普通历史数据存储机械磁盘，物理上提高检索效率。### **2、索引优化实践**Mysql等关系型数据库要分库、分表。Elasticserach的话也要做好充分的考虑。#### **2.1 设置多少个索引？**建议根据业务场景进行存储。不同通道类型的数据要`分索引存储`。举例：知乎采集信息存储到知乎索引；APP采集信息存储到APP索引。#### **2.2 设置多少分片？**建议根据数据量衡量。经验值：建议每个分片大小`不要超过30GB`。#### **2.3 分片数设置？**建议根据集群节点的个数规模，分片个数建议&gt;=集群节点的个数。5节点的集群，5个分片就比较合理。注意：除非reindex操作，`分片数是不可以修改`的。#### **2.4副本数设置？**除非你对系统的健壮性有异常高的要求，比如：银行系统。可以考虑2个副本以上。否则，1个副本足够。注意：`副本数是可以通过配置随时修改`的。#### **2.5不要再在一个索引下创建多个type**即便你是5.X版本，考虑到未来版本升级等后续的可扩展性。建议：一个索引对应一个type。6.x默认对应_doc，5.x你就直接对应type统一为doc。#### **2.6 按照日期规划索引**随着业务量的增加，单一索引和数据量激增给的矛盾凸显。按照日期规划索引是必然选择。好处1：可以实现历史数据秒删。很对历史索引delete即可。注意：一个索引的话需要借助delete_by_query+force_merge操作，慢且删除不彻底。好处2：便于冷热数据分开管理，检索最近几天的数据，直接物理上指定对应日期的索引，速度快的一逼！操作参考：`模板使用+rollover API使用`。#### **2.7 务必使用别名**ES不像mysql方面的更改索引名称。使用别名就是一个相对灵活的选择。## 3、数据模型优化实践**#### **3.1 不要使用默认的Mapping**默认Mapping的字段类型是系统`自动识别`的。其中：string类型默认分成：text和keyword两种类型。如果你的业务中不需要分词、检索，仅需要精确匹配，仅设置为keyword即可。根据业务需要选择合适的类型，有利于节省空间和提升精度，如：浮点型的选择。#### **3.2 Mapping各字段的选型流程**![微信图片_20200315214927](D:\Typora_image\微信图片_20200315214927.jpg)### **3.3 选择合理的分词器**常见的开源中文分词器包括：ik分词器、ansj分词器、hanlp分词器、结巴分词器、海量分词器、“ElasticSearch最全分词器比较及使用方法” 搜索可查看对比效果。如果选择ik，建议使用ik_max_word。因为：粗粒度的分词结果基本包含细粒度ik_smart的结果。### **3.4 date、long、还是keyword**根据业务需要，如果需要基于时间轴做分析，必须date类型；如果仅需要秒级返回，建议使用keyword。###　**4、数据写入优化实践**###  **4.1 要不要秒级响应？**Elasticsearch近实时的本质是：最快1s写入的数据可以被查询到。如果refresh_interval设置为1s，势必会产生大量的segment，检索性能会受到影响。所以，非实时的场景可以调大，设置为30s，甚至-1。###  **4.2 减少副本，提升写入性能。写入前，副本数设置为0，写入后，副本数设置为原来值。###  **4.3 能批量就不单条写入批量接口为bulk，批量的大小要结合队列的大小，而队列大小和线程池大小、机器的cpu核数。###  **4.4 禁用swap在Linux系统上，通过运行以下命令临时禁用交换：sudo swapoff -a### **5、检索聚合优化实战**### **5.1 禁用 wildcard模糊匹配**数据量级达到TB+甚至更高之后，wildcard在多字段组合的情况下很容易出现卡死，甚至导致集群节点崩溃宕机的情况。后果不堪设想。替代方案：方案一：针对精确度要求高的方案:两套分词器结合，standard和ik结合，使用match_phrase检索。方案二：针对精确度要求不高的替代方案：建议ik分词，通过match_phrase和slop结合查询。###  **5.2极小的概率使用match匹配**中文match匹配显然结果是不准确的。很大的业务场景会使用短语匹配“match_phrase&quot;。match_phrase结合合理的分词词典、词库，会使得搜索结果精确度更高，避免噪音数据。###  **5.3 结合业务场景，大量使用filter过滤器**对于不需要使用计算相关度评分的场景，无疑filter缓存机制会使得检索更快。举例：过滤某邮编号码。### **5.4控制返回字段和结果**和mysql查询一样，业务开发中，select * 操作几乎是不必须的。同理，ES中，_source 返回全部字段也是非必须的。要通过_source 控制字段的返回，只返回业务相关的字段。网页正文content，网页快照html_content类似字段的批量返回，可能就是业务上的设计缺陷。显然，摘要字段应该提前写入，而不是查询content后再截取处理。### **5.5 分页深度查询和遍历**分页查询使用：from+size;遍历使用：scroll；并行遍历使用：scroll+slice。斟酌集合业务选型使用。### **5.6 聚合Size的合理设置**聚合结果是不精确的。除非你设置size为2的32次幂-1，否则聚合的结果是取每个分片的Top size元素后综合排序后的值。实际业务场景要求精确反馈结果的要注意。尽量不要获取全量聚合结果——从业务层面取TopN聚合结果值是非常合理的。因为的确排序靠后的结果值意义不大。### **5.7 聚合分页合理实现**聚合结果展示的时，势必面临聚合后分页的问题，而ES官方基于性能原因不支持聚合后分页。如果需要聚合后分页，需要自开发实现。包含但不限于：方案一：每次取聚合结果，拿到内存中分页返回。方案二：scroll结合scroll after集合redis实现。### **6、业务优化**让Elasticsearch做它擅长的事情，很显然，它更擅长基于倒排索引进行搜索。业务层面，用户想最快速度看到自己想要的结果，中间的“字段处理、格式化、标准化”等一堆操作，用户是不关注的。为了让Elasticsearch更高效的检索，建议：1）要做足“前戏”字段抽取、倾向性分析、分类/聚类、相关性判定放在写入ES之前的ETL阶段;2）“睡服”产品经理产品经理基于各种奇葩业务场景可能会提各种无理需求。作为技术人员，要“通知以情晓之以理”，给产品经理讲解明白搜索引擎的原理、Elasticsearch的原理，哪些能做，哪些真的“臣妾做不到”。</code></pre><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image\微信图片_20200315214927.jpg" alt="微信图片_20200315214927"></p><h2 id="presto优化实践"><a href="#presto优化实践" class="headerlink" title="presto优化实践"></a><strong>presto优化实践</strong></h2><pre><code></code></pre><h2 id="hbase参数优化"><a href="#hbase参数优化" class="headerlink" title="hbase参数优化"></a><strong>hbase参数优化</strong></h2><p><strong>zookeeper.session.timeout</strong></p><pre><code class="java">默认值：3分钟（180000ms）说明：RegionServer与Zookeeper间的连接超时时间。当超时时间到后，ReigonServer会被Zookeeper从RS集群清单中移除，HMaster收到移除通知后，会对这台server负责的regions重新balance，让其他存活的RegionServer接管.调优：这个timeout决定了RegionServer是否能够及时的failover。设置成1分钟或更低，可以减少因等待超时而被延长的failover时间。不过需要注意的是，对于一些Online应用，RegionServer从宕机到恢复时间本身就很短的（网络闪断，crash等故障，运维可快速介入），如果调低timeout时间，反而会得不偿失。因为当ReigonServer被正式从RS集群中移除时，HMaster就开始做balance了（让其他RS根据故障机器记录的WAL日志进行恢复）。当故障的RS在人工介入恢复后，这个balance动作是毫无意义的，反而会使负载不均匀，给RS带来更多负担。特别是那些固定分配regions的场景。</code></pre><p><strong>hbase.regionserver.handler.count</strong></p><pre><code class="java">默认值：10说明：RegionServer的请求处理IO线程数。调优：这个参数的调优与内存息息相关。较少的IO线程，适用于处理单次请求内存消耗较高的Big PUT场景（大容量单次PUT或设置了较大cache的scan，均属于Big PUT）或ReigonServer的内存比较紧张的场景。较多的IO线程，适用于单次请求内存消耗低，TPS要求非常高的场景。设置该值的时候，以监控内存为主要参考。这里需要注意的是如果server的region数量很少，大量的请求都落在一个region上，因快速充满memstore触发flush导致的读写锁会影响全局TPS，不是IO线程数越高越好。压测时，开启Enabling RPC-level logging，可以同时监控每次请求的内存消耗和GC的状况，最后通过多次压测结果来合理调节IO线程数。这里是一个案例?Hadoop and HBase Optimization for Read Intensive Search Applications，作者在SSD的机器上设置IO线程数为100，仅供参考。</code></pre><p><strong>hbase.hregion.max.filesize</strong></p><pre><code class="java">默认值：256M说明：在当前ReigonServer上单个Reigon的最大存储空间，单个Region超过该值时，这个Region会被自动split成更小的region。调优：1：小region对split和compaction友好，因为拆分region或compact小region里的storefile速度很快，内存占用低。缺点是split和compaction会很频繁。2：特别是数量较多的小region不停地split, compaction，会导致集群响应时间波动很大，region数量太多不仅给管理上带来麻烦，甚至会引发一些Hbase的bug。3：一般512以下的都算小region。4：大region，则不太适合经常split和compaction，因为做一次compact和split会产生较长时间的停顿，对应用的读写性能冲击非常大。此外，大region意味着较大的storefile，compaction时对内存也是一个挑战。5：当然，大region也有其用武之地。如果你的应用场景中，某个时间点的访问量较低，那么在此时做compact和split，既能顺利完成split和compaction，又能保证绝大多数时间平稳的读写性能。6：既然split和compaction如此影响性能，有没有办法去掉？7：compaction是无法避免的，split倒是可以从自动调整为手动。8：只要通过将这个参数值调大到某个很难达到的值，比如100G，就可以间接禁用自动split（RegionServer不会对未到达100G的region做split）。9：再配合RegionSplitter这个工具，在需要split时，手动split。10：手动split在灵活性和稳定性上比起自动split要高很多，相反，管理成本增加不多，比较推荐online实时系统使用。11：内存方面，小region在设置memstore的大小值上比较灵活，大region则过大过小都不行，过大会导致flush时app的IO wait增高，过小则因store file过多影响读性能。</code></pre><p><strong>hbase.regionserver.global.memstore.upperLimit&#x2F;lowerLimit</strong></p><pre><code class="java">默认值：0.4/0.35upperlimit说明：hbase.hregion.memstore.flush.size 这个参数的作用是当单个Region内所有的memstore大小总和超过指定值时，flush该region的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模式来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。这个参数的作用是防止内存占用过大，当ReigonServer内所有region的memstores所占用内存总和达到heap的40%时，HBase会强制block所有的更新并flush这些region以释放所有memstore占用的内存。lowerLimit说明： 同upperLimit，只不过lowerLimit在所有region的memstores所占用内存达到Heap的35%时，不flush所有的memstore。它会找一个memstore内存占用最大的region，做个别flush，此时写更新还是会被block。lowerLimit算是一个在所有region强制flush导致性能降低前的补救措施。在日志中，表现为 “** Flush thread woke up with memory above low water.”调优：这是一个Heap内存保护参数，默认值已经能适用大多数场景。参数调整会影响读写，如果写的压力大导致经常超过这个阀值，则调小读缓存hfile.block.cache.size增大该阀值，或者Heap余量较多时，不修改读缓存大小。如果在高压情况下，也没超过这个阀值，那么建议你适当调小这个阀值再做压测，确保触发次数不要太多，然后还有较多Heap余量的时候，调大hfile.block.cache.size提高读性能。还有一种可能性是?hbase.hregion.memstore.flush.size保持不变，但RS维护了过多的region，要知道 region数量直接影响占用内存的大小。</code></pre><p><strong>hfile.block.cache.size</strong></p><pre><code class="java">默认值：0.2说明：storefile的读缓存占用Heap的大小百分比，0.2表示20%。该值直接影响数据读的性能。调优：当然是越大越好，如果写比读少很多，开到0.4-0.5也没问题。如果读写较均衡，0.3左右。如果写比读多，果断默认吧。设置这个值的时候，你同时要参考?hbase.regionserver.global.memstore.upperLimit?，该值是memstore占heap的最大百分比，两个参数一个影响读，一个影响写。如果两值加起来超过80-90%，会有OOM的风险，谨慎设置。</code></pre><p><strong>hbase.hstore.blockingStoreFiles</strong></p><pre><code class="java">默认值：说明：在flush时，当一个region中的Store（Coulmn Family）内有超过7个storefile时，则block所有的写请求进行compaction，以减少storefile数量。调优：block写请求会严重影响当前regionServer的响应时间，但过多的storefile也会影响读性能。从实际应用来看，为了获取较平滑的响应时间，可将值设为无限大。如果能容忍响应时间出现较大的波峰波谷，那么默认或根据自身场景调整即可。</code></pre><p><strong>hbase.hregion.memstore.block.multiplier</strong></p><pre><code class="java">默认值：2说明：当一个region里的memstore占用内存大小超过hbase.hregion.memstore.flush.size两倍的大小时，block该region的所有请求，进行flush，释放内存。虽然我们设置了region所占用的memstores总内存大小，比如64M，但想象一下，在最后63.9M的时候，我Put了一个200M的数据，此时memstore的大小会瞬间暴涨到超过预期的hbase.hregion.memstore.flush.size的几倍。这个参数的作用是当memstore的大小增至超过hbase.hregion.memstore.flush.size 2倍时，block所有请求，遏制风险进一步扩大。调优： 这个参数的默认值还是比较靠谱的。如果你预估你的正常应用场景（不包括异常）不会出现突发写或写的量可控，那么保持默认值即可。如果正常情况下，你的写请求量就会经常暴长到正常的几倍，那么你应该调大这个倍数并调整其他参数值，比如hfile.block.cache.sizehbase.regionserver.global.memstore.upperLimit/lowerLimit，以预留更多内存，防止HBase server OOM。</code></pre><p><strong>hbase.hregion.memstore.mslab.enabled</strong></p><pre><code class="java">默认值：true说明：减少因内存碎片导致的Full GC，提高整体性能。</code></pre><p><strong>其它参数</strong></p><pre><code class="python">hbase.regionserver.hlog.splitlog.writer.threads：默认值是3，建议设为10，日志切割所用的线程数hbase.snapshot.enabled：快照功能，默认是false(不开启)，建议设为true，特别是对某些关键的表，定时用快照做备份是一个不错的选择。hbase.hregion.majorcompaction：hbase的region主合并的间隔时间，默认为1天，建议设置为0，禁止自动的major主合并，major合并会把一个store下所有的storefile重写为一个storefile文件，在合并过程中还会把有删除标识的数据删除，在生产集群中，主合并能持续数小时之久，为减少对业务的影响，建议在业务低峰期进行手动或者通过脚本或者api定期进行major合并。hbase.hregion.memstore.flush.size：默认值128M，单位字节，一旦有memstore超过该值将被flush，如果regionserver的jvm内存比较充足(16G以上)，可以调整为256M。hbase.hstore.compaction.min：默认值为3，如果任何一个store里的storefile总数超过该值，会触发默认的合并操作，可以设置5~8，在手动的定期major compact中进行storefile文件的合并，减少合并的次数，不过这会延长合并的时间，以前的对应参数为hbase.hstore.compactionThreshold。HStore的storeFile数量&gt;= compactionThreshold配置的值，则可能会进行compact，默认值为3，可以调大，比如设置为6，在定期的major compact中进行剩下文件的合并。hbase.hstore.compaction.max：默认值为10,一次最多合并多少个storefile，避免OOM。hbase.hstore.blockingStoreFiles：默认为7，如果任何一个store(非.META.表里的store)的storefile的文件数大于该值，则在flush memstore前先进行split或者compact，同时把该region添加到flushQueue，延时刷新，这期间会阻塞写操作直到compact完成或者超过hbase.hstore.blockingWaitTime(默认90s)配置的时间，可以设置为30，避免memstore不及时flush。当regionserver运行日志中出现大量的“Region has too many store files; delaying flush up to 90000ms”时，说明这个值需要调整了hbase.regionserver.thread.compaction.small：默认值为1，regionserver做Minor Compaction时线程池里线程数目,可以设置为5。hbase.regionserver.thread.compaction.large：默认值为1，regionserver做Major Compaction时线程池里线程数目，可以设置为8。hbase.regionserver.lease.period：默认值60000(60s)，客户端连接regionserver的租约超时时间，客户端必须在这个时间内汇报，否则则认为客户端已死掉。这个最好根据实际业务情况进行调整hbase.client.write.buffer：默认为2M，写缓存大小，推荐设置为5M，单位是字节，当然越大占用的内存越多，此外测试过设为10M下的入库性能，反而没有5M好hbase.client.pause：默认是1000(1s),如果你希望低延时的读或者写，建议设为200，这个值通常用于失败重试，region寻找等hbase.client.retries.number：默认值是10，客户端最多重试次数,可以设为11，结合上面的参数，共重试时间71shbase.ipc.client.tcpnodelay：默认是false，建议设为true，关闭消息缓冲hbase.client.scanner.caching：scan缓存，默认为1，避免占用过多的client和rs的内存，一般1000以内合理，如果一条数据太大，则应该设置一个较小的值，通常是设置业务需求的一次查询的数据条数    export HBASE_OPTS=&quot;$HBASE_OPTS -XX:+UseCompressedOops -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSParallelRemarkEnabled  -XX:CMSInitiatingOccupancyFraction=75 -XX:SoftRefLRUPolicyMSPerMB=0&quot;    xport HBASE_REGIONSERVER_OPTS=&quot;-Xms36g -Xmx36g -Xmn1g -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=15 -XX:CMSInitiatingOccupancyFraction=70 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/data/logs/gc-$(hostname)-hbase.log&quot;    hfile.block.index.cacheonwrite在index写入的时候允许put无根（non-root）的多级索引块到block cache里，默认是false，设置为true，或许读性能更好，但是是否有副作用还需调查。io.storefile.bloom.cacheonwrite默认为false，需调查其作用。hbase.regionserver.regionSplitLimit控制最大的region数量，超过则不可以进行split操作，默认是Integer.MAX，可设置为1，禁止自动的split，通过人工，或者写脚本在集群空闲时执行。如果不禁止自动的split，则当region大小超过hbase.hregion.max.filesize时会触发split操作（具体的split有一定的策略，不仅仅通过该参数控制，前期的split会考虑region数据量和memstore大小），每次flush或者compact之后，regionserver都会检查是否需要Split，split会先下线老region再上线split后的region，该过程会很快，但是会存在两个问题：1、老region下线后，新region上线前client访问会失败，在重试过程中会成功但是如果是提供实时服务的系统则响应时长会增加；2、split后的compact是一个比较耗资源的动作。   </code></pre><h2 id="linux高频命令"><a href="#linux高频命令" class="headerlink" title="linux高频命令"></a><strong>linux高频命令</strong></h2><pre><code class="shell">1、查找文件find / -name filename.txt 根据名称查找/目录下的filename.txt文件。find . -name &quot;*.xml&quot; 递归查找所有的xml文件find . -name &quot;*.xml&quot; |xargs grep &quot;hello world&quot; 递归查找所有文件内容中包含hello world的xml文件grep -H &#39;spring&#39; *.xml 查找所以有的包含spring的xml文件find ./ -size 0 | xargs rm -f &amp; 删除文件大小为零的文件ls -l | grep &#39;.jar&#39; 查找当前目录中的所有jar文件grep &#39;test&#39; d* 显示所有以d开头的文件中包含test的行。grep &#39;test&#39; aa bb cc 显示在aa，bb，cc文件中匹配test的行。grep &#39;[a-z]\&#123;5\&#125;&#39; aa 显示所有包含每个字符串至少有5个连续小写字符的字符串的行。2、查看一个程序是否运行ps -ef|grep tomcat 查看所有有关tomcat的进程3、终止线程kill -9 19979 终止线程号位19979的进程4、查看文件，包含隐藏文件ls -al5、当前工作目录pwd6、复制文件cp source dest 复制文件cp -r sourceFolder targetFolder 递归复制整个文件夹scp sourecFile romoteUserName@remoteIp:remoteAddr 远程拷贝7、创建目录mkdir newfolder8、删除目录rmdir deleteEmptyFolder 删除空目录rm -rf deleteFile 递归删除目录中所有内容9、移动文件mv /temp/movefile /targetFolder10、重命名mv oldNameFile newNameFile11、切换用户su -username12、修改文件权限chmod 777 file.java file.java 的权限-rwxrwxrwx，r表示读、w表示写、x表示可执行13、压缩文件tar -czf test.tar.gz /test1 /test2zip -r ./xxx.zip  fileDir14、列出压缩文件列表tar -tzf test.tar.gz15、解压文件tar -xvzf test.tar.gzunzip xxx.zip16、查看文件头10行head -n 10 example.txt17、查看文件尾10行tail -n 10 example.txt18、查看日志类型文件tail -f exmaple.log 这个命令会自动显示新增内容，屏幕只显示10行内容的（可设置）。19、使用超级管理员身份执行命令sudo rm a.txt 使用管理员身份删除文件20、查看端口占用情况netstat -tln | grep 8080 查看端口8080的使用情况netstat -anltp | grep 端口号netstat -ap | grep ssh21、查看端口属于哪个程序lsof -i :808022、查看进程ps aux|grep java 查看java进程ps aux 查看所有进程23、以树状图列出目录的内容tree a24、文件下载wget http://file.tgzcurl http://file.tgz25、网络检测ping www.just-ping.com26、 显示每个文件和目录的磁盘使用空间，也就是文件的大小。du -h 以K  M  G为单位显示，提高可读性（最常用的一个）27、显示磁盘分区上可以使用的磁盘空间df -h 使用-h选项以KB、MB、GB的单位来显示，可读性高28、统计文件命令wc -l filename 统计文件的行数wc passwd fileName 同时统计多个文件29、文本编辑命令sed -i &#39;1d&#39; fileName   删除第一行 sed &#39;3ahello&#39; 1.txt    向第三行后面添加hello，3表示行号30、让某个程序在后台运行nohup ./start-dishi.sh &gt;output 2&gt;&amp;1 &amp;nohup ./start-dishi.sh &gt; /dev/null &amp;    /dev/null文件的作用，这是一个无底洞，任何东西都可以定向到这里，但是却无法打开 00 01 * * * /bin/sh/server/scripts/mysqlbak.sh &gt;/dev/null 2&gt;&amp;1 31、lsof -i:port 查看指定端口有哪些进程在使用lsof  -a -u root -d txt查看root 用户进程所打开的文件类型为txt的文件losf -p pid 通过进程号显示该进行打开的文件losf -p ^pid 列出除了某个进程号，其他进程号所打开的文件信息lsof -a -u username -i 列出某个用户的所有活跃网络端口32、执行telnet指令开启终端机阶段作业，并登入远端主机。telnet 192.168.120.206telnet www.baidu.comservice xinetd restart 启动telnet命令33、反向输出，与cat相反tac testcat -n test  展示文件内容并显示行号-n rz ：行号在自己栏位的最右方显示，且加 0 ；   nl -b a -n rz log2014.log34、比较文件的区别diff -y x.txt y.txt   以并列的方式显示文件的异同之处；diff  -e  1.txt  2.txt  &gt; script.txt   生成一个编辑角本，作为ex 或ed 的输入可将文件1 转换成文件2-r：当diff的参数为文件夹时，diff会遍历整个文件夹对新旧文件夹下同名的文件进行比较-w：忽略所有空格和制表符，将所有其他空白字符串视为一致。例如，if ( a == b ) 与 if(a==b) 相等。-i：忽略字母大小写。例如，小写 a 被认为同大写 A 一样。35 截取字符串echo 123:456:789 | cut -c3  截取第三个字符echo 123:456:789 | cut -c-3 截取前三个字符echo 123:456:789 | cut -c3-6,8-10 提取第三到第六和第八到第十间的字符date |cut -b 1-6cat tmp.cc |&gt;&gt;tmp.cc |&gt;&gt;tmp.cc |&gt;&gt;tmp.cc | head -n10 |&gt;tmp.cc| cut -c7-7cut -c-10 test.txt 截取文件的前10行 ss -an | grep:80    查看80端口的tcp状态cut：将文件的每一行按指定分隔符分割并输出。split：分割文件为不同的小片段。paste：按行合并文件内容。sort：对文件的文本内容排序。uniq：去除重复行。oldboywc：统计文件的行数、单词数或字节数。iconv：转换文件的编码格式。dos2unix：将DOS格式文件转换成UNIX格式。diff：全拼difference，比较文件的差异，常用于文本文件。vimdiff：命令行可视化文件比较工具，常用于文本文件。rev：反向输出文件内容。grep/egrep：过滤字符串，三剑客老三。join：按两个文件的相同字段合并。tr：替换或删除字符。vi/vim：命令行文本编辑器。36 批量杀死进程ps -ef|grep check_os.sh | grep -v grep | awk &#39;&#123;print $2&#125;&#39; | xargs kill -9$2表示第2列,即进程号PID; grep -v grep是列出除开grep命令本身的进程,grep iboss2确认进程关键字kill -9 强杀进程;xargs 使用上一个操作的结果作为下一个命令的参数使用### hdfs 命令hdfs dfs -cat file | wc -l       显示文件行数hdfs dfs -cat file | head -5     显示文件前5行hdfs dfs -du -h  file          显示文件大小hdfs dfs -tail -5 file         显示文件后5行hdfs dfs -getmerge &lt; src&gt; &lt; localdst&gt; [addnl]   将源目录和目标文件作为输入，并将src中的文件连接到目标本地文件（把两个文件的内容合并起来）hdfs dfs -cat file | grep 过滤字段   从hdfs上过滤包含某个字符的行内容hadoop fs -appendToFile /home/dataflair/Desktop/sample /user/dataflair/dir1   将一个或者多个文件添加到HDFS系统中，他也是从标准输入中读取，然后添加到目标文件系统汇总hdfs fs -appedToFile ./hello.txt /input/hello.txt  ---追加一个文件到另一个文件到末尾hdfs job -kill -task &lt;task-id&gt;   杀死任务。被杀死的任务不会不利于失败尝试。hdfs fsck &lt;path&gt; -delete   删除受损文件hdfs jar file.jar 执行jar包程序hdfs job -submit &lt;job-file&gt;  提交作业hdfs job -kill &lt;job-id&gt;  杀死指定作业。hdfs fsck &lt;path&gt; -locations     打印出每个块的位置信息。hdfs fsck &lt;path&gt; -blocks     打印出块信息报告</code></pre><h2 id="Hbase性能优化"><a href="#Hbase性能优化" class="headerlink" title="Hbase性能优化"></a><strong>Hbase性能优化</strong></h2><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image\v2-c4b89ad08f557e7a2d231dad007e1ef0_720w.jpg" alt="v2-c4b89ad08f557e7a2d231dad007e1ef0_720w"></p><pre><code class="python">### HBase写入优化客户端优化## **批量写**采用批量写，可以减少客户端到RegionServer之间的RPC的次数，提高写入性能。批量写请求要么全部成功返回，要么抛出异常。HTable.put(List&lt;Put&gt;);## **异步批量提交**如果业务可以接受异常情况下丢失少量数据，可以使用异步批量提交方式提交请求。用户提交写请求之后，数据会先写入客户端缓存，并返回用户写入成功（此时数据并为提交到RegionServer），当客户端缓存达到阈值（默认2M，可通过hbase.client.write.buffer配置）时才会批量提交给RegionServer。需要注意的是，在某些情况下客户端异常的情况下缓存数据有可能丢失。HTable.setWriteBufferSize(writeBufferSize); // 设置缓存大小HTable.setAutoFlush(false);## **多线程并发写**客户端开启多个HTable写线程，每个写线程负责一个HTable对象的flush操作，这样结合定时flush和写buffer，可以即保证在数据量小的时候，数据可以在较短时间内被flush，同时又保证在数据量大的时候，写buffer一满就即使进行flush。## **使用BulkLoad写入**在HBase中数据都是以HFile形式保存在HDFS中的，当有大量数据需要写入到HBase的时候，可以采用BulkLoad方式完成。使用MapReduce或者Spark直接生成HFile格式的数据文件，然后再通过RegionServer将HFile数据文件移动到相应的Region上去。写数据表设计调优## **COMPRESSION**配置数据的压缩算法，这里的压缩是HFile中block级别的压缩。对于可以压缩的数据，配置压缩算法可以有效减少磁盘的IO，从而达到提高性能的目的。但是并不是所有数据都可以进行有效压缩，如图片，因为图片一般是已经压缩后的数据，所以压缩效果有限。常用的压缩算法是SNAPPY，因为它有较好的压缩和解压速度和可以接受的压缩率。## **IN_MEMORY**配置表的数据优先缓存在内存中，这样可以有效提升读取的性能。适合小表，而且需要频繁进行读取操作的。## **预分区**在HBase中数据是分布在各个Region中的，每个Region都负责一个起始RowKey和结束Rowkey的范围，在向HBase中写数据的时候，会根据RowKey请求到对应的Region上，如果写请求都集中在某一个Region或某几个Region上的时候，性能肯定不如写请求均匀分布在各个Region上好。默认情况下，创建的HBase的只有一个Region分区，会随着数据量的变大，进行split，拆分成多个Region，最开始的性能肯定会很不好建议在设计HBase的的时候，进行预分区，并设计一个良好的Rowkey生成规则（关于RowKey设计，可以参考《一篇文章带你快速搞懂HBase RowKey设计》），尽量将数据分散到各个Region上，那样在进行HBase的读写的时候，对性能会有很好的改善。## **合理设置WAL存储级别**数据在写入HBase的时候，先写WAL，再写入缓存。通常情况下写缓存延迟很低，WAL机制一方面是为了确保数据即使写入缓存后数据丢失也可以通过WAL恢复，另一方面是为了集群之间的复制。默认WAL机制是开启的，并且使用的是同步机制写WAL。如果业务不特别关心异常情况下部分数据的丢失，而更关心数据写入吞吐量，可考虑关闭WAL写，这样可以提升2~3倍数据写入的吞吐量。如果业务不能接受不写WAL，但是可以接受WAL异步写入，这样可以带了1~2倍性能提升。HBase中可以通过设置WAL的持久化等级决定是否开启WAL机制、以及HLog的落盘方式。WAL的持久化等级分为如下四个等级：SKIP_WAL：只写缓存，不写HLog日志。这种方式因为只写内存，因此可以极大的提升写入性能，但是数据有丢失的风险。在实际应用过程中并不建议设置此等级，除非确认不要求数据的可靠性。ASYNC_WAL：异步将数据写入HLog日志中。SYNC_WAL：同步将数据写入日志文件中，需要注意的是数据只是被写入文件系统中，并没有真正落盘，默认。FSYNC_WAL：同步将数据写入日志文件并强制落盘。最严格的日志写入等级，可以保证数据不会丢失，但是性能相对比较差。同样，除了在创建表的时候直接设置WAL存储级别，也可以通过客户端设置WAL持久化等级，代码：put.setDurability(Durability.SYNC_WAL);### HBase读取优化客户端优化## **批量get请求**使用批量请求，可以减少RPC的次数，显著提高吞吐量。需要注意的是，批量get请求要么成功返回所有请求数据，要么抛出异常。Result[] re= table.get(List&lt;Get&gt; gets);## **合理何止scan缓存大小**一次scan可能会返回大量数据，但是实际客户端发起一次scan请求，并不会将所有数据一次性加载到本地，而是分成多次RPC请求进行加载，这样设计一方面是因为大量数据请求可能会导致网络带宽严重消耗进而影响其他业务，另一方面是有可能因为数据量太大导致客户端发生OOM。所以采用先加载一部分数据到本地，然后进行遍历，每次加载一部分数据，如此往复，直至所有数据加载完成。数据加载到本地就存放在scan缓存中，默认100。增大scan的缓存，可以让客户端减少一次scan的RPC次数，从而从整体上提升数据读取的效率。scan.setCaching(int caching); //大scan可以设置为1000## **指定请求列族或者列名**HBase是列族数据库，同一列族的数据存储在一块，不同列族是分开存储的，如果一个表由多个列族，只是根据RowKey而不指定列族进行检索的话，不同列族的数据需要独立进行检索，性能必然会比指定列族的查询差的多。此外指定请求的列的话，不需要将整个列族的所有列的数据返回，这样就减少了网路IO。scan.addColumn();## **设置只读Rowkey过滤器**在只需要Rowkey数据时，可以为Scan添加一个只读取Rowkey的filter（FirstKeyOnlyFilter或KeyOnlyFilter）。## **关闭ResultScanner**在使用table.getScanner之后，记得关闭，否则它会和服务器端一直保持连接，资源无法释放，从而导致服务端的某些资源不可用。scanner.close();## **离线计算访问HBase建议禁用缓存**当离线访问HBase时，往往会对HBase表进行扫描，此时读取的数据没有必要存放在BlockCache中，否则会降低扫描的效率。scan.setBlockCache(false);建议在对HBase表进行扫描时禁用缓存。对于频繁查询HBase的应用场景不需要禁用缓存，并且可以考虑在应用程序和HBase之间加一层缓存系统（如Redis），先查询缓存，缓存没有命中再去查询HBase。读数据表设计调优## **COMPRESSION**同写性能优化COMPRESSION部分。## **BLOCKSIZE**配置HFile中block块的大小，不同的block大小，可以影响HBase读写数据的效率。越大的block块，配置压缩算法，压缩的效率就越好；但是由于HBase的读取数据时以block块为单位的，所以越大的block块，对于随机读的情况，性能可能会比较差，如果要提升写入的性能，一般扩大到128kb或者256kb，可以提升写数据的效率，也不会影响太大的随机读性能。## **DATA_BLOCK_ENCODING**配置HFile中block块的编码方法。当一行数据中存在多个列时，一般可以配置为&quot;FAST_DIFF&quot;，可以有效的节省数据存储的空间，从而提升性能。## **BloomFilter**优化原理：BloomFilter主要用来过滤不存在待检索RowKey或者Row-Col的HFile文件，避免无用的IO操作。它会告诉你在这个HFile文件中是否可能存在待检索的KeyValue，如果不存在，就可以不用小号IO打开文件进行seek。通过设置BloomFilter可以提升读写的性能。BloomFilter是一个列族级别的配置属性，如果列族设置了BloomFilter，那么HBase会在生成StoreFile时包含一份BloomFilter的结构的数据，称为MetaBlock（一旦写入就无法更新）。MetaBlock和DataBlock（真实的KeyValue数据）一起由LRUBlockCache维护，所以开启了BloomFilter会有一定的存储即内存cache开销。HBase利用BloomFilter可以节省必须读磁盘过程，可以提高随机读（get）的性能，但是对于顺序读（scan）而言，设置BloomFilter是没有作用的（0.92版本以后，如果设置了BloomFilter为ROWCOL，对于执行了qualifier的scan有一定的优化）BloomFilter取值有两个，ROW和ROWCOL，需要根据业务来确定具体使用哪种。如果业务大多数随机查询仅仅使用row作为查询条件，BloomFilter一定要设置为ROW。如果大多数随机查询使用row+col作为查询条件，BloomFilter需要设置为ROWCOL。如果不确定业务查询类型，设置为ROW。## **预分区**同写性能优化预分区部分。HBase服务端调优##**GC_OPTS**HBase是利用内存完成读写操作。提高HBase内存可以有效提高HBase性能。GC_OPTS主要需要调整HeapSize和NewSize的大小。调整HeapSize大小的时候，建议将Xms和Xmx设置成相同的值，这样可以避免JVM动态调整HeapSize大小的时候影响性能。调整NewSize大小的时候，建议把其设置为HeapSize大小的1/9。当HBase集群规模越大，Region数量越多时，可以适当调大HMaster的GC_OPTS参数RegionServer需要比HMaster更大的内存，在内存充足的情况下，HeapSize可以相对设置大一些。HMaster的HeapSize为4G的时候，HBase集群可以支持100000个Region的规模。根据经验值，单个RegionServer的HeapSize不建议超过20GB。# HMaster、RegionServer GC_OPTS配置如下：HMaster: -Xms2G -Xmx2G -XX:NewSize=256M -XX:MaxNewSize=256M RegionServer: -Xms4G -Xmx4G -XX:NewSize=512M -XX:MaxNewSize=512M            ## **RegionServer并发请求处理数量**hbase.regionserver.handler.count表示RegionServer在同一时刻能够并发处理多少请求。如果设置过高会导致激烈的线程竞争，如果设置过小，请求将会在RegionServer长时间等待，降低处理能力。应该根据资源情况，适当增加处理线程数。建议根据CPU的使用情况，可以设置为100至300之间的值。## **控制MemStore的大小**hbase.hregion.memstore.flush.size默认值128M，单位字节，一旦有MemStore超过该值将被flush，如果regionserver的jvm内存比较充足(16G以上)，可以调整为256M。在内存足够put负载大情况下可以调整增大。## **BlockCache优化**BlockCache作为读缓存，合理设置对于提高读性能非常重要。默认情况下，BlockCache和MemStore的配置各占40%，可以根据集群业务进行修正，比如读多写少业务可以将BlockCache占比调大。另外BlockCache的策略也很重要，不同策略对读性能来说影响并不大，但是对GC的影响 却很显著。HBase缓存区大小，主要影响查询性能。根据查询模式以及查询记录分布情况来决定缓存区的大小。如果采用随机查询使得缓存区的命中率较低，可以适当降低缓存大小。hfile.block.cache.size，默认0.4，用来提高读性能hbase.regionserver.global.memstore.size，默认0.4，用来提高写性能## **控制HFile个数**MemStore在flush之前，会进行StoreFile的文件数量校验（通过hbase.hstore.blockingStoreFiles参数配置），如果大于设定值，系统将会强制执行Compaction操作进行文件合并，在合并的过程中会阻塞MemStore的数据写入，等待其他线程将StoreFile进行合并。通常情况下发生在数据写入很快的情况下。hbase.hstore.compactionThreshold表示启动Compaction的最低阈值，该值不能太大，否则会积累太多文件，一般建议设置为5～8左右。hbase.hstore.blockingStoreFiles默认设置为7，可以适当调大一些。## **Split优化**hbase.hregion.max.filesize表示HBase中Region的文件总大小的最大值。当Region中的文件大于该参数时，将会导致Region分裂。如果该参数设置过小时，可能会导致Split操作频繁如果该参数设置过大时，会导致Compaction操作需要处理的文件个数增大，影响Compaction执行效率## **Compaction优化**hbase.hstore.compaction.min当一个Store中文件超过该值时，会进行Compaction，适当增大该值，可以减少文件被重复执行Compaction。但是如果过大，会导致Store中文件数过多而影响读取的性能。hbase.hstore.compaction.max控制一次Compaction操作时的文件数据量的最大值。hbase.hstore.compaction.max.size如果一个HFile文件的大小大于该值，那么在Minor Compaction操作中不会选择这个文件进行Compaction操作，除非进行Major Compaction操作。这个值可以防止较大的HFile参与Compaction操作。在禁止Major Compaction后，一个Store中可能存在几个HFile，而不会合并成为一个HFile，这样不会对数据读取造成太大的性能影响。原则是：尽量要减小Compaction的次数和Compaction的执行时间### **HBase 的解决方案-MSLAB**MSLAB，全称是 MemStore-Local Allocation Buffer，是 Cloudera 在 HBase 0.90.1 时提交的一个 patch 里包含的特性。它基于 Arena Allocation 解决了 HBase 因Region flush 导致的内存碎片问题。MSLAB 的实现原理（对照 Arena Allocation，HBase 实现细节）：MemstoreLAB 为 Memstore 提供 Allocator。创建一个 2M（默认）的 Chunk 数组和一个 chunk 偏移量，默认值为 0。当 Memstore 有新的 KeyValue 被插入时，通过 KeyValue.getBuffer()取得 data bytes数组。将 data 复制到 Chunk 数组起始位置为 chunk 偏移量处，并增加偏移量=偏移量+data.length。当一个 chunk 满了以后，再创建一个 chunk。所有操作 lock free，基于CMS 原语。优势：KeyValue 原始数据在 minor gc 时被销毁。数据存放在 2m 大小的 chunk 中，chunk 归属于 memstore。flush 时，只需要释放多个 2m 的 chunks，chunk 未满也强制释放，从而为 Heap 腾出了多个 2M 大小的内存区间，减少碎片密集程度。</code></pre><h2 id="Hbase之MemStore何时Flush-HFile-？"><a href="#Hbase之MemStore何时Flush-HFile-？" class="headerlink" title="Hbase之MemStore何时Flush HFile ？"></a><strong>Hbase之MemStore何时Flush HFile ？</strong></h2><pre><code class="java">1.Region级别的触发刷写。（1）hbase.hregion.memstore.flush.size  单个region内所有的memstore大小总和超过指定值时，flush该region的所有memstore。这里为什么是所有memsotre?因为一张表可能有多个CF，其对应的一个Region自然包含多个CF（即HStore），每个Store都有自己的memstore，这个配置值是所有的store的memstore的总和。当这个总和达到配置值时，即针对每个HSotre，都触发其Memstore，刷写成storefile(HFile的封装)文件。（2）hbase.hstore.blockingStoreFiles 默认值：7  说明：在flush时，当一个region中的Store（Coulmn Family）内有超过7个storefile时，则block所有的写请求进行compaction，以减少storefile数量。  调优：block写请求会严重影响当前regionServer的响应时间，但过多的storefile也会影响读性能。从实际应用来看，为了获取较平滑的响应时间，可将值设为无限大。如果能容忍响应时间出现较大的波峰波谷，那么默认或根据自身场景调整即可。这个值设置比较大，会增加客户端的负载处理能力（即影响读取性能），但是如果你的服务器一直处于一个高的水平，那说明你的机器已经达到性能瓶颈，需要其他方式解决。（3）hbase.hregion.memstore.block.multiplier默认值：2说明：当一个region里总的memstore占用内存大小超过hbase.hregion.memstore.flush.size两倍的大小时，block该region的所有请求，进行flush，释放内存。虽然我们设置了region所占用的memstores总内存大小，比如64M，但想象一下，在最后63.9M的时候，我Put了一个200M的数据，此时memstore的大小会瞬间暴涨到超过预期的hbase.hregion.memstore.flush.size的几倍。这个参数的作用是当memstore的大小增至超过hbase.hregion.memstore.flush.size2倍时，block所有请求，遏制风险进一步扩大。调优：这个参数的默认值还是比较靠谱的。如果你预估你的正常应用场景（不包括异常）不会出现突发写或写的量可控，那么保持默认值即可。如果正常情况下，你的写请求量就会经常暴长到正常的几倍，那么你应该调大这个倍数并调整其他参数值，比如hfile.block.cache.size和hbase.regionserver.global.memstore.upperLimit/lowerLimit，以预留更多内存，防止HBase server OOM。2.RegionServer全局性的触发刷写。（1）hbase.regionserver.global.memstore.upperLimit当ReigonServer内所有region的memstores所占用内存总和达到heap的40%时，HBase会强制block所有的更新并flush这些region以释放所有memstore占用的内存。（2）hbase.regionserver.global.memstore.lowerLimit同upperLimit，只不过lowerLimit在所有region的memstores所占用内存达到Heap的35%时，不flush所有的memstore。它会找一个memstore内存占用最大的region，做个别flush，此时写更新还是会被block。lowerLimit算是一个在所有region强制flush导致性能降低前的补救措施。在日志中，表现为“** Flush thread woke up with memory above low water.”。调优：这是一个Heap内存保护参数，默认值已经能适用大多数场景。 3. HLog (WAL)引起的regionserver全局性的触发刷写。当数据被写入时会默认先写入Write-ahead Log(WAL)。WAL中包含了所有已经写入Memstore但还未Flush到HFile的更改(edits)。在Memstore中数据还没有持久化，当RegionSever宕掉的时候，可以使用WAL恢复数据。若是关闭WAL，则在hbase-site.xml新增hbase.regionserver.hlog.enabled配置，设为false即可，不建议关闭。当WAL(在HBase中成为HLog)变得很大的时候，在恢复的时候就需要很长的时间。因此，对WAL的大小也有一些限制，当达到这些限制的时候，就会触发Memstore的flush。Memstore flush会使WAL减少，因为数据持久化之后(写入到HFile)，就没有必要在WAL中再保存这些修改。有两个属性可以配置：（1）hbase.regionserver.hlog.blocksize（2）hbase.regionserver.maxlogsWAL的最大值由hbase.regionserver.maxlogs*hbase.regionserver.hlog.blocksize (2GB by default)决定。一旦达到这个值，Memstore flush就会被触发。所以，当你增加Memstore的大小以及调整其他的Memstore的设置项时，你也需要去调整HLog的配置项。否则，WAL的大小限制可能会首先被触发，因而，你将利用不到其他专门为Memstore而设计的优化。抛开这些不说，通过WAL限制来触发Memstore的flush并非最佳方式，这样做可能会会一次flush很多Region，尽管“写数据”是很好的分布于整个集群，进而很有可能会引发flush“大风暴”。提示：最好将hbase.regionserver.hlog.blocksize* hbase.regionserver.maxlogs设置为稍微大于hbase.regionserver.global.memstore.lowerLimit* HBASE_HEAPSIZE.</code></pre><h2 id="StringUtils工具类"><a href="#StringUtils工具类" class="headerlink" title="StringUtils工具类"></a><strong>StringUtils工具类</strong></h2><pre><code class="Java">public static boolean isEmpty(String str)    判断某字符串是否为空，为空的标准是 str==null 或 str.length()==0 public static boolean isNotEmpty(String str) 判断某字符串是否非空，等于 !isEmpty(String str) public static boolean isBlank(String str) 判断某字符串是否为空或长度为0或由空白符(whitespace) 构成public static boolean isNotBlank(String str) 判断某字符串是否不为空且长度不为0且不由空白符(whitespace) 构成，等于!isBlank(String str)    public static String removeEnd(final String str, final String remove) &#123;        if (isEmpty(str) || isEmpty(remove)) &#123;            return str;        &#125;        if (str.endsWith(remove)) &#123;            return str.substring(0, str.length() - remove.length());        &#125;        return str;    &#125;  public static String joinWith(final String separator, final Object... objects) &#123;        if (objects == null) &#123;            throw new IllegalArgumentException(&quot;Object varargs must not be null&quot;);        &#125;        final String sanitizedSeparator = defaultString(separator);        final StringBuilder result = new StringBuilder();        final Iterator&lt;Object&gt; iterator = Arrays.asList(objects).iterator();        while (iterator.hasNext()) &#123;            final String value = Objects.toString(iterator.next(), &quot;&quot;);            result.append(value);            if (iterator.hasNext()) &#123;                result.append(sanitizedSeparator);            &#125;        &#125;        return result.toString();    &#125;    public static boolean isWhitespace(final CharSequence cs) &#123;        if (cs == null) &#123;            return false;        &#125;        final int sz = cs.length();        for (int i = 0; i &lt; sz; i++) &#123;            if (!Character.isWhitespace(cs.charAt(i))) &#123;                return false;            &#125;        &#125;        return true;    &#125;          public static boolean isNumericSpace(final CharSequence cs) &#123;        if (cs == null) &#123;            return false;        &#125;        final int sz = cs.length();        for (int i = 0; i &lt; sz; i++) &#123;            if (!Character.isDigit(cs.charAt(i)) &amp;&amp; cs.charAt(i) != &#39; &#39;) &#123;                return false;            &#125;        &#125;        return true;    &#125; public static boolean isNumeric(final CharSequence cs) &#123;        if (isEmpty(cs)) &#123;            return false;        &#125;        final int sz = cs.length();        for (int i = 0; i &lt; sz; i++) &#123;            if (!Character.isDigit(cs.charAt(i))) &#123;                return false;            &#125;        &#125;        return true;    &#125;  public static boolean isBlank(final CharSequence cs) &#123;        int strLen;        if (cs == null || (strLen = cs.length()) == 0) &#123;            return true;        &#125;        for (int i = 0; i &lt; strLen; i++) &#123;            if (!Character.isWhitespace(cs.charAt(i))) &#123;                return false;            &#125;        &#125;        return true;    &#125;         public static boolean isEmpty(String str) &#123;        // 判断字符串是否为空或长度为0        return str == null || str.length() == 0; &#125;// 首字母转大写public static String capitalize(final String str) &#123;        final int strLen = length(str);        if (strLen == 0) &#123;            return str;        &#125;        final int firstCodepoint = str.codePointAt(0);        final int newCodePoint = Character.toTitleCase(firstCodepoint);        if (firstCodepoint == newCodePoint) &#123;            // already capitalized            return str;        &#125;        final int newCodePoints[] = new int[strLen]; // cannot be longer than the char array        int outOffset = 0;        newCodePoints[outOffset++] = newCodePoint; // copy the first codepoint        for (int inOffset = Character.charCount(firstCodepoint); inOffset &lt; strLen; ) &#123;            final int codepoint = str.codePointAt(inOffset);            newCodePoints[outOffset++] = codepoint; // copy the remaining ones            inOffset += Character.charCount(codepoint);         &#125;        return new String(newCodePoints, 0, outOffset);    &#125;     // 首字母转小写public static String uncapitalize(final String str) &#123;        final int strLen = length(str);        if (strLen == 0) &#123;            return str;        &#125;        final int firstCodepoint = str.codePointAt(0);        final int newCodePoint = Character.toLowerCase(firstCodepoint);        if (firstCodepoint == newCodePoint) &#123;            // already capitalized            return str;        &#125;        final int newCodePoints[] = new int[strLen]; // cannot be longer than the char array        int outOffset = 0;        newCodePoints[outOffset++] = newCodePoint; // copy the first codepoint        for (int inOffset = Character.charCount(firstCodepoint); inOffset &lt; strLen; ) &#123;            final int codepoint = str.codePointAt(inOffset);            newCodePoints[outOffset++] = codepoint; // copy the remaining ones            inOffset += Character.charCount(codepoint);         &#125;        return new String(newCodePoints, 0, outOffset);    &#125;// 大小写互换public static String swapCase(final String str) &#123;        if (isEmpty(str)) &#123;            return str;        &#125;        final int strLen = str.length();        final int newCodePoints[] = new int[strLen]; // cannot be longer than the char array        int outOffset = 0;        for (int i = 0; i &lt; strLen; ) &#123;            final int oldCodepoint = str.codePointAt(i);            final int newCodePoint;            if (Character.isUpperCase(oldCodepoint)) &#123;                newCodePoint = Character.toLowerCase(oldCodepoint);            &#125; else if (Character.isTitleCase(oldCodepoint)) &#123;                newCodePoint = Character.toLowerCase(oldCodepoint);            &#125; else if (Character.isLowerCase(oldCodepoint)) &#123;                newCodePoint = Character.toUpperCase(oldCodepoint);            &#125; else &#123;                newCodePoint = oldCodepoint;            &#125;            newCodePoints[outOffset++] = newCodePoint;            i += Character.charCount(newCodePoint);         &#125;        return new String(newCodePoints, 0, outOffset);    &#125; </code></pre><h2 id="Hive行专列和列转行"><a href="#Hive行专列和列转行" class="headerlink" title="Hive行专列和列转行"></a><strong>Hive行专列和列转行</strong></h2><pre><code class="python">### 列转行函数——collect_set和collect_listhive里通常通过collect_set和collect_list来进行列转行，其中collect_list为不去重转换，collect_set为去重转换。</code></pre><table><thead><tr><th align="center">学号</th><th align="center">姓名</th><th align="center">科目</th><th align="center">分数</th></tr></thead><tbody><tr><td align="center">1001</td><td align="center">张三</td><td align="center">语文</td><td align="center">88</td></tr><tr><td align="center">1001</td><td align="center">张三</td><td align="center">数学</td><td align="center">87</td></tr><tr><td align="center">1001</td><td align="center">张三</td><td align="center">英语</td><td align="center">94</td></tr><tr><td align="center">1001</td><td align="center">张三</td><td align="center">历史</td><td align="center">86</td></tr><tr><td align="center">1001</td><td align="center">张三</td><td align="center">地理</td><td align="center">84</td></tr><tr><td align="center">1002</td><td align="center">李四</td><td align="center">语文</td><td align="center">78</td></tr><tr><td align="center">1002</td><td align="center">李四</td><td align="center">数学</td><td align="center">89</td></tr><tr><td align="center">1002</td><td align="center">李四</td><td align="center">英语</td><td align="center">75</td></tr><tr><td align="center">1002</td><td align="center">李四</td><td align="center">历史</td><td align="center">79</td></tr><tr><td align="center">1002</td><td align="center">李四</td><td align="center">地理</td><td align="center">68</td></tr><tr><td align="center">1003</td><td align="center">王五</td><td align="center">语文</td><td align="center">98</td></tr><tr><td align="center">1003</td><td align="center">王五</td><td align="center">数学</td><td align="center">97</td></tr><tr><td align="center">1003</td><td align="center">王五</td><td align="center">英语</td><td align="center">91</td></tr><tr><td align="center">1003</td><td align="center">王五</td><td align="center">历史</td><td align="center">93</td></tr><tr><td align="center">1003</td><td align="center">王五</td><td align="center">地理</td><td align="center">92</td></tr><tr><td align="center">1004</td><td align="center">朱六</td><td align="center">语文</td><td align="center">66</td></tr><tr><td align="center">1004</td><td align="center">朱六</td><td align="center">数学</td><td align="center">63</td></tr><tr><td align="center">1004</td><td align="center">朱六</td><td align="center">英语</td><td align="center">64</td></tr><tr><td align="center">1004</td><td align="center">朱六</td><td align="center">历史</td><td align="center">67</td></tr><tr><td align="center">1004</td><td align="center">朱六</td><td align="center">地理</td><td align="center">68</td></tr></tbody></table><pre><code class="SQL">–使用collect_set函数进行列转行查询SELECTstu_id,stu_name,concat_ws(’,’,collect_set(course)) as course,concat_ws(’,’,collect_set(score)) as scorefrom student_scoregroup by stu_id,stu_name</code></pre><p>查询出的结果为：<br>序号stu_idstu_namecoursescore<br>1 1001张三语文,数学,英语,历史,地理88,87,94,86,84<br>2 1002李四语文,数学,英语,历史,地理78,89,75,79,68<br>3 1003王五语文,数学,英语,历史,地理98,97,91,93,92<br>4 1004朱六语文,数学,英语,历史,地理66,63,64,67,68</p><pre><code class="sql">建立一个新的学生表，使用上面列转行的结果来作为行转列的例表：CREATE table student_score_newasSELECTstu_id,stu_name,concat_ws(’,’,collect_set(course)) as course,concat_ws(’,’,collect_set(score)) as scorefrom student_scoregroup by stu_id,stu_name;</code></pre><pre><code class="python">### 行转列函数——explode和posexplodeexplode函数可以把array或map格式的字段转换成行的形式，当然string形式的字段其实也可以转换，只需要用split函数把字段分割成一个数组的形式即可，比如我们可以把表格中的每个玩家的科目以列的形式查询出来：</code></pre><pre><code class="sql">–使用explode函数进行列转行查询SELECT stu_id,stu_name,ecoursefrom student_score_newlateral view explode(split(course,’,’)) cr as ecourse</code></pre><p>查询结果如下：<br>序号stu_idstu_nameecourse<br>1 1001张三语文<br>2 1001张三数学<br>3 1001张三英语<br>4 1001张三历史<br>5 1001张三地理<br>6 1002李四语文<br>7 1002李四数学<br>8 1002李四英语<br>9 1002李四历史<br>10 1002李四地理<br>11 1003王五语文<br>12 1003王五数学<br>13 1003王五英语<br>14 1003王五历史<br>15 1003王五地理<br>16 1004朱六语文<br>17 1004朱六数学<br>18 1004朱六英语<br>19 1004朱六历史<br>20 1004朱六地理</p><p>但是当我们想要查询每个学生课程对应的分数时，使用explode函数会出现如下结果：</p><p>例如使用如下语句：<br>SELECT stu_id,<br>stu_name,<br>ecourse,<br>escore<br>from student_score_new<br>lateral view explode(split(course,’,’)) cr as ecourse<br>lateral view explode(split(score,’,’)) sc as escore;<br>查询的结果如下:</p><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image\微信图片_20200319230153.png" alt="微信图片_20200319230153"></p><pre><code class="sql">出现这种情况是因为两个并列的explode的sql没办法识别每个科目对应的成绩是多少，对于多个数组的行转列可以使用posexplode函数，例如使用如下查询语句：–使用posexplode函数进行列转行查询SELECT stu_id,stu_name,ecourse,escorefrom student_score_newlateral view posexplode(split(course,’,’)) cr as a,ecourselateral view posexplode(split(score,’,’)) sc as b,escorewhere a=b;</code></pre><p>查询结果如下：<br>序号stu_idstu_nameecourseescore<br>1 1001张三语文88<br>2 1001张三数学87<br>3 1001张三英语94<br>4 1001张三历史86<br>5 1001张三地理84<br>6 1002李四语文78<br>7 1002李四数学89<br>8 1002李四英语75<br>9 1002李四历史79<br>10   1002李四地理68<br>11   1003王五语文98<br>12   1003王五数学97<br>13   1003王五英语91<br>14       1003王五历史93<br>15       1003王五地理92<br>16       1004朱六语文66<br>17       1004朱六数学63<br>18       1004朱六英语64<br>19   1004朱六历史67<br>20       1004朱六地理68</p><h2 id="Java工具类"><a href="#Java工具类" class="headerlink" title="Java工具类"></a><strong>Java工具类</strong></h2><pre><code class="java">import java.text.DecimalFormat;import java.text.FieldPosition;import java.text.Format;import java.text.NumberFormat;import java.text.SimpleDateFormat;import java.util.Calendar;/** * 根据时间生成唯一序列ID * 时间精确到秒，ID最大值为99999且循环使用 *  * @Author:chenssy * @date:2016年4月17日 */public class GenerateSequenceUtil &#123;    private static final FieldPosition HELPER_POSITION = new FieldPosition(0);        /** 时间：精确到秒 */    private final static Format dateFormat = new SimpleDateFormat(&quot;YYYYMMddHHmmss&quot;);        private final static NumberFormat numberFormat = new DecimalFormat(&quot;00000&quot;);        private static int seq = 0;         private static final int MAX = 99999;        public static synchronized String generateSequenceNo() &#123;                 Calendar rightNow = Calendar.getInstance();               StringBuffer sb = new StringBuffer();         dateFormat.format(rightNow.getTime(), sb, HELPER_POSITION);         numberFormat.format(seq, sb, HELPER_POSITION);         if (seq == MAX) &#123;            seq = 0;        &#125; else &#123;            seq++;        &#125;         return sb.toString();    &#125;&#125;import java.io.BufferedReader;import java.io.File;import java.io.FileInputStream;import java.io.InputStreamReader;import java.util.HashMap;import java.util.HashSet;import java.util.Iterator;import java.util.Map;import java.util.Set;/** *  初始化敏感词库，将敏感词加入到HashMap中，构建DFA算法模型 *   * @Author : chenssy * @Date ： 2014年4月20日 下午2:27:06 */public class SensitiveWordInit &#123;    private String ENCODING = &quot;GBK&quot;;    //字符编码    @SuppressWarnings(&quot;rawtypes&quot;)    public HashMap sensitiveWordMap;        SensitiveWordInit()&#123;        super();    &#125;        /**     * @author chenssy      * @date 2014年4月20日 下午2:28:32     * @version 1.0     */    @SuppressWarnings(&quot;rawtypes&quot;)    Map initKeyWord()&#123;        try &#123;            //读取敏感词库            Set&lt;String&gt; keyWordSet = readSensitiveWordFile();            //将敏感词库加入到HashMap中            addSensitiveWordToHashMap(keyWordSet);        &#125; catch (Exception e) &#123;            e.printStackTrace();        &#125;        return sensitiveWordMap;    &#125;    /**     * 读取敏感词库，将敏感词放入HashSet中，构建一个DFA算法模型：     * @author chenssy      * @date 2014年4月20日 下午3:04:20     * @param keyWordSet  敏感词库     * @version 1.0     */    @SuppressWarnings(&#123; &quot;rawtypes&quot;, &quot;unchecked&quot; &#125;)    private void addSensitiveWordToHashMap(Set&lt;String&gt; keyWordSet) &#123;        sensitiveWordMap = new HashMap(keyWordSet.size());     //初始化敏感词容器，减少扩容操作        String key = null;          Map nowMap = null;        Map&lt;String, String&gt; newWorMap = null;        //迭代keyWordSet        Iterator&lt;String&gt; iterator = keyWordSet.iterator();        while(iterator.hasNext())&#123;            key = iterator.next();    //关键字            nowMap = sensitiveWordMap;            for(int i = 0 ; i &lt; key.length() ; i++)&#123;                char keyChar = key.charAt(i);       //转换成char型                Object wordMap = nowMap.get(keyChar);       //获取                                if(wordMap != null)&#123;        //如果存在该key，直接赋值                    nowMap = (Map) wordMap;                &#125;                else&#123;     //不存在则，则构建一个map，同时将isEnd设置为0，因为他不是最后一个                    newWorMap = new HashMap&lt;String,String&gt;();                    newWorMap.put(&quot;isEnd&quot;, &quot;0&quot;);     //不是最后一个                    nowMap.put(keyChar, newWorMap);                    nowMap = newWorMap;                &#125;                                if(i == key.length() - 1)&#123;                    nowMap.put(&quot;isEnd&quot;, &quot;1&quot;);    //最后一个                &#125;            &#125;        &#125;    &#125;    /**     * 读取敏感词库中的内容，将内容添加到set集合中     * @author chenssy      * @date 2014年4月20日 下午2:31:18     * @return     * @version 1.0     * @throws Exception      */    @SuppressWarnings(&quot;resource&quot;)    private Set&lt;String&gt; readSensitiveWordFile() throws Exception&#123;        Set&lt;String&gt; set = null;                File file = new File(&quot;D:\\SensitiveWord.txt&quot;);    //读取文件        InputStreamReader read = new InputStreamReader(new FileInputStream(file),ENCODING);        try &#123;            if(file.isFile() &amp;&amp; file.exists())&#123;      //文件流是否存在                set = new HashSet&lt;String&gt;();                BufferedReader bufferedReader = new BufferedReader(read);                String txt = null;                while((txt = bufferedReader.readLine()) != null)&#123;    //读取文件，将文件内容放入到set中                    set.add(txt);                &#125;            &#125;            else&#123;         //不存在抛出异常信息                throw new Exception(&quot;敏感词库文件不存在&quot;);            &#125;        &#125; catch (Exception e) &#123;            throw e;        &#125;finally&#123;            read.close();     //关闭文件流        &#125;        return set;    &#125;&#125;import java.util.HashSet;import java.util.Iterator;import java.util.Map;import java.util.Set;/** * 铭感词过滤工具类 *  * @Author:chenssy * @date:2014年8月5日 */public class SensitivewordFilterUtil&#123;    @SuppressWarnings(&quot;rawtypes&quot;)    private Map sensitiveWordMap = null;    public static int minMatchTYpe = 1;      //最小匹配规则    public static int maxMatchType = 2;      //最大匹配规则        /**     * 构造函数，初始化敏感词库     */    public SensitivewordFilterUtil()&#123;        sensitiveWordMap = new SensitiveWordInit().initKeyWord();    &#125;        /**     * 判断文字是否包含敏感字符     * @author chenssy      * @date 2014年4月20日 下午4:28:30     * @param txt  文字     * @param matchType  匹配规则&amp;nbsp;1：最小匹配规则，2：最大匹配规则     * @return 若包含返回true，否则返回false     * @version 1.0     */    public boolean isContaintSensitiveWord(String txt,int matchType)&#123;        boolean flag = false;        for(int i = 0 ; i &lt; txt.length() ; i++)&#123;            int matchFlag = this.CheckSensitiveWord(txt, i, matchType); //判断是否包含敏感字符            if(matchFlag &gt; 0)&#123;    //大于0存在，返回true                flag = true;            &#125;        &#125;        return flag;    &#125;        /**     * 获取文字中的敏感词     * @author chenssy      * @date 2014年4月20日 下午5:10:52     * @param txt 文字     * @param matchType 匹配规则&amp;nbsp;1：最小匹配规则，2：最大匹配规则     * @return     * @version 1.0     */    public Set&lt;String&gt; getSensitiveWord(String txt , int matchType)&#123;        Set&lt;String&gt; sensitiveWordList = new HashSet&lt;String&gt;();                for(int i = 0 ; i &lt; txt.length() ; i++)&#123;            int length = CheckSensitiveWord(txt, i, matchType);    //判断是否包含敏感字符            if(length &gt; 0)&#123;    //存在,加入list中                sensitiveWordList.add(txt.substring(i, i+length));                i = i + length - 1;    //减1的原因，是因为for会自增            &#125;        &#125;                return sensitiveWordList;    &#125;        /**     * 替换敏感字字符     * @author chenssy      * @date 2014年4月20日 下午5:12:07     * @param txt     * @param matchType     * @param replaceChar 替换字符，默认*     * @version 1.0     */    public String replaceSensitiveWord(String txt,int matchType,String replaceChar)&#123;        String resultTxt = txt;        Set&lt;String&gt; set = getSensitiveWord(txt, matchType);     //获取所有的敏感词        Iterator&lt;String&gt; iterator = set.iterator();        String word = null;        String replaceString = null;        while (iterator.hasNext()) &#123;            word = iterator.next();            replaceString = getReplaceChars(replaceChar, word.length());            resultTxt = resultTxt.replaceAll(word, replaceString);        &#125;                return resultTxt;    &#125;        /**     * 获取替换字符串     * @author chenssy      * @date 2014年4月20日 下午5:21:19     * @param replaceChar     * @param length     * @return     * @version 1.0     */    private String getReplaceChars(String replaceChar,int length)&#123;        String resultReplace = replaceChar;        for(int i = 1 ; i &lt; length ; i++)&#123;            resultReplace += replaceChar;        &#125;                return resultReplace;    &#125;        /**     * 检查文字中是否包含敏感字符，检查规则如下：&lt;br&gt;     * @author chenssy      * @date 2014年4月20日 下午4:31:03     * @param txt     * @param beginIndex     * @param matchType     * @return，如果存在，则返回敏感词字符的长度，不存在返回0     * @version 1.0     */    @SuppressWarnings(&#123; &quot;rawtypes&quot;&#125;)    public int CheckSensitiveWord(String txt,int beginIndex,int matchType)&#123;        boolean  flag = false;    //敏感词结束标识位：用于敏感词只有1位的情况        int matchFlag = 0;     //匹配标识数默认为0        char word = 0;        Map nowMap = sensitiveWordMap;        for(int i = beginIndex; i &lt; txt.length() ; i++)&#123;            word = txt.charAt(i);            nowMap = (Map) nowMap.get(word);     //获取指定key            if(nowMap != null)&#123;     //存在，则判断是否为最后一个                matchFlag++;     //找到相应key，匹配标识+1                 if(&quot;1&quot;.equals(nowMap.get(&quot;isEnd&quot;)))&#123;                           //如果为最后一个匹配规则,结束循环，返回匹配标识数                    flag = true;       //结束标志位为true                       if(SensitivewordFilterUtil.minMatchTYpe == matchType)&#123;                           //最小规则，直接返回,最大规则还需继续查找                        break;                    &#125;                &#125;            &#125;            else&#123;     //不存在，直接返回                break;            &#125;        &#125;        if(matchFlag &lt; 2 || !flag)&#123;        //长度必须大于等于1，为词             matchFlag = 0;        &#125;        return matchFlag;    &#125;&#125;import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.io.InputStream;import java.io.OutputStream;import java.math.BigInteger;import java.security.MessageDigest;import com.JUtils.date.DateUtils;import com.JUtils.math.RandomUtils;/** * @desc:文件工具类 * @Author:chenssy * @date:2014年8月7日 */public class FileUtils &#123;    private static final String FOLDER_SEPARATOR = &quot;/&quot;;    private static final char EXTENSION_SEPARATOR = &#39;.&#39;;        /**     * @desc:判断指定路径是否存在，如果不存在，根据参数决定是否新建     * @autor:chenssy     * @date:2014年8月7日     * @param filePath   指定的文件路径     * @param isNew      true：新建、false：不新建     * @return 存在返回TRUE，不存在返回FALSE     */    public static boolean isExist(String filePath,boolean isNew)&#123;        File file = new File(filePath);        if(!file.exists() &amp;&amp; isNew)&#123;                return file.mkdirs();    //新建文件路径        &#125;        return false;    &#125;        /**     * 获取文件名，构建结构为 prefix + yyyyMMddHH24mmss + 10位随机数 + suffix + .type     * @autor:chenssy     * @date:2014年8月11日     * @param type    文件类型     * @param prefix  前缀     * @param suffix  后缀     * @return     */    public static String getFileName(String type,String prefix,String suffix)&#123;        String date = DateUtils.getCurrentTime(&quot;yyyyMMddHH24mmss&quot;);   //当前时间        String random = RandomUtils.generateNumberString(10);   //10位随机数                //返回文件名          return prefix + date + random + suffix + &quot;.&quot; + type;    &#125;        /**     * 获取文件名，文件名构成:当前时间 + 10位随机数 + .type     * @autor:chenssy     * @date:2014年8月11日     * @param type  文件类型     * @return     */    public static String getFileName(String type)&#123;        return getFileName(type, &quot;&quot;, &quot;&quot;);    &#125;        /**     * 获取文件名，文件构成：当前时间 + 10位随机数     * @autor:chenssy     * @date:2014年8月11日     * @return     */    public static String getFileName()&#123;        String date = DateUtils.getCurrentTime(&quot;yyyyMMddHH24mmss&quot;);   //当前时间        String random = RandomUtils.generateNumberString(10);   //10位随机数        //返回文件名          return date + random;    &#125;        /**     * 获取指定文件的大小     * @param file     * @return     * @throws Exception     * @author:chenssy     * @date : 2016年4月30日 下午9:10:12     */    @SuppressWarnings(&quot;resource&quot;)    public static long getFileSize(File file) throws Exception &#123;        long size = 0;        if (file.exists()) &#123;            FileInputStream fis = null;            fis = new FileInputStream(file);            size = fis.available();        &#125; else &#123;            file.createNewFile();        &#125;        return size;    &#125;        /**     * 删除所有文件，包括文件夹     * @author : chenssy     * @date : 2016年5月23日 下午12:41:08     * @param dirpath     */    public void deleteAll(String dirpath) &#123;           File path = new File(dirpath);           try &#123;               if (!path.exists())                   return;// 目录不存在退出                if (path.isFile()) // 如果是文件删除                &#123;                   path.delete();                   return;               &#125;               File[] files = path.listFiles();// 如果目录中有文件递归删除文件                for (int i = 0; i &lt; files.length; i++) &#123;                   deleteAll(files[i].getAbsolutePath());               &#125;               path.delete();           &#125; catch (Exception e) &#123;               e.printStackTrace();           &#125;       &#125;        /**     * 复制文件或者文件夹     * @author : chenssy     * @date : 2016年5月23日 下午12:41:59     * @param inputFile    源文件     * @param outputFile    目的文件     * @param isOverWrite   是否覆盖文件     * @throws java.io.IOException     */    public static void copy(File inputFile, File outputFile, boolean isOverWrite)            throws IOException &#123;        if (!inputFile.exists()) &#123;            throw new RuntimeException(inputFile.getPath() + &quot;源目录不存在!&quot;);        &#125;        copyPri(inputFile, outputFile, isOverWrite);    &#125;        /**     * 复制文件或者文件夹     * @author : chenssy     * @date : 2016年5月23日 下午12:43:24     * @param inputFile    源文件     * @param outputFile    目的文件     * @param isOverWrite   是否覆盖文件     * @throws java.io.IOException     */    private static void copyPri(File inputFile, File outputFile, boolean isOverWrite) throws IOException &#123;        if (inputFile.isFile()) &#123;//文件            copySimpleFile(inputFile, outputFile, isOverWrite);        &#125; else &#123;            if (!outputFile.exists()) &#123;//文件夹                outputFile.mkdirs();            &#125;            // 循环子文件夹            for (File child : inputFile.listFiles()) &#123;                copy(child, new File(outputFile.getPath() + &quot;/&quot; + child.getName()), isOverWrite);            &#125;        &#125;    &#125;        /**     * 复制单个文件     * @author : chenssy     * @date : 2016年5月23日 下午12:44:07     * @param inputFile源文件     * @param outputFile目的文件     * @param isOverWrite是否覆盖     * @throws java.io.IOException     */    private static void copySimpleFile(File inputFile, File outputFile,            boolean isOverWrite) throws IOException &#123;        if (outputFile.exists()) &#123;            if (isOverWrite) &#123;//可以覆盖                if (!outputFile.delete()) &#123;                    throw new RuntimeException(outputFile.getPath() + &quot;无法覆盖！&quot;);                &#125;            &#125; else &#123;                // 不允许覆盖                return;            &#125;        &#125;        InputStream in = new FileInputStream(inputFile);        OutputStream out = new FileOutputStream(outputFile);        byte[] buffer = new byte[1024];        int read = 0;        while ((read = in.read(buffer)) != -1) &#123;            out.write(buffer, 0, read);        &#125;        in.close();        out.close();    &#125;        /**     * 获取文件的MD5     * @author : chenssy     * @param file 文件     * @return     */    public static String getFileMD5(File file)&#123;        if (!file.exists() || !file.isFile()) &#123;              return null;          &#125;          MessageDigest digest = null;          FileInputStream in = null;          byte buffer[] = new byte[1024];          int len;          try &#123;              digest = MessageDigest.getInstance(&quot;MD5&quot;);              in = new FileInputStream(file);              while ((len = in.read(buffer, 0, 1024)) != -1) &#123;                  digest.update(buffer, 0, len);              &#125;              in.close();          &#125; catch (Exception e) &#123;              e.printStackTrace();              return null;          &#125;          BigInteger bigInt = new BigInteger(1, digest.digest());          return bigInt.toString(16);      &#125;        /**     * 获取文件的后缀     * @author : chenssy     * @date : 2016年5月23日 下午12:51:59     * @param file 文件     * @return     */    public static String getFileSuffix(String file) &#123;        if (file == null) &#123;            return null;        &#125;        int extIndex = file.lastIndexOf(EXTENSION_SEPARATOR);        if (extIndex == -1) &#123;            return null;        &#125;        int folderIndex = file.lastIndexOf(FOLDER_SEPARATOR);        if (folderIndex &gt; extIndex) &#123;            return null;        &#125;        return file.substring(extIndex + 1);    &#125;        /**     * 文件重命名     * @author : chenssy     * @date : 2016年5月23日 下午12:56:05     * @param oldPath   老文件     * @param newPath   新文件     */    public boolean renameDir(String oldPath, String newPath) &#123;          File oldFile = new File(oldPath);// 文件或目录           File newFile = new File(newPath);// 文件或目录                   return oldFile.renameTo(newFile);// 重命名       &#125;&#125;import java.io.BufferedOutputStream;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.util.zip.ZipEntry;import java.util.zip.ZipOutputStream;/** * 文件压缩、解压工具类。文件压缩格式为zip * * @Author:chenssy * @date:2016年5月24日 下午9:16:01 */public class ZipUitls &#123;    /** 文件后缀名 */    private static final String ZIP_FILE_SUFFIX = &quot;.zip&quot;;        /**     * 压缩文件     * @author:chenssy     * @date : 2016年5月24日 下午9:56:36     * @param resourcePath  源文件     * @param targetPath    目的文件,保存文件路径     */    public static void zipFile(String resourcePath,String targetPath)&#123;        File resourcesFile = new File(resourcePath);         File targetFile = new File(targetPath);                //目的文件不存在，则新建        if(!targetFile.exists())&#123;            targetFile.mkdirs();        &#125;        //文件名        String targetName = resourcesFile.getName() + ZIP_FILE_SUFFIX;                ZipOutputStream out = null;        try &#123;            FileOutputStream outputStream = new FileOutputStream(targetPath+&quot;//&quot;+targetName);            out = new ZipOutputStream(new BufferedOutputStream(outputStream));            compressedFile(out, resourcesFile, &quot;&quot;);        &#125; catch (FileNotFoundException e) &#123;            e.printStackTrace();        &#125;finally&#123;            if (out != null) &#123;                try &#123;                    out.close();                &#125; catch (IOException e) &#123;                    e.printStackTrace();                &#125;             &#125;        &#125;    &#125;    /**     *     * @author:chenssy     * @date : 2016年5月24日 下午10:00:22     * @param out     * @param resourcesFile     * @param dir     */    private static void compressedFile(ZipOutputStream out, File file, String dir) &#123;        FileInputStream fis = null;        try &#123;            if (file.isDirectory()) &#123;//文件夹                // 得到文件列表信息                File[] files = file.listFiles();                // 将文件夹添加到下一级打包目录                out.putNextEntry(new ZipEntry(dir + &quot;/&quot;));                dir = dir.length() == 0 ? &quot;&quot; : dir + &quot;/&quot;;                // 循环将文件夹中的文件打包                for (int i = 0; i &lt; files.length; i++) &#123;                    compressedFile(out, files[i], dir + files[i].getName()); // 递归处理                &#125;            &#125; else &#123; //如果是文件则打包处理                fis = new FileInputStream(file);                out.putNextEntry(new ZipEntry(dir));                // 进行写操作                int j = 0;                byte[] buffer = new byte[1024];                while ((j = fis.read(buffer)) &gt; 0) &#123;                    out.write(buffer, 0, j);                &#125;                // 关闭输入流            &#125;        &#125; catch (FileNotFoundException e) &#123;            e.printStackTrace();        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125; finally&#123;            if(fis != null)&#123;                try &#123;                    fis.close();                &#125; catch (IOException e) &#123;                    e.printStackTrace();                &#125;            &#125;        &#125;    &#125;&#125;import java.text.ParsePosition;import java.text.SimpleDateFormat;import java.util.Date;/** * 日期格式化工具类 * @Author:chenssy * @date:2016年5月26日 下午12:39:57 */public class DateFormatUtils &#123;    public static final String DATE_YEAR = &quot;yyyy&quot;;    /** yyyy:年 */    public static final String DATE_MONTH = &quot;MM&quot;;    /** MM：月 */    public static final String DATE_DAY = &quot;dd&quot;;              /** DD：日 */    public static final String DATE_HOUR = &quot;HH&quot;;    /** HH：时 */    public static final String DATE_MINUTE = &quot;mm&quot;;    /** mm：分 */    public static final String DATE_SECONDES = &quot;ss&quot;;    /** ss：秒 */    public static final String DATE_FORMAT1 = &quot;yyyy-MM-dd&quot;;/** yyyy-MM-dd */    public static final String DATE_FORMAT2 = &quot;yyyy-MM-dd HH:mm:ss&quot;;    /** yyyy-MM-dd hh:mm:ss */    public static final String TIME_FORMAT_SSS = &quot;yyyy-MM-dd HH:mm:ss|SSS&quot;;/** yyyy-MM-dd hh:mm:ss|SSS */    public static final String DATE_NOFUll_FORMAT = &quot;yyyyMMdd&quot;;/** yyyyMMdd */    public static final String TIME_NOFUll_FORMAT = &quot;yyyyMMddHHmmss&quot;;   /** yyyyMMddhhmmss */        /**     *      * 格式转换&lt;br&gt;     * yyyy-MM-dd hh:mm:ss 和 yyyyMMddhhmmss 相互转换&lt;br&gt;     * yyyy-mm-dd 和yyyymmss 相互转换     * @author chenssy     * @date Dec 26, 2013     * @param value  日期     * @return String     */    public static String formatString(String value) &#123;        String sReturn = &quot;&quot;;        if (value == null || &quot;&quot;.equals(value))            return sReturn;        if (value.length() == 14) &#123;   //长度为14格式转换成yyyy-mm-dd hh:mm:ss            sReturn = value.substring(0, 4) + &quot;-&quot; + value.substring(4, 6) + &quot;-&quot; + value.substring(6, 8) + &quot; &quot;                    + value.substring(8, 10) + &quot;:&quot; + value.substring(10, 12) + &quot;:&quot; + value.substring(12, 14);            return sReturn;        &#125;        if (value.length() == 19) &#123;   //长度为19格式转换成yyyymmddhhmmss            sReturn = value.substring(0, 4) + value.substring(5, 7) + value.substring(8, 10) + value.substring(11, 13)                    + value.substring(14, 16) + value.substring(17, 19);            return sReturn;        &#125;        if(value.length() == 10)&#123;     //长度为10格式转换成yyyymmhh            sReturn = value.substring(0, 4) + value.substring(5,7) + value.substring(8,10);        &#125;        if(value.length() == 8)&#123;      //长度为8格式转化成yyyy-mm-dd            sReturn = value.substring(0, 4) + &quot;-&quot; + value.substring(4, 6) + &quot;-&quot; + value.substring(6, 8);        &#125;        return sReturn;    &#125;        public static String formatDate(String date, String format) &#123;        if (date == null || &quot;&quot;.equals(date))&#123;            return &quot;&quot;;        &#125;        Date dt = null;        SimpleDateFormat inFmt = null;        SimpleDateFormat outFmt = null;        ParsePosition pos = new ParsePosition(0);        date = date.replace(&quot;-&quot;, &quot;&quot;).replace(&quot;:&quot;, &quot;&quot;);        if ((date == null) || (&quot;&quot;.equals(date.trim())))            return &quot;&quot;;        try &#123;            if (Long.parseLong(date) == 0L)                return &quot;&quot;;        &#125; catch (Exception nume) &#123;            return date;        &#125;        try &#123;            switch (date.trim().length()) &#123;            case 14:                inFmt = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);                break;            case 12:                inFmt = new SimpleDateFormat(&quot;yyyyMMddHHmm&quot;);                break;            case 10:                inFmt = new SimpleDateFormat(&quot;yyyyMMddHH&quot;);                break;            case 8:                inFmt = new SimpleDateFormat(&quot;yyyyMMdd&quot;);                break;            case 6:                inFmt = new SimpleDateFormat(&quot;yyyyMM&quot;);                break;            case 7:            case 9:            case 11:            case 13:            default:                return date;            &#125;            if ((dt = inFmt.parse(date, pos)) == null)                return date;            if ((format == null) || (&quot;&quot;.equals(format.trim()))) &#123;                outFmt = new SimpleDateFormat(&quot;yyyy年MM月dd日&quot;);            &#125; else &#123;                outFmt = new SimpleDateFormat(format);            &#125;            return outFmt.format(dt);        &#125; catch (Exception ex) &#123;        &#125;        return date;    &#125;    /**     * 格式化日期     * @author chenming     * @date 2016年5月31日     * @param date     * @param format     * @return     */    public static String formatDate(Date date,String format)&#123;        return formatDate(DateUtils.date2String(date), format);    &#125;        /**     * @desc:格式化是时间，采用默认格式（yyyy-MM-dd HH:mm:ss）     * @autor:chenssy     * @date:2014年8月6日     * @param value     * @return     */    public static String formatDate(String value)&#123;        return getFormat(DATE_FORMAT2).format(DateUtils.string2Date(value, DATE_FORMAT2));    &#125;        /**     * 格式化日期     * @author : chenssy     * @date : 2016年5月31日 下午5:40:58     * @param value     * @return     */    public static String formatDate(Date value)&#123;        return formatDate(DateUtils.date2String(value));    &#125;        /**     * 获取日期显示格式，为空默认为yyyy-mm-dd HH:mm:ss     * @author chenssy     * @date Dec 30, 2013     * @param format     * @return     * @return SimpleDateFormat     */    protected static SimpleDateFormat getFormat(String format)&#123;        if(format == null || &quot;&quot;.equals(format))&#123;            format = DATE_FORMAT2;        &#125;        return new SimpleDateFormat(format);    &#125;&#125;import java.text.DateFormat;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Calendar;import java.util.Date;/** * @desc:时间处理工具类 *  * @Author:chenssy * @date:2014年8月4日 */public class DateUtils &#123;    private static final String[] weeks = &#123;&quot;星期日&quot;,&quot;星期一&quot;,&quot;星期二&quot;,&quot;星期三&quot;,&quot;星期四&quot;,&quot;星期五&quot;,&quot;星期六&quot;&#125;;    /**     * 根据指定格式获取当前时间     * @author chenssy     * @date Dec 27, 2013     * @param format     * @return String     */    public static String getCurrentTime(String format)&#123;        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);        Date date = new Date();        return sdf.format(date);    &#125;        /**     * 获取当前时间，格式为：yyyy-MM-dd HH:mm:ss     * @author chenssy     * @date Dec 27, 2013     * @return String     */    public static String getCurrentTime()&#123;        return getCurrentTime(DateFormatUtils.DATE_FORMAT2);    &#125;        /**     * 获取指定格式的当前时间：为空时格式为yyyy-mm-dd HH:mm:ss     * @author chenssy     * @date Dec 30, 2013     * @param format     * @return Date     */    public static Date getCurrentDate(String format)&#123;         SimpleDateFormat sdf = DateFormatUtils.getFormat(format);         String dateS = getCurrentTime(format);         Date date = null;         try &#123;            date = sdf.parse(dateS);        &#125; catch (ParseException e) &#123;            e.printStackTrace();        &#125;        return date;    &#125;        /**     * 获取当前时间，格式为yyyy-MM-dd HH:mm:ss     * @author chenssy     * @date Dec 30, 2013     * @return Date     */    public static Date getCurrentDate()&#123;        return getCurrentDate(DateFormatUtils.DATE_FORMAT2);    &#125;        /**     * 给指定日期加入年份，为空时默认当前时间     * @author chenssy     * @date Dec 30, 2013     * @param year 年份  正数相加、负数相减     * @param date 为空时，默认为当前时间     * @param format 默认格式为：yyyy-MM-dd HH:mm:ss     * @return String     */    public static String addYearToDate(int year,Date date,String format)&#123;        Calendar calender = getCalendar(date,format);        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);                calender.add(Calendar.YEAR, year);                return sdf.format(calender.getTime());    &#125;        /**     * 给指定日期加入年份，为空时默认当前时间     * @author chenssy     * @date Dec 30, 2013     * @param year 年份  正数相加、负数相减     * @param date 为空时，默认为当前时间     * @param format 默认格式为：yyyy-MM-dd HH:mm:ss     * @return String     */    public static String addYearToDate(int year,String date,String format)&#123;        Date newDate = new Date();        if(null != date &amp;&amp; !&quot;&quot;.equals(date))&#123;            newDate = string2Date(date, format);        &#125;                return addYearToDate(year, newDate, format);    &#125;        /**     * 给指定日期增加月份 为空时默认当前时间     * @author chenssy     * @date Dec 30, 2013     * @param month  增加月份  正数相加、负数相减     * @param date 指定时间     * @param format 指定格式 为空默认 yyyy-mm-dd HH:mm:ss     * @return String     */    public static String addMothToDate(int month,Date date,String format) &#123;        Calendar calender = getCalendar(date,format);        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);                calender.add(Calendar.MONTH, month);                return sdf.format(calender.getTime());    &#125;        /**     * 给指定日期增加月份 为空时默认当前时间     * @author chenssy     * @date Dec 30, 2013     * @param month  增加月份  正数相加、负数相减     * @param date 指定时间     * @param format 指定格式 为空默认 yyyy-mm-dd HH:mm:ss     * @return String     */    public static String addMothToDate(int month,String date,String format) &#123;        Date newDate = new Date();        if(null != date &amp;&amp; !&quot;&quot;.equals(date))&#123;            newDate = string2Date(date, format);        &#125;                return addMothToDate(month, newDate, format);    &#125;        /**     * 给指定日期增加天数，为空时默认当前时间     * @author chenssy     * @date Dec 31, 2013     * @param day 增加天数 正数相加、负数相减     * @param date 指定日期     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss     * @return String     */    public static String addDayToDate(int day,Date date,String format) &#123;        Calendar calendar = getCalendar(date, format);        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);                calendar.add(Calendar.DATE, day);                return sdf.format(calendar.getTime());    &#125;        /**     * 给指定日期增加天数，为空时默认当前时间     * @author chenssy     * @date Dec 31, 2013     * @param day 增加天数 正数相加、负数相减     * @param date 指定日期     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss     * @return String     */    public static String addDayToDate(int day,String date,String format) &#123;        Date newDate = new Date();        if(null != date &amp;&amp; !&quot;&quot;.equals(date))&#123;            newDate = string2Date(date, format);        &#125;                return addDayToDate(day, newDate, format);    &#125;        /**     * 给指定日期增加小时，为空时默认当前时间     * @author chenssy     * @date Dec 31, 2013     * @param hour 增加小时  正数相加、负数相减     * @param date 指定日期     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss     * @return String     */    public static String addHourToDate(int hour,Date date,String format) &#123;        Calendar calendar = getCalendar(date, format);        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);                calendar.add(Calendar.HOUR, hour);                return sdf.format(calendar.getTime());    &#125;        /**     * 给指定日期增加小时，为空时默认当前时间     * @author chenssy     * @date Dec 31, 2013     * @param hour 增加小时  正数相加、负数相减     * @param date 指定日期     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss     * @return String     */    public static String addHourToDate(int hour,String date,String format) &#123;        Date newDate = new Date();        if(null != date &amp;&amp; !&quot;&quot;.equals(date))&#123;            newDate = string2Date(date, format);        &#125;                return addHourToDate(hour, newDate, format);    &#125;        /**     * 给指定的日期增加分钟，为空时默认当前时间     * @author chenssy     * @date Dec 31, 2013     * @param minute 增加分钟  正数相加、负数相减     * @param date 指定日期      * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss     * @return String     */    public static String addMinuteToDate(int minute,Date date,String format) &#123;        Calendar calendar = getCalendar(date, format);        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);                calendar.add(Calendar.MINUTE, minute);                return sdf.format(calendar.getTime());    &#125;        /**     * 给指定的日期增加分钟，为空时默认当前时间     * @author chenssy     * @date Dec 31, 2013     * @param minute 增加分钟  正数相加、负数相减     * @param date 指定日期      * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss     * @return String     */    public static String addMinuteToDate(int minute,String date,String format)&#123;        Date newDate = new Date();        if(null != date &amp;&amp; !&quot;&quot;.equals(date))&#123;            newDate = string2Date(date, format);        &#125;                return addMinuteToDate(minute, newDate, format);    &#125;        /**     * 给指定日期增加秒，为空时默认当前时间     * @author chenssy     * @date Dec 31, 2013     * @param second 增加秒 正数相加、负数相减     * @param date 指定日期     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss     * @return String     */    public static String addSecondToDate(int second,Date date,String format)&#123;        Calendar calendar = getCalendar(date, format);        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);                calendar.add(Calendar.SECOND, second);                return sdf.format(calendar.getTime());    &#125;        /**     * 给指定日期增加秒，为空时默认当前时间     * @author chenssy     * @date Dec 31, 2013     * @param second 增加秒 正数相加、负数相减     * @param date 指定日期     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss     * @return String     * @throws Exception      */    public static String addSecondToDate(int second,String date,String format)&#123;        Date newDate = new Date();        if(null != date &amp;&amp; !&quot;&quot;.equals(date))&#123;            newDate = string2Date(date, format);        &#125;                return addSecondToDate(second, newDate, format);    &#125;        /**     * 获取指定格式指定时间的日历     * @author chenssy     * @date Dec 30, 2013     * @param date 时间      * @param format 格式     * @return Calendar     */    public static Calendar getCalendar(Date date,String format)&#123;        if(date == null)&#123;            date = getCurrentDate(format);        &#125;                Calendar calender = Calendar.getInstance();        calender.setTime(date);                return calender;    &#125;        /**     * 字符串转换为日期，日期格式为     *      * @author : chenssy     * @date : 2016年5月31日 下午5:20:22     *     * @param value     * @return     */    public static Date string2Date(String value)&#123;        if(value == null || &quot;&quot;.equals(value))&#123;            return null;        &#125;                SimpleDateFormat sdf = DateFormatUtils.getFormat(DateFormatUtils.DATE_FORMAT2);        Date date = null;                try &#123;            value = DateFormatUtils.formatDate(value, DateFormatUtils.DATE_FORMAT2);            date = sdf.parse(value);        &#125; catch (Exception e) &#123;            e.printStackTrace();        &#125;        return date;    &#125;        /**     * 将字符串(格式符合规范)转换成Date     * @author chenssy     * @date Dec 31, 2013     * @param value 需要转换的字符串     * @param format 日期格式      * @return Date     */    public static Date string2Date(String value,String format)&#123;        if(value == null || &quot;&quot;.equals(value))&#123;            return null;        &#125;                SimpleDateFormat sdf = DateFormatUtils.getFormat(format);        Date date = null;                try &#123;            value = DateFormatUtils.formatDate(value, format);            date = sdf.parse(value);        &#125; catch (Exception e) &#123;            e.printStackTrace();        &#125;        return date;    &#125;        /**     * 将日期格式转换成String     * @author chenssy     * @date Dec 31, 2013     *      * @param value 需要转换的日期     * @param format 日期格式     * @return String     */    public static String date2String(Date value,String format)&#123;        if(value == null)&#123;            return null;        &#125;                SimpleDateFormat sdf = DateFormatUtils.getFormat(format);        return sdf.format(value);    &#125;        /**     * 日期转换为字符串     *      * @author : chenssy     * @date : 2016年5月31日 下午5:21:38     *     * @param value     * @return     */    public static String date2String(Date value)&#123;        if(value == null)&#123;            return null;        &#125;                SimpleDateFormat sdf = DateFormatUtils.getFormat(DateFormatUtils.DATE_FORMAT2);        return sdf.format(value);    &#125;        /**     * 获取指定日期的年份     * @author chenssy     * @date Dec 31, 2013     * @param value 日期     * @return int     */    public static int getCurrentYear(Date value)&#123;        String date = date2String(value, DateFormatUtils.DATE_YEAR);        return Integer.valueOf(date);    &#125;        /**     * 获取指定日期的年份     * @author chenssy     * @date Dec 31, 2013     * @param value 日期     * @return int     */    public static int getCurrentYear(String value) &#123;        Date date = string2Date(value, DateFormatUtils.DATE_YEAR);        Calendar calendar = getCalendar(date, DateFormatUtils.DATE_YEAR);        return calendar.get(Calendar.YEAR);    &#125;        /**     * 获取指定日期的月份     * @author chenssy     * @date Dec 31, 2013     * @param value 日期     * @return int     */    public static int getCurrentMonth(Date value)&#123;        String date = date2String(value, DateFormatUtils.DATE_MONTH);        return Integer.valueOf(date);    &#125;        /**     * 获取指定日期的月份     * @author chenssy     * @date Dec 31, 2013     * @param value 日期     * @return int     */    public static int getCurrentMonth(String value) &#123;        Date date = string2Date(value, DateFormatUtils.DATE_MONTH);        Calendar calendar = getCalendar(date, DateFormatUtils.DATE_MONTH);                return calendar.get(Calendar.MONTH);    &#125;        /**     * 获取指定日期的天份     * @author chenssy     * @date Dec 31, 2013     * @param value 日期     * @return int     */    public static int getCurrentDay(Date value)&#123;        String date = date2String(value, DateFormatUtils.DATE_DAY);        return Integer.valueOf(date);    &#125;        /**     * 获取指定日期的天份     * @author chenssy     * @date Dec 31, 2013     * @param value 日期     * @return int     */    public static int getCurrentDay(String value)&#123;        Date date = string2Date(value, DateFormatUtils.DATE_DAY);        Calendar calendar = getCalendar(date, DateFormatUtils.DATE_DAY);                return calendar.get(Calendar.DATE);    &#125;        /**     * 获取当前日期为星期几     * @author chenssy     * @date Dec 31, 2013     * @param value 日期     * @return String     */    public static String getCurrentWeek(Date value) &#123;        Calendar calendar = getCalendar(value, DateFormatUtils.DATE_FORMAT1);        int weekIndex = calendar.get(Calendar.DAY_OF_WEEK) - 1 &lt; 0 ? 0 : calendar.get(Calendar.DAY_OF_WEEK) - 1;                return weeks[weekIndex];    &#125;        /**     * 获取当前日期为星期几     * @author chenssy     * @date Dec 31, 2013     * @param value 日期     * @return String     */    public static String getCurrentWeek(String value) &#123;        Date date = string2Date(value, DateFormatUtils.DATE_FORMAT1);        return getCurrentWeek(date);    &#125;        /**     * 获取指定日期的小时     * @author chenssy     * @date Dec 31, 2013     * @param value 日期     * @return int     */    public static int getCurrentHour(Date value)&#123;        String date = date2String(value, DateFormatUtils.DATE_HOUR);        return Integer.valueOf(date);    &#125;        /**     * 获取指定日期的小时     * @author chenssy     * @date Dec 31, 2013     * @param value 日期     * @return     * @return int     */    public static int getCurrentHour(String value) &#123;        Date date = string2Date(value, DateFormatUtils.DATE_HOUR);        Calendar calendar = getCalendar(date, DateFormatUtils.DATE_HOUR);                return calendar.get(Calendar.DATE);    &#125;        /**     * 获取指定日期的分钟     * @author chenssy     * @date Dec 31, 2013     * @param value 日期     * @return int     */    public static int getCurrentMinute(Date value)&#123;        String date = date2String(value, DateFormatUtils.DATE_MINUTE);        return Integer.valueOf(date);    &#125;        /**     * 获取指定日期的分钟     * @author chenssy     * @date Dec 31, 2013     * @param value 日期     * @return int     */    public static int getCurrentMinute(String value)&#123;        Date date = string2Date(value, DateFormatUtils.DATE_MINUTE);        Calendar calendar = getCalendar(date, DateFormatUtils.DATE_MINUTE);                return calendar.get(Calendar.MINUTE);    &#125;        /**       * 比较两个日期相隔多少天(月、年) &lt;br&gt;     * 例：&lt;br&gt;     * &amp;nbsp;compareDate(&quot;2009-09-12&quot;, null, 0);//比较天 &lt;br&gt;     * &amp;nbsp;compareDate(&quot;2009-09-12&quot;, null, 1);//比较月 &lt;br&gt;      * &amp;nbsp;compareDate(&quot;2009-09-12&quot;, null, 2);//比较年 &lt;br&gt;     *      * @author chenssy     * @date Dec 31, 2013      * @param startDay 需要比较的时间 不能为空(null),需要正确的日期格式 ,如：2009-09-12        * @param endDay 被比较的时间  为空(null)则为当前时间         * @param stype 返回值类型   0为多少天，1为多少个月，2为多少年         * @return int     */        public static int compareDate(String startDay,String endDay,int stype) &#123;             int n = 0;             startDay = DateFormatUtils.formatDate(startDay, &quot;yyyy-MM-dd&quot;);        endDay = DateFormatUtils.formatDate(endDay, &quot;yyyy-MM-dd&quot;);                String formatStyle = &quot;yyyy-MM-dd&quot;;        if(1 == stype)&#123;            formatStyle = &quot;yyyy-MM&quot;;        &#125;else if(2 == stype)&#123;            formatStyle = &quot;yyyy&quot;;        &#125;                        endDay = endDay==null ? getCurrentTime(&quot;yyyy-MM-dd&quot;) : endDay;                          DateFormat df = new SimpleDateFormat(formatStyle);             Calendar c1 = Calendar.getInstance();             Calendar c2 = Calendar.getInstance();             try &#123;                 c1.setTime(df.parse(startDay));                 c2.setTime(df.parse(endDay));           &#125; catch (Exception e) &#123;                e.printStackTrace();        &#125;             while (!c1.after(c2)) &#123;                   // 循环对比，直到相等，n 就是所要的结果                 n++;                 if(stype==1)&#123;                     c1.add(Calendar.MONTH, 1);          // 比较月份，月份+1                 &#125;                 else&#123;                     c1.add(Calendar.DATE, 1);           // 比较天数，日期+1                 &#125;             &#125;             n = n-1;             if(stype==2)&#123;                 n = (int)n/365;             &#125;                return n;         &#125;           /**     * 比较两个时间相差多少小时(分钟、秒)     * @author chenssy     * @date Jan 2, 2014     * @param startTime 需要比较的时间 不能为空，且必须符合正确格式：2012-12-12 12:12:     * @param endTime 需要被比较的时间 若为空则默认当前时间     * @param type 1：小时   2：分钟   3：秒     * @return int     */    public static int compareTime(String startTime , String endTime , int type) &#123;        //endTime是否为空，为空默认当前时间        if(endTime == null || &quot;&quot;.equals(endTime))&#123;            endTime = getCurrentTime();        &#125;                SimpleDateFormat sdf = DateFormatUtils.getFormat(&quot;&quot;);        int value = 0;        try &#123;            Date begin = sdf.parse(startTime);            Date end = sdf.parse(endTime);            long between = (end.getTime() - begin.getTime()) / 1000;  //除以1000转换成豪秒            if(type == 1)&#123;   //小时                value = (int) (between % (24 * 36000) / 3600);            &#125;            else if(type == 2)&#123;                value = (int) (between % 3600 / 60);            &#125;            else if(type == 3)&#123;                value = (int) (between % 60 / 60);            &#125;        &#125; catch (ParseException e) &#123;            e.printStackTrace();        &#125;        return value;    &#125;        /**     * 比较两个日期的大小。&lt;br&gt;     * 若date1 &gt; date2 则返回 1&lt;br&gt;     * 若date1 = date2 则返回 0&lt;br&gt;     * 若date1 &lt; date2 则返回-1     * @autor:chenssy     * @date:2014年9月9日     *     * @param date1       * @param date2     * @param format  待转换的格式     * @return 比较结果     */    public static int compare(String date1, String date2,String format) &#123;        DateFormat df = DateFormatUtils.getFormat(format);        try &#123;            Date dt1 = df.parse(date1);            Date dt2 = df.parse(date2);            if (dt1.getTime() &gt; dt2.getTime()) &#123;                return 1;            &#125; else if (dt1.getTime() &lt; dt2.getTime()) &#123;                return -1;            &#125; else &#123;                return 0;            &#125;        &#125; catch (Exception exception) &#123;            exception.printStackTrace();        &#125;        return 0;    &#125;        /**     * 获取指定月份的第一天      *      * @author : chenssy     * @date : 2016年5月31日 下午5:31:10     *     * @param date     * @return     */    public static String getMonthFirstDate(String date)&#123;        date = DateFormatUtils.formatDate(date);        return DateFormatUtils.formatDate(date, &quot;yyyy-MM&quot;) + &quot;-01&quot;;    &#125;        /**     * 获取指定月份的最后一天     *      * @author : chenssy     * @date : 2016年5月31日 下午5:32:09     *     * @param strdate     * @return     */    public static String getMonthLastDate(String date) &#123;        Date strDate = DateUtils.string2Date(getMonthFirstDate(date));        Calendar calendar = Calendar.getInstance();        calendar.setTime(strDate);        calendar.add(Calendar.MONTH, 1);        calendar.add(Calendar.DAY_OF_YEAR, -1);        return DateFormatUtils.formatDate(calendar.getTime());    &#125;        /**     * 获取所在星期的第一天     *      * @author : chenssy     * @date : 2016年6月1日 下午12:38:53     *     * @param date     * @return     */    @SuppressWarnings(&quot;static-access&quot;)    public static Date getWeekFirstDate(Date date) &#123;        Calendar now = Calendar.getInstance();        now.setTime(date);        int today = now.get(Calendar.DAY_OF_WEEK);        int first_day_of_week = now.get(Calendar.DATE) + 2 - today; // 星期一        now.set(now.DATE, first_day_of_week);        return now.getTime();    &#125;        /**     * 获取所在星期的最后一天     * @author : chenssy     * @date : 2016年6月1日 下午12:40:31     * @param date     * @return     */    @SuppressWarnings(&quot;static-access&quot;)    public static Date geWeektLastDate(Date date) &#123;        Calendar now = Calendar.getInstance();        now.setTime(date);        int today = now.get(Calendar.DAY_OF_WEEK);        int first_day_of_week = now.get(Calendar.DATE) + 2 - today; // 星期一        int last_day_of_week = first_day_of_week + 6; // 星期日        now.set(now.DATE, last_day_of_week);        return now.getTime();    &#125;&#125;import java.sql.Timestamp;import java.text.SimpleDateFormat;import java.util.Date;/** * TimeStamp工具类，提供TimeStamp与String、Date的转换 * @author chenssy * @date 2016-09-24 * @since 1.0.0 */public class TimestampUtils &#123;/** * String转换为TimeStamp * @param value 待转换的String，格式必须为 yyyy-mm-dd hh:mm:ss[.f...] 这样的格式，中括号表示可选，否则报错 * @return java.sql.Timestamp * @author chenssy * @date 2016-09-24 * @since v1.0.0 */    public static Timestamp string2Timestamp(String value)&#123;        if(value == null &amp;&amp; !&quot;&quot;.equals(value.trim()))&#123;            return null;        &#125;        Timestamp ts = new Timestamp(System.currentTimeMillis());        ts = Timestamp.valueOf(value);        return ts;    &#125;    /**     * 将Timestamp 转换为String类型，format为null则使用默认格式 yyyy-MM-dd HH:mm:ss     * @param value     待转换的Timestamp     * @param format    String的格式     * @return java.lang.String     * @author chenssy     * @date 2016-09-24     * @since v1.0.0     */    public static String timestamp2String(Timestamp value,String format)&#123;        if(null == value)&#123;            return &quot;&quot;;        &#125;        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);                return sdf.format(value);    &#125;    /**     * Date转换为Timestamp     * @param date    待转换的Date     * @return java.sql.Timestamp     * @author chenssy     * @date 2016-09-24     * @since v1.0.0     */    public static Timestamp date2Timestamp(Date date)&#123;        if(date == null)&#123;            return null;        &#125;        return new Timestamp(date.getTime());    &#125;    /**     * Timestamp转换为Date     * @param time  待转换的Timestamp     * @return java.util.Date     * @author chenssy     * @date 2016-09-24     * @since v1.0.0     */    public static Date timestamp2Date(Timestamp time)&#123;        return time == null ? null : time;    &#125;&#125;mport java.io.ByteArrayInputStream;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.io.Serializable;import java.util.Collection;/** * 克隆工具类，进行深克隆,包括对象、集合 *  * @Author:chenssy * @date:2014年8月9日 */public class CloneUtils &#123;    /**     * 采用对象的序列化完成对象的深克隆     * @autor:chenssy     * @date:2014年8月9日     *     * @param obj     * 待克隆的对象     * @return     */    @SuppressWarnings(&quot;unchecked&quot;)    public static &lt;T extends Serializable&gt; T cloneObject(T obj) &#123;        T cloneObj = null;        try &#123;            // 写入字节流            ByteArrayOutputStream out = new ByteArrayOutputStream();            ObjectOutputStream obs = new ObjectOutputStream(out);            obs.writeObject(obj);            obs.close();            // 分配内存，写入原始对象，生成新对象            ByteArrayInputStream ios = new ByteArrayInputStream(out.toByteArray());            ObjectInputStream ois = new ObjectInputStream(ios);            // 返回生成的新对象            cloneObj = (T) ois.readObject();            ois.close();        &#125; catch (Exception e) &#123;            e.printStackTrace();        &#125;        return cloneObj;    &#125;        /**     * 利用序列化完成集合的深克隆     * @autor:chenssy     * @date:2014年8月9日     *     * @param collection     * 待克隆的集合     * @return     * @throws ClassNotFoundException     * @throws java.io.IOException     */    @SuppressWarnings(&quot;unchecked&quot;)    public static &lt;T&gt; Collection&lt;T&gt; cloneCollection(Collection&lt;T&gt; collection) throws ClassNotFoundException, IOException&#123;        ByteArrayOutputStream byteOut = new ByteArrayOutputStream();          ObjectOutputStream out = new ObjectOutputStream(byteOut);          out.writeObject(collection);        out.close();              ByteArrayInputStream byteIn = new ByteArrayInputStream(byteOut.toByteArray());          ObjectInputStream in = new ObjectInputStream(byteIn);          Collection&lt;T&gt; dest = (Collection&lt;T&gt;) in.readObject();          in.close();                return dest;      &#125;&#125;import java.beans.BeanInfo;import java.beans.IntrospectionException;import java.beans.Introspector;import java.beans.PropertyDescriptor;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;import java.util.HashMap;import java.util.Map;/** * Bean与Map的转换 * * @author chenssy * @date 2016-09-24 * @since 1.0.0 */public class BeanMapConvert &#123;    /**     * Bean转换为Map     * @param object     * @return String-Object的HashMap     * @author chenssy     * @date 2016-09-25     * @since v1.0.0     */    public static Map&lt;String,Object&gt; bean2MapObject(Object object)&#123;        if(object == null)&#123;            return null;        &#125;        Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;();        try &#123;            BeanInfo beanInfo = Introspector.getBeanInfo(object.getClass());            PropertyDescriptor[] propertyDescriptors = beanInfo.getPropertyDescriptors();            for (PropertyDescriptor property : propertyDescriptors) &#123;                String key = property.getName();                // 过滤class属性                if (!key.equals(&quot;class&quot;)) &#123;                    // 得到property对应的getter方法                    Method getter = property.getReadMethod();                    Object value = getter.invoke(object);                    map.put(key, value);                &#125;            &#125;        &#125; catch (Exception e) &#123;           e.printStackTrace();        &#125;        return map;    &#125;    /**     * Map转换为Java Bean     * @param map     待转换的Map     * @param object  Java Bean     * @return java.lang.Object     * @author chenssy     * @date 2016-09-25     * @since v1.0.0     */    public static Object map2Bean(Map map,Object object)&#123;        if(map == null || object == null)&#123;            return null;        &#125;        try &#123;            BeanInfo beanInfo = Introspector.getBeanInfo(object.getClass());            PropertyDescriptor[] propertyDescriptors = beanInfo.getPropertyDescriptors();            for (PropertyDescriptor property : propertyDescriptors) &#123;                String key = property.getName();                if (map.containsKey(key)) &#123;                    Object value = map.get(key);                    // 得到property对应的setter方法                    Method setter = property.getWriteMethod();                    setter.invoke(object, value);                &#125;            &#125;        &#125; catch (IntrospectionException e) &#123;            e.printStackTrace();        &#125; catch (InvocationTargetException e) &#123;            e.printStackTrace();        &#125; catch (IllegalAccessException e) &#123;            e.printStackTrace();        &#125;        return object;    &#125;&#125;</code></pre><h2 id="Flink的exactly-once是如何保证的？"><a href="#Flink的exactly-once是如何保证的？" class="headerlink" title="Flink的exactly-once是如何保证的？"></a><strong>Flink的exactly-once是如何保证的？</strong></h2><pre><code class="java"></code></pre><h2 id="Hbase列族设计"><a href="#Hbase列族设计" class="headerlink" title="Hbase列族设计"></a><strong>Hbase列族设计</strong></h2><pre><code class="python">###  列族属性配置版本数量最小版本数存活时间(TTL)数据块大小(blockSize)块缓存(BlockCache)激进缓存(IN_MEMORY)压缩(compression)布隆过滤器数据库编码复制范围预分区版本数量（VERSIONS）每个列族可以单独设置行版本数，默认是3。这个设置很重要，因为HBase是不会去覆盖一个值的，它只会在后面追加写，用时间戳（版本号）来区分，过早的版本会在执行Major Compaction时删除。这个版本的值可以根据具体的场景来增加或减少。不推荐将版本最大值设置成一个很高的水平，除非老数据对你也非常重要。过多的版本，会导致存储文件变大，以至于影响查询效率。最小版本数（MIN_VERSIONS ）每个列族可以设置最小版本数，最小版本数缺省值是0，表示禁用该特性。最小版本数参数和存活时间是一起使用的，允许配置“如保存最后T秒有价值的数据，最多N个版本，但最少M个版本”（M是最小版本，M&lt;N）。该参数仅在存活时间对列族启用，且必须小于行版本数。存活时间（TTL）HBase支持配置版本数据的存活时间（TTL），TTL设置了一个基于时间戳的临界值，HBase会自动检查TTL值是否达到上限，如果TTL达到上限，则该数据会在Major Compaction过程中被删除。数据块大小（BLOCKSIZE ）hbase默认的块大小是64kb，不同于HDFS默认64MB的块大小。原因是hbase需要支持随机访问，一旦找到了行键所在的块，接下来就会定位对应的单元格。使用64kb的块扫描的速度显然优于64MB大小的块。对于不同的业务数据，块大小的合理设置对读写性能有很大的影响。如果业务请求以Get请求为主，可以考虑将块大小设置较小；如果以Scan请求为主，可以将块大小调大；默认的64K块大小是在Scan和Get之间取得的一个平衡。注意：默认块大小适用于多种数据使用模式，调整块大小是比较高级的操作。配置错误将对性能产生负面影响。因此建议在调整之后进行测试，根据测试结果决定是否可以线上使用。块缓存（BLOCKCACHE）默认是true。缓存是内存存储，hbase使用块缓存将最近使用的块加载到内存中。块缓存会根据最近最久未使用（LRU）”的规则删除数据块。如果你的使用场景是经常顺序访问或者很少被访问，可以关闭列族的缓存。列族缓存默认是打开的。激进缓存的配置（IN_MEMORY）HBase可以选择一个列族赋予更高的优先级缓存，激进缓存（表示优先级更高），IN_MEMORY 默认是false。如果设置为true，hbase会尝试将整个列族保存在内存中，只有在需要保存是才会持久化写入磁盘。但是在运行时hbase会尝试将整张表加载到内存里。这个参数通常适合较小的列族。压缩（COMPRESSION）HBase在写入数据块到HDFS之前会首先对数据进行压缩，再落盘，从而减少磁盘空间使用量。而在读数据的时候首先从HDFS中加载出block块之后进行解压缩，然后再缓存到BlockCache，最后返回给用户。使用压缩其实就是使用CPU资源换取磁盘空间资源。HBase支持三种压缩方式：LZO、Snappy和GZIP。默认为NONE，不适用压缩，| **压缩算法**  | 压缩比率  | 压缩速度 | 解压速度  || ------------ | -------- | -------- | -------- || GZIP         | 13.4%    | 21 MB/s  | 118 MB/s || LZO          | 20.5%    | 135 MB/s | 410 MB/s || Snappy       | 22.2%    | 172 MB/s | 409 MB/s || ------------ | -------- | -------- | -------- |其中：1：GZIP的压缩率最高，但是其实CPU密集型的，对CPU的消耗比其他算法要多，压缩和解压速度也慢；LZO的压缩率居中，比GZIP要低一些，但是压缩和解压速度明显要比GZIP快很多，其中解压速度快的更多；2：Snappy的压缩率最低，而压缩和解压速度要稍微比LZO要快一些。综合来看，Snappy的压缩率最低，但是编解码速率最高，对CPU的消耗也最小，目前一般建议使用Snappy。复制范围（REPLICATION_SCOPE ）HBase提供了跨级群同步的功能，本地集群的数据更新可以及时同步到其他集群。复制范围（replication scope）的参数默认为0，表示复制功能处于关闭状态。预分区（SPLITS ）在默认情况下，HBase表在刚刚被创建的时候，只有1个分区（Region），当一个Region的大小达到阈值（通过hbase.hregion.max.filesize参数控制），Region会进行split，分裂成2个Region。但是在进行split的时候，会消耗大量的资源，频繁的split会对HBase的性能造成巨大的影响。HBase提供了预分区的功能，用户可以在创建表的时候对表按照一定的规则提前进行分区。这样是进行HBase数据读写的时候，会按照Region分区情况，在集群内做数据的负载均衡。常用分区方法：create &#39;table&#39;,&#39;cf&#39;, SPLITS =&gt; [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;]create &#39;table&#39;,&#39;cf&#39;, &#123; NUMREGIONS =&gt; 8 , SPLITALGO =&gt; &#39;UniformSplit&#39; &#125;create &#39;table&#39;,&#39;cf&#39;, &#123; NUMREGIONS =&gt; 10, SPLITALGO =&gt; &#39;HexStringSplit&#39; &#125; BLOOMFILTERBloomFilter主要用来过滤不存在待检索RowKey或者Row-Col的HFile文件，避免无用的IO操作。它会告诉你在这个HFile文件中是否可能存在待检索的KV，如果不存在，就可以不用消耗IO打开文件进行seek。通过设置BloomFilter可以提升随机读写的性能。BloomFilter是一个列族级别的配置属性，如果在表中设置了BloomFilter，那么HBase会在生成StoreFile时包含一份BloomFilter结构的数据，称其为MetaBlock和DataBlock(真实KeyValue数据)一起由LRUBlockCache维护。所以开启BloomFilter会有一定的存储即内存Cache的开销。BloomFilter取值有两个，row和rowcol，需要根据业务来确定具体使用哪种。如果业务大多数随机查询时仅仅使用row作为查询条件，BloomFilter一定要设置为row；如果大多数随机查询使用row+cf作为查询条件，BloomFilter需要设置为rowcol；如果不确定查询类型，建议设置为row。列族设置列族数量不要在一张表中定义太多的列族。目前HBase并不能很好的处理2~3以上的列族，flush和compaction 操作是针对一个Region的。当一个列族操作大量数据的时候会引发一个flush，它邻近的列族也会因关联效应被触发flush，尽管它没有操作多少数据。compaction操作是根据一个列族下的全部文件的数量触发的，而不是根据文件大小触发的。当很多的列族在flush和compaction时，会造成很多没用的IO负载。尽量在模式中只针对一个列族进行操作。将使用率相近的列归为一个列族，这样每次访问就只用访问一个列族，既能提升查询效率，也能保持尽可能少的访问不同的磁盘文件。列族的基数如果一个表存在多个列族，要注意列族之间基数（如行数）相差不要太大。例如列族A有100万行，列族B有10亿行，按照RowKey切分后，列族A可能被分散到很多很多Region（及RegionServer），这导致扫描列族A十分低效。列族名、列名长度列族名和列名越短越好，冗长的名字虽然可读性好，但是更短的名字在HBase中更好。一个具体的值由存储该值的行键、对应的列（列族:列）以及该值的时间戳决定。HBase中索引是为了加速随机访问的速度，索引的创建是基于“行键+列族:列+时间戳+值”的，如果行键和列族的大小过大，甚至超过值本身的大小，那么将会增加索引的大小。并且在HBase中数据记录往往非常之多，重复的行键、列将不但使索引的大小过大，也将加重系统的负担        总结根据HBase列族的这些属性配置，结合我们的使用场景，HBase列族可以进行如下优化：列族不宜过多，将相关性很强的key-value都放在同一个列族下，；尽量最小化行键和列族的大小；提前预估数据量，再根据Rowkey规则，提前规划好Region分区，在创建表的时候进行预分区；在业务上没有特别要求的情况下，只使用一个版本，即最大版本和最小版本一样，均为1；根据业务需求合理设置好失效时间（存储的时间越短越好）；根据查询条件，设置合理的BloomFilter配置；合理设计RowKey，可以参考《一篇文章带你快速搞懂HBase RowKey设计》。</code></pre><h2 id="布谷鸟过滤器"><a href="#布谷鸟过滤器" class="headerlink" title="布谷鸟过滤器"></a><strong>布谷鸟过滤器</strong></h2><pre><code class="python">布隆过滤器有exists方法通过对位数组的hash计算判断某元素是否在集合中，实现去重功能。但布隆过滤器有一下缺点：   ## 不支持反向删除元素：一旦对位数组进行了赋值，无法将其删除。   ## 查询性能弱：布隆过滤器使用多个hash函数计算位图多个不同位点，由于多个位点在内存中不连续，CPU寻址花销较大。   ## 空间利用率低     #** 布谷鸟过滤器 **#首先要说明布谷鸟过滤器并不是使用位图实现的, 而是一维数组. 它所存储的是数据的指纹(fingerprint).布谷鸟过滤器使用两个 hash 算法将新来的元素映射到数组的两个位置. 如果两个位置中有一个位置位空, 那么就可以将元素直接放进去. 但是如果这两个位置都满了, 它就会随机踢走一个, 然后自己霸占了这个位置.正如布谷鸟那样, 把蛋下到其它鸟的窝里. 这也是得名的由来. 但它并不是像布谷鸟那样, 管杀不管埋, 还会为这个被踢走的数据, 找一个新家.## 但布谷鸟过滤器有一个明显的弱点, 无法对同一个数据连续插入!https://www.cnblogs.com/chuxiuhong/p/8215719.html</code></pre><h2 id="Python面向对象"><a href="#Python面向对象" class="headerlink" title="Python面向对象"></a><strong>Python面向对象</strong></h2><pre><code class="python">这篇文章用很简单的例子把python类的内置方法串起来梳理，使得知识点之间具有很强关联性，便于理解。引入定义一个类并实例化class info(object):    # python3中，新定义的类默认都是object的子类，所以如果只写 class info 没有指明它的父类，那么也是可以的，父类就是object。    # 之所以有如此多内置方法可用，即使我们自己定义的类里根本看不到这些内容，是因为内置方法正是在其父类object中实现的。    # 例如，内置方法__str__在我们类定义里找不到，这时候就往其父类object中找，于是该方法就能起到object中所定义的作用。    # 如果内置方法被我们自己亲自定义了，那么显然，我们自己定义的内置方法取代了它默认的行为。    message = &#39;student&#39;    # 可以在类里存放共享的数据，它可以直接访问：类名.message，也可以用对象去访问：对象.message。    # 一般情况下如果没有重写过任何内置方法，对象.message会先寻找属性字典，再找有没有共享数据info.message    # 注意，这个数据是一个变量。在类的定义体里可以显式地修改:info.message=...    def __init__(self, name, age):      # 构造函数__init__是内置方法，以 m = info(...) 的方式实例化对象时自动执行。    # 同时还有析构函数__del__，但是程序执行结束会自动删除对象，这样__del__便自动执行了。    # 默认的__del__只是单纯删除对象，其他什么也不做。        self.name = name          # self.name可以当做一个变量名来赋值。其中self指代对象自己，&quot;.&quot;后面的名字是自己自定义的一个名字。        # 这个&quot;.&quot;后面的名字，作为key，存储在对象的内置字典__dict__中。字典的key是可hash的，当然，属性名就是一个字符串。        # 本质上，这里做的事情是self.__dict__[name]=...        self.age = age    def get_name(self):        # self.name能访问到属性的值，本质上做的事情是在self.__dict__字典中搜寻name这个key，返回它的value。        return self.namestudent = info(&#39;poincare&#39;, 23)1.反射：setattr，getattr，delattr反射是用属性名字字符串的方式来访问对象的属性或操作对象的属性的一种手段。难道直接用student.name不可以吗？可以，但是现在我有一个特殊的需求，就是让用户自己输入属性的名字，来操作属性。def operater(student=student):    attrname = input(&#39;请输入你要获取的student的属性的属性名称：&#39;)    print(student.attrname)operater()报错。解释器根本不知道这里的student.attrname是什么东西。首先，student.attrname不是一个已定义过的变量，其次，student对象并没有一个属性的名字叫&quot;attrname&quot;。解决方法是，通过对象内置的字典__dict__来找:def operater(student=student):    attrname = input(&#39;请输入你要获取的student的属性的属性名称：&#39;)    print(student.__dict__[attrname])operater()确实，因为对象的属性一般存在__dict__中，所以直接调用__dict__做查、改、删操作屡试不爽。但是这样显得太粗暴了。python有反射函数可以完成这个任务：def operater(student=student):    attrname = input(&#39;请输入你要获取的student的属性的属性名称：&#39;)    print(getattr(student, attrname))operater()同理，可以使用反射函数setattr,getattr,delattr来替代对对象__dict__的直接操控。2.内置方法：__setattr__,__getattr__,__delattr__,__getattribute__有机会重写对属性的赋值、修改、查找、删除行为。不管在类的内部还是外部，涉及对属性的赋值或修改，不管是采取student.name=&#39;bla&#39;的方式，还是采取反射setattr(student, &#39;name&#39;, &#39;bla&#39;)的方式，只要重写了__setattr__,程序将优先找到自己重写的__setattr__执行。怎么会有这种奇葩需求？当然有。最简单的例子就是实现功能：任何方式任何时刻student的name属性被改动，都要在屏幕上给出警告&quot;student.name被修改！&quot;class info(object):    message = &#39;student&#39;    def __init__(self, name, age):        self.name = name        self.age = age    def __setattr__(self, key, value):        if key == &#39;name&#39;：            print(&#39;student的name属性被修改！&#39;)        self.key = value    def get_name(self):        return self.name    看到这里，马上指出问题。因为前面的反射已经提过了，不可以直接self.key=value，这实际上是创建了一个名叫key的属性。key指代一个字符串对象。考虑其他方式:class info(object):    message = &#39;student&#39;    def __init__(self, name, age):        self.name = name        self.age = age    def __setattr__(self, key, value):        if key == &#39;name&#39;:            self.name = value  # 方式一            # setattr(self, key, value)  # 方式二            print(&#39;student的name属性被修改！&#39;)        setattr(self, key, value)    def get_name(self):        return self.name实例化对象stundent的过程中，要把name属性进行设置。此时报错。原因是递归溢出。实际上，在实例化时，首先转到__init__，执行self.name=value,因为重写了__setattr__方法，所以转到__setattr__，又准备执行self.name=value,因为重写了__setattr__方法，又要执行self.name=value，无限递归，永不停止。方式二把self.name=value改成反射setattr(self,name,value),仍然报错，因为本质上都是在修改这个属性的值，都要触发__setattr__。于是居然陷入了自相矛盾的过程！我这个方法需要实现赋值，但是赋值又会触发这个方法...解决方案，就是暴力地修改__dict__中的键了。注意！要区分修改__dict__和修改__dict__中的键值的区别。作为一个字典，字典中键值的修改并不会影响到__dict__所指向的字典“容器”本身。class info(object):    message = &#39;student&#39;    def __init__(self, name, age):        self.name = name        self.age = age    def __setattr__(self, key, value):        if key == &#39;name&#39;:            print(&#39;student的name属性被修改！&#39;)        self.__dict__(key) = value  # 直接通过修改__dict__字典的办法操纵对象的属性。__dict__本身也是对象的属性，但是这里修改的是__dict__的键，而不是__dict__，所以不会触发__setattr__。    def get_name(self):        return self.name还有一种好办法。之前曾提到过，如果自己没有实现__setattr__，解释器就会从父类中找__setattr__。因此完全可以在子类中引用父类的__setattr__来完成赋值：class info(object):    message = &#39;student&#39;    def __init__(self, name, age):        self.name = name        self.age = age    def __setattr__(self, key, value):        if key == &#39;name&#39;:            print(&#39;student的name属性被修改！&#39;)        super().__setattr__(key,value)  # 简化的写法，实际上super()内带有默认参数，与如下同理。        # super(type(self), self).__setattr__(key,value)  # 直接引用父类未经重写的__setattr__方法操纵对象的属性        # super(info, self).__setattr__(key,value)  # 也可以显式地在super()内用类自己的名称。不建议。因为显式引用失去了通用性。直接type(self)就能表示self所属的类info    def get_name(self):        return self.name再来看__getattr__，任何对属性值的获取，当属性不存在时，就可能会触发这个方法。简单的场景：如果用户想要获取的属性名不存在，不要报错，而是打印信息，并返回Noneclass info(object):    message = &#39;student&#39;    def __init__(self, name, age):        self.name = name        self.age = age    def __setattr__(self, key, value):        if key == &#39;name&#39;:            print(&#39;student的name属性被修改！&#39;)        super().__setattr__(key,value)    def __getattr__(self,item):        print(&#39;该属性&#123;&#125;不存在！&#39;.format(item))        return None    def get_name(self):        return self.name        再来看__getattribute__，任何对属性值的访问，无论属性是否存在，都要触发这个方法。实现这个方法后如果同时实现了__getattr__，在__getattribute__没报错或没有显式调用__setattr__的情况下，就不会执行__getattr__。之所以未亲自实现__getattribute__方法时，属性找不到时才会找__getattr__，就是因为解释器已经先调用__getattribute__找过一遍，报了属性未找到的错误，才转到__getattr__的。简单的场景：任何时刻用户获取name属性时要给出警告&quot;有人访问了name属性！&quot;class info(object):    message = &#39;student&#39;    def __init__(self, name, age):        self.name = name        self.age = age    def __setattr__(self, key, value):        if key == &#39;name&#39;:            print(&#39;student的name属性被修改！&#39;)        super().__setattr__(key,value)    def __getattr__(self,item):  # 显式调用或__getattribute__方法报错时才调用。在本程序中，未找到属性怎么办这个事情在__getattribute__中实现了，所以此处永远不会被触发。        print(&#39;我是__getattr__，报警：属性&#123;&#125;不存在！&#39;.format(item))        return None    def __getattribute__(self,item):  # 任何对属性值的访问，注意！包括__dict__，都会先触发这个方法。对__dict__中键值的访问，必须先访问__dict__，所以本质上还是访问了属性__dict__！        if item not in super().__getattribute__(&#39;__dict__&#39;):  # 利用父类未重写的__getattribute__来避免查找属性__dict__时发生无限递归。            print(&#39;我是__getattribute__,报警：属性&#123;&#125;不存在！&#39;.format(item))            return None        return super().__getattribute__(item)  # 利用父类未重写的__getattribute__来实现本身应该有的功能并避免无限递归：返回属性的值    def get_name(self):        return self.name介绍到这，对对象的属性的操纵的触发，已经有了“优先级”的概念。默认的查找： __dict__字典 → (找不到才)向上父类__dict__字典...对于内置方法们，如果实现了__getattribute__，那么一定先跳转到__getattribute__，按：__getattribute__  →  (报错才)__getattr__  →  (报错才)向上父类...  的顺序进行的。（注：原始的__getattribute__做的事：先从属性字典找，再从类共享数据（类也有个字典，存了数和函数）找，再看是否实现了__getattr__，有就跳转，没有就报错。）至于向上父类的顺序，遵循MRO线性表。MRO线性表的计算规则已经在本博客随笔有详细介绍和python算法实现。3.内置方法：__set__,__get__,__delete__有机会代理对属性的赋值、修改、查找、删除等行为。简单的例子：实现功能：当用户调用不同的student对象的age的时候，返回None，并报警。class des:    count = dict()    def __get__(self, instance, owner):        des.count[instance] = des.count.get(instance,0)+1        print(&#39;你这是第&#123;&#125;次在对象&#123;&#125;中找不到age属性了！停，不会再去__getattr__了&#39;.format(des.count[instance, default=0],instance))        return Noneclass info:    age = des()  # age成为了共享属性，因为构造函数没给出self.age    def __init__(self,name):  # 可以看出，构造函数根本没有实现age属性self.age        self.name = name    def __setattr__(self,key,value):        super().__setattr__(key,value)    def __getattr__(self,item):  # 实在找不到就只返回个None        return None    def __getattribute__(self,item):        return super().__getattribute__(item)  # 其实重写等于没有重写。因为完全借鉴了原始方法student1,student2=info(&#39;student1&#39;),info(&#39;student2&#39;)student1.age # 你这是第一次在student1中找不到属性age！student1.age # 你这是第二次在student1中找不到属性age！student2.age # 你这是第一次在student2中找不到属性age！以上程序可以用查找顺序很容易理解：首先，必然找__getattribute__，无论是否重写了，都要尝试找它。对于默认的__getattribute__，先在对象的属性字典找，没找到age。再在类定义中找共享属性，找到了age。发现age是一个被des类实例化过的对象，本来应该返回这个对象，但是注意内置方法__get__代理了此行为！再看__set__。简单的例子：实现功能：当用户初始化时，或修改属性值时，如果value值类型不对，就要报警。class des:    def __set__(self,instance,value):        if isinstance(value,str):            print(&#39;类型正确！&#39;)            self.__dict__[instance]=value        else:            print(&#39;类型错误！没能成功设置值！&#39;)class info:    name = des()    def __init__(self,name):        self.name=name    def __setattr__(self,key,value):        super().__setattr__(key,value)    def __getattr__(self,item):        return None    def __getattribute__(self,item):        return super().__getattribute__(item)student = info(&#39;aaa&#39;)  # 类型正确！student.name = 3  # 类型错误！没能成功设置值！以上程序的执行顺序是：首先，构造函数执行到self.name=name这步，必然找__setattr__，无论是否重写了。然后，找属性name准备修改。但是发现此时并不能在对象的属性字典中找到name。再在类的定义中找共享属性，找到了name。name是一个被des类实例化过的对象，本来应该把这个对象直接改成name，但是注意内置方法__set__代理了此行为！在__set__中，将student对象和name值，传入函数，然后把name值存到了类共享属性name对象的属性字典中，key值为student对象。student对象确实可以当做key值，因为它是可hash的。为什么这么做？因为下次实例另一个学生：student_2 = info(&#39;aaa&#39;)的时候，这个类共享属性name对象的属性字典中key值就是student_2对象。因此虽然name是它们共有的属性，但是name这个对象的属性字典里，已经把每个student全部区分开了，不同的student，对应不同的name值！接下来，要访问student的name值：print(student.name)显然不行！这样打印出来的，其实是info类里的共享属性：对象name。联想到上面实现__set__时，是将name值存在了对象name的字典里。因此，正确的访问方式是：print(student.name.__dict__[student])这也太麻烦了！这时，联想到__get__方法：__get__方法有机会代理__attribute__属性查找中找到类共享属性后的部分行为。所以，应当把__get__和__set__一起用，放在des类的定义里面：class des:    def __get__(self,instance,owner):        return self.__dict__[instance]    def __set__(self,instance,value):        if isinstance(value,str):            print(&#39;类型正确！&#39;)            self.__dict__[instance]=value        else:            print(&#39;类型错误！没能成功设置值！&#39;)class info:    name = des()    def __init__(self,name):        self.name=name    def __setattr__(self,key,value):        super().__setattr__(key,value)    def __getattr__(self,item):        return None    def __getattribute__(self,item):        return super().__getattribute__(item)student = info(&#39;aaa&#39;)  # 类型正确！student.name = 3  # 类型错误！没能成功设置值！print(student.name)  # 打印出了aaa这就是很简单的一种理解描述符作用机制的方式。所谓描述符，就是实现了__get__或__set__或__delete__的类。描述符一定要在另一个类的共享属性中实例化，这样就可以作用于另一个类的对象的属性操作。只实现__get__的，是非数据描述符，当属性字典查无时，会跳转到该非数据描述符的__get__中。实现__set__或__delete__的，是数据描述符，但是一般至少同时实现__set__和__get__。这样做出的描述符，对值的修改或查询，都会触发。注意，如果存在删除属性的动作，记得实现__delete__，否则它会找不到属性删。因为属性并非在__dict__里。数据描述符是把属性值存到另一个类中，而不是属性字典中，所以，直接使用__dict__来查看对象的属性和属性值会出问题，因为属性字典的查找先于__get__。对吗？实际上，对于数据描述符，一但实现，属性查找的优先级就被悄悄的提升了。原来__get__是在字典里找不到才跳转到它，现在是先于字典查找就要跳转到它了！优先级：类属性(这里应指直接以类名.属性名来操纵的属性，例如info.des=None就直接把描述符删了。) &gt; 数据描述符 &gt; 实例属性 &gt; 非数据描述符 &gt; __getattr__使用描述符的优势：可以观察到，描述符定义清晰，可重用，“只做一件事”，甚至还有机会把构造函数引入描述符中，从而实现更深层次定制。所以，描述符已经被大量使用在很多场景。Learning about descriptors not only provides access to a larger toolset, it creates a deeper understanding of how Python works and an appreciation for the elegance of its design.使用描述符的劣势：直接调用对象的__dict__，发现并不存在name属性，与常见范式不是很统一。当然，可以利用描述符内定制性极强这一优势解决：在__set__中增加一行代码：instance.__dict__[&#39;name&#39;] = value  # 保持使用描述符后，__dict__中属性的统一性。当然，__get__并不会去查找它。杀器祭出：定制化类型限制：class Type:    def __init__(self, attrname, typename):        self.attrname = attrname        self.typename = typename    def __set__(self, instance, value):        if not isinstance(value, self.typename):            raise TypeError(                &#39;&#123;&#125;必须为&#123;&#125;类型，而接收到的&#123;&#125;却是&#123;&#125;类型。&#39;.format(                    self.attrname,                    self.typename,                    value,                    type(value)))        else:            instance.__dict__[self.attrname] = value            print(&#39;赋值属性&#123;&#125;值为&#123;&#125;&#39;.format(self.attrname, value))    def __get__(self, instance, cls):        if instance is None:            return self        else:            return instance.__dict__[self.attrname]    def __delete__(self, instance):        del instance.__dict__[self.attrname]def typeassert(**kwargs):    def decorator(cls):        def wrapper(*kw_):            for attrname, typename in kwargs.items():                setattr(cls, attrname, Type(attrname, typename))            return cls(*kw_)        return wrapper    return decorator@typeassert(name=str, age=int, salary=float)class Info:    def __init__(self, name, age, salary):        self.name = name        self.age = age        self.salary = salary    def __str__(self):        return &#39;name:&#123;&#125;,age:&#123;&#125;,salary:&#123;&#125;&#39;.format(            self.name, self.age, self.salary)dai = Info(&#39;poincare&#39;, 23, 1000.0)描述符实现绑定到类的方法：class Classmethod:  # 类装饰器写法    def __init__(self, funcname):        self.funcname = funcname    def __get__(self, instance, cls):        def wrappers(*kw, **kwargs):            k = self.funcname(cls, *kw, **kwargs)            return k        return wrappersclass Info:    info = [1, 2, 3]    def __init__(self, name):        self.name = name    @Classmethod    def sayinfo(self, alladd):        self.info = [k + alladd for k in self.info]        return self.infop = Info(&#39;poin&#39;)print(p.sayinfo(1))print(Info.sayinfo(1))描述符实现静态方法：class Staticmethod:    def __init__(self, funcname):        self.funcname = funcname    def __get__(self, instance, cls):        def wrappers(*kw, **kwargs):            k = self.funcname(*kw, **kwargs)            return k        return wrappersclass Info:    info = [1, 2, 3]    def __init__(self, name):        self.name = name    @Staticmethod    def add(x, y):        print(x + y)        return x + yp = Info(&#39;dai&#39;)  # 实例化# p.add(1)  # 报错，缺少一个位置参数p.add(1, 2)  # 可以返回结果# Info.add(1)  # 报错，缺少一个位置参数Info.add(1, 2)  # 可以返回结果   实际上，实例.add 和 类.add 是同一函数，有完全一样的地址。print(p.add)print(Info.add)</code></pre><h2 id="python配置文件的封装"><a href="#python配置文件的封装" class="headerlink" title="python配置文件的封装"></a><strong>python配置文件的封装</strong></h2><pre><code class="python"># config文件封装# 对配置文件进行封装# 导入配置文件模块from configparser import ConfigParser# 创建一个配置文件类class HandleConfig:    &quot;&quot;&quot;    处理配置文件    &quot;&quot;&quot;    # 定义一个实例属性    def __init__(self,filename):        # 定义一个名称：filename实例属性        self.filename = filename        # 创建配置解释器config对象        self.config = ConfigParser()        # 指定读取的配置文件, 无需变量接收读取内容        self.config.read(self.filename,encoding=&quot;utf8&quot;)    # 定义一个获取配置文件数据的实例方法，获取配置文件中对应不同数据类型的值    # get方法和字典中的get有区别    # 第一个参数section为区域名，第二个参数option为选项名    # 从配置文件中，使用索引(方括号)或者使用get方法，读取出来的所有数据都是字符串类型    def get_value(self,section,option):        # 通过定义的实例属性self.config对象，调用ConfigParser类中的get方法，并将结果进行返回        return self.config.get(section,option)    # 可以使用getint(区域名，选项名)只能读取int类型的数据，否则会报错    def get_int(self,section,option):        return self.config.getint(section,option)    # 可以使用getfloat(区域名，选项名)只能读取int类型和浮点类型的数据，否则会报错    def get_float(self,section,option):        return self.config.getfloat(section,option)    # 可以使用getboolean(区域名，选项名)来读取布尔类似的数据    def get_boolean(self,section,option):        return self.config.getboolean(section,option)    # 通过eval函数对get获取的字符串进行转义，获取配置文件中不同的数据类型    def get_eval_data(self,section,option):        return eval(self.config.get(section, option))    # 定义写入配置文件的静态方法：    # 为什么不能定义在内属性，内属性是每个对象的公共资源，每个对象都可以访问    # 读配置和写配置都用相同的对象，读配置和写配置尽量不要用相同的对象    @staticmethod    # 定义write_config方法    # datas里面传入需要写入的数据，往往是一个嵌套字典的格式    # filename是文件名    def write_config(datas,filename):        # 创建一个config对象，可以用来调用ConfigParser类中的方法        config = ConfigParser()        # 在空配置config中写入配置        # config还没有读取数据时，可以类似一个空字典        # datas是需要写入的数据        for key in datas:            config[key] = datas[key]        with open(filename,&quot;w&quot;,encoding=&quot;utf8&quot;) as file:            config.write(file)# 定义do_config对象，用来调用HandleConfig类中的方法# testcase.conf为对应的文件名称do_config = HandleConfig(&quot;testcase.conf&quot;)# 魔法变量if __name__ == &quot;__main__&quot;:    # 创建do_config类的对象    do_config = HandleConfig(&quot;testcase.conf&quot;)    # 需要写入到配置文件的数据    datas = &#123;        &quot;excel&quot;: &#123;            &quot;cases_path&quot;: &quot;cases.xlsx&quot;        &#125;,        &quot;user&quot;: &#123;            &quot;username&quot;: &quot;hc&quot;,            &quot;password&quot;: &quot;123456&quot;        &#125;    &#125;    # 通过do_config对象调用write_config静态方法，将datas数据写入到write_cases.ini文件中    do_config.write_config(datas,&quot;write_cases.ini&quot;)    pass# 读取excel文件的封装import openpyxlclass CaseData:    &quot;&quot;&quot;测试用例数据类，专门用来创建对象，存放用例数据&quot;&quot;&quot;    passclass ReadExcle(object):    def __init__(self, filename, sheetname):        self.filename = filename        self.sheetname = sheetname    def open(self):        &quot;&quot;&quot;打开工作表和表单&quot;&quot;&quot;        self.wb = openpyxl.load_workbook(self.filename)        self.sh = self.wb[self.sheetname]    def read_data(self):        &quot;&quot;&quot;读取数据的方法&quot;&quot;&quot;        # 打开工作簿和表单        self.open()        # 将表单中的内容，按行获取所有的格子        rows = list(self.sh.rows)        # 创建一个空列表，用例存放所有的用例数据        cases = []        # 获取表头，放到一个列表中        title = [c.value for c in rows[0]]        # 获取除表头以外的其他行中的数据        for r in rows[1:]:            # 每遍历一行，创建一个列表，用例存放该行的数据            data = [c.value for c in r]            # 将表头和该行的数据进行聚合打包，转换字典            case_data = dict(zip(title, data))            # 将该行的用例数据加入到cases这个列表中            cases.append(case_data)        # 关闭工作簿对象        self.wb.close()        # 将读取好的数据返回出去        return cases    def read_data_obj(self):        &quot;&quot;&quot;读取数据的方法,数据返回的是列表嵌套对象的形式&quot;&quot;&quot;        # 打开工作簿和表单        self.open()        # 将表单中的内容，按行获取所有的格子        rows = list(self.sh.rows)        # 创建一个空列表，用例存放所有的用例数据        cases = []        # 通过列表推导式获取表头，放到一个列表中        title = [c.value for c in rows[0]]        # 获取除表头以外的其他行中的数据        for r in rows[1:]:            # 通过列表推导式，获取改行的数据，放到一个列表中            data = [c.value for c in r]            # 创建一个用例数据对象            case = CaseData()            # 将表头和该行的数据进行聚合打包，然后进行遍历            for i in zip(title, data):                # 通过反射机制，将表头设为对象属性，对应值设为对象的属性值                setattr(case, i[0], i[1])            # 将该行的用例数据加入到cases这个列表中            cases.append(case)        # 关闭工作薄        self.wb.close()        # 将读取好的数据返回出去        return cases    def write_data(self, row, column, value):        &quot;&quot;&quot;写入数据&quot;&quot;&quot;        # 打开工作簿和表单        self.open()        # 写入内容        self.sh.cell(row=row, column=column, value=value)        # 保存文件        self.wb.save(self.filename)        # 关闭工作簿        self.wb.close()if __name__ == &#39;__main__&#39;:    read = ReadExcle(&#39;cases.xlsx&#39;, &#39;register&#39;)    # 读取    # data = read.read_data_obj()    # print(data)    # read.write_data(2, 4, &#39;通过&#39;)    # read.write_data(3, 4, &#39;未通过&#39;)    # 获取日志封装import loggingfrom Day17_2020_03_11.Python_1102_handle_config.handle_yaml import do_yamlclass MyLogger(object):    @classmethod    def create_logger(cls):        &quot;&quot;&quot;创建日志收集器&quot;&quot;&quot;        # 创建一个日志收集器        my_log = logging.getLogger(do_yaml.read(&quot;log&quot;, &quot;log_name&quot;))        # 设置日志收集器的收集等级        my_log.setLevel(do_yaml.read(&quot;log&quot;, &quot;logger_level&quot;))        # 设置日志输出的格式        formater = logging.Formatter(do_yaml.read(&quot;log&quot;, &quot;formatter&quot;))        # 创建一个输出导控制台的日志输出渠道        sh = logging.StreamHandler()        sh.setLevel(do_yaml.read(&quot;log&quot;, &quot;stream_level&quot;))        # 设置输出导控制台的格式        sh.setFormatter(formater)        # 将输出渠道添加到日志收集器中        my_log.addHandler(sh)        # 创建一个输出导文件的渠道        fh = logging.FileHandler(filename=do_yaml.read(&quot;log&quot;, &quot;logfile_name&quot;),                                 encoding=&#39;utf8&#39;)        fh.setLevel(do_yaml.read(&quot;log&quot;, &quot;logfile_level&quot;))        # 设置输出导文件的日志格式        fh.setFormatter(formater)        # 将输出渠道添加到日志收集器中        my_log.addHandler(fh)        return my_logdo_log = MyLogger.create_logger()# yaml读取配置文件信息封装import yamlclass HandleYaml:    def __init__(self, filename):        with open(filename, encoding=&quot;utf-8&quot;) as one_file:            self.datas = yaml.full_load(one_file)    def read(self, section, option):        &quot;&quot;&quot;        读数据        :param section: 区域名        :param option: 选项名        :return:        &quot;&quot;&quot;        return self.datas[section][option]    @staticmethod    def write(datas, filename):        &quot;&quot;&quot;        写数据        :param datas: 嵌套字典的字典        :param filename: yaml文件路径        :return:        &quot;&quot;&quot;        with open(filename, mode=&quot;w&quot;, encoding=&quot;utf-8&quot;) as one_file:            yaml.dump(datas, one_file, allow_unicode=True)do_yaml = HandleYaml(&quot;testcase01.yaml&quot;)if __name__ == &#39;__main__&#39;:    do_yaml = HandleYaml(&quot;testcase01.yaml&quot;)    datas = &#123;        &quot;excel&quot;: &#123;            &quot;cases_path&quot;: &quot;cases.xlsx&quot;,            &quot;result_col&quot;: 5        &#125;,        &quot;msg&quot;: &#123;            &quot;success_result&quot;: &quot;通过&quot;,            &quot;fail_result&quot;: &quot;Fail&quot;        &#125;    &#125;    do_yaml.write(datas, &quot;write_datas.yaml&quot;)    pass# 测试用例类：test_casesimport unittestfrom Day17_2020_03_11.Python_1102_handle_config.register import registerfrom Day17_2020_03_11.Python_1102_handle_config.handle_excel import ReadExclefrom Day17_2020_03_11.Python_1102_handle_config.ddt import ddt, datafrom Day17_2020_03_11.Python_1102_handle_config.handle_yaml import do_yamlfrom Day17_2020_03_11.Python_1102_handle_config.handle_log import do_log@ddtclass RegisterTestCase(unittest.TestCase):    # excle = ReadExcle(&quot;cases.xlsx&quot;, &#39;register&#39;)    excle = ReadExcle(do_yaml.read(&quot;excel&quot;, &quot;cases_path&quot;), &#39;register&#39;)    cases = excle.read_data_obj()    @data(*cases)    def test_register(self, case):        # 第一步  准备用例数据        # 获取用例的行号        row = case.case_id + 1        # 获取预期结果        excepted = eval(case.excepted)        # 获取用例入参        data = eval(case.data)        # 第二步： 调用功能函数，获取实际结果        res = register(*data)        # 第三步：比对预期结果和实际结果        try:            self.assertEqual(excepted, res)        except AssertionError as e:            self.excle.write_data(row=row,                                  column=do_yaml.read(&quot;excel&quot;, &quot;result_col&quot;),                                  value=do_yaml.read(&quot;msg&quot;, &quot;fail_result&quot;))            # do_log.error(&quot;断言异常: &#123;&#125;&quot;.format(e))            do_log.error(f&quot;断言异常: &#123;e&#125;&quot;)            raise e        else:            self.excle.write_data(row=row,                                  column=do_yaml.read(&quot;excel&quot;, &quot;result_col&quot;),                                  value=do_yaml.read(&quot;msg&quot;, &quot;success_result&quot;))            # 测试运行程序：run_testimport unittest# 导入模块的快捷键：alt + enterfrom datetime import datetimefrom Day17_2020_03_11.Python_1102_handle_config import test_casesfrom Day17_2020_03_11.Python_1102_handle_config.HTMLTestRunnerNew import HTMLTestRunnerfrom Day17_2020_03_11.Python_1102_handle_config.handle_yaml import do_yaml# 创建测试套件suite = unittest.TestSuite()# 加载用例用例到套件loader = unittest.TestLoader()suite.addTest(loader.loadTestsFromModule(test_cases))result_full_path = do_yaml.read(&#39;report&#39;, &#39;name&#39;) + &#39;_&#39; +  \                   datetime.strftime(datetime.now(), &#39;%Y%m%d%H%M%S&#39;) + &#39;.html&#39;with open(result_full_path, &#39;wb&#39;) as fb:    # 创建测试运行程序    runner = HTMLTestRunner(stream=fb,                            title=do_yaml.read(&#39;report&#39;, &#39;title&#39;),                            description=do_yaml.read(&#39;report&#39;, &#39;description&#39;),                            tester=do_yaml.read(&#39;report&#39;, &#39;tester&#39;))    # 执行测试套件中的用例    runner.run(suite)</code></pre><h2 id="pyspark操作hive"><a href="#pyspark操作hive" class="headerlink" title="pyspark操作hive"></a><strong>pyspark操作hive</strong></h2><pre><code class="python">import syssys.path.append(&quot;./xxx.zip&quot;)import os ssos.environ.get(&quot;SPARK_HOME&quot;)os.environ.get(&quot;JAVA_HOME&quot;)from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(&quot;test_super&quot;).enableHiveSupport().getOrCreate()</code></pre><h2 id="Hbase-RowKey设计的二三事"><a href="#Hbase-RowKey设计的二三事" class="headerlink" title="Hbase RowKey设计的二三事"></a><strong>Hbase RowKey设计的二三事</strong></h2><pre><code class="scala"></code></pre><h2 id="浅谈hbase二级索引"><a href="#浅谈hbase二级索引" class="headerlink" title="浅谈hbase二级索引"></a><strong>浅谈hbase二级索引</strong></h2><pre><code class="scala"></code></pre><h2 id="kafka-api-java版"><a href="#kafka-api-java版" class="headerlink" title="kafka api java版"></a><strong>kafka api java版</strong></h2><pre><code></code></pre><h2 id="kafka-api-scala版"><a href="#kafka-api-scala版" class="headerlink" title="kafka api scala版"></a><strong>kafka api scala版</strong></h2><pre><code class="scala">package com.linys.scala.KAFKA_producerimport java.util.Propertiesimport org.apache.kafka.clients.producer.&#123;KafkaProducer, ProducerRecord, RecordMetadata&#125;/**  * 实现producer  */object KafkaProducerDemo &#123;  def main(args: Array[String]): Unit = &#123;    val prop = new Properties    object ProduceDemo &#123;  def main(args: Array[String]): Unit = &#123;    // 指定topic    val topic = &quot;testTopic&quot;    val props = new Properties()    // 必配的参数    props.setProperty(&quot;bootstrap.servers&quot;, &quot;hdp-01:9092,hdp-02:9092,hdp-03:9092&quot;)    props.setProperty(&quot;key.serializer&quot;, classOf[StringSerializer].getName)    props.setProperty(&quot;value.serializer&quot;, classOf[StringSerializer].getName)    // 其他配置    props.setProperty(&quot;acks&quot;, &quot;all&quot;)    props.setProperty(&quot;retries&quot;,0)    props.setProperty(&quot;batch.size&quot;,10)     // 参数配置的第二种方式。使用 props.put(k,v)方法。       // 构造生产者的API    val producer: KafkaProducer[String, String] = new KafkaProducer[String, String](props)    for (i &lt;- 1 to 100000) &#123;      Thread.sleep(500)      // 利用ProducerRecord 来封装消息      //    val record = new ProducerRecord[String, String](topic, &quot;123&quot;) // topic  value      //    new ProducerRecord[String,String](&quot;&quot;,&quot;&quot;,&quot;&quot;) ///topic  key   value      val part = i % 3 // 取分区编号      val record = new ProducerRecord[String, String](topic, part, &quot;&quot;, &quot;i&quot; + i)      producer.send(record)    &#125;    producer.close()  &#125;&#125;    import java.utilimport java.util.Propertiesimport org.apache.kafka.clients.consumer.&#123;ConsumerRecords, KafkaConsumer&#125;import org.apache.kafka.common.serialization.&#123;StringDeserializer&#125;/**  * Created by Huige   * Email: 824203453@qq.com   * DATE: 2019/3/19  * Desc:   */object ConsumerDemo &#123;  def main(args: Array[String]): Unit = &#123;    val topic = &quot;testTopic&quot;    val props = new Properties()    // 必配的参数    props.setProperty(&quot;bootstrap.servers&quot;, &quot;hdp-01:9092,hdp-02:9092,hdp-03:9092&quot;)    // key 和value的反序列化    props.setProperty(&quot;key.deserializer&quot;, classOf[StringDeserializer].getName)    props.setProperty(&quot;value.deserializer&quot;, classOf[StringDeserializer].getName)    props.setProperty(&quot;group.id&quot;,&quot;1232&quot;)    // 其他配置    // [latest, earliest, none]    props.setProperty(&quot;auto.offset.reset&quot;,&quot;earliest&quot;)    // 是否自动提交偏移量管理  默认是true    //    props.setProperty(&quot;enable.auto.commit&quot;,&quot;false&quot;)    //    props.setProperty(&quot;auto.commit.interval.ms&quot;,&quot;5000&quot;) // 自定提交时间  默认5000ms    val consumer: KafkaConsumer[String, String] = new KafkaConsumer[String,String](props)    val lst = new util.ArrayList[String]()    lst.add(&quot;testTopic&quot;)    // 订阅主题    consumer.subscribe(lst)     //    consumer.subscribe(util.Arrays.asList(&quot;&quot;))    while(true)&#123;      // 如果Kafak中没有消息，会隔timeout这个值读一次。比如上面代码设置了2秒，也是就2秒后会查一次。      // 如果Kafka中还有消息没有消费的话，会马上去读，而不需要等待。      val records: ConsumerRecords[String, String] = consumer.poll(2000)      import scala.collection.JavaConversions._      for(i &lt;- records)&#123;        // ConsumerRecord        // ConsumerRecord(topic = testTopic, partition = 0, offset = 660, CreateTime = 1552984524657, checksum = 3225686705, serialized key size = 0, serialized value size = 4, key = , value = i966)        println(i)       //  println(i.partition())      &#125;    &#125;  &#125;&#125;</code></pre><h2 id="pulsar基础命令"><a href="#pulsar基础命令" class="headerlink" title="pulsar基础命令*"></a><em>pulsar基础命令</em>*</h2><pre><code class="powershell">### stanalone 启动bin/pulsar standalone ： 当前terminal运行，terminal关闭，服务关闭pulsar-daemon start/stop standalone ： 后台运行的standalone服务模式###  client生产bin/pulsar-client produce my-topic --messages &quot;hello-pulsar&quot;向my-topic这个topic生产数据，内容为“hello-pulsar”，如果topic不存在，pulsar会自动创建消费bin/pulsar-client consume my-topic -s &quot;first-subscription&quot;消费my-topic的数据，订阅名称为“first-subscription&quot;, 如果topic不存在，pulsar会自动创建### tenants查看所有tenants                /pulsar-admin tenants list创建tenants                    pulsar-admin tenants create my-tenant删除tenants                    pulsar-admin tenants delete my-tenant### broker查看存活的broker信息            pulsar-admin brokers list use查看broke如上的namesapce        pulsar-admin brokers namespaces use --url broker1.use.org.com:8080查看可以动态更新的配置           pulsar-admin brokers list-dynamic-config查看已经动态更新过的配置         pulsar-admin brokers get-all-dynamic-config动态更新配置                    pulsar-admin brokers update-dynamic-config brokerShutdownTimeoutMs 100### namespace查看tenant下的所有namespace     pulsar-admin namespaces list test-tenant创建namespace                  pulsar-admin namespaces create test-tenant/test-namespace查看namespace策略               pulsar-admin namespaces policies test-tenant/test-namespace删除namespace                  pulsar-admin namespaces delete test-tenant/ns1### permissionpulsar的权限控制是在namespace级别的，授权pulsar-admin namespaces grant-permission test-tenant/ns1 \--actions produce,consume \--role admin10注意： 当broker.conf中的authorizationAllowWildcardsMatching 为true时，支持通配符匹配，例如，pulsar-admin namespaces grant-permission test-tenant/ns1 \--actions produce,consume \--role &#39;my.role.*&#39;获取授权信息pulsar-admin namespaces permissions test-tenant/ns1撤销授权pulsar-admin namespaces revoke-permission test-tenant/ns1 \--role admin10### persistent topics格式： persistent://tenant/namespace/topic查看namespace下的topic信息    pulsar-admin persistent list my-tenant/my-namespace列举persistent topic          pulsar-admin topics list tenant/namespace给客户端添加针对于某个topic的role（许可）pulsar-admin persistent grant-permission--actions produce,consume --role application1persistent://test-tenant/ns1/topic1获取许可信息pulsar-admin persistent permissions \persistent://test-tenant/ns1/tp1回滚许可pulsar-admin persistent revoke-permission \--role application1 \persistent://test-tenant/ns1/tp1 \删除topicpulsar-admin persistent delete \persistent://test-tenant/ns1/tp1 \下线topicpulsar-admin persistent unload \persistent://test-tenant/ns1/tp1查看topic相关的统计信息pulsar-admin persistent stats \persistent://test-tenant/ns1/tp1查看topic内部统计信息pulsar-admin persistent stats-internal \persistent://test-tenant/ns1/tp1peek 消息pulsar-admin persistent peek-messages \--count 10 --subscription my-subscription \persistent://test-tenant/ns1/tp1跳过消费部分消息pulsar-admin persistent skip \--count 10 --subscription my-subscription \persistent://test-tenant/ns1/tp1跳过所有数据pulsar-admin persistent skip-all \--subscription my-subscription \persistent://test-tenant/ns1/tp1 \重置消费cursor到几分钟之前pulsar-admin persistent reset-cursor \--subscription my-subscription --time 10 \persistent://test-tenant/ns1/tp1 \查找topic所在的broker信息pulsar-admin persistent lookup \persistent://test-tenant/ns1/tp1 \获取topic的bundle信息pulsar-admin persistent bundle-range \persistent://test-tenant/ns1/tp1 \&quot;0x00000000_0xffffffff&quot;查询topic的订阅信息pulsar-admin persistent subscriptions \persistent://test-tenant/ns1/tp1 \取消订阅pulsar-admin persistent unsubscribe \--subscription my-subscription \persistent://test-tenant/ns1/tp1 \最后一条消息的MessageIDpulsar-admin topics last-message-id topic-namenon-persistent topics格式 ： non-persistent://tenant/namespace/topic获取统计信息pulsar-admin non-persistent stats \non-persistent://test-tenant/ns1/tp1 \获取内存统计信息pulsar-admin non-persistent stats-internal \non-persistent://test-tenant/ns1/tp1 \创建分区topicbin/pulsar-admin non-persistent create-partitioned-topic \non-persistent://my-tenant/my-namespace/my-topic \--partitions 4注意：需要指明topic名称和分区数量分区topic的元数据信息pulsar-admin non-persistent get-partitioned-topic-metadata \non-persistent://my-tenant/my-namespace/my-topic下线topicpulsar-admin non-persistent unload \non-persistent://test-tenant/ns1/tp1分区topic格式： persistent://tenant/namespace/topic创建topicbin/pulsar-admin topics create-partitioned-topic \persistent://my-tenant/my-namespace/my-topic \--partitions 4创建非分区topic$ bin/pulsar-admin topics create persistent://my-tenant/my-namespace/my-topic获取分区topic的元数据信息pulsar-admin topics get-partitioned-topic-metadata \persistent://my-tenant/my-namespace/my-topic更新topic信息pulsar-admin topics update-partitioned-topic \persistent://my-tenant/my-namespace/my-topic \--partitions 8注意：修改分区数量时，只能比原来的分区数大删除topicbin/pulsar-admin topics delete-partitioned-topic \persistent://my-tenant/my-namespace/my-topic获取统计信息pulsar-admin topics partitioned-stats \persistent://test-tenant/namespace/topic \--per-partition获取内部统计信息pulsar-admin topics stats-internal \persistent://test-tenant/namespace/topic### Schema上传schemapulsar-admin schemas upload &lt;topic-name&gt; --filename /path/to/schema-definition-file获取schemapulsar-admin schemas get &lt;topic-name&gt;删除schemapulsar-admin schemas delete &lt;topic-name&gt;</code></pre><p><a href="https://www.bilibili.com/video/BV1UW411i7Qa?from=search&amp;seid=8467458502881112354">https://www.bilibili.com/video/BV1UW411i7Qa?from=search&amp;seid=8467458502881112354</a></p><h2 id="pulsar-api"><a href="#pulsar-api" class="headerlink" title="pulsar api"></a><strong>pulsar api</strong></h2><pre><code></code></pre><h2 id="Spark自定义外部数据源"><a href="#Spark自定义外部数据源" class="headerlink" title="Spark自定义外部数据源"></a><strong>Spark自定义外部数据源</strong></h2><pre><code class="scala">import org.apache.spark.rdd.RDDimport org.apache.spark.sql.&#123;Row, SQLContext&#125;import org.apache.spark.sql.sources._import org.apache.spark.sql.types._/**  * Created by rana on 29/9/16.  */class CustomDatasourceRelation(override val sqlContext : SQLContext, path : String, userSchema : StructType)  extends BaseRelation with TableScan with PrunedScan with PrunedFilteredScan with Serializable &#123;  override def schema: StructType = &#123;    if (userSchema != null) &#123;      userSchema    &#125; else &#123;      StructType(        StructField(&quot;id&quot;, IntegerType, false) ::        StructField(&quot;name&quot;, StringType, true) ::        StructField(&quot;gender&quot;, StringType, true) ::        StructField(&quot;salary&quot;, LongType, true) ::        StructField(&quot;expenses&quot;, LongType, true) :: Nil      )    &#125;  &#125;  override def buildScan(): RDD[Row] = &#123;    println(&quot;TableScan: buildScan called...&quot;)    val schemaFields = schema.fields    // Reading the file&#39;s content    val rdd = sqlContext.sparkContext.wholeTextFiles(path).map(f =&gt; f._2)    val rows = rdd.map(fileContent =&gt; &#123;      val lines = fileContent.split(&quot;\n&quot;)      val data = lines.map(line =&gt; line.split(&quot;,&quot;).map(word =&gt; word.trim).toSeq)      val tmp = data.map(words =&gt; words.zipWithIndex.map&#123;        case (value, index) =&gt;          val colName = schemaFields(index).name          Util.castTo(if (colName.equalsIgnoreCase(&quot;gender&quot;)) &#123;if(value.toInt == 1) &quot;Male&quot; else &quot;Female&quot;&#125; else value,            schemaFields(index).dataType)      &#125;)      tmp.map(s =&gt; Row.fromSeq(s))    &#125;)    rows.flatMap(e =&gt; e)  &#125;  override def buildScan(requiredColumns: Array[String]): RDD[Row] = &#123;    println(&quot;PrunedScan: buildScan called...&quot;)    val schemaFields = schema.fields    // Reading the file&#39;s content    val rdd = sqlContext.sparkContext.wholeTextFiles(path).map(f =&gt; f._2)    val rows = rdd.map(fileContent =&gt; &#123;      val lines = fileContent.split(&quot;\n&quot;)      val data = lines.map(line =&gt; line.split(&quot;,&quot;).map(word =&gt; word.trim).toSeq)      val tmp = data.map(words =&gt; words.zipWithIndex.map&#123;        case (value, index) =&gt;          val colName = schemaFields(index).name          val castedValue = Util.castTo(if (colName.equalsIgnoreCase(&quot;gender&quot;)) &#123;if(value.toInt == 1) &quot;Male&quot; else &quot;Female&quot;&#125; else value,                                        schemaFields(index).dataType)          if (requiredColumns.contains(colName)) Some(castedValue) else None      &#125;)      tmp.map(s =&gt; Row.fromSeq(s.filter(_.isDefined).map(value =&gt; value.get)))    &#125;)    rows.flatMap(e =&gt; e)  &#125;  override def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row] = &#123;    println(&quot;PrunedFilterScan: buildScan called...&quot;)    println(&quot;Filters: &quot;)    filters.foreach(f =&gt; println(f.toString))    var customFilters: Map[String, List[CustomFilter]] = Map[String, List[CustomFilter]]()    filters.foreach( f =&gt; f match &#123;      case EqualTo(attr, value) =&gt;        println(&quot;EqualTo filter is used!!&quot; + &quot;Attribute: &quot; + attr + &quot; Value: &quot; + value)        /**          * as we are implementing only one filter for now, you can think that this below line doesn&#39;t mak emuch sense          * because any attribute can be equal to one value at a time. so what&#39;s the purpose of storing the same filter          * again if there are.          * but it will be useful when we have more than one filter on the same attribute. Take the below condition          * for example:          * attr &gt; 5 &amp;&amp; attr &lt; 10          * so for such cases, it&#39;s better to keep a list.          * you can add some more filters in this code and try them. Here, we are implementing only equalTo filter          * for understanding of this concept.          */        customFilters = customFilters ++ Map(attr -&gt; &#123;          customFilters.getOrElse(attr, List[CustomFilter]()) :+ new CustomFilter(attr, value, &quot;equalTo&quot;)        &#125;)      case _ =&gt; println(&quot;filter: &quot; + f.toString + &quot; is not implemented by us!!&quot;)    &#125;)    val schemaFields = schema.fields    // Reading the file&#39;s content    val rdd = sqlContext.sparkContext.wholeTextFiles(path).map(f =&gt; f._2)    val rows = rdd.map(file =&gt; &#123;      val lines = file.split(&quot;\n&quot;)      val data = lines.map(line =&gt; line.split(&quot;,&quot;).map(word =&gt; word.trim).toSeq)      val filteredData = data.map(s =&gt; if (customFilters.nonEmpty) &#123;        var includeInResultSet = true        s.zipWithIndex.foreach &#123;          case (value, index) =&gt;            val attr = schemaFields(index).name            val filtersList = customFilters.getOrElse(attr, List())            if (filtersList.nonEmpty) &#123;              if (CustomFilter.applyFilters(filtersList, value, schema)) &#123;              &#125; else &#123;                includeInResultSet = false              &#125;            &#125;        &#125;        if (includeInResultSet) s else Seq()      &#125; else s)      val tmp = filteredData.filter(_.nonEmpty).map(s =&gt; s.zipWithIndex.map &#123;        case (value, index) =&gt;          val colName = schemaFields(index).name          val castedValue = Util.castTo(if (colName.equalsIgnoreCase(&quot;gender&quot;)) &#123;            if (value.toInt == 1) &quot;Male&quot; else &quot;Female&quot;          &#125; else value,            schemaFields(index).dataType)          if (requiredColumns.contains(colName)) Some(castedValue) else None      &#125;)      tmp.map(s =&gt; Row.fromSeq(s.filter(_.isDefined).map(value =&gt; value.get)))    &#125;)    rows.flatMap(e =&gt; e)  &#125;&#125;</code></pre><pre><code class="scala">import org.apache.spark.sql.types.StructTypecase class CustomFilter(attr : String, value : Any, filter : String)object CustomFilter &#123;  def applyFilters(filters : List[CustomFilter], value : String, schema : StructType): Boolean = &#123;    var includeInResultSet = true    val schemaFields = schema.fields    val index = schema.fieldIndex(filters.head.attr)    val dataType = schemaFields(index).dataType    val castedValue = Util.castTo(value, dataType)    filters.foreach(f =&gt; &#123;      val givenValue = Util.castTo(f.value.toString, dataType)      f.filter match &#123;        case &quot;equalTo&quot; =&gt; &#123;          includeInResultSet = castedValue == givenValue          println(&quot;custom equalTo filter is used!!&quot;)        &#125;        case _ =&gt; throw new UnsupportedOperationException(&quot;this filter is not supported!!&quot;)      &#125;    &#125;)    includeInResultSet  &#125;&#125;</code></pre><pre><code class="scala">import org.apache.hadoop.fs.Pathimport org.apache.spark.sql.&#123;DataFrame, SQLContext, SaveMode&#125;import org.apache.spark.sql.sources.&#123;BaseRelation, CreatableRelationProvider, RelationProvider, SchemaRelationProvider&#125;import org.apache.spark.sql.types.StructType/**  * Created by rana on 29/9/16.  */class DefaultSource extends RelationProvider with SchemaRelationProvider with CreatableRelationProvider &#123;  override def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation = &#123;    createRelation(sqlContext, parameters, null)  &#125;  override def createRelation(sqlContext: SQLContext, parameters: Map[String, String], schema: StructType): BaseRelation = &#123;    val path = parameters.get(&quot;path&quot;)    path match &#123;      case Some(p) =&gt; new CustomDatasourceRelation(sqlContext, p, schema)      case _ =&gt; throw new IllegalArgumentException(&quot;Path is required for custom-datasource format!!&quot;)    &#125;  &#125;  override def createRelation(sqlContext: SQLContext, mode: SaveMode, parameters: Map[String, String],                              data: DataFrame): BaseRelation = &#123;    val path = parameters.getOrElse(&quot;path&quot;, &quot;./output/&quot;) //can throw an exception/error, it&#39;s just for this tutorial    val fsPath = new Path(path)    val fs = fsPath.getFileSystem(sqlContext.sparkContext.hadoopConfiguration)    mode match &#123;      case SaveMode.Append =&gt; sys.error(&quot;Append mode is not supported by &quot; + this.getClass.getCanonicalName); sys.exit(1)      case SaveMode.Overwrite =&gt; fs.delete(fsPath, true)      case SaveMode.ErrorIfExists =&gt; sys.error(&quot;Given path: &quot; + path + &quot; already exists!!&quot;); sys.exit(1)      case SaveMode.Ignore =&gt; sys.exit()    &#125;    val formatName = parameters.getOrElse(&quot;format&quot;, &quot;customFormat&quot;)    formatName match &#123;      case &quot;customFormat&quot; =&gt; saveAsCustomFormat(data, path, mode)      case &quot;json&quot; =&gt; saveAsJson(data, path, mode)      case _ =&gt; throw new IllegalArgumentException(formatName + &quot; is not supported!!!&quot;)    &#125;    createRelation(sqlContext, parameters, data.schema)  &#125;  private def saveAsJson(data : DataFrame, path : String, mode: SaveMode): Unit = &#123;    /**      * Here, I am using the dataframe&#39;s Api for storing it as json.      * you can have your own apis and ways for saving!!      */    data.write.mode(mode).json(path)  &#125;  private def saveAsCustomFormat(data : DataFrame, path : String, mode: SaveMode): Unit = &#123;    /**      * Here, I am  going to save this as simple text file which has values separated by &quot;|&quot;.      * But you can have your own way to store without any restriction.      */    val customFormatRDD = data.rdd.map(row =&gt; &#123;      row.toSeq.map(value =&gt; value.toString).mkString(&quot;|&quot;)    &#125;)    customFormatRDD.saveAsTextFile(path)  &#125;&#125;</code></pre><pre><code class="scala">import org.apache.spark.sql.types.&#123;DataType, IntegerType, LongType, StringType&#125;/**  * Created by rana on 30/9/16.  */object Util &#123;  def castTo(value : String, dataType : DataType) = &#123;    dataType match &#123;      case _ : IntegerType =&gt; value.toInt      case _ : LongType =&gt; value.toLong      case _ : StringType =&gt; value    &#125;  &#125;&#125;</code></pre><pre><code class="scala">import org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSession/**  * Created by rana on 29/9/16.  */object app extends App &#123;  println(&quot;Application started...&quot;)  val conf = new SparkConf().setAppName(&quot;spark-custom-datasource&quot;)  val spark = SparkSession.builder().config(conf).master(&quot;local&quot;).getOrCreate()  val df = spark.sqlContext.read.format(&quot;cn.zj.spark.sql.datasource&quot;).load(&quot;1229practice/data/&quot;)  //print the schema//  df.printSchema()  //print the data//  df.show()  //save the data//  df.write.options(Map(&quot;format&quot; -&gt; &quot;customFormat&quot;)).mode(SaveMode.Overwrite).format(&quot;io.dcengines.rana.datasource&quot;).save(&quot;out_custom/&quot;)//  df.write.options(Map(&quot;format&quot; -&gt; &quot;json&quot;)).mode(SaveMode.Overwrite).format(&quot;io.dcengines.rana.datasource&quot;).save(&quot;out_json/&quot;)//  df.write.mode(SaveMode.Overwrite).format(&quot;io.dcengines.rana.datasource&quot;).save(&quot;out_none/&quot;)  //select some specific columns//  df.createOrReplaceTempView(&quot;test&quot;)//  spark.sql(&quot;select id, name, salary from test&quot;).show()  //filter data  df.createOrReplaceTempView(&quot;test&quot;)  spark.sql(&quot;select * from test where salary = 50000&quot;).show()  println(&quot;Application Ended...&quot;)&#125;</code></pre><h2 id="Hive行级更新和生成代理键"><a href="#Hive行级更新和生成代理键" class="headerlink" title="Hive行级更新和生成代理键"></a><strong>Hive行级更新和生成代理键</strong></h2><pre><code class="python">### hive支持行级更新1.编辑hive-site.xml文件:客户端hive.support.concurrency – truehive.enforce.bucketing – true (Not required as of Hive 2.0)hive.exec.dynamic.partition.mode – nonstricthive.txn.manager – org.apache.hadoop.hive.ql.lockmgr.DbTxnManager服务端hive.compactor.initiator.on – true (See table below for more details)hive.compactor.worker.threads – a positive number on at least one instance of the Thrift metastore service2.如果一个表要实现update和delete功能，该表就必须支持ACID，而支持ACID，就必须满足以下条件：表的存储格式必须是目前只支持ORCFileformat和AcidOutputFormat表必须进行分桶（CLUSTERED BY (col_name, col_name, …) INTO num_buckets BUCKETS）；Table property中参数transactional必须设定为True（tblproperties(‘transactional’=‘true’)）；### hive生成代理键代理键 ：      维度表中必须有一个能够唯一标识一行记录的列，通过该列维护维度表与事实表之间的关系，一般在维度表中业务主键符合条件可以当作维度主键。补充：      是由数据仓库处理过程中产生的，与业务本身无关的, 唯一标识维度表中一条记录并充当维度表主键的列，也是描述维度表与事实表关系的纽带。所以在设计有代理键的维度表中，事实表中的关联键是代理键而不是原有的业务主键，即业务关系是靠代理键维护，这样有效避免源系统变化对数据仓库的影响。在实际业务中，代理键通常是数值型，自增的值。### 1、用row_number()函数生成代理键INSERT OVERWRITE TABLE testTableselect row_number() over (order by a.acc_no) id,a.acc_nofrom ba_pay_out.app_intf_web_cli_his_view a补充：指定自增基数insert into table User_Attribute select (row_number() over())+1000 as id,customid from tbl_custom;### 2、用UDFRowSequence生成代理键add jar hdfs:///user/hive-contrib-2.0.0.jar; create temporary function row_sequence as &#39;org.apache.hadoop.hive.contrib.udf.udfrowsequence&#39;; INSERT OVERWRITE TABLE testTableselect row_sequence() id,a.acc_nofrom ba_pay_out.app_intf_web_cli_his_view ahive-contrib-2.0.0.jar中包含一个生成记录序号的自定义函数udfrowsequence。上面的语句先加载JAR包，然后创建一个名为row_sequence()的临时函数作为调用UDF的接口，这样可以为查询的结果集生成一个自增伪列。之后就和row_number()写法类似了，只不过将窗口函数row_number()替换为row_sequence()函数。以上两种方法，第二种的性能要由于第一种，第一种执行慢，且当数据超过约几千万（本人经验超过4千万）时，就报内存不够的了，这个可能与hadoop的资源配置也有关系，而第二中方法在数据超过1.5亿的情况下依然能够快速运行。</code></pre><p><a href="https://blog.csdn.net/wzy0623/article/details/53893174">https://blog.csdn.net/wzy0623/article/details/53893174</a></p><h2 id="Spark高级操作之JSON和复杂嵌套结构的操作-一）"><a href="#Spark高级操作之JSON和复杂嵌套结构的操作-一）" class="headerlink" title="Spark高级操作之JSON和复杂嵌套结构的操作(一）"></a><strong>Spark高级操作之JSON和复杂嵌套结构的操作(一）</strong></h2><p>本文主要讲spark2.0版本以后存在的Sparksql的一些实用的函数，帮助解决复杂嵌套的json数据格式，比如，map和嵌套结构。Spark2.1在spark 的Structured Streaming也可以使用这些功能函数。</p><p>下面几个是本文重点要讲的方法。</p><p>A),get_json_object()</p><p>B),from_json()</p><p>C),to_json()</p><p>D),explode()</p><p>E),selectExpr()</p><pre><code class="scala">##    准备阶段首先，创建一个没有任何嵌套的JSon Schemaimport org.apache.spark.sql.types._import org.apache.spark.sql.functions._val jsonSchema = new StructType().add(&quot;battery_level&quot;, LongType).add(&quot;c02_level&quot;, LongType).add(&quot;cca3&quot;,StringType).add(&quot;cn&quot;, StringType).add(&quot;device_id&quot;, LongType).add(&quot;device_type&quot;, StringType).add(&quot;signal&quot;, LongType).add(&quot;ip&quot;, StringType).add(&quot;temp&quot;, LongType).add(&quot;timestamp&quot;, TimestampType)使用上面的schema，我在这里创建一个Dataframe，使用的是scala 的case class，同时会产生一些json格式的数据。当然，生产中这些数据也可以来自于kafka。这个case class总共有两个字段：整型(作为device id)和一个字符串(json的数据结构，代表设备的事件)case class DeviceData (id: Int, device: String)val eventsDS = Seq (  (0, &quot;&quot;&quot;&#123;&quot;device_id&quot;: 0, &quot;device_type&quot;: &quot;sensor-ipad&quot;, &quot;ip&quot;: &quot;68.161.225.1&quot;, &quot;cca3&quot;: &quot;USA&quot;, &quot;cn&quot;: &quot;United States&quot;, &quot;temp&quot;: 25, &quot;signal&quot;: 23, &quot;battery_level&quot;: 8, &quot;c02_level&quot;: 917, &quot;timestamp&quot; :1475600496 &#125;&quot;&quot;&quot;),  (1, &quot;&quot;&quot;&#123;&quot;device_id&quot;: 1, &quot;device_type&quot;: &quot;sensor-igauge&quot;, &quot;ip&quot;: &quot;213.161.254.1&quot;, &quot;cca3&quot;: &quot;NOR&quot;, &quot;cn&quot;: &quot;Norway&quot;, &quot;temp&quot;: 30, &quot;signal&quot;: 18, &quot;battery_level&quot;: 6, &quot;c02_level&quot;: 1413, &quot;timestamp&quot; :1475600498 &#125;&quot;&quot;&quot;),  (2, &quot;&quot;&quot;&#123;&quot;device_id&quot;: 2, &quot;device_type&quot;: &quot;sensor-ipad&quot;, &quot;ip&quot;: &quot;88.36.5.1&quot;, &quot;cca3&quot;: &quot;ITA&quot;, &quot;cn&quot;: &quot;Italy&quot;, &quot;temp&quot;: 18, &quot;signal&quot;: 25, &quot;battery_level&quot;: 5, &quot;c02_level&quot;: 1372, &quot;timestamp&quot; :1475600500 &#125;&quot;&quot;&quot;),...).toDF(&quot;id&quot;, &quot;device&quot;).as[DeviceData]### 如何使用get_json_object()该方法从spark1.6开始就有了，从一个json 字符串中根据指定的json 路径抽取一个json 对象。从上面的dataset中取出部分数据，然后抽取部分字段组装成新的json 对象。比如，我们仅仅抽取：id，devicetype，ip，CCA3 code.val eventsFromJSONDF = Seq (  (0, &quot;&quot;&quot;&#123;&quot;device_id&quot;: 0, &quot;device_type&quot;: &quot;sensor-ipad&quot;, &quot;ip&quot;: &quot;68.161.225.1&quot;, &quot;cca3&quot;: &quot;USA&quot;, &quot;cn&quot;: &quot;United States&quot;, &quot;temp&quot;: 25, &quot;signal&quot;: 23, &quot;battery_level&quot;: 8, &quot;c02_level&quot;: 917, &quot;timestamp&quot; :1475600496 &#125;&quot;&quot;&quot;),  (1, &quot;&quot;&quot;&#123;&quot;device_id&quot;: 1, &quot;device_type&quot;: &quot;sensor-igauge&quot;, &quot;ip&quot;: &quot;213.161.254.1&quot;, &quot;cca3&quot;: &quot;NOR&quot;, &quot;cn&quot;: &quot;Norway&quot;, &quot;temp&quot;: 30, &quot;signal&quot;: 18, &quot;battery_level&quot;: 6, &quot;c02_level&quot;: 1413, &quot;timestamp&quot; :1475600498 &#125;&quot;&quot;&quot;),  (2, &quot;&quot;&quot;&#123;&quot;device_id&quot;: 2, &quot;device_type&quot;: &quot;sensor-ipad&quot;, &quot;ip&quot;: &quot;88.36.5.1&quot;, &quot;cca3&quot;: &quot;ITA&quot;, &quot;cn&quot;: &quot;Italy&quot;, &quot;temp&quot;: 18, &quot;signal&quot;: 25, &quot;battery_level&quot;: 5, &quot;c02_level&quot;: 1372, &quot;timestamp&quot; :1475600500 &#125;&quot;&quot;&quot;),...).toDF(&quot;id&quot;, &quot;json&quot;)测试及输出val jsDF = eventsFromJSONDF.select($&quot;id&quot;, get_json_object($&quot;json&quot;, &quot;$.device_type&quot;).alias(&quot;device_type&quot;),get_json_object($&quot;json&quot;, &quot;$.ip&quot;).alias(&quot;ip&quot;),get_json_object($&quot;json&quot;, &quot;$.cca3&quot;).alias(&quot;cca3&quot;))jsDF.printSchemajsDF.show### 如何使用from_json()与get_json_object不同的是该方法，使用schema去抽取单独列。在dataset的api select中使用from_json()方法，我可以从一个json 字符串中按照指定的schema格式抽取出来作为DataFrame的列。还有，我们也可以将所有在json中的属性和值当做一个devices的实体。我们不仅可以使用device.arrtibute去获取特定值，也可以使用*通配符。下面的例子，主要实现如下功能：A),使用上述schema从json字符串中抽取属性和值，并将它们视为devices的独立列。B),select所有列C),使用.,获取部分列。val devicesDF = eventsDS.select(from_json($&quot;device&quot;, jsonSchema) as &quot;devices&quot;).select($&quot;devices.*&quot;).filter($&quot;devices.temp&quot; &gt; 10 and $&quot;devices.signal&quot; &gt; 15)### 如何使用to_json()下面使用to_json()将获取的数据转化为json格式。将结果重新写入kafka或者保存partquet文件。val stringJsonDF = eventsDS.select(to_json(struct($&quot;*&quot;))).toDF(&quot;devices&quot;)stringJsonDF.show保存数据到kafkastringJsonDF.write.format(&quot;kafka&quot;).option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;).option(&quot;topic&quot;, &quot;iot-devices&quot;).save()### 如何使用selectExpr()将列转化为一个JSON对象的另一种方式是使用selectExpr()功能函数。例如我们可以将device列转化为一个JSON对象。val stringsDF = eventsDS.selectExpr(&quot;CAST(id AS INT)&quot;, &quot;CAST(device AS STRING)&quot;)stringsDF.showSelectExpr()方法的另一个用法，就是使用表达式作为参数，将它们转化为指定的列。如下：devicesDF.selectExpr(&quot;c02_level&quot;, &quot;round(c02_level/temp) as ratio_c02_temperature&quot;).orderBy($&quot;ratio_c02_temperature&quot; desc).show使用Sparksql的slq语句是很好写的首先注册成临时表，然后写sqldevicesDF.createOrReplaceTempView(&quot;devicesDFT&quot;)spark.sql(&quot;select c02_level,round(c02_level/temp) as ratio_c02_temperature from devicesDFT order by ratio_c02_temperature desc&quot;).show### 验证为了验证我们的DataFrame转化为json String是成功的我们将结果写入本地磁盘。stringJsonDF.write.mode(&quot;overwrite&quot;).format(&quot;parquet&quot;).save(&quot;file:///opt/jules&quot;)</code></pre><h2 id="Spark高级操作之JSON和复杂嵌套结构的操作-二）"><a href="#Spark高级操作之JSON和复杂嵌套结构的操作-二）" class="headerlink" title="Spark高级操作之JSON和复杂嵌套结构的操作(二）"></a><strong>Spark高级操作之JSON和复杂嵌套结构的操作(二）</strong></h2><pre><code class="scala">一，准备阶段Json格式里面有map结构和嵌套json也是很合理的。本文将举例说明如何用spark解析包含复杂的嵌套数据结构，map。现实中的例子是，一个设备的检测事件，二氧化碳的安全你浓度，高温数据等，需要实时产生数据，然后及时的告警处理。1，定义schemaimport org.apache.spark.sql.types._val schema = new StructType()  .add(&quot;dc_id&quot;, StringType)                    // data center where data was posted to Kafka cluster  .add(&quot;source&quot;,                               // info about the source of alarm    MapType(                                   // define this as a Map(Key-&gt;value)      StringType,      new StructType()        .add(&quot;description&quot;, StringType)        .add(&quot;ip&quot;, StringType)        .add(&quot;id&quot;, LongType)        .add(&quot;temp&quot;, LongType)        .add(&quot;c02_level&quot;, LongType)        .add(&quot;geo&quot;,          new StructType()            .add(&quot;lat&quot;, DoubleType)            .add(&quot;long&quot;, DoubleType)        )    )  )2，准备数据val dataDS = Seq(&quot;&quot;&quot;&#123;&quot;dc_id&quot;: &quot;dc-101&quot;,&quot;source&quot;: &#123;    &quot;sensor-igauge&quot;: &#123;      &quot;id&quot;: 10,      &quot;ip&quot;: &quot;68.28.91.22&quot;,      &quot;description&quot;: &quot;Sensor attached to the container ceilings&quot;,      &quot;temp&quot;:35,      &quot;c02_level&quot;: 1475,      &quot;geo&quot;: &#123;&quot;lat&quot;:38.00, &quot;long&quot;:97.00&#125;                            &#125;,    &quot;sensor-ipad&quot;: &#123;      &quot;id&quot;: 13,      &quot;ip&quot;: &quot;67.185.72.1&quot;,      &quot;description&quot;: &quot;Sensor ipad attached to carbon cylinders&quot;,      &quot;temp&quot;: 34,      &quot;c02_level&quot;: 1370,      &quot;geo&quot;: &#123;&quot;lat&quot;:47.41, &quot;long&quot;:-122.00&#125;    &#125;,    &quot;sensor-inest&quot;: &#123;      &quot;id&quot;: 8,      &quot;ip&quot;: &quot;208.109.163.218&quot;,      &quot;description&quot;: &quot;Sensor attached to the factory ceilings&quot;,      &quot;temp&quot;: 40,      &quot;c02_level&quot;: 1346,      &quot;geo&quot;: &#123;&quot;lat&quot;:33.61, &quot;long&quot;:-111.89&#125;    &#125;,    &quot;sensor-istick&quot;: &#123;      &quot;id&quot;: 5,      &quot;ip&quot;: &quot;204.116.105.67&quot;,      &quot;description&quot;: &quot;Sensor embedded in exhaust pipes in the ceilings&quot;,      &quot;temp&quot;: 40,      &quot;c02_level&quot;: 1574,      &quot;geo&quot;: &#123;&quot;lat&quot;:35.93, &quot;long&quot;:-85.46&#125;    &#125;  &#125;&#125;&quot;&quot;&quot;).toDS()  // should only be one item  dataDS.count()3，准备处理val df = spark.read.schema(schema).json(dataDS.rdd)查看schemadf.printSchema二，如何使用explode()Explode()方法在spark1.3的时候就已经存在了，在这里展示一下如何抽取嵌套的数据结构。在一些场合，会结合explode，to_json,from_json一起使用。Explode为给定的map的每一个元素创建一个新的行。比如上面准备的数据，source就是一个map结构。Map中的每一个key/value对都会是一个独立的行。val explodedDF = df.select($&quot;dc_id&quot;, explode($&quot;source&quot;))explodedDF.printSchema获取内部的数据case class DeviceAlert(dcId: String, deviceType:String, ip:String, deviceId:Long, temp:Long, c02_level: Long, lat: Double, lon: Double)val notifydevicesDS = explodedDF.select( $&quot;dc_id&quot; as &quot;dcId&quot;,  $&quot;key&quot; as &quot;deviceType&quot;,  &#39;value.getItem(&quot;ip&quot;) as &#39;ip,  &#39;value.getItem(&quot;id&quot;) as &#39;deviceId,  &#39;value.getItem(&quot;c02_level&quot;) as &#39;c02_level,  &#39;value.getItem(&quot;temp&quot;) as &#39;temp,  &#39;value.getItem(&quot;geo&quot;).getItem(&quot;lat&quot;) as &#39;lat,  //note embedded level requires yet another level of fetching.  &#39;value.getItem(&quot;geo&quot;).getItem(&quot;long&quot;) as &#39;lon)  .as[DeviceAlert]  // return as a Dataset查看schema信息notifydevicesDS.printSchema三，再复杂一点在物联网场景里，通畅物联网设备会将很多json 事件数据发给他的收集器。收集器可以是附近的数据中心，也可以是附近的聚合器，也可以是安装在家里的一个设备，它会有规律的周期的将数据通过加密的互联网发给远程的数据中心。说白一点，数据格式更复杂。我们下面会有三个map的数据格式：恒温计，摄像机，烟雾报警器。import org.apache.spark.sql.types._// a bit longish, nested, and convuloted JSON schema :)val nestSchema2 = new StructType()  .add(&quot;devices&quot;,    new StructType()      .add(&quot;thermostats&quot;, MapType(StringType,      new StructType()        .add(&quot;device_id&quot;, StringType)        .add(&quot;locale&quot;, StringType)        .add(&quot;software_version&quot;, StringType)        .add(&quot;structure_id&quot;, StringType)        .add(&quot;where_name&quot;, StringType)        .add(&quot;last_connection&quot;, StringType)        .add(&quot;is_online&quot;, BooleanType)        .add(&quot;can_cool&quot;, BooleanType)        .add(&quot;can_heat&quot;, BooleanType)        .add(&quot;is_using_emergency_heat&quot;, BooleanType)        .add(&quot;has_fan&quot;, BooleanType)        .add(&quot;fan_timer_active&quot;, BooleanType)        .add(&quot;fan_timer_timeout&quot;, StringType)        .add(&quot;temperature_scale&quot;, StringType)        .add(&quot;target_temperature_f&quot;, DoubleType)        .add(&quot;target_temperature_high_f&quot;, DoubleType)        .add(&quot;target_temperature_low_f&quot;, DoubleType)        .add(&quot;eco_temperature_high_f&quot;, DoubleType)        .add(&quot;eco_temperature_low_f&quot;, DoubleType)        .add(&quot;away_temperature_high_f&quot;, DoubleType)        .add(&quot;away_temperature_low_f&quot;, DoubleType)        .add(&quot;hvac_mode&quot;, StringType)        .add(&quot;humidity&quot;, LongType)        .add(&quot;hvac_state&quot;, StringType)        .add(&quot;is_locked&quot;, StringType)        .add(&quot;locked_temp_min_f&quot;, DoubleType)        .add(&quot;locked_temp_max_f&quot;, DoubleType)))      .add(&quot;smoke_co_alarms&quot;, MapType(StringType,      new StructType()        .add(&quot;device_id&quot;, StringType)        .add(&quot;locale&quot;, StringType)        .add(&quot;software_version&quot;, StringType)        .add(&quot;structure_id&quot;, StringType)        .add(&quot;where_name&quot;, StringType)        .add(&quot;last_connection&quot;, StringType)        .add(&quot;is_online&quot;, BooleanType)        .add(&quot;battery_health&quot;, StringType)        .add(&quot;co_alarm_state&quot;, StringType)        .add(&quot;smoke_alarm_state&quot;, StringType)        .add(&quot;is_manual_test_active&quot;, BooleanType)        .add(&quot;last_manual_test_time&quot;, StringType)        .add(&quot;ui_color_state&quot;, StringType)))      .add(&quot;cameras&quot;, MapType(StringType,      new StructType()        .add(&quot;device_id&quot;, StringType)        .add(&quot;software_version&quot;, StringType)        .add(&quot;structure_id&quot;, StringType)        .add(&quot;where_name&quot;, StringType)        .add(&quot;is_online&quot;, BooleanType)        .add(&quot;is_streaming&quot;, BooleanType)        .add(&quot;is_audio_input_enabled&quot;, BooleanType)        .add(&quot;last_is_online_change&quot;, StringType)        .add(&quot;is_video_history_enabled&quot;, BooleanType)        .add(&quot;web_url&quot;, StringType)        .add(&quot;app_url&quot;, StringType)        .add(&quot;is_public_share_enabled&quot;, BooleanType)        .add(&quot;activity_zones&quot;,          new StructType()            .add(&quot;name&quot;, StringType)            .add(&quot;id&quot;, LongType))        .add(&quot;last_event&quot;, StringType))))对应的数据val nestDataDS2 = Seq(&quot;&quot;&quot;&#123;  &quot;devices&quot;: &#123;     &quot;thermostats&quot;: &#123;        &quot;peyiJNo0IldT2YlIVtYaGQ&quot;: &#123;          &quot;device_id&quot;: &quot;peyiJNo0IldT2YlIVtYaGQ&quot;,          &quot;locale&quot;: &quot;en-US&quot;,          &quot;software_version&quot;: &quot;4.0&quot;,          &quot;structure_id&quot;: &quot;VqFabWH21nwVyd4RWgJgNb292wa7hG_dUwo2i2SG7j3-BOLY0BA4sw&quot;,          &quot;where_name&quot;: &quot;Hallway Upstairs&quot;,          &quot;last_connection&quot;: &quot;2016-10-31T23:59:59.000Z&quot;,          &quot;is_online&quot;: true,          &quot;can_cool&quot;: true,          &quot;can_heat&quot;: true,          &quot;is_using_emergency_heat&quot;: true,          &quot;has_fan&quot;: true,          &quot;fan_timer_active&quot;: true,          &quot;fan_timer_timeout&quot;: &quot;2016-10-31T23:59:59.000Z&quot;,          &quot;temperature_scale&quot;: &quot;F&quot;,          &quot;target_temperature_f&quot;: 72,          &quot;target_temperature_high_f&quot;: 80,          &quot;target_temperature_low_f&quot;: 65,          &quot;eco_temperature_high_f&quot;: 80,          &quot;eco_temperature_low_f&quot;: 65,          &quot;away_temperature_high_f&quot;: 80,          &quot;away_temperature_low_f&quot;: 65,          &quot;hvac_mode&quot;: &quot;heat&quot;,          &quot;humidity&quot;: 40,          &quot;hvac_state&quot;: &quot;heating&quot;,          &quot;is_locked&quot;: true,          &quot;locked_temp_min_f&quot;: 65,          &quot;locked_temp_max_f&quot;: 80          &#125;        &#125;,        &quot;smoke_co_alarms&quot;: &#123;          &quot;RTMTKxsQTCxzVcsySOHPxKoF4OyCifrs&quot;: &#123;            &quot;device_id&quot;: &quot;RTMTKxsQTCxzVcsySOHPxKoF4OyCifrs&quot;,            &quot;locale&quot;: &quot;en-US&quot;,            &quot;software_version&quot;: &quot;1.01&quot;,            &quot;structure_id&quot;: &quot;VqFabWH21nwVyd4RWgJgNb292wa7hG_dUwo2i2SG7j3-BOLY0BA4sw&quot;,            &quot;where_name&quot;: &quot;Jane&#39;s Room&quot;,            &quot;last_connection&quot;: &quot;2016-10-31T23:59:59.000Z&quot;,            &quot;is_online&quot;: true,            &quot;battery_health&quot;: &quot;ok&quot;,            &quot;co_alarm_state&quot;: &quot;ok&quot;,            &quot;smoke_alarm_state&quot;: &quot;ok&quot;,            &quot;is_manual_test_active&quot;: true,            &quot;last_manual_test_time&quot;: &quot;2016-10-31T23:59:59.000Z&quot;,            &quot;ui_color_state&quot;: &quot;gray&quot;            &#125;          &#125;,       &quot;cameras&quot;: &#123;        &quot;awJo6rH0IldT2YlIVtYaGQ&quot;: &#123;          &quot;device_id&quot;: &quot;awJo6rH&quot;,          &quot;software_version&quot;: &quot;4.0&quot;,          &quot;structure_id&quot;: &quot;VqFabWH21nwVyd4RWgJgNb292wa7hG_dUwo2i2SG7j3-BOLY0BA4sw&quot;,          &quot;where_name&quot;: &quot;Foyer&quot;,          &quot;is_online&quot;: true,          &quot;is_streaming&quot;: true,          &quot;is_audio_input_enabled&quot;: true,          &quot;last_is_online_change&quot;: &quot;2016-12-29T18:42:00.000Z&quot;,          &quot;is_video_history_enabled&quot;: true,          &quot;web_url&quot;: &quot;https://home.nest.com/cameras/device_id?auth=access_token&quot;,          &quot;app_url&quot;: &quot;nestmobile://cameras/device_id?auth=access_token&quot;,          &quot;is_public_share_enabled&quot;: true,          &quot;activity_zones&quot;: &#123; &quot;name&quot;: &quot;Walkway&quot;, &quot;id&quot;: 244083 &#125;,          &quot;last_event&quot;: &quot;2016-10-31T23:59:59.000Z&quot;          &#125;        &#125;      &#125;     &#125;&quot;&quot;&quot;).toDS通过创建一个简单的dataset，我们可以使用所有的dataset的方法来进行ETL操作，比如from_json(), to_json(), explode() and selectExpr()。val nestDF2 = spark                            // spark session   .read                             //  get DataFrameReader  .schema(nestSchema2)             //  use the defined schema above and read format as JSON  .json(nestDataDS2.rdd)将整个json对象，转化为一个json stringval stringJsonDF = nestDF2.select(to_json(struct($&quot;*&quot;))).toDF(&quot;nestDevice&quot;)将三个json object 的map对象转化为三个单独的map列，然后可以是使用explode方法访问其属性val mapColumnsDF = nestDF2.select($&quot;devices&quot;.getItem(&quot;smoke_co_alarms&quot;).alias (&quot;smoke_alarms&quot;),  $&quot;devices&quot;.getItem(&quot;cameras&quot;).alias (&quot;cameras&quot;),  $&quot;devices&quot;.getItem(&quot;thermostats&quot;).alias (&quot;thermostats&quot;))转化为三个dataframeval explodedThermostatsDF = mapColumnsDF.select(explode($&quot;thermostats&quot;))val explodedCamerasDF = mapColumnsDF.select(explode($&quot;cameras&quot;))//or you could use the original nestDF2 and use the devices.X notationval explodedSmokedAlarmsDF =  nestDF2.select(explode($&quot;devices.smoke_co_alarms&quot;))访问三个map内部的元素val thermostateDF = explodedThermostatsDF.select($&quot;value&quot;.getItem(&quot;device_id&quot;).alias(&quot;device_id&quot;),  $&quot;value&quot;.getItem(&quot;locale&quot;).alias(&quot;locale&quot;),  $&quot;value&quot;.getItem(&quot;where_name&quot;).alias(&quot;location&quot;),  $&quot;value&quot;.getItem(&quot;last_connection&quot;).alias(&quot;last_connected&quot;),  $&quot;value&quot;.getItem(&quot;humidity&quot;).alias(&quot;humidity&quot;),  $&quot;value&quot;.getItem(&quot;target_temperature_f&quot;).alias(&quot;target_temperature_f&quot;),  $&quot;value&quot;.getItem(&quot;hvac_mode&quot;).alias(&quot;mode&quot;),  $&quot;value&quot;.getItem(&quot;software_version&quot;).alias(&quot;version&quot;))val cameraDF = explodedCamerasDF.select($&quot;value&quot;.getItem(&quot;device_id&quot;).alias(&quot;device_id&quot;),  $&quot;value&quot;.getItem(&quot;where_name&quot;).alias(&quot;location&quot;),  $&quot;value&quot;.getItem(&quot;software_version&quot;).alias(&quot;version&quot;),  $&quot;value&quot;.getItem(&quot;activity_zones&quot;).getItem(&quot;name&quot;).alias(&quot;name&quot;),  $&quot;value&quot;.getItem(&quot;activity_zones&quot;).getItem(&quot;id&quot;).alias(&quot;id&quot;))val smokedAlarmsDF = explodedSmokedAlarmsDF.select($&quot;value&quot;.getItem(&quot;device_id&quot;).alias(&quot;device_id&quot;),  $&quot;value&quot;.getItem(&quot;where_name&quot;).alias(&quot;location&quot;),  $&quot;value&quot;.getItem(&quot;software_version&quot;).alias(&quot;version&quot;),  $&quot;value&quot;.getItem(&quot;last_connection&quot;).alias(&quot;last_connected&quot;),  $&quot;value&quot;.getItem(&quot;battery_health&quot;).alias(&quot;battery_health&quot;))</code></pre><h2 id="shell-定时脚本"><a href="#shell-定时脚本" class="headerlink" title="shell 定时脚本"></a><strong>shell 定时脚本</strong></h2><pre><code class="shell">1：查看crontab是否开启ps aux | grep cron启动命令service crond restart设置开机自启动 chkconfig crond on2:使用方法。可以通过用户的crobtab设置（命令的方式），也可以通过系统的crontab的配置文件(可以指定其它用户）。通过命令行的方式（root用户）：corntab -e 进入编辑器* * * * * 执行的任务        （五个*分别表示分时日月周，*/n表示每个n分钟执行一次任务）参数说明：星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。逗号（,）：可以用逗号隔开的值指定一个列表范围，（代表不连续的时间）例如，“1,2,5,7,8,9”中杠（-）：可以用整数之间的中杠表示一个整数范围（代表连续的时间范围），例如“2-6”表示“2,3,4,5,6”正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。命令：30 21 * * * /etc/init.d/smb restart     每晚的21:30重启smb45 4 1,10,22 * * /etc/init.d/smb restart    每月1、10、22日的4 : 45重启smb0,30 18-23 * * * /etc/init.d/smb restart   每天18 : 00至23 : 00之间每隔30分钟重启smb3,15 8-11 * * * command    在上午8点到11点的第3和第15分钟执行3,15 8-11 */2 * * command   每隔两天的上午8点到11点的第3和第15分钟执行0 4 1 jan * /etc/init.d/smb restart 一月一号的4点重启smb配置文件的corntab命令</code></pre><h2 id="基于shell的azkban容错插件"><a href="#基于shell的azkban容错插件" class="headerlink" title="基于shell的azkban容错插件"></a><strong>基于shell的azkban容错插件</strong></h2><pre><code class="shell"></code></pre><h2 id="shell编程整理"><a href="#shell编程整理" class="headerlink" title="shell编程整理"></a><strong>shell编程整理</strong></h2><pre><code></code></pre><h2 id="Redis管理kafka的偏移量"><a href="#Redis管理kafka的偏移量" class="headerlink" title="Redis管理kafka的偏移量"></a><strong>Redis管理kafka的偏移量</strong></h2><p>注：想要redis管理kafka的偏移量实现exatly-oncle,需要开启redis的pIpeline机制。本篇幅未做管理。</p><pre><code class="scala">import java.utilimport cn.huige.spark05.utils.JedisPoolUtilsimport org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.TopicPartitionimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.&#123;SparkConf, TaskContext&#125;import org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka010._import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import scala.collection.mutableobject StreamingOffsetRedis &#123;  Logger.getLogger(&quot;org&quot;).setLevel(Level.ERROR)  def main(args: Array[String]): Unit = &#123;    val conf: SparkConf = new SparkConf()      .setAppName(this.getClass.getSimpleName)      .setMaster(&quot;local[*]&quot;)    val ssc: StreamingContext = new StreamingContext(conf, Seconds(2))    val groupId = &quot;aaa&quot;    val concat_sep = &quot;:_:&quot;    // kafka消费者的参数    val kafkaParams = Map[String, Object](      &quot;bootstrap.servers&quot; -&gt; &quot;hdp-01:9092,hdp-02:9092,hdp-03:9092&quot;,      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],      &quot;group.id&quot; -&gt; groupId,      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,      // 手动维护偏移量，这里需要设置为false      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)    )    val topics = Array(&quot;wctopic&quot;)    // 指定offset的位置    val offsetMap = mutable.HashMap[TopicPartition, Long]()    val jedis2 = JedisPoolUtils()    val values: util.Map[String, String] = jedis2.hgetAll(groupId + concat_sep + topics(0))    import scala.collection.JavaConversions._    for(tp &lt;- values)&#123;      // 赋值      val topicAndPart = new TopicPartition(topics(0),tp._1.toInt)      offsetMap(topicAndPart)=tp._2.toLong    &#125;    val dstream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream(      ssc,      // 任务尽量均匀的分布在各个executor节点      LocationStrategies.PreferConsistent,      ConsumerStrategies.Subscribe[String, String](topics, kafkaParams, offsetMap))    dstream.foreachRDD(rdd =&gt; &#123;      // 判断rdd是否为空      if(!rdd.isEmpty())&#123;      println(rdd.partitions.size)      // 获取到当前批次的偏移量数据      val ofr: Array[OffsetRange] = rdd.asInstanceOf[HasOffsetRanges].offsetRanges      ofr.foreach(println)                // 方式二: 业务逻辑的处理。即处理一条数据就更新一条偏移量。        rdd.foreachPartition(it =&gt;&#123;          val jedis = JedisPoolUtils()          it.foreach(tp =&gt;&#123;            // 具体业务逻辑的实现            // 业务逻辑写mysql            // 方式一:偏移量的维护            val part = TaskContext.getPartitionId()            val range: OffsetRange = ofr.filter(_.partition == part)(0)            val uofset = range.untilOffset            val groupAndTopic = groupId + concat_sep + topics(0)            // 把最新的offset 写到redis中            jedis.hset(groupAndTopic, part + &quot;&quot;, uofset + &quot;&quot;)          &#125;)          jedis.close()        &#125;)      &#125;    &#125;)      // 提交更新偏移量      val jedis = JedisPoolUtils()      ofr.foreach(or =&gt; &#123;        val uofset = or.untilOffset        val groupAndTopic = groupId + concat_sep + or.topic        // 把最新的offset 写到redis中        jedis.hset(groupAndTopic, or.partition + &quot;&quot;, uofset + &quot;&quot;)      &#125;)      jedis.close()      &#125;    &#125;)    ssc.start()    ssc.awaitTermination()  &#125;&#125;</code></pre><h2 id="kafka-exactly-once保证—mysql"><a href="#kafka-exactly-once保证—mysql" class="headerlink" title="kafka exactly-once保证—mysql"></a><strong>kafka exactly-once保证—mysql</strong></h2><pre><code class="scala">import java.sql.PreparedStatementimport kafka.common.TopicAndPartitionimport kafka.message.MessageAndMetadataimport kafka.serializer.StringDecoderimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka.&#123;KafkaUtils, OffsetRange&#125;import streamingtest.&#123;ConfigurationConstants, ConnectPool&#125;import streamingtest.FlumePollingEvent.loggerobject OffsetReadAndSave &#123;  // 从mysql中读取kafka偏移量，并产生kafka DStream  def KafkaOffsetRead(ssc: StreamingContext, kafkaParams: Map[String, String], consumerTopics: Set[String]): InputDStream[(String, String)] = &#123;    val connOffset = ConnectPool.getConnection    val psOffsetCnt: PreparedStatement = connOffset.prepareStatement(&quot;SELECT SUM(1) FROM `kafka_offset` WHERE `topic`=?&quot;)    psOffsetCnt.setString(1, ConfigurationConstants.kafkaConsumerTopics)    val rs = psOffsetCnt.executeQuery()    var parCount = 0    while (rs.next()) &#123;      parCount = rs.getInt(1)      println(parCount.toString)    &#125;    var kafkaStream : InputDStream[(String, String)] = null    var fromOffsets: Map[TopicAndPartition, Long] = Map()    val psOffsetRead: PreparedStatement = connOffset.prepareStatement(&quot;SELECT offset FROM `kafka_offset` WHERE `topic`=? AND `partition`=?&quot;)    if (parCount &gt; 0) &#123;      for (i &lt;- 0 until parCount) &#123;        psOffsetRead.setString(1, ConfigurationConstants.kafkaConsumerTopics)        psOffsetRead.setInt(2, i)        val rs1 = psOffsetRead.executeQuery()        while (rs1.next()) &#123;          val partitionOffset = rs1.getInt(1)          val tp = TopicAndPartition(ConfigurationConstants.kafkaConsumerTopics, i)          fromOffsets += (tp -&gt; partitionOffset.toLong)        &#125;      &#125;      val messageHandler = (mmd : MessageAndMetadata[String, String]) =&gt; (mmd.topic, mmd.message())      kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder, (String, String)](ssc, kafkaParams, fromOffsets, messageHandler)    &#125;    else &#123;      kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, consumerTopics)    &#125;    ConnectPool.closeCon(psOffsetCnt,connOffset)    ConnectPool.closeCon(psOffsetRead,connOffset)    kafkaStream  &#125;  // 将kafka偏移量写入mysql中保存  def KafkaOffsetSave(offsetRanges: Array[OffsetRange]): Unit = &#123;    val connOffset = ConnectPool.getConnection    connOffset.setAutoCommit(false)    val psOffset: PreparedStatement = connOffset.prepareStatement(&quot;REPLACE INTO `kafka_offset` (`topic`, `partition`, `offset`) VALUES (?,?,?)&quot;)    for (o &lt;- offsetRanges) &#123;      println(s&quot;$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;&quot;)      psOffset.setString(1, o.topic.toString)      psOffset.setInt(2, o.partition.toInt)      psOffset.setLong(3, o.fromOffset.toLong)      psOffset.addBatch()    &#125;    psOffset.executeBatch()    connOffset.commit()    ConnectPool.closeCon(psOffset,connOffset)  &#125;&#125;</code></pre><h2 id="kafka-exactly-once-保证redis"><a href="#kafka-exactly-once-保证redis" class="headerlink" title="kafka exactly-once 保证redis"></a><strong>kafka exactly-once 保证redis</strong></h2><pre><code class="scala">package com.mouse.redisPoolimport com.mouse.ExactlyOnce.GetLog.MyRecordimport com.mouse.ExactlyOnce.&#123;GetLog, RedisClient&#125;import org.apache.kafka.common.TopicPartitionimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.streaming.kafka010._import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import redis.clients.jedis.Pipeline/**  * 注释：  *  * 在Spark Streaming中消费Kafka数据，保证Exactly-once的核心有三点：  * 使用Direct方式连接Kafka；自己保存和维护Offset；更新Offset和计算在同一事务中完成；  *  *  思路步骤：  *  1.启动后，先从Redis中获取上次保存的Offset，Redis中的key为”topic_partition”，即每个分区维护一个Offset；  *  2.使用获取到的Offset，创建DirectStream；  *  3.在处理每批次的消息时，利用Redis的事务机制，确保在Redis中指标的计算和Offset的更新维护，  *  在同一事务中完成。只有这两者同步，才能真正保证消息的Exactly-once。  *  *  注意事项：  *   1.在启动Spark Streaming程序时候，有个参数最好指定：  *   2.spark.streaming.kafka.maxRatePerPartition=20000（每秒钟从topic的每个partition最多消费的消息条数）  *   如果程序第一次运行，或者因为某种原因暂停了很久重新启动时候，会积累很多消息，  *   如果这些消息同时被消费，很有可能会因为内存不够而挂掉，因此，需要根据实际的数据量大小，  *   以及批次的间隔时间来设置该参数，以限定批次的消息量。  *   3.如果该参数设置20000，而批次间隔时间未10秒，那么每个批次最多从Kafka中消费20万消息。  *  *  */object SparkStreaming_ExactlyOnce &#123;  def main(args: Array[String]): Unit = &#123;    val brokers = &quot;kafka:9092&quot;    val topic = &quot;save_redis_offset&quot;    val partition : Int = 0 //测试topic只有一个分区    //Kafka参数    val kafkaParams = Map[String, Object](      &quot;bootstrap.servers&quot; -&gt; brokers,      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],      &quot;group.id&quot; -&gt; &quot;exactly-once&quot;,      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean),      &quot;auto.offset.reset&quot; -&gt; &quot;none&quot;   //letest    )/* * earliest   当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费* latest   当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据* none   topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常*/         // Redis configurations    val maxTotal = 10    val maxIdle = 10    val minIdle = 1    val redisHost = &quot;192.168.1.1&quot;    val redisPort = 6379    val redisTimeout = 30000    //默认db，用户存放Offset和pv数据    val dbDefaultIndex = 8    RedisClient.makePool(redisHost, redisPort, redisTimeout, maxTotal, maxIdle, minIdle)    val conf = new SparkConf().setAppName(&quot;TestSparkStreaming&quot;).setIfMissing(&quot;spark.master&quot;, &quot;local[2]&quot;)    val ssc = new StreamingContext(conf, Seconds(10))    //从Redis获取上一次存的Offset    val jedis = RedisClient.getPool.getResource    jedis.select(dbDefaultIndex)    val topic_partition_key = topic + &quot;_&quot; + partition    var lastOffset = 0L    val lastSavedOffset = jedis.get(topic_partition_key)    if(null != lastSavedOffset) &#123;      try &#123;        lastOffset = lastSavedOffset.toLong      &#125; catch &#123;        case ex : Exception =&gt; println(ex.getMessage)          println(&quot;get lastSavedOffset error, lastSavedOffset from redis [&quot; + lastSavedOffset + &quot;] &quot;)          System.exit(1)      &#125;    &#125;    RedisClient.getPool.returnResource(jedis)    println(&quot;lastOffset from redis -&gt; &quot; + lastOffset)    //设置每个分区起始的Offset    val fromOffsets = Map&#123;new TopicPartition(topic, partition) -&gt; lastOffset&#125;    //使用Direct API 创建Stream    val stream = KafkaUtils.createDirectStream[String, String](      ssc,      LocationStrategies.PreferConsistent,      ConsumerStrategies.Assign[String, String](fromOffsets.keys.toList, kafkaParams, fromOffsets)    )    //开始处理批次消息    stream.foreachRDD &#123;      rdd =&gt;        /**          * 获取 RDD 每一个分区里的offset;          * OffsetRange里面核心字段&#123;            val topic: String,    主题            val partition: Int,   分区            val fromOffset: Long, 该拉去数据的开始偏移量            val untilOffset: Long 该分区拉去数据的最后偏移量            &#125;          */        val offsetRanges: Array[OffsetRange] = rdd.asInstanceOf[HasOffsetRanges].offsetRanges        val result: Array[MyRecord] = GetLog.processLogs(rdd)        println(&quot;=============== Total &quot; + result.length + &quot; events in this batch ..&quot;)        val jedis = RedisClient.getPool.getResource        val p1 : Pipeline = jedis.pipelined();        p1.select(dbDefaultIndex)        p1.multi() //开启事务        //逐条处理消息        result.foreach &#123;          record =&gt;            //增加小时总pv            val pv_by_hour_key = &quot;pv_&quot; + record.hour            p1.incr(pv_by_hour_key)            //增加网站小时pv            val site_pv_by_hour_key = &quot;site_pv_&quot; + record.site_id + &quot;_&quot; + record.hour            p1.incr(site_pv_by_hour_key)            //使用set保存当天的uv            val uv_by_day_key = &quot;uv_&quot; + record.hour.substring(0, 10)            p1.sadd(uv_by_day_key, record.user_id)        &#125;        //更新Offset        offsetRanges.foreach &#123; offsetRange =&gt;          println(&quot;partition : &quot; + offsetRange.partition + &quot; fromOffset:  &quot; + offsetRange.fromOffset + &quot; untilOffset: &quot; + offsetRange.untilOffset)          val topic_partition_key = offsetRange.topic + &quot;_&quot; + offsetRange.partition          p1.set(topic_partition_key, offsetRange.untilOffset.toString)        &#125;        p1.exec();//提交事务        p1.sync();//关闭pipeline        RedisClient.getPool.returnResource(jedis)    &#125;    ssc.start()    ssc.awaitTermination()  &#125;&#125;</code></pre><h2 id="redis常用命令及其含义"><a href="#redis常用命令及其含义" class="headerlink" title="redis常用命令及其含义"></a><strong>redis常用命令及其含义</strong></h2><pre><code class="scala"></code></pre><h2 id="Spark性能优化总结"><a href="#Spark性能优化总结" class="headerlink" title="Spark性能优化总结"></a><strong>Spark性能优化总结</strong></h2><pre><code class="python">0. Overview1. 开发调优   - 避免创建重复的RDD   - 尽可能复用同一个RDD   - 对多次使用的RDD进行持久化   - 尽量避免使用shuffle类算子   - 使用map-side预聚合的shuffle操作   - 使用高性能的算子   - 广播大变量   - 使用Kryo优化序列化性能   - 优化数据结构2. 资源参数调优   - 运行时架构   - 运行流程   - 调优     - executor配置     - driver配置     - 并行度     - 网络超时     - 数据本地化     - JVM/gc配置3. 数据倾斜调优   - 使用Hive ETL预处理数据   - 过滤少数导致倾斜的key   - 提高shuffle操作的并行度   - 两阶段聚合   - 将reduce join转为map join   - 使用随机前缀和扩容RDD进行join4. Shuffle调优   - shuffle原理   - shuffle演进   - 调优   - join类型5. 其他优化项   - 使用DataFrame/DataSet</code></pre><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a><strong>Overview</strong></h4><p>Spark的瓶颈一般来自于集群(standalone, yarn, mesos, k8s)的资源紧张，CPU，网络带宽，内存。通过都会将数据序列化，降低其内存memory和网络带宽shuffle的消耗。</p><blockquote><p>Spark的性能，想要它快，就得充分利用好系统资源，尤其是内存和CPU：核心思想就是能用内存cache就别spill落磁盘，CPU 能并行就别串行，数据能local就别shuffle。</p></blockquote><h4 id="开发调优"><a href="#开发调优" class="headerlink" title="开发调优"></a><strong>开发调优</strong></h4><ol><li><p>避免创建重复的RDD</p></li><li><ul><li>比如多次读可以persist；但如果input太大，persist可能得不偿失</li></ul></li><li><p>尽可能复用同一个RDD</p></li><li><ul><li>但是如果rdd的lineage太长，最好checkpoint下来，避免长重建</li></ul></li><li><p>对多次使用的RDD进行持久化</p></li><li><ul><li>持久化级别（SER，MEM，DISK，_N）</li></ul></li><li><p>尽量避免使用shuffle类算子</p></li><li><ul><li>shuffle算子如distinct（实际调用reduceByKey）、reduceByKey、aggregateByKey、sortByKey、groupByKey、join、cogroup、repartition等，入参中会有一个并行度参数numPartitions</li><li>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key</li></ul></li><li><p>使用map-side预聚合的shuffle操作</p></li><li><ul><li><p>reduceByKey(combiner)，groupByKey(没有combiner)</p><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/202041-91701.jpg"></p></li></ul></li><li><p>使用高性能的算子</p></li><li><ul><li>一边进行重分区的shuffle操作，一边进行排序</li></ul></li><li><ul><li>减少小文件数量</li></ul></li><li><ul><li>特别是在写DB的时候，避免每条写记录都new一个connection；推荐是每个partition new一个connection；更好的是new connection池，每个partition从中取即可，减少partitionNum个new的消耗</li></ul></li><li><ul><li>使用reduceByKey&#x2F;aggregateByKey替代groupByKey</li><li>使用mapPartitions替代普通map</li><li>使用foreachPartitions替代foreach</li><li>使用filter之后进行coalesce操作</li><li>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</li></ul></li><li><p>广播大变量</p></li><li><ul><li>广播变量是executor内所有task共享的，避免了每个task自己维护一个变量，OOM</li></ul></li><li><p>使用Kryo优化序列化性能</p></li><li><p>优化数据结构</p></li><li><ul><li>原始类型(Int, Long)</li><li>字符串，每个字符串内部都有一个字符数组以及长度等额外信息</li><li>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间</li><li>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry</li><li>尽量使用<code>字符串</code>替代<code>对象</code>，使用<code>原始类型</code>（比如Int、Long）替代<code>字符串</code>，使用<code>数组</code>替代<code>集合类型</code>，这样尽可能地减少内存占用，从而降低GC频率，提升性能</li></ul></li></ol><h4 id="资源参数调优"><a href="#资源参数调优" class="headerlink" title="资源参数调优"></a><strong>资源参数调优</strong></h4><p>运行时架构</p><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/202041-92929.jpg" alt="202041-92929"></p><ul><li><p>Client：客户端进程，负责提交作业</p></li><li><p>Driver&#x2F;SC：运行应用程序&#x2F;业务代码的main()函数并且创建SparkContext，其中创建SparkContext的目的是为了准备Spark应用程序的运行环境。在Spark中由SparkContext负责和ClusterManager&#x2F;ResourceManager通信，进行资源的申请、任务的分配和监控等；当Executor部分运行完毕后，Driver负责将SparkContext关闭。通常用SparkContext代表Drive</p></li><li><ul><li>SparkContext：整个应用程序的上下文，控制应用的生命周期</li><li>DAGScheduler：实现将Spark作业分解成一到多个Stage，每个Stage根据RDD的Partition个数决定Task的个数，然后生成相应的Task set放到TaskScheduler中</li><li>TaskScheduler：分配Task到Executor上执行，并维护Task的运行状态</li></ul></li><li><p>Executor：应用程序Application运行在Worker节点上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上。在Spark on Yarn模式下，其进程名称为CoarseGrainedExecutorBackend，负责将Task包装成taskRunner，并从线程池中抽取出一个空闲线程运行Task。每个CoarseGrainedExecutorBackend能并行运行Task的数量就取决于分配给它的CPU的个数</p></li><li><p>Job：一个job包含多个RDD及作用于相应RDD上的各种Operation。每执行一个action算子（foreach, count, collect, take, saveAsTextFile）就会生成一个 job</p></li><li><p>Stage：每个Job会被拆分很多组Task，每组Task被称为Stage，亦称TaskSet。一个作业job分为多个阶段stages（shuffle，串行），一个stage包含一系列的tasks（并行）</p></li><li><p>Task：被送往各个Executor上的执行的内容，task之间无状态传递，可以并行执行</p><p><strong>运行流程</strong></p><ol><li><p>client向YARN的ResourceManager&#x2F;RM申请启动ApplicationMaster&#x2F;AM（单个应用程序&#x2F;作业的资源管理和任务监控）</p></li><li><p>RM收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，spark在此启动其AM，其中AM进行SparkContext&#x2F;SC&#x2F;Driver初始化启动并创建RDD Object、DAGScheduler、TASKScheduler</p></li><li><p>SC根据RDD的依赖关系构建DAG图，并将DAG提交给DAGScheduler解析为<strong>stage</strong>。Stages以TaskSet的形式提交给TaskScheduler，TaskScheduler维护所有TaskSet，当Executor向Driver发送心跳时，TaskScheduler会根据其资源剩余情况分配相应的Task，另外TaskScheduler还维护着所有Task的运行状态，重试失败了的Task</p></li><li><p>AM向RM申请container资源，资源到位后便与NodeManager通信，要求它在获得的Container中(executor)启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向AM中的SC注册并申请Task</p></li><li><p>AM中的SC分配Task给CoarseGrainedExecutorBackend&#x2F;executor执行，CoarseGrainedExecutorBackend运行Task并向AM汇报运行的状态和进度，以便让AM随时掌握各个task的运行状态，从而可以在任务失败时重新启动任务或者推测执行</p></li><li><p>应用程序运行完成后，AM向RM申请注销并关闭自己</p><pre><code class="python">调优### executor配置spark.executor.memoryspark.executor.instancesspark.executor.cores### driver配置spark.driver.memory（如果没有collect操作，一般不需要很大，1~4g即可）spark.driver.cores### 并行度spark.default.parallelism (used for RDD API)spark.sql.shuffle.partitions (usef for DataFrame/DataSet API)### 网络超时spark.network.timeout (所有网络交互的默认超时)### 数据本地化spark.locality.wait### JVM/gc配置spark.executor.extraJavaOptionsspark.driver.extraJavaOptions</code></pre></li></ol></li><li><pre><code class="java">调优executor配置spark.executor.memoryspark.executor.instancesspark.executor.cores    driver配置spark.driver.memory（如果没有collect操作，一般不需要很大，1~4g即可）spark.driver.cores    并行度spark.default.parallelism (used for RDD API)spark.sql.shuffle.partitions (usef for DataFrame/DataSet API)    网络超时spark.network.timeout (所有网络交互的默认超时)    数据本地化spark.locality.wait    JVM/gc配置spark.executor.extraJavaOptionsspark.driver.extraJavaOptions</code></pre></li></ul><h4 id="数据倾斜调优"><a href="#数据倾斜调优" class="headerlink" title="数据倾斜调优"></a><strong>数据倾斜调优</strong></h4><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/202041-93654.jpg" alt="202041-93654"></p><p>使用Hive ETL预处理数据<br>            治标不治本（利用了mr的走disk特性），还多了一条skew pipeline<br>过滤少数导致倾斜的key<br>            但有些场景倾斜是常态<br>提高shuffle操作的并行度<br>            让每个task处理比原来更少的数据（之前可能task会%parNum分到2个key），但是如果单key倾斜，方法失效</p><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/202041-93827.jpg" alt="202041-93827"></p><p>两阶段聚合（局部聚合+全局聚合）<br>            附加随机前缀 -&gt; 局部聚合 -&gt; 去除随机前缀 -&gt; 全局聚合<br>            适用于聚合类shuffle（计算sum，count），但是对于join类shuffle不适用</p><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/微信图片_20200401094020.jpg" alt="微信图片_20200401094020"></p><p>将reduce join转为map join<br>            适用于join类shuffle，因为shuffle变成map操作了<br>            只适用于一个大表和一个小表，将小表广播，并不适合两个都是大表<br>使用随机前缀和扩容RDD进行join<br>            leftDf添加随机前缀(1<del>N的)；复制rightDf每条record至N条并依次打上前缀(1</del>N)<br>            缺点是复制后的rightDf增大了N-1倍</p><h4 id="Shuffle调优"><a href="#Shuffle调优" class="headerlink" title="Shuffle调优"></a><strong>Shuffle调优</strong></h4><p><strong>shuffle原理</strong></p><ul><li><p>Spark在DAG阶段以宽依赖shuffle为界，划分stage，上游stage做map task，每个map task将计算结果数据分成多份，每一份对应到下游stage的每个partition中，并将其临时写到磁盘，该过程叫做shuffle write</p></li><li><p>下游stage做reduce task，每个reduce task通过网络拉取上游stage中所有map task的指定分区结果数据，该过程叫做shuffle read，最后完成reduce的业务逻辑</p></li><li><p>下图中，上游stage有3个map task，下游stage有4个reduce task，那么这3个map task中<strong>每个map task都会产生4份数据</strong>。而4个reduce task中的每个reduce task都会拉取上游3个map task对应的那份数据</p><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/202041-100128.jpg" alt="202041-100128"></p><p><strong>shuffle演进</strong></p><ol><li><p><strong>&lt;0.8</strong> hashBasedShuffle</p></li><li><ul><li>每个map端的task为每个reduce端的partition&#x2F;task生成一个文件，通常会产生大量的文件，伴随大量的随机磁盘IO操作与大量的内存开销<code>M*R</code></li></ul></li><li><p><strong>0.8.1</strong> 引入文件合并File Consolidation机制</p></li><li><ul><li>每个executor为每个reduce端的partition生成一个文件<code>E*R</code></li></ul></li><li><p><strong>0.9</strong> 引入External AppendOnlyMap</p></li><li><ul><li>combine时可以将数据spill到磁盘，然后通过堆排序merge</li></ul></li><li><p><strong>1.1</strong> 引入sortBasedShuffle</p></li><li><ul><li>每个map task不会为每个reducer task生成一个单独的文件，而是会将所有的结果写到一个文件里，同时会生成一个index文件，reducer可以通过这个index文件取得它需要处理的数据<code>M</code></li></ul></li><li><p><strong>1.4</strong> 引入Tungsten-Sort Based Shuffle</p></li><li><ul><li>亦称unsafeShuffle，将数据记录用序列化的二进制方式存储，把排序转化成指针数组的排序，引入堆外内存空间和新的内存管理模型</li></ul></li><li><p><strong>1.6</strong> Tungsten-sort并入Sort Based Shuffle</p></li><li><ul><li>由SortShuffleManager自动判断选择最佳Shuffle方式，如果检测到满足Tungsten-sort条件会自动采用Tungsten-sort Based Shuffle，否则采用Sort Based Shuffle</li></ul></li><li><p><strong>2.0</strong> hashBasedShuffle退出历史舞台</p></li><li><ul><li><p>从此Spark只有sortBasedShuffle</p><pre><code class="python">调优shuffle是一个涉及到CPU（序列化反序列化）、网络IO（跨节点数据传输）以及磁盘IO（shuffle中间结果落盘）的操作。所以用户在编写Spark应用程序的过程中应当尽可能避免shuffle算子和考虑shuffle相关的优化，提升spark应用程序的性能。要减少shuffle的开销，主要有两个思路，减少shuffle次数，尽量不改变key，把数据处理在local完成减少shuffle的数据规模先去重，再合并A.union(B).distinct() vs. A.distinct().union(B.distinct()).distinct()用broadcast + filter来代替joinspark.shuffle.file.buffer设置shuffle write task的buffer大小，将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘spark.reducer.maxSizeInFlight设置shuffle read task的buffer大小，决定了每次能够拉取pull多少数据。减少拉取数据的次数，也就减少了网络传输的次数spark.shuffle.sort.bypassMergeThresholdshuffle read task的数量小于这个阈值（默认是200），则map-side/shuffle write过程中不会进行排序操作</code></pre></li></ul></li></ol></li></ul><h4 id="Spark的join类型"><a href="#Spark的join类型" class="headerlink" title="Spark的join类型"></a><strong>Spark的join类型</strong></h4><p>Shuffled Hash Join<br>Sort Merge Join<br>Broadcast Join</p><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/微信图片_20200401100531.jpg" alt="微信图片_20200401100531"></p><h4 id="其他优化项"><a href="#其他优化项" class="headerlink" title="其他优化项"></a><strong>其他优化项</strong></h4><ol><li><p>使用DataFrame&#x2F;DataSet</p></li><li><ul><li><p>spark sql 的catalyst优化器，</p></li><li><p>堆外内存（有了Tungsten后，感觉off-head没有那么明显的性能提升了）</p><table><thead><tr><th>**Type **</th><th>**RDD **</th><th><em><strong>*DataFrame*</strong></em></th><th><em><strong>*DataSet*</strong></em></th></tr></thead><tbody><tr><td>definition</td><td>RDD是分布式的Java对象的集合</td><td>DataFrame是分布式的Row对象的集合</td><td>DataSet是分布式的Java对象的集合 ds &#x3D; df.as[ElementType] df &#x3D; Dataset[Row]</td></tr><tr><td>pros</td><td>* 编译时类型安全 * 面向对象的编程风格</td><td>* 引入schema结构信息 * 减少数据读取，优化执行计划，如filter下推，剪裁 * off-heap堆外存储</td><td>* Encoder序列化 * 支持结构与非结构化数据 * 和rdd一样，支持自定义对象存储 * 和dataframe一样，支持结构化数据的sql查询 * 采用堆外内存存储，gc友好 * 类型转化安全，代码有好</td></tr><tr><td>cons</td><td>* 对于结构化数据不友好 * 默认采用的是java序列化方式，序列化结果比较大，而且数据存储在java堆内存中，导致gc比较频繁</td><td>* rdd内部数据直接以java对象存储，dataframe内存存储的是Row对象而不能是自定义对象 * 编译时不能类型转化安全检查，运行时才能确定是否有问题</td><td>* 可能需要额外定义Encoder</td></tr></tbody></table></li></ul></li></ol><h2 id="Kafka知识补充"><a href="#Kafka知识补充" class="headerlink" title="Kafka知识补充"></a>Kafka知识补充</h2><pre><code class="python">session.timeout.ms    默认是10000ms，会话超时时间。当我们使用consumer_group的模式进行消费时，kafka如果检测到某个consumer挂掉，就会记性rebalance。consumer每隔一段时间(heartbeat.interval.ms)给broker发送心跳消息，如果超过这个时间没有发送，broker就会认为这个consumer挂了。这个参数的有效取值范围是broker端的设group.min.session.timeout.ms(6000)和group.max.session.timeout.ms(300000)之间。    max.poll.interval.ms    在使用消费者组管理时，调用poll（）之间的最大延迟。这提出了消费者在获取更多记录之前可以闲置的时间量的上界。如果在此超时到期之前未调用poll（），则认为使用者失败，并且组将重新平衡以将分区重新分配给其他成员。    max.partition.fetch.bytes    一次fetch请求，从一个partition中取得的records最大大小。如果在从topic中第一个非空的partition取消息时，如果取到的第一个record的大小就超过这个配置时，仍然会读取这个record，也就是说在这片情况下，只会返回这一条record。 broker、topic都会对producer发给它的message size做限制。所以在配置这值时，可以参考broker的message.max.bytes 和 topic的max.message.bytes的配置.    session.timeout.ms是针对这个线程到底能不能按时发送心跳的。但是如果这个线程运行正常，但是消费线程挂了呢？这就无法检测了啊。所以就引进了max.poll.interval.ms，用来解决这个问题。    heartbeat.interval.ms    在使用Kafka的团队管理设施时，心跳与团队协调员之间的预期时间。心跳信号用于确保工作人员的会话保持活动状态，并便于在新成员加入或离开组时重新平衡。该值必须设置为低于session.timeout.ms，但通常应设置为不高于该值的1/3。它可以调整得更低，以控制正常再平衡的预期时间。    ### timestamp_index文件的作用和理解？？？##</code></pre><h2 id="Flink-WaterMark的理解"><a href="#Flink-WaterMark的理解" class="headerlink" title="Flink WaterMark的理解"></a><strong>Flink WaterMark的理解</strong></h2><pre><code class="java">一种延迟触发窗口机制。当前数据流中最大时间-延迟时间  &gt;= 窗口最大时间则触发窗口执行。是一种衡量Event Time进展的机制，它是数据本身的一个隐藏属性。通常基于Event Time的数据，自身都包含一个timestamp.watermark是用于处理乱序事件的，而正确的处理乱序事件，通常用watermark机制结合window来实现。watermark+window机制。window中可以对input进行按照Event Time排序，使得完全按照Event Time发生的顺序去处理数据，以达到处理乱序数据的目的。    基于Event Time的事件处理，Flink默认的事件触发条件为:对于out-of-order及正常的数据而言    watermark的时间戳 &gt; = window endTime    在 [window_start_time,window_end_time] 中有数据存在。对于late element太多的数据而言    Event Time &gt; watermark的时间戳AssignerWithPeriodicWatermarks   定时提取watermark，这种方式会定时提取更新wartermark。    AssignerWithPunctuatedWatermarks  伴随event的到来就提取watermark，就是每一个event到来的时候，就会提取一次Watermark。这样的方式当然设置watermark更为精准，但是当数据量大的时候，频繁的更新wartermark会比较影响性能。通常情况下采用定时提取就足够了。</code></pre><h2 id="Hbase的Region划分"><a href="#Hbase的Region划分" class="headerlink" title="Hbase的Region划分"></a><strong>Hbase的Region划分</strong></h2><h2 id="ES-的Setting参数和优化"><a href="#ES-的Setting参数和优化" class="headerlink" title="ES 的Setting参数和优化"></a><strong>ES 的Setting参数和优化</strong></h2><pre><code class="java">&#123;-    &quot;app_centurily_cbest_order_data_2020-06-12&quot;:&#123;        &quot;settings&quot;:&#123;-            &quot;index&quot;:&#123;-                &quot;routing&quot;:&#123;-                    &quot;reblance&quot;:&#123;-                        &quot;enable&quot;:&quot;none&quot;                    &#125;,                    &quot;allocation&quot;:&#123;-                        &quot;total_shards_per_node&quot;:&quot;4&quot;                    &#125;                &#125;,                &quot;refresh_interval&quot;:&quot;120s&quot;,                --主分片数，默认为5.只能在创建索引时设置，不能修改                &quot;number_of_shards&quot;: &quot;20&quot;,                                &quot;translog&quot;:&#123;-                    &quot;flush_threshold_size&quot;:&quot;1024mb&quot;,                    &quot;sync_interval&quot;:&quot;120s&quot;,                    &quot;durability&quot;:&quot;async&quot;                &#125;,                --减少磁盘争用（ssd可以忽略该条设置）                &quot;merge&quot;:&#123;-                    &quot;scheduler&quot;:&#123;-                        &quot;max_thread_count&quot;:&quot;1&quot;                    &#125;                &#125;                &quot;provided_name&quot;:&quot;app_centurily_cbest_order_data_2020-06-12&quot;,                &quot;max_result_window&quot;:&quot;100000000&quot;,                &quot;create_date&quot;:&quot;1591923600447&quot;,                -- 每个主分片的副本数。默认为 1。                &quot;number_of_replicas&quot;:&quot;1&quot;,                &quot;uuid&quot;:&quot;wGzBij23445454VpQ&quot;,                &quot;version&quot;:&#123;-                    &quot;created&quot;:&quot;7070099&quot;                &#125;            &#125;        &#125;    &#125;&#125;</code></pre><p>1：静态设置：</p><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>index.number_of_shards</td><td>主分片数，默认为5.只能在创建索引时设置，不能修改</td></tr><tr><td>index.shard.check_on_startup</td><td>是否应在索引打开前检查分片是否损坏，当检查到分片损坏将禁止分片被打开。false：默认值；checksum：检查物理损坏；true:检查物理和逻辑损坏，这将消耗大量内存和CPU；fix：检查物理和逻辑损坏。有损坏的分片将被集群自动删除，这可能导致数据丢失</td></tr><tr><td>index.routing_partition_size</td><td>自定义路由值可以转发的目的分片数。默认为 1，只能在索引创建时设置。此值必须小于index.number_of_shards</td></tr><tr><td>index.codec</td><td>默认使用LZ4压缩方式存储数据，也可以设置为 best_compression，它使用 DEFLATE 方式以牺牲字段存储性能为代价来获得更高的压缩比例</td></tr></tbody></table><p>2.动态设置</p><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>index.number_of_replicas</td><td>每个主分片的副本数。默认为 1。</td></tr><tr><td>index.auto_expand_replicas</td><td>基于可用节点的数量自动分配副本数量,默认为 false（即禁用此功能）</td></tr><tr><td>index.refresh_interval</td><td>执行刷新操作的频率，这使得索引的最近更改可以被搜索。默认为 1s。可以设置为 -1 以禁用刷新</td></tr><tr><td>index.max_result_window</td><td>用于索引搜索的 from+size 的最大值。默认为 10000</td></tr><tr><td>index.max_rescore_window</td><td>在搜索此索引中 rescore 的 window_size 的最大值</td></tr><tr><td>index.blocks.read_only</td><td>设置为 true 使索引和索引元数据为只读，false 为允许写入和元数据更改。</td></tr><tr><td>index.blocks.read</td><td>设置为 true 可禁用对索引的读取操作</td></tr><tr><td>index.blocks.write</td><td>设置为 true 可禁用对索引的写入操作</td></tr><tr><td>index.blocks.metadata</td><td>设置为 true 可禁用对索引元数据的读取和写入</td></tr><tr><td>index.max_refresh_listeners</td><td>索引的每个分片上可用的最大刷新侦听器数</td></tr></tbody></table><h2 id="ES-Mapping的参数说明和优化"><a href="#ES-Mapping的参数说明和优化" class="headerlink" title="ES Mapping的参数说明和优化"></a><strong>ES Mapping的参数说明和优化</strong></h2><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/20180123193245289.png" alt="20180123193245289"></p><pre><code class="python">Mapping类似数据库中的表结构定义，主要作用如下：    定义Index下的字段名（Field Name）    定义字段的类型，比如数据型、字符串型、布尔型等    定义倒排索引相关配置，比如是否索引、记录position等    自定义mapping：Mapping中字段类型一旦设定后，禁止直接修改（Lucene实现的倒排索引生成后不允许修改）重新建立新的索引，然后做reindex操作允许新增字段## 通过dynamic参数来控制字段的新增 ##     true:默认值，允许自动新增字段    false：不允许自动新增字段，但是文档可以正常写入，但无法对字段进行查询等操作    strict：文档不能写入，报错        &#123;    &quot;zhidao_index&quot;: &#123;        &quot;mappings&quot;: &#123;            &quot;zhidao_type&quot;: &#123;                &quot;_ttl&quot;: &#123;                    &quot;enabled&quot;: false                &#125;,                &quot;properties&quot;: &#123;                    &quot;answer&quot;: &#123;                        &quot;type&quot;: &quot;string&quot;,                        &quot;index&quot;: &quot;not_analyzed&quot;                    &#125;,                    &quot;answerAuthor&quot;: &#123;                        &quot;type&quot;: &quot;string&quot;                    &#125;,                    &quot;answerDate&quot;: &#123;                        &quot;type&quot;: &quot;date&quot;,                        &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot;//这里出现了复合类型                    &#125;,                    &quot;answer_author&quot;: &#123;                        &quot;type&quot;: &quot;string&quot;,                        &quot;index&quot;: &quot;not_analyzed&quot;                    &#125;,                    &quot;answer_date&quot;: &#123;                        &quot;type&quot;: &quot;string&quot;,                        &quot;index&quot;: &quot;not_analyzed&quot;                    &#125;,                    &quot;author&quot;: &#123;                        &quot;type&quot;: &quot;string&quot;,                        &quot;index&quot;: &quot;not_analyzed&quot;                    &#125;,                    &quot;category&quot;: &#123;                        &quot;type&quot;: &quot;string&quot;,                        &quot;index&quot;: &quot;not_analyzed&quot;                    &#125;,                    &quot;date&quot;: &#123;                        &quot;type&quot;: &quot;string&quot;,                        &quot;index&quot;: &quot;not_analyzed&quot;                    &#125;,                    &quot;description&quot;: &#123;                        &quot;type&quot;: &quot;string&quot;,                        &quot;index&quot;: &quot;not_analyzed&quot;                    &#125;,                    &quot;id&quot;: &#123;                        &quot;type&quot;: &quot;string&quot;,                        &quot;index&quot;: &quot;not_analyzed&quot;                    &#125;,                    &quot;keywords&quot;: &#123;                        &quot;type&quot;: &quot;string&quot;,                        &quot;index&quot;: &quot;not_analyzed&quot;                    &#125;,                    &quot;list&quot;: &#123;                        &quot;type&quot;: &quot;object&quot;                    &#125;,                    &quot;question&quot;: &#123;                        &quot;type&quot;: &quot;string&quot;,                        &quot;index&quot;: &quot;not_analyzed&quot;                    &#125;,                    &quot;readCount&quot;: &#123;                        &quot;type&quot;: &quot;long&quot;                    &#125;,                    &quot;read_count&quot;: &#123;                        &quot;type&quot;: &quot;integer&quot;                    &#125;,                    &quot;title&quot;: &#123;                        &quot;type&quot;: &quot;string&quot;                    &#125;                &#125;            &#125;        &#125;    &#125;&#125;</code></pre><p>1.基本数据类型</p><table><thead><tr><th>类型</th><th>说明</th></tr></thead><tbody><tr><td>字符串类型</td><td>string,text,keyword</td></tr><tr><td>整数类型</td><td>integer,long,short,byte</td></tr><tr><td>浮点类型</td><td>double,float,half_float,scaled_float</td></tr><tr><td>逻辑类型</td><td>boolean</td></tr><tr><td>日期类型</td><td>date</td></tr><tr><td>范围类型</td><td>range</td></tr><tr><td>二进制类型</td><td>binary</td></tr></tbody></table><p>分片查询</p><p>Es会将数据均衡的存储在分片中，我们可以指定es去具体的分片或节点钟查询从而进一步的实现es极速查询。</p><p><strong>1：randomizeacross shards</strong></p><p>随机选择分片查询数据，es的默认方式</p><p><strong>2：_local</strong></p><p>优先在本地节点上的分片查询数据然后再去其他节点上的分片查询，本地节点没有IO问题但有可能造成负载不均问题。数据量是完整的。</p><p><strong>3：_primary</strong></p><p>只在主分片中查询不去副本查，一般数据完整。</p><p><strong>4：_primary_first</strong></p><p>优先在主分片中查，如果主分片挂了则去副本查，一般数据完整。</p><p><strong>5：_only_node</strong></p><p>只在指定id的节点中的分片中查询，数据可能不完整。</p><p><strong>6：_prefer_node</strong></p><p>优先在指定你给节点中查询，一般数据完整。</p><p><strong>7：_shards</strong></p><p>在指定分片中查询，数据可能不完整。</p><p><strong>8：_only_nodes</strong></p><p>可以自定义去指定的多个节点查询，es不提供此方式需要改源码。</p><p><a href="https://segmentfault.com/a/1190000019057909">https://segmentfault.com/a/1190000019057909</a></p><h2 id="Kylin的性能优化"><a href="#Kylin的性能优化" class="headerlink" title="Kylin的性能优化"></a><strong>Kylin的性能优化</strong></h2><h2 id="Flink性能调优"><a href="#Flink性能调优" class="headerlink" title="Flink性能调优"></a>Flink性能调优</h2><h2 id="HiveSQL常用优化方法全面总结"><a href="#HiveSQL常用优化方法全面总结" class="headerlink" title="HiveSQL常用优化方法全面总结"></a>HiveSQL常用优化方法全面总结</h2><p><strong>优化手段</strong></p><pre><code class="python">## 列裁剪和分区裁剪所谓列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。以我们的日历记录表为例：select uid,event_type,record_datafrom calendar_record_logwhere pt_date &gt;= 20190201 and pt_date &lt;= 20190224and status = 0;select *或者不指定分区，全列扫描和全表扫描效率都很低。Hive中与列裁剪优化相关的配置项是hive.optimize.cp，与分区裁剪优化相关的则是hive.optimize.pruner，默认都是true。在HiveQL解析阶段对应的则是ColumnPruner逻辑优化器。## 谓词下推它就是将SQL语句中的where谓词逻辑都尽可能提前执行，减少下游处理的数据量。例如以下HiveQL语句select a.uid,a.event_type,b.topic_id,b.titlefrom calendar_record_log aleft outer join (  select uid,topic_id,title from forum_topic  where pt_date = 20190224 and length(content) &gt;= 100) b on a.uid = b.uidwhere a.pt_date = 20190224 and status = 0;对forum_topic做过滤的where语句写在子查询内部，而不是外部。Hive中有谓词下推优化的配置项hive.optimize.ppd，默认值true，与它对应的逻辑优化器是PredicatePushDown。该优化器就是将OperatorTree中的FilterOperator向上提，见下图。</code></pre><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200615231829.jpg"></p><pre><code class="python">上面的链接中是一篇讲解HiveQL解析与执行过程的好文章，前文提到的优化器、OperatorTree等概念在其中也有详细的解释，非常推荐。## sort by代替order byHiveQL中的order by与其他SQL方言中的功能一样，就是将结果按某字段全局排序，这会导致所有map端数据都进入一个reducer中，在数据量大时可能会长时间计算不完。如果使用sort by，那么还是会视情况启动多个reducer进行排序，并且保证每个reducer内局部有序。为了控制map端数据分配到reducer的key，往往还要配合distribute by一同使用。如果不加distribute by的话，map端数据就会随机分配到reducer。举个例子，假如要以UID为key，以上传时间倒序、记录类型倒序输出记录数据：select uid,upload_time,event_type,record_datafrom calendar_record_logwhere pt_date &gt;= 20190201 and pt_date &lt;= 20190224distribute by uidsort by upload_time desc,event_type desc;.## group by代替distinct当要统计某一列的去重数时，如果数据量很大，count(distinct)就会非常慢，原因与order by类似，count(distinct)逻辑只会有很少的reducer来处理。这时可以用group by来改写：select count(1) from (  select uid from calendar_record_log  where pt_date &gt;= 20190101  group by uid) t;但是这样写会启动两个MR job（单纯distinct只会启动一个），所以要确保数据量大到启动job的overhead远小于计算耗时，才考虑这种方法。当数据集很小或者key的倾斜比较明显时，group by还可能会比distinct慢。那么如何用group by方式同时统计多个列？下面是解决方法：select t.a,sum(t.b),count(t.c),count(t.d) from (  select a,b,null c,null d from some_table  union all  select a,0 b,c,null d from some_table group by a,c  union all  select a,0 b,null c,d from some_table group by a,d) t;## group by配置调整    # - map端预聚合    group by时，如果先起一个combiner在map端做部分预聚合，可以有效减少shuffle数据量。预聚合的配置项是hive.map.aggr，默认值true，对应的优化器为GroupByOptimizer，简单方便。通过hive.groupby.mapaggr.checkinterval参数也可以设置map端预聚合的行数阈值，超过该值就会分拆job，默认值100000。    # - 倾斜均衡配置项    group by时如果某些key对应的数据量过大，就会发生数据倾斜。Hive自带了一个均衡数据倾斜的配置项hive.groupby.skewindata，默认值false。其实现方法是在group by时启动两个MR job。第一个job会将map端数据随机输入reducer，每个reducer做部分聚合，相同的key就会分布在不同的reducer中。第二个job再将前面预处理过的数据按key聚合并输出结果，这样就起到了均衡的效果。但是，配置项毕竟是死的，单纯靠它有时不能根本上解决问题，因此还是建议自行了解数据倾斜的细节，并优化查询语句。## join基础优化    # - build table（小表）前置    在最常见的hash join方法中，一般总有一张相对小的表和一张相对大的表，小表叫build table，大表叫probe table。如下图所示：</code></pre><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200615233237.jpg"></p><pre><code class="python">Hive在解析带join的SQL语句时，会默认将最后一个表作为probe table，将前面的表作为build table并试图将它们读进内存。如果表顺序写反，probe table在前面，引发OOM的风险就高了。在维度建模数据仓库中，事实表就是probe table，维度表就是build table。假设现在要将日历记录事实表和记录项编码维度表来join：select a.event_type,a.event_code,a.event_desc,b.upload_timefrom calendar_event_code ainner join (  select event_type,upload_time from calendar_record_log  where pt_date = 20190225) b on a.event_type = b.event_type;# - 多表join时key相同这种情况会将多个join合并为一个MR job来处理，例如：select a.event_type,a.event_code,a.event_desc,b.upload_timefrom calendar_event_code ainner join (  select event_type,upload_time from calendar_record_log  where pt_date = 20190225) b on a.event_type = b.event_typeinner join (  select event_type,upload_time from calendar_record_log_2  where pt_date = 20190225) c on a.event_type = c.event_type;如果上面两个join的条件不相同，比如改成a.event_code = c.event_code，就会拆成两个MR job计算。负责这个的是相关性优化器CorrelationOptimizer，它的功能除此之外还非常多，逻辑复杂，参考Hive官方的文档可以获得更多细节：https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer        # - 利用map join特性 map join特别适合大小表join的情况。Hive会将build table和probe table在map端直接完成join过程，消灭了reduce，效率很高。    select /*+mapjoin(a)*/ a.event_type,b.upload_time    from calendar_event_code a    inner join (          select event_type,upload_time from calendar_record_log          where pt_date = 20190225    ) b on a.event_type &lt; b.event_type;上面的语句中加了一条map join hint，以显式启用map join特性。早在Hive 0.8版本之后，就不需要写这条hint了。map join还支持不等值连接，应用更加灵活。map join的配置项是hive.auto.convert.join，默认值true，对应逻辑优化器是MapJoinProcessor。还有一些参数用来控制map join的行为，比如hive.mapjoin.smalltable.filesize，当build table大小小于该值就会启用map join，默认值25000000（25MB）。还有hive.mapjoin.cache.numrows，表示缓存build table的多少行数据到内存，默认值25000。    # - 分桶表map joinmap join对分桶表还有特别的优化。由于分桶表是基于一列进行hash存储的，因此非常适合抽样（按桶或按块抽样）。它对应的配置项是hive.optimize.bucketmapjoin，优化器是BucketMapJoinOptimizer。但我们的业务中用分桶表较少，所以就不班门弄斧了，只是提一句。sort merge bucket join    # - 倾斜均衡配置项    这个配置与上面group by的倾斜均衡配置项异曲同工，通过hive.optimize.skewjoin来配置，默认false。如果开启了，在join过程中Hive会将计数超过阈值hive.skewjoin.key（默认100000）的倾斜key对应的行临时写进文件中，然后再启动另一个job做map join生成结果。通过hive.skewjoin.mapjoin.map.tasks参数还可以控制第二个job的mapper数量，默认10000。再重复一遍，通过自带的配置项经常不能解决数据倾斜问题。join是数据倾斜的重灾区，后面还要介绍在SQL层面处理倾斜的各种方法。## 优化SQL处理join数据倾斜上面已经多次提到了数据倾斜，包括已经写过的sort by代替order by，以及group by代替distinct方法，本质上也是为了解决它。join操作更是数据倾斜的重灾区，需要多加注意。    # - 空值或无意义值    这种情况很常见，比如当事实表是日志类数据时，往往会有一些项没有记录到，我们视情况会将它置为null，或者空字符串、-1等。如果缺失的项很多，在做join时这些空值就会非常集中，拖累进度。因此，若不需要空值数据，就提前写where语句过滤掉。需要保留的话，将空值key用随机方式打散，例如将用户ID为null的记录随机改为负值：select a.uid,a.event_type,b.nickname,b.agefrom (  select   (case when uid is null then cast(rand()*-10240 as int) else uid end) as uid,  event_type from calendar_record_log  where pt_date &gt;= 20190201) a left outer join (  select uid,nickname,age from user_info where status = 4) b on a.uid = b.uid;    # - 单独处理倾斜key    这其实是上面处理空值方法的拓展，不过倾斜的key变成了有意义的。一般来讲倾斜的key都很少，我们可以将它们抽样出来，对应的行单独存入临时表中，然后打上一个较小的随机数前缀（比如0~9），最后再进行聚合。SQL语句与上面的相仿，不再赘述。        # - 不同数据类型    这种情况不太常见，主要出现在相同业务含义的列发生过逻辑上的变化时。举个例子，假如我们有一旧一新两张日历记录表，旧表的记录类型字段是(event_type int)，新表的是(event_type string)。为了兼容旧版记录，新表的event_type也会以字符串形式存储旧版的值，比如&#39;17&#39;。当这两张表join时，经常要耗费很长时间。其原因就是如果不转换类型，计算key的hash值时默认是以int型做的，这就导致所有“真正的”string型key都分配到一个reducer上。所以要注意类型转换：elect a.uid,a.event_type,b.record_datafrom calendar_record_log aleft outer join (  select uid,event_type from calendar_record_log_2  where pt_date = 20190228) b on a.uid = b.uid and b.event_type = cast(a.event_type as string)where a.pt_date = 20190228;    # - build table过大    有时，build table会大到无法直接使用map join的地步，比如全量用户维度表，而使用普通join又有数据分布不均的问题。这时就要充分利用probe table的限制条件，削减build table的数据量，再使用map join解决。代价就是需要进行两次join。举个例子：    select /*+mapjoin(b)*/ a.uid,a.event_type,b.status,b.extra_info    from calendar_record_log a    left outer join (          select /*+mapjoin(s)*/ t.uid,t.status,t.extra_info          from (select distinct uid from calendar_record_log where pt_date = 20190228) s          inner join user_info t on s.uid = t.uid    ) b on a.uid = b.uidwhere a.pt_date = 20190228;## MapReduce优化</code></pre><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200615234331.jpg"></p><pre><code class="python">    # -调整mapper数mapper数量与输入文件的split数息息相关，在Hadoop源码org.apache.hadoop.mapreduce.lib.input.FileInputFormat类中可以看到split划分的具体逻辑。这里不贴代码，直接叙述mapper数是如何确定的。    可以直接通过参数mapred.map.tasks（默认值2）来设定mapper数的期望值，但它不一定会生效，下面会提到。    设输入文件的总大小为total_input_size。HDFS中，一个块的大小由参数dfs.block.size指定，默认值64MB或128MB。在默认情况下，mapper数就是：default_mapper_num = total_input_size / dfs.block.size。    参数mapred.min.split.size（默认值1B）和mapred.max.split.size（默认值64MB）分别用来指定split的最小和最大大小。split大小和split数计算规则是：split_size = MAX(mapred.min.split.size, MIN(mapred.max.split.size, dfs.block.size))；split_num = total_input_size / split_size。    得出mapper数：mapper_num = MIN(split_num, MAX(default_num, mapred.map.tasks))。可见，如果想减少mapper数，就适当调高mapred.min.split.size，split数就减少了。如果想增大mapper数，除了降低mapred.min.split.size之外，也可以调高mapred.map.tasks。一般来讲，如果输入文件是少量大文件，就减少mapper数；如果输入文件是大量非小文件，就增大mapper数；至于大量小文件的情况，得参考下面“合并小文件”一节的方法处理。    # -调整reducer数reducer数量的确定方法比mapper简单得多。使用参数mapred.reduce.tasks可以直接设定reducer数量，不像mapper一样是期望值。但如果不设这个参数的话，Hive就会自行推测，逻辑如下：    参数hive.exec.reducers.bytes.per.reducer用来设定每个reducer能够处理的最大数据量，默认值1G（1.2版本之前）或256M（1.2版本之后）。    参数hive.exec.reducers.max用来设定每个job的最大reducer数量，默认值999（1.2版本之前）或1009（1.2版本之后）。    得出reducer数：reducer_num = MIN(total_input_size / reducers.bytes.per.reducer, reducers.max)。reducer数量与输出文件的数量相关。如果reducer数太多，会产生大量小文件，对HDFS造成压力。如果reducer数太少，每个reducer要处理很多数据，容易拖慢运行时间或者造成OOM。    # -合并小文件输入阶段合并    需要更改Hive的输入文件格式，即参数hive.input.format，默认值是org.apache.hadoop.hive.ql.io.HiveInputFormat，我们改成org.apache.hadoop.hive.ql.io.CombineHiveInputFormat。这样比起上面调整mapper数时，又会多出两个参数，分别是mapred.min.split.size.per.node和mapred.min.split.size.per.rack，含义是单节点和单机架上的最小split大小。如果发现有split大小小于这两个值（默认都是100MB），则会进行合并。具体逻辑可以参看Hive源码中的对应类。输出阶段合并    直接将hive.merge.mapfiles和hive.merge.mapredfiles都设为true即可，前者表示将map-only任务的输出合并，后者表示将map-reduce任务的输出合并。另外，hive.merge.size.per.task可以指定每个task输出后合并文件大小的期望值，hive.merge.size.smallfiles.avgsize可以指定所有输出文件大小的均值阈值，默认值都是1GB。如果平均大小不足的话，就会另外启动一个任务来进行合并。    # -启用压缩压缩job的中间结果数据和输出数据，可以用少量CPU时间节省很多空间。压缩方式一般选择Snappy，效率最高。要启用中间压缩，需要设定hive.exec.compress.intermediate为true，同时指定压缩方式hive.intermediate.compression.codec为org.apache.hadoop.io.compress.SnappyCodec。另外，参数hive.intermediate.compression.type可以选择对块（BLOCK）还是记录（RECORD）压缩，BLOCK的压缩率比较高。输出压缩的配置基本相同，打开hive.exec.compress.output即可。    # -JVM重用在MR job中，默认是每执行一个task就启动一个JVM。如果task非常小而碎，那么JVM启动和关闭的耗时就会很长。可以通过调节参数mapred.job.reuse.jvm.num.tasks来重用。例如将这个参数设成5，那么就代表同一个MR job中顺序执行的5个task可以重复使用一个JVM，减少启动和关闭的开销。但它对不同MR job中的task无效。## 并行执行与本地模式并行执行    Hive中互相没有依赖关系的job间是可以并行执行的，最典型的就是多个子查询union all。在集群资源相对充足的情况下，可以开启并行执行，即将参数hive.exec.parallel设为true。另外hive.exec.parallel.thread.number可以设定并行执行的线程数，默认为8，一般都够用。本地模式    Hive也可以不将任务提交到集群进行运算，而是直接在一台节点上处理。因为消除了提交到集群的overhead，所以比较适合数据量很小，且逻辑不复杂的任务。设置hive.exec.mode.local.auto为true可以开启本地模式。但任务的输入数据总量必须小于hive.exec.mode.local.auto.inputbytes.max（默认值128MB），且mapper数必须小于hive.exec.mode.local.auto.tasks.max（默认值4），reducer数必须为0或1，才会真正用本地模式执行。## 严格模式所谓严格模式，就是强制不允许用户执行3种有风险的HiveQL语句，一旦执行会直接失败。这3种语句是：查询分区表时不限定分区列的语句；    两表join产生了笛卡尔积的语句；    用order by来排序但没有指定limit的语句。    要开启严格模式，需要将参数hive.mapred.mode设为strict## 采用合适的存储格式在HiveQL的create table语句中，可以使用stored as ...指定表的存储格式。Hive表支持的存储格式有TextFile、SequenceFile、RCFile、Avro、ORC、Parquet等。存储格式一般需要根据业务进行选择，在我们的实操中，绝大多数表都采用TextFile与Parquet两种存储格式之一。TextFile是最简单的存储格式，它是纯文本记录，也是Hive的默认格式。虽然它的磁盘开销比较大，查询效率也低，但它更多地是作为跳板来使用。RCFile、ORC、Parquet等格式的表都不能由文件直接导入数据，必须由TextFile来做中转。Parquet和ORC都是Apache旗下的开源列式存储格式。列式存储比起传统的行式存储更适合批量OLAP查询，并且也支持更好的压缩和编码。我们选择Parquet的原因主要是它支持Impala查询引擎，并且我们对update、delete和事务性操作需求很低。这里就不展开讲它们的细节，可以参考各自的官网：https://parquet.apache.org/https://orc.apache.org/</code></pre><h2 id="spark数据倾斜"><a href="#spark数据倾斜" class="headerlink" title="spark数据倾斜"></a><strong>spark数据倾斜</strong></h2><p>场景1：大量不同的Key被分配到了相同的Task造成该Task数据量过大。<br>        ***案例***<br>现有一张测试表，名为student_external，内有10.5亿条数据，每条数据有一个唯一的id值。现从中取出id取值为9亿到10.5亿的共1.5条数据，并通过一些处理，使得id为9亿到9.4亿间的所有数据对12取模后余数为8（即在Shuffle并行度为12时该数据集全部被HashPartition分配到第8个Task），其它数据集对其id除以100取整，从而使得id大于9.4亿的数据在Shuffle时可被均匀分配到所有Task中，而id小于9.4亿的数据全部分配到同一个Task中。处理过程如</p><pre><code class="sql">INSERT OVERWRITE TABLE testSELECT CASE WHEN id &lt; 940000000 THEN (9500000  + (CAST (RAND() * 8 AS INTEGER)) * 12 )       ELSE CAST(id/100 AS INTEGER)       END,       nameFROM student_externalWHERE id BETWEEN 900000000 AND 1050000000;</code></pre><p>这种方式可能会造成数据倾斜。接下来，使用Spark读取该测试数据，并通过*groupByKey(12)*对id分组处理，且Shuffle并行度为12。代码如下：</p><pre><code class="python">public class SparkDataSkew &#123;  public static void main(String[] args) &#123;    SparkSession sparkSession = SparkSession.builder()      .appName(&quot;SparkDataSkewTunning&quot;)      .config(&quot;hive.metastore.uris&quot;, &quot;thrift://hadoop1:9083&quot;)      .enableHiveSupport()      .getOrCreate();    Dataset&lt;Row&gt; dataframe = sparkSession.sql( &quot;select * from test&quot;);    dataframe.toJavaRDD()      .mapToPair((Row row) -&gt; new Tuple2&lt;Integer, String&gt;(row.getInt(0),row.getString(1)))      .groupByKey(12)      .mapToPair((Tuple2&lt;Integer, Iterable&lt;String&gt;&gt; tuple) -&gt; &#123;        int id = tuple._1();        AtomicInteger atomicInteger = new AtomicInteger(0);        tuple._2().forEach((String name) -&gt; atomicInteger.incrementAndGet());        return new Tuple2&lt;Integer, Integer&gt;(id, atomicInteger.get());      &#125;).count();      sparkSession.stop();      sparkSession.close();  &#125;  &#125;提交：   spark-submit --queue ambari --num-executors 4 --executor-cores 12 --executor-memory 12g --class com.jasongj.spark.driver.SparkDataSkew --master yarn --deploy-mode client SparkExample-with-dependencies-1.0.jar    # 自定义Partitioner使用自定义的Partitioner（默认为HashPartitioner），将原本被分配到同一个Task的不同Key分配到不同Task。以上述数据集为例，继续将并发度设置为12，但是在groupByKey算子上，使用自定义的Partitioner（实现如下）.groupByKey(new Partitioner() &#123;  @Override  public int numPartitions() &#123;    return 12;  &#125;  @Override  public int getPartition(Object key) &#123;    int id = Integer.parseInt(key.toString());    if(id &gt;= 9500000 &amp;&amp; id &lt;= 9500084 &amp;&amp; ((id - 9500000) % 12) == 0) &#123;      return (id - 9500000) / 12;    &#125; else &#123;      return id % 12;    &#125;  &#125;&#125;)</code></pre><p>场景二：小数据集join大数据集</p><p>解决方案：在Java&#x2F;Scala代码中将小数据集数据拉取到Driver，然后通过Broadcast方案将小数据集的数据广播到各Executor。或者在使用SQL前，将Broadcast的阈值调整得足够多，从而使用Broadcast生效。进而将Reduce侧Join替换为Map侧Join。</p><p>*<strong>案例*</strong></p><p>通过如下SQL创建一张具有倾斜Key且总记录数为1.5亿的大表test。</p><pre><code class="sql">INSERT OVERWRITE TABLE testSELECT CAST(CASE WHEN id &lt; 980000000 THEN (95000000  + (CAST (RAND() * 4 AS INT) + 1) * 48 )       ELSE CAST(id/10 AS INT) END AS STRING),       nameFROM student_externalWHERE id BETWEEN 900000000 AND 1050000000;</code></pre><p>使用如下SQL创建一张数据分布均匀且总记录数为50万的小表test_new。</p><pre><code class="sql">INSERT OVERWRITE TABLE test_newSELECT CAST(CAST(id/10 AS INT) AS STRING),       nameFROM student_delta_externalWHERE id BETWEEN 950000000 AND 950500000;</code></pre><p>直接通过Spark Thrift Server提交如下SQL将表test与表test_new进行Join并将Join结果存于表test_join中。</p><pre><code class="sql">INSERT OVERWRITE TABLE test_joinSELECT test_new.id, test_new.nameFROM testJOIN test_newON test.id = test_new.id;</code></pre><p>以上sql执行会发生数据倾斜。</p><p>接下来，尝试通过Broadcast实现Map侧Join。实现Map侧Join的方法，并非直接通过<em>CACHE TABLE test_new</em>将小表test_new进行cache。现通过如下SQL进行Join。</p><pre><code class="sql">CACHE TABLE test_new;INSERT OVERWRITE TABLE test_joinSELECT test_new.id, test_new.nameFROM testJOIN test_newON test.id = test_new.id;</code></pre><p>该操作仍分为三个Stage，且仍然有Shuffle存在，唯一不同的是，小表的读取不再直接扫描Hive表，而是扫描内存中缓存的表。并且数据倾斜仍然存在。</p><p><strong>正确的使用Broadcast实现Map侧Join的方式是</strong>，通过*SET spark.sql.autoBroadcastJoinThreshold&#x3D;104857600;*将Broadcast的阈值设置得足够大。</p><p>最终版：</p><pre><code class="sql">SET spark.sql.autoBroadcastJoinThreshold=104857600;INSERT OVERWRITE TABLE test_joinSELECT test_new.id, test_new.nameFROM testJOIN test_newON test.id = test_new.id;</code></pre><p>场景三：两张表都比较大，无法使用Map则Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p><p>解决：为skew的key增加随机前&#x2F;后缀。<br>为数据量特别大的Key增加随机前&#x2F;后缀，从而使倾斜的数据集分散到不同的Task中，彻底解决数据倾斜问题。Join另一则的数据中，与倾斜Key对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜Key如何加前缀，都能与之正常Join。</p><h3 id="案例"><a href="#案例" class="headerlink" title="*案例*"></a>*<strong>案例*</strong></h3><p>通过如下SQL，将id为9亿到9.08亿共800万条数据的id转为9500048或者9500096，其它数据的id除以100取整。从而该数据集中，id为9500048和9500096的数据各400万，其它id对应的数据记录数均为100条。这些数据存于名为test的表中。</p><p>对于另外一张小表test_new，取出50万条数据，并将id（递增且唯一）除以100取整，使得所有id都对应100条数据。</p><pre><code class="SQL">INSERT OVERWRITE TABLE testSELECT CAST(CASE WHEN id &lt; 908000000 THEN (9500000  + (CAST (RAND() * 2 AS INT) + 1) * 48 )  ELSE CAST(id/100 AS INT) END AS STRING),  nameFROM student_externalWHERE id BETWEEN 900000000 AND 1050000000;INSERT OVERWRITE TABLE test_newSELECT CAST(CAST(id/100 AS INT) AS STRING),  nameFROM student_delta_externalWHERE id BETWEEN 950000000 AND 950500000;</code></pre><p>通过如下代码，读取test表对应的文件夹内的数据并转换为JavaPairRDD存于leftRDD中，同样读取test表对应的数据存于rightRDD中。通过RDD的join算子对leftRDD与rightRDD进行Join，并指定并行度为48。</p><pre><code class="java">public class SparkDataSkew&#123;  public static void main(String[] args) &#123;    SparkConf sparkConf = new SparkConf();    sparkConf.setAppName(&quot;DemoSparkDataFrameWithSkewedBigTableDirect&quot;);    sparkConf.set(&quot;spark.default.parallelism&quot;, String.valueOf(parallelism));    JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf);    JavaPairRDD&lt;String, String&gt; leftRDD = javaSparkContext.textFile(&quot;hdfs://hadoop1:8020/apps/hive/warehouse/default/test/&quot;)      .mapToPair((String row) -&gt; &#123;        String[] str = row.split(&quot;,&quot;);        return new Tuple2&lt;String, String&gt;(str[0], str[1]);      &#125;);    JavaPairRDD&lt;String, String&gt; rightRDD = javaSparkContext.textFile(&quot;hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/&quot;)      .mapToPair((String row) -&gt; &#123;        String[] str = row.split(&quot;,&quot;);          return new Tuple2&lt;String, String&gt;(str[0], str[1]);      &#125;);    leftRDD.join(rightRDD, parallelism)      .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2()))      .foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;        AtomicInteger atomicInteger = new AtomicInteger();          iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());      &#125;);    javaSparkContext.stop();    javaSparkContext.close();  &#125;&#125;</code></pre><p>此代码会产生数据倾斜。<br>现通过如下操作，实现倾斜Key的分散处理</p><ul><li><p>将leftRDD中倾斜的key（即9500048与9500096）对应的数据单独过滤出来，且加上1到24的随机前缀，并将前缀与原数据用逗号分隔（以方便之后去掉前缀）形成单独的leftSkewRDD</p></li><li><p>将rightRDD中倾斜key对应的数据抽取出来，并通过flatMap操作将该数据集中每条数据均转换为24条数据（每条分别加上1到24的随机前缀），形成单独的rightSkewRDD</p></li><li><p>将leftSkewRDD与rightSkewRDD进行Join，并将并行度设置为48，且在Join过程中将随机前缀去掉，得到倾斜数据集的Join结果skewedJoinRDD</p></li><li><p>将leftRDD中不包含倾斜Key的数据抽取出来作为单独的leftUnSkewRDD</p></li><li><p>对leftUnSkewRDD与原始的rightRDD进行Join，并行度也设置为48，得到Join结果unskewedJoinRDD</p></li><li><p>通过union算子将skewedJoinRDD与unskewedJoinRDD进行合并，从而得到完整的Join结果集</p><pre><code class="java">public class SparkDataSkew&#123;    public static void main(String[] args) &#123;      int parallelism = 48;      SparkConf sparkConf = new SparkConf();      sparkConf.setAppName(&quot;SolveDataSkewWithRandomPrefix&quot;);      sparkConf.set(&quot;spark.default.parallelism&quot;, parallelism + &quot;&quot;);      JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf);      JavaPairRDD&lt;String, String&gt; leftRDD = javaSparkContext.textFile(&quot;hdfs://hadoop1:8020/apps/hive/warehouse/default/test/&quot;)        .mapToPair((String row) -&gt; &#123;          String[] str = row.split(&quot;,&quot;);            return new Tuple2&lt;String, String&gt;(str[0], str[1]);        &#125;);        JavaPairRDD&lt;String, String&gt; rightRDD = javaSparkContext.textFile(&quot;hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/&quot;)          .mapToPair((String row) -&gt; &#123;            String[] str = row.split(&quot;,&quot;);              return new Tuple2&lt;String, String&gt;(str[0], str[1]);          &#125;);        String[] skewedKeyArray = new String[]&#123;&quot;9500048&quot;, &quot;9500096&quot;&#125;;        Set&lt;String&gt; skewedKeySet = new HashSet&lt;String&gt;();        List&lt;String&gt; addList = new ArrayList&lt;String&gt;();        for(int i = 1; i &lt;=24; i++) &#123;            addList.add(i + &quot;&quot;);        &#125;        for(String key : skewedKeyArray) &#123;            skewedKeySet.add(key);        &#125;        Broadcast&lt;Set&lt;String&gt;&gt; skewedKeys = javaSparkContext.broadcast(skewedKeySet);        Broadcast&lt;List&lt;String&gt;&gt; addListKeys = javaSparkContext.broadcast(addList);        JavaPairRDD&lt;String, String&gt; leftSkewRDD = leftRDD          .filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1()))          .mapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;((new Random().nextInt(24) + 1) + &quot;,&quot; + tuple._1(), tuple._2()));        JavaPairRDD&lt;String, String&gt; rightSkewRDD = rightRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1()))          .flatMapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; addListKeys.value().stream()          .map((String i) -&gt; new Tuple2&lt;String, String&gt;( i + &quot;,&quot; + tuple._1(), tuple._2()))          .collect(Collectors.toList())          .iterator()        );        JavaPairRDD&lt;String, String&gt; skewedJoinRDD = leftSkewRDD          .join(rightSkewRDD, parallelism)          .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1().split(&quot;,&quot;)[1], tuple._2()._2()));        JavaPairRDD&lt;String, String&gt; leftUnSkewRDD = leftRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; !skewedKeys.value().contains(tuple._1()));        JavaPairRDD&lt;String, String&gt; unskewedJoinRDD = leftUnSkewRDD.join(rightRDD, parallelism).mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2()));        skewedJoinRDD.union(unskewedJoinRDD).foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;          AtomicInteger atomicInteger = new AtomicInteger();          iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());        &#125;);        javaSparkContext.stop();        javaSparkContext.close();    &#125;    &#125;</code></pre><h2 id="kafka-分区的选择"><a href="#kafka-分区的选择" class="headerlink" title="kafka 分区的选择"></a><strong>kafka 分区的选择</strong></h2><pre><code class="python">## 分区多的优点 ##kafka使用分区将topic的消息打散到多个分区分布保存在不同的broker上，实现了producer和consumer消息处理的高吞吐量。Kafka的producer和consumer都可以多线程地并行操作，而每个线程处理的是一个分区的数据。因此分区实际上是调优Kafka并行度的最小单元。对于producer而言，它实际上是用多个线程并发地向不同分区所在的broker发起Socket连接同时给这些分区发送消息；而consumer，同一个消费组内的所有consumer线程都被指定topic的某一个分区进行消费。所以说，如果一个topic分区越多，理论上整个集群所能达到的吞吐量就越大。## 分区不是越多越好 ##分区是否越多越好呢？显然也不是，因为每个分区都有自己的开销:# 一、客户端/服务器端需要使用的内存就越多 #Kafka0.8.2之后，在客户端producer有个参数batch.size，默认是16KB。它会为每个分区缓存消息，一旦满了就打包将消息批量发出。看上去这是个能够提升性能的设计。不过很显然，因为这个参数是分区级别的，如果分区数越多，这部分缓存所需的内存占用也会更多。假设你有10000个分区，按照默认设置，这部分缓存需要占用约157MB的内存。而consumer端呢？我们抛开获取数据所需的内存不说，只说线程的开销。如果还是假设有10000个分区，同时consumer线程数要匹配分区数(大部分情况下是最佳的消费吞吐量配置)的话，那么在consumer client就要创建10000个线程，也需要创建大约10000个Socket去获取分区数据。这里面的线程切换的开销本身已经不容小觑了。服务器端的开销也不小，如果阅读Kafka源码的话可以发现，服务器端的很多组件都在内存中维护了分区级别的缓存，比如controller，FetcherManager等，因此分区数越多，这种缓存的成本就越大。# 二、文件句柄的开销 #每个分区在底层文件系统都有属于自己的一个目录。该目录下通常会有两个文件：base_offset.log和base_offset.index。Kafak的controller和ReplicaManager会为每个broker都保存这两个文件句柄(file handler)。很明显，如果分区数越多，所需要保持打开状态的文件句柄数也就越多，最终可能会突破你的ulimit -n的限制。# 三、降低高可用性 #Kafka通过副本(replica)机制来保证高可用。具体做法就是为每个分区保存若干个副本(replica_factor指定副本数)。每个副本保存在不同的broker上。其中的一个副本充当leader 副本，负责处理producer和consumer请求。其他副本充当follower角色，由Kafka controller负责保证与leader的同步。如果leader所在的broker挂掉了，contorller会检测到然后在zookeeper的帮助下重选出新的leader——这中间会有短暂的不可用时间窗口，虽然大部分情况下可能只是几毫秒级别。但如果你有10000个分区，10个broker，也就是说平均每个broker上有1000个分区。此时这个broker挂掉了，那么zookeeper和controller需要立即对这1000个分区进行leader选举。比起很少的分区leader选举而言，这必然要花更长的时间，并且通常不是线性累加的。如果这个broker还同时是controller情况就更糟了。</code></pre><p><strong>如何确定分区数量呢？</strong>　　</p><p>可以遵循一定的步骤来尝试确定分区数：创建一个只有1个分区的topic，然后测试这个topic的producer吞吐量和consumer吞吐量。假设它们的值分别是Tp和Tc，单位可以是MB&#x2F;s。然后假设总的目标吞吐量是Tt，<strong>那么分区数 &#x3D; Tt &#x2F; max(Tp, Tc)</strong></p><p>说明：Tp表示producer的吞吐量。测试producer通常是很容易的，因为它的逻辑非常简单，就是直接发送消息到Kafka就好了。Tc表示consumer的吞吐量。测试Tc通常与应用的关系更大， 因为Tc的值取决于你拿到消息之后执行什么操作，因此Tc的测试通常也要麻烦一些。</p><p><strong>一条消息如何知道要被发送到哪个分区？</strong></p><p><strong>1，</strong> <em><strong>*如果指定了分区，就写入到指定的分区中。*</strong></em></p><p><strong>2，</strong> <em><strong>*如果没有指定分区，指定了key，按照key的hashcode，取模，写入对应的分区*</strong></em></p><p>def partition(key: Any, numPartitions: Int): Int &#x3D; {   Utils.abs(key.hashCode) % numPartitions }</p><p> 如果<strong>key为null时，从缓存中取分区id或者随机取一个。</strong>如果你没有指定key，那么Kafka是如何确定这条消息去往哪个分区的呢？</p><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200625083640.png"></p><p><strong>3</strong>，<em><strong>没有指定分区和key，轮询机制*</strong></em></p><pre><code class="python">## Consumer个数与分区数有什么关系？## topic下的一个分区只能被同一个consumer group下的一个consumer线程来消费，但反之并不成立，即一个consumer线程可以消费多个分区的数据，比如Kafka提供的ConsoleConsumer，默认就只是一个线程来消费所有分区的数据。所以，如果你的分区数是N，那么最好线程数也保持为N，这样通常能够达到最大的吞吐量。超过N的配置只是浪费系统资源，因为多出的线程不会被分配到任何分区。</code></pre></li></ul><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200625084324.png"></p><pre><code class="python">## Consumer消费Partition的分配策略 ##Kafka提供的两种分配策略：range和roundrobin，由参数partition.assignment.strategy指定，默认是range策略。当以下事件发生时，Kafka 将会进行一次分区分配：    #同一个 Consumer Group 内新增消费者    #消费者离开当前所属的Consumer Group，包括shuts down 或 crashes    #订阅的主题新增分区    Range策略是对每个主题而言的，首先对同一个主题里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。然后将partitions的个数除于消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区.例如：排完序的分区将会是0, 1, 2, 3, 4, 5, 6, 7, 8, 9，10；消费者线程序号排完序将会是C1-0, C2-0, C2-1最后分区分配的结果看起来是这样的.C1-0 将消费 0, 1, 2, 3 分区C2-0 将消费 4, 5, 6, 7 分区C2-1 将消费 8, 9, 10 分区使用RoundRobin策略有两个前提条件必须满足：同一个Consumer Group里面的所有消费者的num.streams必须相等；每个消费者订阅的主题必须相同。所以这里假设前面提到的2个消费者的num.streams = 2。RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序，最后按照round-robin风格将分区分别分配给不同的消费者线程。目前我们还不能自定义分区分配策略，只能通过partition.assignment.strategy参数选择 range 或 roundrobin。</code></pre><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200625085037.png"></p><h2 id="StructedStreaming-的outputMode注意事项："><a href="#StructedStreaming-的outputMode注意事项：" class="headerlink" title="StructedStreaming 的outputMode注意事项："></a><strong>StructedStreaming 的outputMode注意事项：</strong></h2><p>Append 模式（只输出新添加的（原来没有的））</p><ol><li>Append模式用WaterMark删除旧的聚合状态。</li><li>仅在WaterMark &gt;&#x3D; Window End Time时，输出一次最终结果到结果表并写入到接收器。</li><li>如果有聚合操作，必须添加waterMark，否则不支持。</li><li>Join查询，目前只支持Append模式。</li><li>Append模式下，在flatMapGroupsWithState之后，可再有聚合操作。</li></ol><p>Update 模式（只输出变化的部分）</p><ol><li>Update模式用WaterMark删除旧的聚合状态。</li><li>只要WaterMark &lt; Window End Time, 就会触发聚合计算并输出。</li><li>Update模式下, 在flatMapGroupsWithState之后，不允许再有聚合操作。</li><li>支持MapGroupsWithState</li></ol><p>Complete 模式（全部输出）</p><ol><li>Complete模式不会删除旧的聚合状态。</li><li>不论数据迟到多久，都会触发聚合计算。</li><li>必须要聚合操作，否则报错。</li></ol><h2 id="StructedStreaming-DataFrame-x2F-Streaming-DataSet的注意事项"><a href="#StructedStreaming-DataFrame-x2F-Streaming-DataSet的注意事项" class="headerlink" title="StructedStreaming DataFrame&#x2F;Streaming DataSet的注意事项"></a><strong>StructedStreaming DataFrame&#x2F;Streaming DataSet的注意事项</strong></h2><pre><code class="python">## 1 Event time must be defined on a window or a timestamp 时间时间必须为时间戳## 2-Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode;; 排序在流式处理的时候不支持## 3 输出模式必须是append或update. 在输出模式是complete的时候(必须有聚合), 要求每次输出所有的聚合结果. 我们使用 watermark 的目的是丢弃一些过时聚合数据, 所以complete模式使用wartermark无效也无意义## 4 在输出模式是append时, 必须设置 watermask 才能使用聚合操作. 其实, watermask 定义了 append 模式中何时输出聚合聚合结果(状态), 并清理过期状态## 5 在输出模式是update时, watermask 主要用于过滤过期数据并及时清理过期状态.## 6 watermask 会在处理当前批次数据时更新, 并且会在处理下一个批次数据时生效使用. 但如果节点发送故障, 则可能延迟若干批次生效## 7 withWatermark 必须使用与聚合操作中的时间戳列是同一列.df.withWatermark(“time”, “1 min”).groupBy(“time2”).count() 无效## 8 withWatermark 必须在聚合之前调用f.groupBy(“time”).count().withWatermark(“time”, “1 min”) 无效$$ streaming Datasets/DataFrame不支持一些Datasets/DataFrames操作。其中一些如下。streaming Datasets尚不支持多个流聚合（即流DF上的聚合链）。streaming Datasets不支持Limit 和take first N 。不支持对流数据集的去重操作。仅在聚合之后且在“complete”下，流数据集才支持排序操作。有条件地支持流数据集和静态数据集之间的Outer joins。不支持流数据集的Full outer join不支持使用右侧流数据集的Left outer join不支持使用左侧流数据集的Right outer join尚不支持两个流数据集之间的任何类型的连接。此外，还有一些数据集方法无法处理流式数据集。它们是将立即运行查询并返回结果的操作，这在流式数据集上没有意义。相反，这些功能可以通过显式启动流式查询来实现count（）-无法从流数据集返回单个计数。而是使用ds.groupBy（）.count（）返回包含运行计数的流数据集。foreach（）-而是使用ds.writeStream.foreach（…）show（）-而是使用console sink（请参阅下一节）。如果尝试这些操作中的任何一个，您将看到一个AnalysisException，例如“流数据帧/数据集不支持操作XYZ”。尽管将来的Spark版本可能会支持其中的某些功能，但从根本上讲，还有一些功能很难有效地在流数据上实现。例如，不支持对输入流进行排序，因为它需要跟踪流中接收到的所有数据。因此，从根本上讲，这很难有效执行。</code></pre><h2 id="StructedStreaming窗口的划分逻辑"><a href="#StructedStreaming窗口的划分逻辑" class="headerlink" title="StructedStreaming窗口的划分逻辑"></a><strong>StructedStreaming窗口的划分逻辑</strong></h2><pre><code class="scala">maxNumOverlapping &lt;- ceil(windowDuration / slideDuration)   for (i &lt;- 0 until maxNumOverlapping)    windowId &lt;- ceil((timestamp - startTime) / slideDuration)       windowStart &lt;- windowId * slideDuration + (i - maxNumOverlapping) * slideDuration + startTime    windowEnd &lt;- windowStart + windowDuration        return windowStart, windowEnd计算逻辑：输入数据：2019-08-14 10:55:00_dog,hello,word window($&quot;tm&quot;, &quot;4 minutes&quot;, &quot;2 minutes&quot;), //设置的窗口参数 startTime 没传默认是0 windowDuration =4  slideDuration=2  maxNumOverlapping =2  //计算出最大的窗口数为 2 windowId &lt;- ceil((timestamp - startTime) / slideDuration)    55/2 向上取整28   windowStart &lt;- windowId * slideDuration + (i - maxNumOverlapping) * slideDuration + startTime   56+（0-2）*2 = 52   windowStart：2019-08-14 10:52:00  windowEnd &lt;- windowStart + windowDurationwindowEnd ：2019-08-14 10:56:00  最终计算完的窗口如下： [2019-08-14 10:52:00  2019-08-14 10:56:00 ] [2019-08-14 10:54:00  2019-08-14 10:58:00 ]</code></pre><h2 id="structured-streaming的WaterMark"><a href="#structured-streaming的WaterMark" class="headerlink" title="structured-streaming的WaterMark"></a><strong>structured-streaming的WaterMark</strong></h2><pre><code class="scala">水印计算公式：watermark 计算: watermark = MaxEventTime - Threshhod而且, watermark只能逐渐增加, 不能减少初始水印值为0watermark 在用于基于时间的状态聚合操作时, 该时间可以基于窗口, 也可以基于 event-timeb本身.输出模式必须是append或update. 在输出模式是complete的时候(必须有聚合), 要求每次输出所有的聚合结果. 我们使用 watermark 的目的是丢弃一些过时聚合数据, 所以complete模式使用wartermark无效也无意义.在输出模式是append时, 必须设置 watermark 才能使用聚合操作. 其实, watermark 定义了 append 模式中何时输出聚合聚合结果(状态), 并清理过期状态.在输出模式是update时, watermark 主要用于过滤过期数据并及时清理过期状态.watermark 会在处理当前批次数据时更新, 并且会在处理下一个批次数据时生效使用. 但如果节点发送故障, 则可能延迟若干批次生效.withWatermark 必须使用与聚合操作中的时间戳列是同一列.df.withWatermark(“time”, “1 min”).groupBy(“time2”).count() 无效withWatermark 必须在聚合之前调用. f.groupBy(“time”).count().withWatermark(“time”, “1 min”) 无效</code></pre><h2 id="Zookeeper与redis分布式锁的实现"><a href="#Zookeeper与redis分布式锁的实现" class="headerlink" title="Zookeeper与redis分布式锁的实现"></a><strong>Zookeeper与redis分布式锁的实现</strong></h2><p><strong>zk的分布式锁：</strong></p><p>其实可以做的比较简单，就是某个节点尝试创建临时znode，此时创建成功了就获取了这个锁；这个时候别的客户端来创建锁会失败，只能注册个监听器监听这个锁。释放锁就是删除这个znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新枷锁。</p><pre><code class="java">import org.apache.zookeeper.*;import org.apache.zookeeper.data.Stat;import org.springframework.beans.factory.annotation.Autowired; import java.io.IOException;import java.util.Collections;import java.util.List;import java.util.concurrent.CountDownLatch;import java.util.concurrent.TimeUnit; public class ZooKeeperDistributedLock implements Watcher &#123;     private ZooKeeper zk;    private String locksRoot= &quot;/locks&quot;;    private String productId;    private String waitNode;    private String lockNode;    private CountDownLatch latch;    private CountDownLatch connectedLatch = new CountDownLatch(1);    private int sessionTimeout = 30000;     public ZooKeeperDistributedLock(String productId)&#123;        this.productId = productId;        try &#123;            String address = &quot;192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181&quot;;            zk = new ZooKeeper(address, sessionTimeout, this);            connectedLatch.await();        &#125; catch (IOException e) &#123;            throw new LockException(e);        &#125; catch (InterruptedException e) &#123;            throw new LockException(e);        &#125;    &#125;     @Autowired    public void process(WatchedEvent event) &#123;        if(event.getState()== Event.KeeperState.SyncConnected)&#123;            connectedLatch.countDown();            return;        &#125;         if(this.latch != null) &#123;            this.latch.countDown();        &#125;    &#125;     public void acquireDistributedLock() &#123;        try &#123;            if(this.tryLock())&#123;                return;            &#125;            else&#123;                waitForLock(waitNode, sessionTimeout);            &#125;        &#125; catch (KeeperException e) &#123;            throw new LockException(e);        &#125; catch (InterruptedException e) &#123;            throw new LockException(e);        &#125;    &#125;     public boolean tryLock() &#123;        try &#123;            // 传入进去的locksRoot + “/” + productId            // 假设productId代表了一个商品id，比如说1            // locksRoot = locks            // /locks/10000000000，/locks/10000000001，/locks/10000000002            // EPHEMERAL_SEQUENTIAL 临时顺序编号目录节点            lockNode = zk.create(locksRoot + &quot;/&quot; + productId, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);             // 看看刚创建的节点是不是最小的节点            // locks：10000000000，10000000001，10000000002            List&lt;String&gt; locks = zk.getChildren(locksRoot, false);            Collections.sort(locks);             if(lockNode.equals(locksRoot+&quot;/&quot;+ locks.get(0)))&#123;                //如果是最小的节点,则表示取得锁                return true;            &#125;             //如果不是最小的节点，找到比自己小1的节点            int previousLockIndex = -1;            for(int i = 0; i &lt; locks.size(); i++) &#123;                if(lockNode.equals(locksRoot + &quot;/&quot; + locks.get(i))) &#123;                    previousLockIndex = i - 1;                    break;                &#125;            &#125;             this.waitNode = locks.get(previousLockIndex);        &#125; catch (KeeperException e) &#123;            throw new LockException(e);        &#125; catch (InterruptedException e) &#123;            throw new LockException(e);        &#125;        return false;    &#125;     private boolean waitForLock(String waitNode, long waitTime) throws InterruptedException, KeeperException &#123;        Stat stat = zk.exists(locksRoot + &quot;/&quot; + waitNode, true);        if(stat != null)&#123;            this.latch = new CountDownLatch(1);            this.latch.await(waitTime, TimeUnit.MILLISECONDS);                 this.latch = null;        &#125;        return true;    &#125;     public void unlock() &#123;        try &#123;            // 删除/locks/10000000000节点            // 删除/locks/10000000001节点            System.out.println(&quot;unlock &quot; + lockNode);            zk.delete(lockNode,-1);            lockNode = null;            zk.close();        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125; catch (KeeperException e) &#123;            e.printStackTrace();        &#125;    &#125;      public class LockException extends RuntimeException &#123;        private static final long serialVersionUID = 1L;        public LockException(String e)&#123;            super(e);        &#125;        public LockException(Exception e)&#123;            super(e);        &#125;    &#125;// 如果有一把锁，被多个人给竞争，此时多个人会排队，第一个拿到锁的人会执行，然后释放锁，后面的每个人都会去监听排在自己前面的那个人创建的node上，一旦某个人释放了锁，排在自己后面的人就会被zookeeper给通知，一旦被通知了之后，就ok了，自己就获取到了锁，就可以执行代码了</code></pre><p><strong>Redlock算法</strong></p><p>在算法的分布式版本中，我们假设我们有N个Redis母版。这些节点是完全独立的，因此我们不使用复制或任何其他隐式协调系统。我们已经描述了如何在单个实例中安全地获取和释放锁。我们认为该算法将使用此方法在单个实例中获取和释放锁，这是理所当然的。在我们的示例中，我们将N &#x3D; 5设置为一个合理的值，因此我们需要在不同的计算机或虚拟机上运行5个Redis主服务器，以确保它们将以大多数独立的方式发生故障。</p><p>为了获取锁，客户端执行以下操作：</p><ol><li><p>它以毫秒为单位获取当前时间。</p></li><li><p>它尝试在所有N个实例中顺序使用所有实例中相同的键名和随机值来获取锁定。在第2步中，在每个实例中设置锁时，客户端使用的超时时间小于总锁自动释放时间，以便获取该超时时间。例如，如果自动释放时间为10秒，则超时时间可能在5到50毫秒之间。这样可以防止客户端长时间与处于故障状态的Redis节点进行通信：如果某个实例不可用，我们应该尝试与下一个实例尽快进行通信。</p></li><li><p>客户端通过从当前时间中减去在步骤1中获得的时间戳，来计算获取锁所花费的时间。当且仅当客户端能够在大多数实例（至少3个）中获取锁时， ，并且获取锁所花费的总时间小于锁有效时间，则认为已获取锁。</p></li><li><p>如果获取了锁，则将其有效时间视为初始有效时间减去经过的时间，如步骤3中所计算。</p></li><li><p>如果客户端由于某种原因（无法锁定N &#x2F; 2 + 1实例或有效时间为负数）而未能获得该锁，它将尝试解锁所有实例（即使它认为不是该实例）能够锁定）。</p><pre><code class="java">@Componentpublic class RedisLockHelper &#123;     private long sleepTime = 100;    // 1. 配置文件    Config config = new Config();    config.useSingleServer()            .setAddress(&quot;redis://127.0.0.1:6379&quot;)            .setPassword(RedisConfig.PASSWORD)            .setDatabase(0);    //2. 构造RedissonClient    RedissonClient redissonClient = Redisson.create(config);    //3. 设置锁定资源名称    RLock lock = redissonClient.getLock(&quot;redlock&quot;);    lock.lock();    try &#123;        System.out.println(&quot;获取锁成功，实现业务逻辑&quot;);        Thread.sleep(10000);    &#125; catch (InterruptedException e) &#123;        e.printStackTrace();    &#125; finally &#123;        lock.unlock();    &#125;    /**     * 直接使用setnx + expire方式获取分布式锁     * 非原子性     *     * @param key     * @param value     * @param timeout     * @return     */    public boolean lock_setnx(Jedis jedis,String key, String value, int timeout) &#123;        Long result = jedis.setnx(key, value);        // result = 1时，设置成功，否则设置失败        if (result == 1L) &#123;            return jedis.expire(key, timeout) == 1L;        &#125; else &#123;            return false;        &#125;    &#125;    /**     * 使用Lua脚本，脚本中使用setnex+expire命令进行加锁操作     *     * @param jedis     * @param key     * @param UniqueId     * @param seconds     * @return     */    public boolean Lock_with_lua(Jedis jedis,String key, String UniqueId, int seconds) &#123;        String lua_scripts = &quot;if redis.call(&#39;setnx&#39;,KEYS[1],ARGV[1]) == 1 then&quot; +                &quot;redis.call(&#39;expire&#39;,KEYS[1],ARGV[2]) return 1 else return 0 end&quot;;        List&lt;String&gt; keys = new ArrayList&lt;&gt;();        List&lt;String&gt; values = new ArrayList&lt;&gt;();        keys.add(key);        values.add(UniqueId);        values.add(String.valueOf(seconds));        Object result = jedis.eval(lua_scripts, keys, values);        //判断是否成功        return result.equals(1L);    &#125;    /**     * 在Redis的2.6.12及以后中,使用 set key value [NX] [EX] 命令     *     * @param key     * @param value     * @param timeout     * @return     */    public boolean lock(Jedis jedis,String key, String value, int timeout, TimeUnit timeUnit) &#123;        long seconds = timeUnit.toSeconds(timeout);        return &quot;OK&quot;.equals(jedis.set(key, value, &quot;NX&quot;, &quot;EX&quot;, seconds));    &#125;    /**     * 自定义获取锁的超时时间     *     * @param jedis     * @param key     * @param value     * @param timeout     * @param waitTime     * @param timeUnit     * @return     * @throws InterruptedException     */    public boolean lock_with_waitTime(Jedis jedis,String key, String value, int timeout, long waitTime,TimeUnit timeUnit) throws InterruptedException &#123;        long seconds = timeUnit.toSeconds(timeout);        while (waitTime &gt;= 0) &#123;            String result = jedis.set(key, value, &quot;nx&quot;, &quot;ex&quot;, seconds);            if (&quot;OK&quot;.equals(result)) &#123;                return true;            &#125;            waitTime -= sleepTime;            Thread.sleep(sleepTime);        &#125;        return false;    &#125;    /**     * 错误的解锁方法—直接删除key     *     * @param key     */    public void unlock_with_del(Jedis jedis,String key) &#123;        jedis.del(key);    &#125;    /**     * 使用Lua脚本进行解锁操纵，解锁的时候验证value值     *     * @param jedis     * @param key     * @param value     * @return     */    public boolean unlock(Jedis jedis,String key,String value) &#123;        String luaScript = &quot;if redis.call(&#39;get&#39;,KEYS[1]) == ARGV[1] then &quot; +                &quot;return redis.call(&#39;del&#39;,KEYS[1]) else return 0 end&quot;;        return jedis.eval(luaScript, Collections.singletonList(key), Collections.singletonList(value)).equals(1L);    &#125;&#125;总结：    1： 直接使用setnx + expire方式获取分布式锁    SET my:lock 随机值 NX PX 30000，这个命令就ok，这个的NX的意思就是只有key不存在的时候才会设置成功，PX 30000的意思是30秒后锁自动释放。别人创建的时候如果发现已经有了就不能加锁了。            2： 使用Lua脚本，脚本中使用setnex+expire命令进行加锁操作        if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then            return redis.call(&quot;del&quot;,KEYS[1])        else            return 0        end            3：使用 set key value [NX] [EX] 命令,加锁         4:使用Lua脚本进行解锁操作，解锁的时候验证value值（释放锁）    </code></pre><h2 id="kafka分段日志"><a href="#kafka分段日志" class="headerlink" title="kafka分段日志"></a><strong>kafka分段日志</strong></h2><p><strong>Kafka的存储结构</strong></p><p>总所周知，Kafka的Topic可以有多个分区，分区其实就是最小的读取和存储结构，即Consumer看似订阅的是Topic，实则是从Topic下的某个分区获得消息，Producer也是发送消息也是如此。</p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaDFQc2V4RWFpYjU5Z0I0NHNjdVN6Q1dZTFg3VndoS3VKMEk4aFdoZlBXdDI0aDdZRXdrdTROTEEvNjQw?x-oss-process=image/format,png" alt="img" style="zoom:50%;"><p>上图是总体逻辑上的关系，映射到实际代码中在磁盘上的关系则是如下图所示：</p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaHFpYnNVMUxkOWZ2aG1pYVZhVXdLTnBzcGRZNlZ5V0FEUzZXSFRwQzU1UG9kampEeXJwRm9jcWljUS82NDA?x-oss-process=image/format,png" alt="img" style="zoom:50%;"><p>每个分区对应一个Log对象，在磁盘中就是一个子目录，子目录下面会有多组日志段即多Log Segment，每组日志段包含：消息日志文件(以log结尾)、位移索引文件(以index结尾)、时间戳索引文件(以timeindex结尾)。其实还有其它后缀的文件，例如.txnindex、.deleted等等。篇幅有限，暂不提起<br><strong>以下为日志的定义</strong></p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaEFYWGlhaWNnRjI2Wk1OT2IzRFRTQ2ljQ3dqSkR5bE1VemwyWmhyd2x5azR0OXpHdjdpYmd1aWJUb3NRLzY0MA?x-oss-process=image/format,png" alt="img" style="zoom:80%;"><h3 id="以下为日志段的定义"><a href="#以下为日志段的定义" class="headerlink" title="以下为日志段的定义"></a><strong>以下为日志段的定义</strong></h3><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaHJCOEtKcXVobERtcmlhZ3JmY2NoVlBGajRIQm4yczBZZ1VJYVpGaWFOTlRwekZnUkFpYlBCN21XUS82NDA?x-oss-process=image/format,png" alt="img"></p></li></ol><p>***<u>indexIntervalBytes</u><em><strong>可以理解为插了多少消息之后再建一个索引，由此可以看出Kafka的索引其实是</strong></em><u>稀疏索引</u><em><strong>，这样可以</strong></em><u>避免索引文件占用过多的内存，从而可以在内存中保存更多的索引</u>***。对应的就是Broker 端参数log.index.interval.bytes 值，默认4KB。</p><p>实际的通过***<u>索引查找</u><em><strong>消息过程是先</strong></em><u>通过offset找到索引所在的文件，然后通过二分法找到离目标最近的索引，再顺序遍历消息文件找到目标文件</u>***。这波操作时间复杂度为O(log2n)+O(m),n是索引文件里索引的个数，m为稀疏程度。</p><p>这就***<u>是空间和时间的互换，又经过数据结构与算法的平衡</u>***，妙啊！</p><p>再说下***<u>rollJitterMs,</u><em><strong>这其实是个扰动值，对应的参数是log.roll.jitter.ms,这其实就要说到日志段的切分了，<u><em><strong>log.segment.bytes,这个参数控制着日志段文件的大小，默认是1G</strong></em></u>，即当文件存储超过1G之后就新起一个文件写入。这是以</strong></em><u>大小为维度</u><em><strong>的，还有一个参数是</strong></em><u>log.segment.ms,以时间为维度切分</u>***。</p><p>那配置了这个参数之后如果有很多很多分区，然后因为这个参数是全局的，<u><em><strong>因此同一时刻需要做很多文件的切分，这磁盘IO就顶不住了啊，因此需要设置个rollJitterMs</strong></em></u>，来岔开它们。</p><p><em><strong><u>怎么样有没有联想到redis缓存的过期时间？过期时间加个随机数，防止同一时刻大量缓存过期导致缓存击穿数据库。看看知识都是通的啊！</u></strong></em></p><p><em><strong>*日志段的写入*</strong></em></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaHg1bmhIemRKWGJHYkh0RUpGUVdoemNoaWNabThZdW85NXk1enQ1WHBJZ3hGa2tXcHRhdVozb1EvNjQw?x-oss-process=image/format,png" alt="img"></p><pre><code class="python">## 1、判断下当前日志段是否为空，空的话记录下时间，来作为之后日志段的切分依据## 2、确保位移值合法，最终调用的是AbstractIndex.toRelative(..)方法，即使判断offset是否小于0，是否大于int最大值。## 3、append消息，实际上就是通过FileChannel将消息写入，当然只是写入内存中及页缓存，是否刷盘看配置。## 4、更新日志段最大时间戳和最大时间戳对应的位移值。这个时间戳其实用来作为定期删除日志的依据## 5、更新索引项，如果需要的话(bytesSinceLastIndexEntry &gt; indexIntervalBytes)</code></pre><p>最后再来个流程图</p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaFF2VmswaG9qRUMyamswS2UxODlzZjhVYk5ZZDJCVFNpYzhYWGcydXgyYVY0cldUc0FQSE5pYnNBLzY0MA?x-oss-process=image/format,png" alt="img" style="zoom: 50%;"><p><em><strong>*日志段的读取*</strong></em></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaEtlMWJ3R2M0ZEpuVXhzOWZTaWI0cjduRVJ3WnVHQmZQY2hxSWd3M2dFdTFRZlFjMmlhV2d3SWljZy82NDA?x-oss-process=image/format,png" alt="img"></p><pre><code class="python">## 1、根据第一条消息的offset，通过OffsetIndex找到对应的消息所在的物理位置和大小。## 2、获取LogOffsetMetadata,元数据包含消息的offset、消息所在segment的起始offset和物理位置## 3、判断minOneMessage是否为true,若是则调整为必定返回一条消息大小，其实就是在单条消息大于maxSize的情况下得以返回，防止消费者饿死## 4、再计算最大的fetchSize,即（最大物理位移-此消息起始物理位移）和adjustedMaxSize的最小值(这波我不是很懂，因为以上一波操作adjustedMaxSize已经最小为一条消息的大小了)## 5、调用 FileRecords 的 slice 方法从指定位置读取指定大小的消息集合，并且构造FetchDataInfo返回</code></pre><p>再来个流程图：</p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaEsydHZiT0FxemNIbnJ3NkpvWU16aWN3aDhyQkYzYUpFeTN2WkhBbXdOaWE4dzhGREdCQlA4MjZBLzY0MA?x-oss-process=image/format,png" alt="img" style="zoom:50%;"><h2 id="面试题：如何保证kafka消息的顺序性"><a href="#面试题：如何保证kafka消息的顺序性" class="headerlink" title="面试题：如何保证kafka消息的顺序性"></a><strong>面试题：如何保证kafka消息的顺序性</strong></h2><pre><code class="python">## 1：一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。## 2: 每个partition对应一个消费者消费;每个消费者内写 N 个内存 queue，具有相同 key 的数据经过hash都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。</code></pre><img src="https://img-blog.csdnimg.cn/20190519230306612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE0Mzk4Mzk=,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;"><h2 id="kafka日志留存策略"><a href="#kafka日志留存策略" class="headerlink" title="kafka日志留存策略"></a><strong>kafka日志留存策略</strong></h2><pre><code class="python">## 留存策略类型与日志留存方式相关的策略类型主要有两种：delete和compact。这两种留存方式的机制完全不同.delete类型的留存策略    用户可以通过设置broker端参数log.cleanup.policy来指定集群上所有topic默认的策略类型。另外也可以通过topic级别参数cleanup.policy来为某些topic设置不同于默认值的策略类型。当前log.cleanup.policy参数的默认值是[delete,compact]，这是一个list类型的参数，表示集群上所有topic会同时开启delete和compact两种留存策略——这是0.10.1.0新引入的功能，在0.10.1.0之前，该参数只能两选一，不能同时兼顾，但在实际使用中很多用户都抱怨compact类型的topic存在过期key消息未删除的情况，故社区修改了该参数配置，允许一个topic同时开启两种留存策略。    ## Kafka是如何处理日志留存的每个Kafka broker启动时，都会在后台开启一个定时任务，定期地去检查并执行所有topic日志留存，这个定时任务触发的时间周期由broker端参数log.retention.check.interval.ms控制，默认是5分钟，即每台broker每5分钟都会尝试去检查一下是否有可以删除的日志。因此如果你要缩短这个间隔，只需要调小log.retention.check.interval.ms即可。鉴于日志留存和日志删除实际上是一个问题的两个方面，因而我们下面讨论的是关于Kafka根据什么规则来删除日志。但有一点要强调一下，待删除的标的是日志段，即LogSegment，也就是以.log结尾的一个个文件，而非整个文件夹。另外还有一点也很重要，当前日志段（active logsegment）是永远不会被删除的，不管用户配置了哪种留存机制。</code></pre><p><strong>当前留存机制共有3种：</strong></p><ol><li><p><em><strong><u>基于空间维度</u></strong></em></p><p>也称size-based retention，指的是Kafka定期为那些超过磁盘空间阈值的topic进行日志段的删除。这个阈值由broker端参数<u><strong>log.retention.bytes</strong></u>和topic级别参数**<u>retention.bytes</u>**控制，默认是-1，表示Kafka当前未开启这个留存机制，即不管topic日志量涨到多少，Kafka都不视其为“超过阈值”。如果用户要开启这种留存机制，必须显式设置log.retention.bytes（或retention.bytes）。 </p><p>一旦用户设置了阈值，那么Kafka就会在定时任务中尝试比较当前日志量总大小是否超过阈值至少一个日志段的大小。这里所说的总大小是指所有日志段文件的大小，不包括索引文件的大小！如果是则会尝试从最老的日志段文件开始删起。注意这里的“**<u><em>超过阈值至少一个日志段的大小</em></u>**”，这就是说超过阈值的部分必须要大于一个日志段的大小，否则不会进行删除的，原因就是因为删除的标的是日志段文件——即文件只能被当做一个整体进行删除，无法删除部分内容。</p><p>举个例子来说明，假设日志段大小是700MB，当前分区共有4个日志段文件，大小分别是700MB，700MB，700MB和1234B——显然1234B那个文件就是active日志段。此时该分区总的日志大小是3*700MB+1234B&#x3D;2100MB+1234B，如果阈值设置为2000MB，那么超出阈值的部分就是100MB+1234B，小于日志段大小700MB，故Kafka不会执行任何删除操作，即使总大小已经超过了阈值；反之如果阈值设置为1400MB，那么超过阈值的部分就是700MB+1234B &gt; 700MB，此时Kafka会删除最老的那个日志段文件。</p></li><li><p><em><strong><u>基于时间维度</u></strong></em></p><p>也称time-based retention，指的是Kafka定期未那些超过时间阈值的topic进行日志段删除操作。这个阈值由broker端参数<u><strong>log.retention.ms</strong></u>、**<u>log.retention.mintues</u><strong>、</strong><u>log.retention.hours</u><strong>以及topic级别参数</strong><u>retention.ms</u>**控制。如果同时设置了log.retention.ms、log.retention.mintues、log.retention.hours，以log.retention.ms优先级为最高，log.retention.mintues次之，log.retention.hours最次。当前这三个参数的默认值依次是null, null和168，<u><strong>故Kafka为每个topic默认保存7天的日志。</strong></u></p><p>这里需要讨论下这“7天”是如何界定的？在0.10.0.0之前，Kafka每次检查时都会将当前时间与每个日志段文件的最新修改时间做比较，如果两者的差值超过了上面设定的阈值（比如上面说的7天），那么Kafka就会尝试删除该文件。不过这种界定方法是有问题的，因为文件的最新修改时间是可变动的——比如用户在终端通过touch命令查看该日志段文件或Kafka对该文件切分时都可能导致最新修改时间的变化从而扰乱了该规则的判定，因此自0.10.0.0版本起，**<u><em>Kafka在消息体中引入了时间戳字段(当然不是单纯为了修复这个问题)，并且为每个日志段文件都维护一个最大时间戳字段。通过将当前时间与该最大时间戳字段进行比较来判定是否过期。使用当前最大时间戳字段的好处在于它对用户是透明的，用户在外部无法直接修改它，故不会造成判定上的混乱。</em></u>**</p><p>最大时间戳字段的更新机制也很简单，每次日志段写入新的消息时，都会尝试更新该字段。因为消息时间戳通常是递增的，故每次写入操作时都会保证最大时间戳字段是会被更新的，而一旦一个日志段写满了被切分之后它就不再接收任何新的消息，其最大时间戳字段的值也将保持不变。倘若该值距离当前时间超过了设定的阈值，那么该日志段文件就会被删除。</p></li><li><p><em><strong><u>基于起始位移维度</u></strong></em></p><pre><code class="python">## 基于日志起始位移（log start offset)。这实际上是0.11.0.0版本新增加的功能。其实增加这个功能的初衷主要是为了Kafka流处理应用——在流处理应用中存在着大量的中间消息，这些消息可能已经被处理过了，但依然保存在topic日志中，占用了大量的磁盘空间。如果通过设置基于时间维度的机制来删除这些消息就需要用户设置很小的时间阈值，这可能导致这些消息尚未被下游操作算子（operator）处理就被删除；如果设置得过大，则极大地增加了空间占用。故社区在0.11.0.0引入了第三种留存机制：基于起始位移## 所谓起始位移，就是指分区日志的当前起始位移——注意它是分区级别的值，而非日志段级别。故每个分区都只维护一个起始位移值。该值在初始化时被设置为最老日志段文件的基础位移(base offset)，随着日志段的不断删除，该值会被更新当前最老日志段的基础位移。另外Kafka提供提供了一个脚本命令帮助用户设置指定分区的起始位移：kafka-delete-records.sh。 该留存机制是默认开启的，不需要用户任何配置。Kafka会为每个日志段做这样的检查：1. 获取日志段A的下一个日志段B的基础位移；2. 如果该值小于分区当前起始位移则删除此日志段A。依然拿例子还说明，假设我有一个topic，名字是test，该topic只有1个分区，该分区下有5个日志段文件，分别是A1.log, A2.log, A3.log, A4.log和A5.log，其中A5.log是active日志段。这5个日志段文件中消息范围分别是0~9999,10000~19999,20000~29999,30000~39999和40000~43210（A5未写满）。如果此时我确信前3个日志段文件中的消息已经被处理过了，于是想删除这3个日志段，此时我应该怎么做呢？由于我无法预知这些日志段文件产生的速度以及被消费的速度，因此不管是基于时间的删除机制还是基于空间的删除机制都是不适用的。此时我便可以使用kafka-delete-records.sh脚本将该分区的起始位移设置为A4.log的起始位移，即40000。为了做到这点，我需要首先创建一个JSON文件a.json，内容如下：&#123;&quot;partitions&quot;:[&#123;&quot;topic&quot;: &quot;test&quot;, &quot;partition&quot;: 0,&quot;offset&quot;: 40000&#125;],&quot;version&quot;:1&#125;然后执行下列命令：bin/kafka-delete-records.sh --bootstrap-server localhost:9092 --offset-json-file a.json 如果一切正常，应该可以看到类似于这样的输出：Executing records delete operationRecords delete operation completedpartition: test-0 low_watermark: 40000</code></pre><h2 id="kafka最佳实践"><a href="#kafka最佳实践" class="headerlink" title="kafka最佳实践"></a><strong>kafka最佳实践</strong></h2><h2 id="通过Spark生成HFile，并以BulkLoad方式将数据导入到HBase"><a href="#通过Spark生成HFile，并以BulkLoad方式将数据导入到HBase" class="headerlink" title="通过Spark生成HFile，并以BulkLoad方式将数据导入到HBase"></a>通过Spark生成HFile，并以BulkLoad方式将数据导入到HBase</h2><p>在实际生产环境中，将计算和存储进行分离，是我们提高集群吞吐量、确保集群规模水平可扩展的主要方法之一，并且通过集群的扩容、性能的优化，确保在数据大幅增长时，存储不能称为系统的瓶颈。</p><p>具体到我们实际的项目需求中，有一个典型的场景，通常会将Hive中的部分数据，比如热数据，存入到HBase中，进行冷热分离处理。</p><p>我们采用Spark读取Hive表数据存入HBase中，这里主要有两种方式：</p><ol><li>通过HBase的put API进行数据的批量写入</li><li>通过生成HFile文件，然后通过BulkLoad方式将数据存入HBase</li></ol><p>HBase的原生put方式，通过HBase集群的region server向HBase插入数据，但是当数据量非常大时，region会进行split、compact等处理，并且这些处理非常占用计算资源和IO开销，影响性能和集群的稳定性。</p><p>HBase的数据最终是以HFile的形式存储到HDFS上的，如果我们能直接将数据生成为HFile文件，然后将HFile文件保存到HBase对应的表中，可以避免上述的很多问题，效率会相对更高。</p><p>本篇文章主要介绍如何使用Spark生成HFile文件，然后通过BulkLoad方式将数据导入到HBase中，并附批量put数据到HBase以及直接存入数据到HBase中的实际应用示例。</p><p><strong>1. 生成HFile，BulkLoad导入</strong></p><pre><code class="scala">## 1.1 数据样例&#123;&quot;id&quot;:&quot;1&quot;,&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:&quot;18&quot;&#125;&#123;&quot;id&quot;:&quot;2&quot;,&quot;name&quot;:&quot;mike&quot;,&quot;age&quot;:&quot;19&quot;&#125;&#123;&quot;id&quot;:&quot;3&quot;,&quot;name&quot;:&quot;kilos&quot;,&quot;age&quot;:&quot;20&quot;&#125;&#123;&quot;id&quot;:&quot;4&quot;,&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:&quot;21&quot;&#125;...## 1.2 示例代码/**  * @Author bigdatalearnshare  */object App &#123;  def main(args: Array[String]): Unit = &#123;    System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;)    val sparkSession = SparkSession      .builder()      .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)      .master(&quot;local[*]&quot;)      .getOrCreate()        val rowKeyField = &quot;id&quot;        val df = sparkSession.read.format(&quot;json&quot;).load(&quot;/people.json&quot;)    val fields = df.columns.filterNot(_ == &quot;id&quot;).sorted    val data = df.rdd.map &#123; row =&gt;      val rowKey = Bytes.toBytes(row.getAs(rowKeyField).toString)      val kvs = fields.map &#123; field =&gt;        new KeyValue(rowKey, Bytes.toBytes(&quot;hfile-fy&quot;), Bytes.toBytes(field), Bytes.toBytes(row.getAs(field).toString))      &#125;      (new ImmutableBytesWritable(rowKey), kvs)    &#125;.flatMapValues(x =&gt; x).sortByKey()        val hbaseConf = HBaseConfiguration.create(sparkSession.sessionState.newHadoopConf())    hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;linux-1:2181,linux-2:2181,linux-3:2181&quot;)    hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, &quot;hfile&quot;)    val connection = ConnectionFactory.createConnection(hbaseConf)    val tableName = TableName.valueOf(&quot;hfile&quot;)    //没有HBase表则创建    creteHTable(tableName, connection)    val table = connection.getTable(tableName)    try &#123;      val regionLocator = connection.getRegionLocator(tableName)      val job = Job.getInstance(hbaseConf)      job.setMapOutputKeyClass(classOf[ImmutableBytesWritable])      job.setMapOutputValueClass(classOf[KeyValue])      HFileOutputFormat2.configureIncrementalLoad(job, table, regionLocator)      val savePath = &quot;hdfs://linux-1:9000/hfile_save&quot;      delHdfsPath(savePath, sparkSession)      job.getConfiguration.set(&quot;mapred.output.dir&quot;, savePath)      data.saveAsNewAPIHadoopDataset(job.getConfiguration)      val bulkLoader = new LoadIncrementalHFiles(hbaseConf)      bulkLoader.doBulkLoad(new Path(savePath), connection.getAdmin, table, regionLocator)    &#125; finally &#123;      //WARN LoadIncrementalHFiles: Skipping non-directory hdfs://linux-1:9000/hfile_save/_SUCCESS 不影响,直接把文件移到HBASE对应HDFS地址了      table.close()      connection.close()    &#125;    sparkSession.stop()  &#125;  def creteHTable(tableName: TableName, connection: Connection): Unit = &#123;    val admin = connection.getAdmin    if (!admin.tableExists(tableName)) &#123;      val tableDescriptor = new HTableDescriptor(tableName)      tableDescriptor.addFamily(new HColumnDescriptor(Bytes.toBytes(&quot;hfile-fy&quot;)))      admin.createTable(tableDescriptor)    &#125;  &#125;  def delHdfsPath(path: String, sparkSession: SparkSession) &#123;    val hdfs = FileSystem.get(sparkSession.sessionState.newHadoopConf())    val hdfsPath = new Path(path)    if (hdfs.exists(hdfsPath)) &#123;      //val filePermission = new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.READ)      hdfs.delete(hdfsPath, true)    &#125;  &#125;&#125;## 1.3 注意事项上述示例代码可以根据实际业务需求作相应调整，但有一个问题需要特别注意：通过Spark读取过来的数据生成HFile时，要确保HBase的主键、列族、列按照有序排列。否则，会抛出以下异常:Caused by: java.io.IOException: Added a key not lexically larger than previous. Current cell = 1/hfile-fy:age/1588230543677/Put/vlen=2/seqid=0, lastCell = 1/hfile-fy:name/1588230543677/Put/vlen=4/seqid=0</code></pre><p><strong>2. 批量put</strong>**</p><pre><code class="scala">val rowKeyField = &quot;id&quot;val df = sparkSession.read.format(&quot;json&quot;).load(&quot;/stats.json&quot;)val fields = df.columns.filterNot(_ == &quot;id&quot;)df.rdd.foreachPartition &#123; partition =&gt;      val hbaseConf = HBaseConfiguration.create()      hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;linux-1:2181,linux-2:2181,linux-3:2181&quot;)      hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, &quot;batch_put&quot;)      val conn = ConnectionFactory.createConnection(hbaseConf)      val table = conn.getTable(TableName.valueOf(&quot;batch_put&quot;))      val res = partition.map &#123; row =&gt;        val rowKey = Bytes.toBytes(row.getAs(rowKeyField).toString)        val put = new Put(rowKey)        val family = Bytes.toBytes(&quot;hfile-fy&quot;)        fields.foreach &#123; field =&gt;          put.addColumn(family, Bytes.toBytes(field), Bytes.toBytes(row.getAs(field).toString))        &#125;        put      &#125;.toList      Try(table.put(res)).getOrElse(table.close())      table.close()      conn.close()&#125;</code></pre><p>在实际应用中，我们也可以将经常一起查询的数据拼接在一起存入一个列中，比如将上述的pv和uv拼接在一起使用，可以降低KeyValue带来的结构化开销。</p><p><strong>3.saveAsNewAPIHadoopDataset</strong></p><pre><code class="scala">val hbaseConf = sparkSession.sessionState.newHadoopConf()hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;linux-1:2181,linux-2:2181,linux-3:2181&quot;)hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, &quot;direct&quot;)val job = Job.getInstance(hbaseConf)job.setMapOutputKeyClass(classOf[ImmutableBytesWritable])job.setMapOutputValueClass(classOf[Result])job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])val rowKeyField = &quot;id&quot;val df = sparkSession.read.format(&quot;json&quot;).load(&quot;/stats.json&quot;)val fields = df.columns.filterNot(_ == &quot;id&quot;)df.rdd.map &#123; row =&gt;    val put = new Put(Bytes.toBytes(row.getAs(rowKeyField).toString))    val family = Bytes.toBytes(&quot;hfile-fy&quot;)    fields.foreach &#123; field =&gt;      put.addColumn(family, Bytes.toBytes(field), Bytes.toBytes(row.getAs(field).toString))    &#125;    (new ImmutableBytesWritable(), put)&#125;.saveAsNewAPIHadoopDataset(job.getConfiguration)</code></pre><h2 id="sparkSQL技术指北"><a href="#sparkSQL技术指北" class="headerlink" title="sparkSQL技术指北"></a><strong>sparkSQL技术指北</strong></h2><pre><code class="python">## 1:SparkSQL 中的 hint SparkSQL 2.2 增加了 Hint Framework 的支持，允许在查询中加入注释，让查询优化器优化逻辑计划。目前支持的 hint 有三个：COALESCE、REPARTITION、BROADCAST，其中 COALESCE、REPARTITION 这两个是 SparkSQL 2.4 开始支持。SELECT /*+ COALESCE(2) */ ...SELECT /*+ REPARTITION(10) */ ...# 这两个 hint 是从 SparkSQL 2.4 开始支持SELECT /*+ MAPJOIN(a) */ ...SELECT /*+ BROADCASTJOIN(a) */ ...SELECT /*+ BROADCAST(a) */ ...# 该 hint 是从 SparkSQL 2.2 开始支持</code></pre><pre><code class="python">## SparkSQL 性能调优参数# 1，spark.hadoopRDD.ignoreEmptySplits默认是false，如果是true，则会忽略那些空的splits，减小task的数量。# 2，spark.hadoop.mapreduce.input.fileinputformat.split.minsize是用于聚合input的小文件，用于控制每个mapTask的输入文件，防止小文件过多时候，产生太多的task。# 3，spark.sql.autoBroadcastJoinThreshold &amp;&amp; spark.sql.broadcastTimeout用于控制在 spark sql 中使用 BroadcastJoin 时候表的大小阈值，适当增大可以让一些表走 BroadcastJoin，提升性能，但是如果设置太大又会造成 driver 内存压力，而 broadcastTimeout 是用于控制 Broadcast 的 Future 的超时时间，默认是 300s，可根据需求进行调整。# 4，spark.sql.adaptive.enabled &amp;&amp; spark.sql.adaptive.shuffle.targetPostShuffleInputSize该参数是用于开启 spark 的自适应执行，后面的 targetPostShuffleInputSize 是用于控制之后的 shuffle 阶段的平均输入数据大小，防止产生过多的task。# 5，spark.sql.parquet.mergeSchema默认 false。当设为 true，parquet 会聚合所有 parquet 文件的 schema，否则是直接读取 parquet summary 文件，或者在没有 parquet summary 文件时候随机选择一个文件的 schema 作为最终的 schema。# 6，spark.sql.files.opencostInBytes该参数默认 4M，表示小于 4M 的小文件会合并到一个分区中，用于减小小文件，防止太多单个小文件占一个分区情况。# 7，spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version1 或者 2，默认是 1. MapReduce-4815 详细介绍了 fileoutputcommitter 的原理，实践中设置了 version=2 的比默认 version=1 的减少了70%以上的 commit 时间，但是1更健壮，能处理一些情况下的异常。</code></pre><h2 id="Spark内存管理详解"><a href="#Spark内存管理详解" class="headerlink" title="Spark内存管理详解"></a><strong>Spark内存管理详解</strong></h2><p>我们知道在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p><p>另外，Spark 1.6 之前使用的是静态内存管理 (StaticMemoryManager) 机制，</p><p>StaticMemoryManager 也是 Spark 1.6 之前唯一的内存管理器。在 Spark1.6 之后引入了统一内存管理</p><p>(UnifiedMemoryManager) 机制，UnifiedMemoryManager 是 Spark 1.6 之后默认的内存管理器，1.6 之前采用的静态管理（StaticMemoryManager）方式仍被保留，可通过配置 spark.memory.useLegacyMode 参数启用。</p><p>这里仅对<u><em><strong>统一内存管理模块</strong></em></u> (UnifiedMemoryManager) 机制进行分析。</p><h3 id="一、Executor内存总体布局"><a href="#一、Executor内存总体布局" class="headerlink" title="一、Executor内存总体布局"></a><strong>一、Executor内存总体布局</strong></h3><p>默认情况下，Executor不开启堆外内存，因此整个 Executor 端内存布局如下图所示：</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712101630401.png" alt="image-20200712101630401" style="zoom:50%;"><p>我们可以看到在Yarn集群管理模式中，Spark 以 Executor Container 的形式在 NodeManager 中运行，其可使用的内存上限由</p><p>yarn.scheduler.maximum-allocation-mb 指定，我们称之为 MonitorMemory。</p><p>整个Executor内存区域分为两块：</p><pre><code class="python">## 1. JVM堆外内存大小由 spark.yarn.executor.memoryOverhead 参数指定。默认大小为 executorMemory * 0.10, with minimum of 384m。此部分内存主要用于JVM自身，字符串, NIO Buffer（Driect Buffer）等开销。此部分为用户代码及Spark 不可操作的内存，不足时可通过调整参数解决。## 2. 堆内内存（ExecutorMemory）大小由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置，即JVM最大分配的堆内存 (-Xmx)。Spark为了更高效的使用这部分内存，对这部分内存进行了逻辑上的划分管理。我们在下面的统一内存管理会详细介绍。# NOTES:对于Yarn集群，存在: ExecutorMemory + MemoryOverhead &lt;= MonitorMemory，若应用提交之时，指定的 ExecutorMemory 与 MemoryOverhead 之和大于 MonitorMemory，则会导致 Executor 申请失败；若运行过程中，实际使用内存超过上限阈值，Executor 进程会被 Yarn 终止掉 (kill)。</code></pre><h3 id="二、统一内存管理"><a href="#二、统一内存管理" class="headerlink" title="二、统一内存管理"></a><strong>二、统一内存管理</strong></h3><p>Spark 1.6之后引入了统一内存管理，包括了堆内内存 (On-heap Memory) 和堆外内存 (Off-heap Memory) 两大区域，下面对这两块区域进行详细的说明。</p><p><strong>1. 堆内内存 (On-heap Memory)</strong></p><p>默认情况下，Spark 仅仅使用了堆内内存。Spark 对堆内内存的管理是一种逻辑上的“规划式”的管理，Executor 端的堆内内存区域在逻辑上被划分为以下四个区域：</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712102003104.png" alt="image-20200712102003104" style="zoom:50%;"><pre><code class="python">#1 执行内存 (Execution Memory) : 主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据；#2 存储内存 (Storage Memory) : 主要用于存储 spark 的 cache 数据，例如RDD的缓存、unroll数据；#3 用户内存（User Memory）: 主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息；#4 预留内存（Reserved Memory）: 系统预留内存，会用来存储Spark内部对象。Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前 记录 这些内存，我们来看其具体流程：    申请内存 ：        Spark 在代码中 new 一个对象实例        JVM 从堆内内存分配空间，创建对象并返回对象引用        Spark 保存该对象的引用，记录该对象占用的内存    释放内存 ：        Spark 记录该对象释放的内存，删除该对象的引用        等待 JVM 的垃圾回收机制释放该对象占用的堆内内存</code></pre><p>下面的图对这个四个内存区域的分配比例做了详细的描述：</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712102136759.png" alt="image-20200712102136759" style="zoom:50%;"></li></ol><pre><code class="python">## （1）预留内存 (Reserved Memory)系统预留内存，会用来存储Spark内部对象。其大小在代码中是写死的，其值等于 300MB，这个值是不能修改的（如果在测试环境下，我们可以通过 spark.testing.reservedMemory 参数进行修改）；如果Executor分配的内存小于 1.5 * 300 = 450M 时，Executor将无法执行。##（2）存储内存 (Storage Memory)主要用于存储 spark 的 cache 数据，例如 RDD 的缓存、广播（Broadcast）数据、和 unroll 数据。内存占比为 UsableMemory * spark.memory.fraction * spark.memory.storageFraction，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的30%（1 * 0.6 * 0.5 = 0.3）。在 UnifiedMemory 管理中，这两部分内存可以相互借用，具体借用机制我们下一小节会详细介绍。##（3）执行内存 (Execution Memory)主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据。内存占比为 UsableMemory * spark.memory.fraction * (1 - spark.memory.storageFraction)，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的30%（1 * 0.6 * (1 - 0.5) = 0.3）。在 UnifiedMemory 管理中，这两部分内存可以相互借用，具体借用机制我们下一小节会详细介绍。##（4）其他/用户内存 (Other/User Memory) 主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息。内存占比为 UsableMemory * (1 - spark.memory.fraction)，在Spark2+ 中，默认占可用内存的40%（1 * (1 - 0.6) = 0.4）。其中，usableMemory = executorMemory - reservedMemory，这个就是 Spark 可用内存。NOTES:# （1）为什么设置300M预留内存统一内存管理最初版本other这部分内存没有固定值 300M 设置，而是和静态内存管理相似，设置的百分比，最初版本占 25%。百分比设置在实际使用中出现了问题，若给定的内存较低时，例如 1G，会导致 OOM，具体讨论参考这里 Make unified memory management work with small heaps。因此，other这部分内存做了修改，先划出 300M 内存。#（2）spark.memory.fraction 由 0.75 降至 0.6spark.memory.fraction 最初版本的值是 0.75，很多分析统一内存管理这块的文章也是这么介绍的，同样的，在使用中发现这个值设置的偏高，导致了 gc 时间过长，spark 2.0 版本将其调整为 0.6，详细谈论参见 Reduce spark.memory.fraction default to avoid overrunning old gen in JVM default config。</code></pre><p><strong>2. 堆外内存 (Off-heap Memory)</strong></p><p>Spark 1.6 开始引入了 Off-heap memory 。这种模式不在 JVM 内申请内存，而是调用 Java 的 unsafe 相关 API 进行诸如 C 语言里面的 malloc() 直接向操作系统申请内存。这种方式下 Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。另外，堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。，缺点是必须自己编写内存申请和释放的逻辑。</p><p>默认情况下Off-heap模式的内存并不启用，我们可以通过 <u><em><strong>spark.memory.offHeap.enabled</strong></em></u> 参数开启，并由 <em><strong><u>spark.memory.offHeap.size</u></strong></em> 指定堆外内存的大小，单位是字节（占用的空间划归 JVM OffHeap 内存）。</p><p>如果堆外内存被启用，那么 Executor 内将同时存在堆内和堆外内存，两者的使用互补影响，这个时候 Executor 中的 Execution 内存是堆内的 Execution 内存和堆外的 Execution 内存之和，同理，Storage 内存也一样。其内存分布如下图所示：</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712102544470.png" alt="image-20200712102544470" style="zoom:50%;"><pre><code class="python">相比堆内内存，堆外内存只区分 Execution 内存和 Storage 内存：#（1）存储内存 (Storage Memory)内存占比为 maxOffHeapMemory * spark.memory.storageFraction，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的50%（1 * 0.5 = 0.5）。在 UnifiedMemory 管理中，这两部分内存可以相互借用，具体借用机制我们下一小节会详细介绍。#（2）执行内存 (Execution Memory)内存占比为 maxOffHeapMemory * (1 - spark.memory.storageFraction)，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的50%（1 * (1 - 0.5) = 0.5）。在 UnifiedMemory 管理中，这两部分内存可以相互借用，具体借用机制我们下一小节会详细介绍。</code></pre><p><strong>3. Execution 内存和 Storage 内存动态占用机制</strong></p><p>在 Spark 1.5 之前，Execution 内存和 Storage 内存分配是静态的，换句话说就是如果 Execution 内存不足，即使 Storage 内存有很大空闲程序也是无法利用到的；反之亦然。</p><p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成”一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。</p><p>统一内存管理机制，与静态内存管理最大的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域：</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712103204824.png" alt="image-20200712103204824" style="zoom:50%;"><pre><code class="python">其中最重要的优化在于动态占用机制，其规则如下：## 程序提交的时候我们都会设定基本的 Execution 内存和 Storage 内存区域（通过 spark.memory.storageFraction 参数设置）。我们用 onHeapStorageRegionSize 来表示 spark.storage.storageFraction 划分的存储内存区域。这部分内存是不可以被驱逐(Evict)的存储内存（但是如果空闲是可以被占用的）。## 当计算内存不足时，可以借用 onHeapStorageRegionSize 中未使用部分，且 Storage 内存的空间被对方占用后，需要等待执行内存自己释放，不能抢占。## 若实际 StorageMemory 使用量超过 onHeapStorageRegionSize，那么当计算内存不足时，可以驱逐并借用 StorageMemory – onHeapStorageRegionSize 部分，而 onHeapStorageRegionSize 部分不可被抢占。## 反之，当存储内存不足时（存储空间不足是指不足以放下一个完整的 Block），也可以借用计算内存空间；但是 Execution 内存的空间被存储内存占用后，是可让对方将占用的部分转存到硬盘，然后“归还”借用的空间。## 如果双方的空间都不足时，则存储到硬盘；将内存中的块存储到磁盘的策略是按照 LRU 规则进行的。说明（1）出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。可通过配置spark.memory.useLegacyMode 参数启用。（2）spark.memory.storageFraction 是不可被驱逐的内存空间。只有空闲的时候能够被执行内存占用，但是不能被驱逐抢占。（3）Storage 内存的空间被对方占用后，目前的实现是无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂；而且 Shuffle 过程产生的文件在后面一定会被使用到，而 Cache 在内存的数据不一定在后面使用。在 Unified Memory Management in Spark 1.6 中详细讲解了为何选择这种策略，简单总结如下:数据清除的开销 : 驱逐storage内存的开销取决于 storage level，MEMORY_ONLY 可能是最昂贵的，因为需要重新计算，MEMORY_AND_DISK_SER 正好相反，只涉及到磁盘IO。溢写 execution 内存到磁盘的开销并不昂贵，因为 execution 存储的数据格式紧凑(compact format)，序列化开销低。并且，清除的 storage 内存可能不会被用到，但是，可以预见的是，驱逐的 execution 内存是必然会再被读到内存的，频繁的驱除重读 execution 内存将导致昂贵的开销。实现的复杂度 : storage 内存的驱逐是容易实现的，只需要使用已有的方法，drop 掉 block。execution 则复杂的多，首先，execution 以 page 为单位管理这部分内存，并且确保相应的操作至少有 one page ，如果把这 one page 内存驱逐了，对应的操作就会处于饥饿状态。此外，还需要考虑 execution 内存被驱逐的情况下，等待 cache 的 block 如何处理。（4）上面说的借用对方的内存需要借用方和被借用方的内存类型都一样，都是堆内内存或者都是堆外内存，不存在堆内i内存不够去借用堆外内存的空间。</code></pre><h3 id="小结："><a href="#小结：" class="headerlink" title="小结："></a><strong>小结：</strong></h3><pre><code class="python">## 1：内存组成部分总的内存  =  预留内存(300Mb) +  可用内存可用内存 =   统一内存（60%）  +   其他（40%）统一内存 =  存储内存（Storeage）（50%）  +  执行内存(Execution)（50%）--executor-memory  1g可用内存  =  1g – 300mb  = 724mb统一内存  = （1g-300） *0.6 = 434.4mb存储内存 =  （1g-300） *0.6 *0.5 = 217.2mb执行内存 =  （1g-300） *0.6 *0.5 = 217.2mb## 2:动态占用机制1，如果双方的空间都满了，溢出到磁盘。2，如果对方有空闲，都可以占用   双向占用3，单向收回，Execution收回 自己被Storage占用的空间。</code></pre><p><strong>4. 任务内存管理（Task Memory Manager）</strong></p><pre><code class="python">Executor 中任务以线程的方式执行，各线程共享JVM的资源（即 Execution 内存），任务之间的内存资源没有强隔离（任务没有专用的Heap区域）。因此，可能会出现这样的情况：先到达的任务可能占用较大的内存，而后到的任务因得不到足够的内存而挂起。在 Spark 任务内存管理中，使用 HashMap 存储任务与其消耗内存的映射关系。每个任务可占用的内存大小为潜在可使用计算内存（ 潜在可使用计算内存为: 初始计算内存 + 可抢占存储内存）的 1/2n ~ 1/n，当剩余内存为小于 1/2n 时，任务将被挂起，直至有其他任务释放执行内存，而满足内存下限 1/2n，任务被唤醒。其中 n 为当前 Executor 中活跃的任务树。比如如果 Execution 内存大小为 10GB，当前 Executor 内正在运行的 Task 个数为5，则该 Task 可以申请的内存范围为 10 / (2 * 5) ~ 10 / 5，也就是 1GB ~ 2GB 的范围。任务执行过程中，如果需要更多的内存，则会进行申请，如果存在空闲内存，则自动扩容成功，否则，将抛出 OutOffMemroyError。每个 Executor 中可同时运行的任务数由 Executor 分配的 CPU 的核数 N 和每个任务需要的 CPU 核心数 C 决定。其中：N = spark.executor.coresC = spark.task.cpus## 由此每个 Executor 的最大任务并行度可表示为 : TP = N / C 。其中，C 值与应用类型有关，大部分应用使用默认值 1 即可，因此，影响 Executor 中最大任务并行度（最大活跃task数）的主要因素是 N。依据 Task 的内存使用特征，前文所述的 Executor 内存模型可以简单抽象为下图所示模型：</code></pre><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712104935038.png" alt="image-20200712104935038" style="zoom:50%;"><p>其中，Executor 向 yarn 申请的总内存可表示为 : M &#x3D; M1 + M2 。<br>如果考虑堆外内存则大概是如下结构：</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712105012761.png" alt="image-20200712105012761" style="zoom:50%;"><p><strong>5. 一个示例</strong></p><p><strong>(1）只用了堆内内存</strong></p><p>现在我们提交的 Spark 作业关于内存的配置如下：–executor-memory 18g<br>由于没有设置spark.memory.fraction 和 spark.memory.storageFraction 参数，我们可以看到 Spark UI 关于 Storage Memory 的显示如下：</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712105156727.png" alt="image-20200712105156727" style="zoom:50%;"><p>上图很清楚地看到 Storage Memory 的可用内存是 10.1GB，这个数是咋来的呢？根据前面的规则，我们可以得出以下的计算：</p><pre><code class="python">systemMemory = spark.executor.memoryreservedMemory = 300MBusableMemory = systemMemory - reservedMemoryStorageMemory= usableMemory * spark.memory.fraction * spark.memory.storageFraction#1 如果我们把数据代进去，得出以下的结果：systemMemory = 18Gb = 19327352832 字节reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800usableMemory = systemMemory - reservedMemory = 19327352832 - 314572800 = 19012780032StorageMemory = usableMemory * spark.memory.fraction * spark.memory.storageFraction              = 19012780032 * 0.6 * 0.5 = 5703834009.6 = 5.312109375GB#2 和上面的 10.1GB 对不上啊。为什么呢？这是因为 Spark UI 上面显示的 Storage Memory 可用内存其实等于 Execution 内存和 Storage 内存之和，也就是 usableMemory * spark.memory.fraction :StorageMemory = usableMemory * spark.memory.fraction              = 19012780032 * 0.6 = 11407668019.2 = 10.62421GB#3 还是不对，这是因为我们虽然设置了 --executor-memory 18g ，但是 Spark 的 Executor 端通过 Runtime.getRuntime.maxMemory 拿到的内存其实没这么大，只有 17179869184 字节，所以 systemMemory=17179869184，然后计算的数据如下：ystemMemory = 17179869184 字节reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384StorageMemory= usableMemory * spark.memory.fraction             = 16865296384 * 0.6 = 9.42421875 GB#4 我们通过将上面的 16865296384 * 0.6 字节除于 1024 * 1024 * 1024 转换成 9.42421875 GB，和 UI 上显示的还是对不上，这是因为 Spark UI 是通过除于 1000 * 1000 * 1000 将字节转换成 GB，如下：systemMemory = 17179869184 字节reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384StorageMemory = usableMemory * spark.memory.fraction              = 16865296384 * 0.6 字节 =  16865296384 * 0.6 / (1000 * 1000 * 1000) = 10.1GB现在终于对上了。</code></pre><p>具体将字节转换成 GB 的计算逻辑如下(core 模块下面的 &#x2F;core&#x2F;src&#x2F;main&#x2F;resources&#x2F;org&#x2F;apache&#x2F;spark&#x2F;ui&#x2F;static&#x2F;utils.js)：</p><pre><code class="javascript">functionformatBytes(bytes, type) &#123;    if(type !==&#39;display&#39;)returnbytes;    if(bytes == 0)return&#39;0.0 B&#39;;    vark = 1000;    vardm = 1;    varsizes = [&#39;B&#39;,&#39;KB&#39;,&#39;MB&#39;,&#39;GB&#39;,&#39;TB&#39;,&#39;PB&#39;,&#39;EB&#39;,&#39;ZB&#39;,&#39;YB&#39;];    vari = Math.floor(Math.log(bytes) / Math.log(k));    returnparseFloat((bytes / Math.pow(k, i)).toFixed(dm)) +&#39; &#39;+ sizes[i];&#125;</code></pre><p>我们设置了 –executor-memory 18g，但是 Spark 的 Executor 端通过</p><p>Runtime.getRuntime.maxMemory 拿到的内存其实没这么大，只有 17179869184 字节，这个数据是怎么计算的？</p><p>**<u><em>Runtime.getRuntime.maxMemory 是程序能够使用的最大内存，其值会比实际配置的执行器内存的值小。这是因为内存分配池的堆部分划分为 Eden，Survivor 和 Tenured 三部分空间，而这里面一共包含了两个 Survivor 区域，</em></u>**而这两个 Survivor 区域在任何时候我们只能用到其中一个，所以我们可以使用下面的公式进行描述 :</p><pre><code class="scala">ExecutorMemory = Eden + 2 * Survivor + TenuredRuntime.getRuntime.maxMemory = Eden + Survivor + Tenured上面的 17179869184 字节可能因为你的 GC 配置不一样得到的数据不一样，但是上面的计算公式是一样的。</code></pre><p><strong>（2）用了堆内和堆外内存</strong></p><p>现在如果我们启用了堆外内存，情况会怎样呢？我们的内存相关配置如下：</p><pre><code class="scala">spark.executor.memory           18gspark.memory.offHeap.enabled   truespark.memory.offHeap.size       10737418240</code></pre><p>从上面可以看出，堆外内存为 10GB，现在 Spark UI 上面显示的 Storage Memory 可用内存为 20.9GB，如下：</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712110238920.png" alt="image-20200712110238920" style="zoom:50%;"><p>其实 Spark UI 上面显示的 Storage Memory 可用内存等于堆内内存和堆外内存之和，计算公式如下：</p><p>堆内:</p><pre><code class="scala">systemMemory = 17179869184 字节reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384totalOnHeapStorageMemory = usableMemory * spark.memory.fraction                         = 16865296384 * 0.6 = 10119177830</code></pre><p>堆外:</p><pre><code class="scala">totalOffHeapStorageMemory = spark.memory.offHeap.size = 10737418240 总 Storage 内存:StorageMemory = totalOnHeapStorageMemory + totalOffHeapStorageMemory              = (10119177830 + 10737418240) 字节              = (20856596070 / (1000 * 1000 * 1000)) GB              = 20.9 GB</code></pre><p><strong>6. Executor内存参数调优</strong></p><p>（1<u><em>） <strong>Executor JVM Used Memory Heuristic</strong></em>**</u></p><ul><li><p><strong>现象：</strong>配置的executor内存比实际使用的JVM最大使用内存还要大很多。</p></li><li><p><strong>原因：</strong>这意味着 executor 内存申请过多了，实际上并不需要使用这么多内存。</p></li><li><p><strong>解决方案：</strong>将 spark.executor.memory 设置为一个比较小的值。</p><pre><code class="scala">spark.executor.memory : 16 GBMax executor peak JVM used memory : 6.6 GB Suggested spark.executor.memory : 7 GB</code></pre></li></ul><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712110643913.png" alt="image-20200712110643913" style="zoom:50%;"><p><em><strong><u>（2） Executor Unified Memory Heuristic</u></strong></em></p><ul><li><p><strong>现象：</strong>分配的统一内存 (Unified Memory &#x3D; Storage Memory + Execution Memory) 比 executor 实际使用的统一内存大的多。</p></li><li><p><strong>原因：</strong>这意味着不需要这么大的统一内存。</p></li><li><p><strong>解决方案：</strong>降低 spark.memory.fraction 的比例。</p><pre><code class="scala">spark.executor.memory : 10 GBspark.memory.fraction : 0.6Allocated unified memory : 6 GBMax peak JVM userd memory : 7.2 GBMax peak unified memory : 1.2 GB Suggested spark.memory.fraction : 0.2</code></pre></li></ul><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712111944876.png" alt="image-20200712111944876" style="zoom:50%;"><p><strong>（3）Executor OOM类错误 （错误代码 137、143等）</strong></p><p>该类错误一般是由于 Heap（M2）已达上限，Task 需要更多的内存，而又得不到足够的内存而导致。因此，解决方案要从增加每个 Task 的内存使用量，满足任务需求 或 降低单个 Task 的内存消耗量，从而使现有内存可以满足任务运行需求两个角度出发。因此有如下解决方案：</p><pre><code class="python">## 法一：增加单个task的内存使用量增加最大 Heap值，即上图中 M2 的值，使每个 Task 可使用内存增加。降低 Executor 的可用 Core 的数量 N , 使 Executor 中同时运行的任务数减少，在总资源不变的情况下，使每个 Task 获得的内存相对增加。当然，这会使得 Executor 的并行度下降。可以通过调高 spark.executor.instances 参数来申请更多的 executor 实例（或者通过 spark.dynamicAllocation.enabled 启动动态分配），提高job的总并行度。## 法二：降低单个Task的内存消耗量降低单个Task的内存消耗量可从配置方式和调整应用逻辑两个层面进行优化：配置方式减少每个 Task 处理的数据量，可降低 Task 的内存开销，在 Spark 中，每个 partition 对应一个处理任务 Task，因此，在数据总量一定的前提下，可以通过增加 partition 数量的方式来减少每个 Task 处理的数据量，从而降低 Task 的内存开销。针对不同的 Spark 应用类型，存在不同的 partition 配置参数 :P = spark.default.parallism (非SQL应用)P = spark.sql.shuffle.partition (SQL 应用)通过增加 P 的值，可在一定程度上使 Task 现有内存满足任务运行。注: 当调整一个参数不能解决问题时，上述方案应进行协同调整。</code></pre><p><strong>a.调整应用逻辑</strong></p><p>Executor OOM 一般发生 Shuffle 阶段，该阶段需求计算内存较大，且应用逻辑对内存需求有较大影响，下面举例就行说明：</p><p>选择合适的算子，如 groupByKey 转换为 reduceByKey。</p><p>一般情况下，groupByKey 能实现的功能使用 reduceByKey 均可实现，而 ReduceByKey 存在 Map 端的合并，可以有效减少传输带宽占用及 Reduce 端内存消耗。</p><p><strong>b.避免数据倾斜 (data skew)</strong></p><p>Data Skew 是指任务间处理的数据量存大较大的差异。</p><p>如左图所示，key 为 010 的数据较多，当发生 shuffle 时，010 所在分区存在大量数据，不仅拖慢 Job 执行（Job 的执行时间由最后完成的任务决定）。而且导致 010 对应 Task 内存消耗过多，可能导致 OOM。</p><p>右图，经过预处理（加盐，此处仅为举例说明问题，解决方法不限于此）可以有效减少 Data Skew 导致的问题。</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712112334330.png" alt="image-20200712112334330" style="zoom:50%;"><p><em><u><strong>（4）Execution Memory Spill Heuristic</strong></u></em></p><ul><li><p><strong>现象：</strong>在 stage 3 发现执行内存溢出。Shuffle read bytes 和 spill 分布均匀。这个 stage 有 200 个 tasks。</p></li><li><p><strong>原因：</strong>执行内存溢出，意味着执行内存不足。跟上面的 OOM 错误一样，只是执行内存不足的情况下不会报 OOM 而是会将数据溢出到磁盘。但是整个性能很难接受。</p></li><li><p><strong>解决方案：</strong>同 3。</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712112421393.png" alt="image-20200712112421393" style="zoom:50%;"><p><em><u><strong>5） Executor GC Heuristic</strong></u></em></p><ul><li><p><strong>现象：</strong>Executor 花费很多时间在 GC。</p></li><li><p><strong>原因：</strong>可以通过-verbose:gc</p><p>-XX:+PrintGCDetails</p><p>-XX:+PrintGCTimeStamps 查看 GC 情况</p></li><li><p><strong>解决方案：</strong> Garbage Collection Tuning</p></li></ul><p><strong>（6）Beyond … memory, killed by yarn.</strong></p><p>出现该问题原因是由于实际使用内存上限超过申请的内存上限而被 Yarn 终止掉了, 首先说明 Yarn 中 Container 的内存监控机制：</p><ul><li>Container 进程的内存使用量 : 以 Container 进程为根的进程树中所有进程的内存使用总量。</li><li>Container 被杀死的判断依据 : 进程树总内存（物理内存或虚拟内存）使用量超过向 Yarn 申请的内存上限值，则认为该 Container 使用内存超量，可以被“杀死”。</li></ul><p>因此，对该异常的分析要从是否存在子进程两个角度出发。</p><p><strong>a. 不存在子进程</strong></p><p>Overhead) 不足，依据 Yarn 内存使用情况有如下两种方案:</p><p><strong>法一：</strong>如果，M (spark.executor.memory) 未达到 Yarn 单个 Container 允许的上限时，可仅增加 M1（spark.yarn.executor.memoryOverhead），从而增加 M；如果，M 达到 Yarn 单个 Container 允许的上限时，增加 M1，降低 M2。</p><p>注意二者之各要小于 Container 监控内存量，否则伸请资源将被 yarn 拒绝。</p><p><strong>法二：</strong>减少可用的 Core 的数量 N，使并行任务数减少，从而减少 Overhead 开销</p><p><strong>b. 存在子进程</strong></p><p>Spark 应用中 Container 以 Executor（JVM进程）的形式存在，因此根进程为 Executor 对应的进程，而 Spark 应用向Yarn申请的总资源 M &#x3D; M1 + M2，都是以 Executor(JVM) 进程（非进程树）可用资源的名义申请的。</p><p>申请的资源并非一次性全量分配给 JVM 使用，而是先为 JVM 分配初始值，随后内存不足时再按比率不断进行扩容，直致达到 Container 监控的最大内存使用量 M。当 Executor 中启动了子进程（如调用 shell 等）时，子进程占用的内存（记为 S）就被加入 Container 进程树，此时就会影响 Executor 实际可使用内存资源（Executor 进程实际可使用资源变为: M - S），然而启动 JVM 时设置的可用最大资源为 M，且 JVM 进程并不会感知 Container 中留给自己的使用量已被子进程占用。因此，当 JVM 使用量达到 M - S，还会继续开劈内存空间，这就会导致 Executor 进程树使用的总内存量大于 M 而被 Yarn 杀死。</p><p>典形场景有:</p><ul><li>PySpark（Spark已做内存限制，一般不会占用过大内存）</li><li>自定义Shell调用</li></ul><p>其解决方案分别为:</p><p>a. PySpark场景：</p><ul><li>如果，M 未达到 Yarn 单个 Container 允许的上限时，可仅增加 M1 ，从而增加 M；如果，M 达到 Yarn 单个 Container 允许的上限时，增加 M1，降低 M2。</li><li>减少可用的 Core 的数量 N，使并行任务数减少，从而减少 Overhead 开销</li></ul><p>b. 自定义 Shell 场景:（OverHead 不足为假象）</p><p>调整子进程可用内存量 (通过单机测试，内存控制在 Container 监控内存以内，且为 Spark 保留内存等留有空间）。</p><h3 id="7-淘汰和罗盘"><a href="#7-淘汰和罗盘" class="headerlink" title="7. 淘汰和罗盘"></a><strong>7. 淘汰和罗盘</strong></h3><pre><code class="python">由于同一个Executor的所有的计算任务共享有限的存储内存空间，当有新的Block需要缓存但是剩余空间不足且无法动态占用时，就要对LinkedHashMap中的旧Block进行淘汰（Eviction)，而被淘汰的Block如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘（Drop），否则直接删除该Block。存储内存的淘汰规则为：    #1:被淘汰的旧Block要与新Block的MemoryMode相同，即同属于堆外或堆内内存    #2:新旧Block不能属于同一个RDD，避免循环淘汰    #3:旧Block所属RDD不能处于被读状态，避免引发一致性问题    #4:遍历LinkedHashMap中Block，按照最近最少使用（LRU）的顺序淘汰，直到满足新Block所需的空间。其中LRU是LinkedHashMap的特性。落盘的流程则比较简单，如果其存储级别符合_useDisk为true的条件，再根据其_deserialized判断是否是非序列化的形式，若是则对其进行序列化，最后将数据存储到磁盘，在Storage模块中更新其信息。</code></pre><h3 id="8-Shuffle的内存占用"><a href="#8-Shuffle的内存占用" class="headerlink" title="8: Shuffle的内存占用"></a><strong>8: Shuffle的内存占用</strong></h3><pre><code class="python">执行内存主要用来存储任务在执行Shuffle时占用的内存，Shuffle是按照一定规则对RDD数据重新分区的过程，我们来看Shuffle的Write和Read两阶段对执行内存的使用：    ## Shuffle Write        若在map端选择普通的排序方式，会采用ExternalSorter进行外排，在内存中存储数据时主要占用堆内执行空间。        若在map端选择Tungsten的排序方式，则采用ShuffleExternalSorter直接对以序列化形式存储的数据排序，在内存中存储数据时可以占用堆外或堆内执行空间，取决于用户是否开启了堆外内存以及堆外执行内存是否足够。    ## Shuffle Read        在对reduce端的数据进行聚合时，要将数据交给Aggregator处理，在内存中存储数据时占用堆内执行空间。        如果需要进行最终结果排序，则要将再次将数据交给ExternalSorter处理，占用堆内执行空间。在ExternalSorter和Aggregator中，Spark会使用一种叫AppendOnlyMap的哈希表在堆内执行内存中存储数据，但在Shuffle过程中所有数据并不能都保存到该哈希表中，当这个哈希表占用的内存会进行周期性地采样估算，当其大到一定程度，无法再从MemoryManager申请到新的执行内存时，Spark就会将其全部内容存储到磁盘文件中，这个过程被称为溢存(Spill)，溢存到磁盘的文件最后会被归并(Merge)。Shuffle Write阶段中用到的Tungsten是Databricks公司提出的对Spark优化内存和CPU使用的计划[4]，解决了一些JVM在性能上的限制和弊端。Spark会根据Shuffle的情况来自动选择是否采用Tungsten排序。Tungsten采用的页式内存管理机制建立在MemoryManager之上，即Tungsten对执行内存的使用进行了一步的抽象，这样在Shuffle过程中无需关心数据具体存储在堆内还是堆外。每个内存页用一个MemoryBlock来定义，并用Object obj和long offset这两个变量统一标识一个内存页在系统内存中的地址。堆内的MemoryBlock是以long型数组的形式分配的内存，其obj的值为是这个数组的对象引用，offset是long型数组的在JVM中的初始偏移地址，两者配合使用可以定位这个数组在堆内的绝对地址；堆外的MemoryBlock是直接申请到的内存块，其obj为null，offset是这个内存块在系统内存中的64位绝对地址。Spark用MemoryBlock巧妙地将堆内和堆外内存页统一抽象封装，并用页表(pageTable)管理每个Task申请到的内存页。Tungsten页式管理下的所有内存用64位的逻辑地址表示，由页号和页内偏移量组成：#1. 页号：占13位，唯一标识一个内存页，Spark在申请内存页之前要先申请空闲页号。2. 页内偏移量：占51位，是在使用内存页存储数据时，数据在页内的偏移地址。有了统一的寻址方式，Spark可以用64位逻辑地址的指针定位到堆内或堆外的内存，整个Shuffle Write排序的过程只需要对指针进行排序，并且无需反序列化，整个过程非常高效，对于内存访问效率和CPU使用效率带来了明显的提升。</code></pre><h2 id="SparkSQL用UDAF实现Bitmap函数"><a href="#SparkSQL用UDAF实现Bitmap函数" class="headerlink" title="SparkSQL用UDAF实现Bitmap函数"></a><strong>SparkSQL用UDAF实现Bitmap函数</strong></h2><pre><code class="SQL">使用phoenix在HBase中创建测试表，字段使用VARBINARY类型CREATE TABLE IF NOT EXISTS test_binary (date VARCHAR NOT NULL,dist_mem VARBINARY CONSTRAINT test_binary_pk PRIMARY KEY (date) ) SALT_BUCKETS=6;</code></pre><h3 id="实现自定义聚合函数bitmap"><a href="#实现自定义聚合函数bitmap" class="headerlink" title="实现自定义聚合函数bitmap"></a>实现自定义聚合函数bitmap</h3><pre><code class="java">import org.apache.spark.sql.Row;import org.apache.spark.sql.expressions.MutableAggregationBuffer;import org.apache.spark.sql.expressions.UserDefinedAggregateFunction;import org.apache.spark.sql.types.DataType;import org.apache.spark.sql.types.DataTypes;import org.apache.spark.sql.types.StructField;import org.apache.spark.sql.types.StructType;import org.roaringbitmap.RoaringBitmap; import java.io.*;import java.util.ArrayList;import java.util.List; /** * 实现自定义聚合函数Bitmap */public class UdafBitMap extends UserDefinedAggregateFunction &#123;    @Override    public StructType inputSchema() &#123;        List&lt;StructField&gt; structFields = new ArrayList&lt;&gt;();        structFields.add(DataTypes.createStructField(&quot;field&quot;, DataTypes.BinaryType, true));        return DataTypes.createStructType(structFields);    &#125;     @Override    public StructType bufferSchema() &#123;        List&lt;StructField&gt; structFields = new ArrayList&lt;&gt;();        structFields.add(DataTypes.createStructField(&quot;field&quot;, DataTypes.BinaryType, true));        return DataTypes.createStructType(structFields);    &#125;     @Override    public DataType dataType() &#123;        return DataTypes.LongType;    &#125;     @Override    public boolean deterministic() &#123;        //是否强制每次执行的结果相同        return false;    &#125;     @Override    public void initialize(MutableAggregationBuffer buffer) &#123;        //初始化        buffer.update(0, null);    &#125;     @Override    public void update(MutableAggregationBuffer buffer, Row input) &#123;        // 相同的executor间的数据合并        // 1. 输入为空直接返回不更新        Object in = input.get(0);        if(in == null)&#123;            return ;        &#125;        // 2. 源为空则直接更新值为输入        byte[] inBytes = (byte[]) in;        Object out = buffer.get(0);        if(out == null)&#123;            buffer.update(0, inBytes);            return ;        &#125;        // 3. 源和输入都不为空使用bitmap去重合并        byte[] outBytes = (byte[]) out;        byte[] result = outBytes;        RoaringBitmap outRR = new RoaringBitmap();        RoaringBitmap inRR = new RoaringBitmap();        try &#123;            outRR.deserialize(new DataInputStream(new ByteArrayInputStream(outBytes)));            inRR.deserialize(new DataInputStream(new ByteArrayInputStream(inBytes)));            outRR.or(inRR);            ByteArrayOutputStream bos = new ByteArrayOutputStream();            outRR.serialize(new DataOutputStream(bos));            result = bos.toByteArray();        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;        buffer.update(0, result);    &#125;     @Override    public void merge(MutableAggregationBuffer buffer1, Row buffer2) &#123;        //不同excutor间的数据合并        update(buffer1, buffer2);    &#125;     @Override    public Object evaluate(Row buffer) &#123;        //根据Buffer计算结果        long r = 0l;        Object val = buffer.get(0);        if (val != null) &#123;            RoaringBitmap rr = new RoaringBitmap();            try &#123;                rr.deserialize(new DataInputStream(new ByteArrayInputStream((byte[]) val)));                r = rr.getLongCardinality();            &#125; catch (IOException e) &#123;                e.printStackTrace();            &#125;        &#125;        return r;    &#125;&#125;</code></pre><h3 id="调用示例"><a href="#调用示例" class="headerlink" title="调用示例"></a>调用示例</h3><pre><code class="java">/**     * 使用自定义函数解析bitmap     *     * @param sparkSession     * @return     */    private static void udafBitmap(SparkSession sparkSession) &#123;        try &#123;            Properties prop = PropUtil.loadProp(DB_PHOENIX_CONF_FILE);            // JDBC连接属性            Properties connProp = new Properties();            connProp.put(&quot;driver&quot;, prop.getProperty(DB_PHOENIX_DRIVER));            connProp.put(&quot;user&quot;, prop.getProperty(DB_PHOENIX_USER));            connProp.put(&quot;password&quot;, prop.getProperty(DB_PHOENIX_PASS));            connProp.put(&quot;fetchsize&quot;, prop.getProperty(DB_PHOENIX_FETCHSIZE));            // 注册自定义聚合函数            sparkSession.udf().register(&quot;bitmap&quot;,new UdafBitMap());            sparkSession                    .read()                    .jdbc(prop.getProperty(DB_PHOENIX_URL), &quot;test_binary&quot;, connProp)                    // sql中必须使用global_temp.表名，否则找不到                    .createOrReplaceGlobalTempView(&quot;test_binary&quot;);            //sparkSession.sql(&quot;select YEAR(TO_DATE(date)) year,bitmap(dist_mem) memNum from global_temp.test_binary group by YEAR(TO_DATE(date))&quot;).show();            sparkSession.sql(&quot;select date,bitmap(dist_mem) memNum from global_temp.test_binary group by date&quot;).show();        &#125; catch (Exception e) &#123;            e.printStackTrace();        &#125;    &#125;</code></pre><h2 id="ETL常见数据质量监控"><a href="#ETL常见数据质量监控" class="headerlink" title="ETL常见数据质量监控"></a><strong>ETL常见数据质量监控</strong></h2><img src="https://img-blog.csdnimg.cn/20190528154230375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mjg5MzY1MA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:80%;"><h2 id="数据质量监控方法"><a href="#数据质量监控方法" class="headerlink" title="数据质量监控方法"></a>数据质量监控方法</h2><h3 id="1、校验每天的记录数"><a href="#1、校验每天的记录数" class="headerlink" title="1、校验每天的记录数"></a>1、校验每天的记录数</h3><p>分析师遇到的最常见数据异常是其报告的输出突然降至0。</p><p>我们通常会发现最后的罪魁祸首是当天没有将新记录添加到相应的表中。</p><p>一种简单的检查方法是确保每天一个表中的新记录数&gt;0。</p><h3 id="2、NULL和0值校验"><a href="#2、NULL和0值校验" class="headerlink" title="2、NULL和0值校验"></a>2、NULL和0值校验</h3><p>分析师常遇到的第二个问题是NULL或0值。我们要保证每天增量数据中的NULL或0值不能超过新增数据的99%。要检查这一点，只需将一个循环脚本设置为每天用NULL或0计数一个表中的新记录数。如果看到记录数急剧增加，则可能存在转换错误或源业务系统就存在异常。</p><h3 id="3、每天新增的记录数波动范围"><a href="#3、每天新增的记录数波动范围" class="headerlink" title="3、每天新增的记录数波动范围"></a>3、每天新增的记录数波动范围</h3><p>某一天你发现数据量出现大幅增长或下降，而规则1和2都已校验通过。这种波动可能是正常的，比如电商行业某天的大促活动，或者社交软件的营销活动。但是也可能这就是异常的，是因为从源系统抽取了重复的记录。所以针对此种情况，我们也要制定数据质量规则，检查这些波动何时发生，并主动进行诊断。比如自动执行的一个简单的SQL过程，每天检查COUNT个新记录是否在7天跟踪平均值的误差范围内。阈值和误差范围可能因公司和产品而异，经验值一般是加减25％。当然，你可也可以直接和前一天的数据对比，增量不超过前一天的1倍。</p><h3 id="4、重复记录数据校验"><a href="#4、重复记录数据校验" class="headerlink" title="4、重复记录数据校验"></a>4、重复记录数据校验</h3><p>不管是电商系统或者是社交系统或者是物联网设备上报的数据，正常情况下都不会出现两条完全一样的记录（包括ID，时间，值都一样）。笔者曾遇到一个终端上报的两条数据完全一样的场景，导致我在做时间分段时候，划分不正确。所以，对数据值唯一性校验是有必要的。</p><h3 id="5、数据时间校验"><a href="#5、数据时间校验" class="headerlink" title="5、数据时间校验"></a>5、数据时间校验</h3><p>一般我们业务系统的数据都是带有时间戳的，这个时间戳肯定比当前的时间要小。但是由于采集数据设备异常（业务系统异常），我们会碰到“未来时间”的数据，那如果我们以时间作为分区，后期可能就会出现异常的分析结果。当然，如果你的公司业务是跨国的，你需要考虑时差因素。</p><h2 id="Hive实现数据抽样的三种方式"><a href="#Hive实现数据抽样的三种方式" class="headerlink" title="Hive实现数据抽样的三种方式"></a><strong>Hive实现数据抽样的三种方式</strong></h2><p>Hive提供了数据取样（SAMPLING）的功能，能够根据一定的规则进行数据抽样，目前支持数据块抽样，分桶抽样和随机抽样，具体如下所示：</p><pre><code class="SQL">1. 数据块抽样（tablesample()函数）1&gt;   tablesample(n percent)  根据hive表数据的大小按比例抽取数据，并保存到新的hive表中。如：抽取原hive表中10%的数据（注意：测试过程中发现，select语句不能带where条件且不支持子查询，可通过新建中间表或使用随机抽样解决）create table xxx_new as select * from xxx tablesample(10 percent)2&gt;  tablesample(n M)  指定抽样数据的大小，单位为M。3&gt;  tablesample(n rows)  指定抽样数据的行数，其中n代表每个map任务均取n行数据，map数量可通过hive表的简单查询语句确认（关键词：number of mappers: x)2.分桶抽样hive中分桶其实就是根据某一个字段Hash取模，放入指定数据的桶中，比如将表table_1按照ID分成100个桶，其算法是hash(id) % 100，这样，hash(id) % 100 = 0的数据被放到第一个桶中，hash(id) % 100 = 1的记录被放到第二个桶中。创建分桶表的关键语句为：CLUSTER BY语句。分桶抽样语法：TABLESAMPLE (BUCKET x OUT OF y [ON colname])其中x是要抽样的桶编号，桶编号从1开始，colname表示抽样的列，y表示桶的数量。例如：将表随机分成10组，抽取其中的第一个桶的数据select * from table_01 tablesample(bucket 1 out of 10 on rand())3. 随机抽样（rand()函数）1&gt;  使用rand()函数进行随机抽样，limit关键字限制抽样返回的数据，其中rand函数前的distribute和sort关键字可以保证数据在mapper和reducer阶段是随机分布的，案例如下：select * from table_name where col=xxx distribute by rand() sort by rand() limit num;2&gt;  使用order 关键词案例如下：select * from table_name where col=xxx order by rand() limit num;经测试对比，千万级数据中进行随机抽样 order by方式耗时更长，大约多30秒左右。</code></pre><h2 id="Mysql使用规范"><a href="#Mysql使用规范" class="headerlink" title="Mysql使用规范"></a><strong>Mysql使用规范</strong></h2><h2 id="从B-树到LSM树，及LSM树在HBase中的应用"><a href="#从B-树到LSM树，及LSM树在HBase中的应用" class="headerlink" title="从B+树到LSM树，及LSM树在HBase中的应用"></a>从B+树到LSM树，及LSM树在HBase中的应用</h2><h2 id="Sparksql-的-catalyst"><a href="#Sparksql-的-catalyst" class="headerlink" title="Sparksql 的 catalyst"></a><strong>Sparksql 的 catalyst</strong></h2><h2 id="SparkSQL读写部数据源——csv文件的读写"><a href="#SparkSQL读写部数据源——csv文件的读写" class="headerlink" title="SparkSQL读写部数据源——csv文件的读写"></a><strong>SparkSQL读写部数据源——csv文件的读写</strong></h2><p><code>1、sep 和 delimiter的功能都是一样，都是表示csv的切割符，(默认是,)(读写参数)</code></p><pre><code class="scala">spark.read.option(&quot;sep&quot;, &quot; &quot;).csv(Seq(&quot;jeffy&quot;, &quot;katy&quot;).toDS()).show()spark.read.option(&quot;delimiter&quot;, &quot; &quot;).csv(Seq(&quot;jeffy&quot;, &quot;katy&quot;).toDS()).show()scads.write.mode(SaveMode.Overwrite).option(&quot;sep&quot;, &quot;|&quot;).csv(s&quot;$&#123;path&#125;&quot;)</code></pre><p><code>2、header(默认是false) 表示是否将csv文件中的第一行作为schema(读写参数)</code></p><pre><code class="scala">spark.read.option(&quot;header&quot;, true).csv(s&quot;$&#123;path&#125;&quot;)</code></pre><p><code>3.inferSchema 表示是否支持从数据中推导出schema(只读参数)</code></p><pre><code class="scala">spark.read.option(&quot;header&quot;, true).option(&quot;inferSchema&quot;, true).csv(s&quot;$&#123;path&#125;&quot;)</code></pre><p><code>4.charset和encoding(默认是UTF-8)，根据指定的编码器对csv文件进行解码(只读参数)</code></p><pre><code class="scala">spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;encoding&quot;, &quot;iso-8859-1&quot;).option(&quot;sep&quot;, &quot;þ&quot;).csv(s&quot;$&#123;path&#125;&quot;).show()</code></pre><p><code>5.quote(默认值是&quot; ) 表示将不需要切割的字段值用quote标记起来(读写参数)</code></p><pre><code> var optMap = Map(&quot;quote&quot; -&gt; &quot;\&#39;&quot;, &quot;delimiter&quot; -&gt; &quot; &quot;)    spark.read.options(optMap).csv(Seq(&quot;23 &#39;jeffy tang&#39;&quot;, &quot;34 katy&quot;).toDS()).show()</code></pre><p>6.escape(默认值是<code>\</code>) 如果在quote标记的字段值中还含有quote,则用escape来避免(读写参数)</p><pre><code class="scala">val optMap = Map(&quot;quote&quot; -&gt; &quot;\&#39;&quot;, &quot;delimiter&quot; -&gt; &quot; &quot;, &quot;escape&quot; -&gt; &quot;\&quot;&quot;)spark.read.options(optMap).csv(Seq(&quot;23 &#39;jeffy \&quot;&#39;tang&#39;&quot;, &quot;34 katy&quot;).toDS()).show()</code></pre><p><code>7.comment(默认是空字符串，表示关闭这个功能) 表示csv中的注释的标记符(读写参数)</code></p><pre><code class="scala">val optMap = Map(&quot;comment&quot; -&gt; &quot;~&quot;, &quot;header&quot; -&gt; &quot;false&quot;)spark.read.options(optMap).csv(s&quot;$&#123;BASE_PATH&#125;/comments.csv&quot;).show()</code></pre><p><code>8.(读写参数)ignoreLeadingWhiteSpace(默认是false) 表示是否忽略字段值前面的空格 /ignoreTrailingWhiteSpace(默认是false) 表示是否忽略字段值后面的空格</code></p><pre><code class="scala"> val optMap = Map(&quot;ignoreLeadingWhiteSpace&quot; -&gt; &quot;true&quot;, &quot;ignoreTrailingWhiteSpace&quot; -&gt; &quot;true&quot;) spark.read.options(optMap).csv(Seq(&quot; a,b  , c &quot;).toDS()).show()</code></pre><p><code>9. multiLine(默认是false) 是否支持一条记录被拆分成了多行的csv的读取解析(类似于execl单元格多行)(只读参数)</code></p><pre><code class="scala">spark.read.option(&quot;header&quot;, true).option(&quot;multiLine&quot;, true).csv(s&quot;$&#123;path&#125;&quot;).show()</code></pre><p><code>10 、mode(默认是PERMISSIVE) (只读参数)</code></p><p><code>\``1) PERMISSIVE 表示碰到解析错误的时候，将字段都置为null</code></p><p><code>\``2) DROPMALFORMED 表示忽略掉解析错误的记录</code></p><pre><code class="scala">3) FAILFAST 当有解析错误的时候，立马抛出异常 spark.read.option(&quot;mode&quot;, &quot;PERMISSIVE&quot;).schema(schema).csv(s&quot;$&#123;path&#125;&quot;)</code></pre><p><code>11. nullValue(默认是空字符串)， 表示需要将nullValue指定的字符串解析成null(读写参数)</code></p><pre><code class="scala">spark.read.option(&quot;nullValue&quot;, &quot;--&quot;).csv(Seq(&quot;0,2013-11-11,--&quot;, &quot;1,1983-08-04,3&quot;).toDS()).show()</code></pre><p><code>12.nanValue(默认值为NaN) (只读参数)</code></p><p><code>1) positiveInf</code></p><p><code>2) negativeInf</code></p><pre><code class="scala">   spark.read.format(&quot;csv&quot;).schema(StructType(List(        StructField(&quot;int&quot;, IntegerType, true),        StructField(&quot;long&quot;, LongType, true),        StructField(&quot;float&quot;, FloatType, true),        StructField(&quot;double&quot;, DoubleType, true)      ))).options(Map(        &quot;header&quot; -&gt; &quot;true&quot;,        &quot;mode&quot; -&gt; &quot;DROPMALFORMED&quot;,        &quot;nullValue&quot; -&gt; &quot;--&quot;,        &quot;nanValue&quot; -&gt; &quot;NAN&quot;,        &quot;negativeInf&quot; -&gt; &quot;-INF&quot;,        &quot;positiveInf&quot; -&gt; &quot;INF&quot;)).load(s&quot;$&#123;BASE_PATH&#125;/numbers.csv&quot;)</code></pre><p><code>13.codec和compression 压缩格式，支持的压缩格式有：</code></p><p><code>none 和 uncompressed表示不压缩;</code></p><p><code>bzip2、deflate、gzip、lz4、snappy (只写参数)</code></p><pre><code class="scala">inferSchemaDF.write.mode(SaveMode.Overwrite).option(&quot;compression&quot;, &quot;gzip&quot;).csv(s&quot;$&#123;path&#125;&quot;)</code></pre><p><code>14.maxColumns(默认是20480) 规定一个csv的一条记录最大的列数 (只读参数)</code></p><pre><code class="scala"> spark.read.option(&quot;maxColumns&quot;, &quot;3&quot;).csv(Seq(&quot;test,as,g&quot;, &quot;h,bm,s&quot;).toDS()).show() //会报错</code></pre></li></ul><h2 id="Spark-Kudu的广告业务项目实战笔记"><a href="#Spark-Kudu的广告业务项目实战笔记" class="headerlink" title="Spark+Kudu的广告业务项目实战笔记"></a>Spark+Kudu的广告业务项目实战笔记</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p><code>本项目需要实现：将广告数据的json文件放置在HDFS上，并利用spark进行ETL操作、分析操作，之后存储在kudu上，最后设定每天凌晨三点自动执行广告数据的分析存储操作。</code></p><h3 id="2-项目需求"><a href="#2-项目需求" class="headerlink" title="2.项目需求"></a>2.项目需求</h3><p><code>数据ETL：原始文件为JSON格式数据，需原始文件与IP库中数据进行解析</code></p><p><code>统计各省市的地域分布情况</code></p><p><code>统计广告投放的地域分布情况</code></p><p><code>统计广告投放APP分布情况</code></p><h3 id="3-项目架构"><a href="#3-项目架构" class="headerlink" title="3.项目架构"></a>3.项目架构</h3><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200823135336754.png" alt="image-20200823135336754" style="zoom:50%;"><h3 id="4-日志字段"><a href="#4-日志字段" class="headerlink" title="4.日志字段"></a>4.日志字段</h3><pre><code class="JSON">&#123;  &quot;sessionid&quot;: &quot;qld2dU4cfhEa3yhADzgphOf3ySv9vMml&quot;,  &quot;advertisersid&quot;: 66,  &quot;adorderid&quot;: 142848,  &quot;adcreativeid&quot;: 212312,  &quot;adplatformproviderid&quot;: 174663,  &quot;sdkversion&quot;: &quot;Android 5.0&quot;,  &quot;adplatformkey&quot;: &quot;PLMyYnDKQgOPL55frHhxkUIQtBThHfHq&quot;,  &quot;putinmodeltype&quot;: 1,  &quot;requestmode&quot;: 1,  &quot;adprice&quot;: 8410.0,  &quot;adppprice&quot;: 5951.0,  &quot;requestdate&quot;: &quot;2018-10-07&quot;,  &quot;ip&quot;: &quot;182.91.190.221&quot;,  &quot;appid&quot;: &quot;XRX1000014&quot;,  &quot;appname&quot;: &quot;支付宝 - 让生活更简单&quot;,  &quot;uuid&quot;: &quot;QtxDH9HUueM2IffUe8z2UqLKuZueZLqq&quot;,  &quot;device&quot;: &quot;HUAWEI GX1手机&quot;,  &quot;client&quot;: 1,  &quot;osversion&quot;: &quot;&quot;,  &quot;density&quot;: &quot;&quot;,  &quot;pw&quot;: 1334,  &quot;ph&quot;: 750,  &quot;lang&quot;: &quot;&quot;,  &quot;lat&quot;: &quot;&quot;,  &quot;provincename&quot;: &quot;&quot;,  &quot;cityname&quot;: &quot;&quot;,  &quot;ispid&quot;: 46007,  &quot;ispname&quot;: &quot;移动&quot;,  &quot;networkmannerid&quot;: 1,  &quot;networkmannername&quot;: &quot;4G&quot;,  &quot;iseffective&quot;: 1,  &quot;isbilling&quot;: 1,  &quot;adspacetype&quot;: 3,  &quot;adspacetypename&quot;: &quot;全屏&quot;,  &quot;devicetype&quot;: 1,  &quot;processnode&quot;: 3,  &quot;apptype&quot;: 0,  &quot;district&quot;: &quot;district&quot;,  &quot;paymode&quot;: 1,  &quot;isbid&quot;: 1,  &quot;bidprice&quot;: 6812.0,  &quot;winprice&quot;: 89934.0,  &quot;iswin&quot;: 0,  &quot;cur&quot;: &quot;rmb&quot;,  &quot;rate&quot;: 0.0,  &quot;cnywinprice&quot;: 0.0,  &quot;imei&quot;: &quot;&quot;,  &quot;mac&quot;: &quot;52:54:00:41:ba:02&quot;,  &quot;idfa&quot;: &quot;&quot;,  &quot;openudid&quot;: &quot;FIZHDPIKQYVNHOHOOAWMTQDFTPNWAABZTAFVHTEL&quot;,  &quot;androidid&quot;: &quot;&quot;,  &quot;rtbprovince&quot;: &quot;&quot;,  &quot;rtbcity&quot;: &quot;&quot;,  &quot;rtbdistrict&quot;: &quot;&quot;,  &quot;rtbstreet&quot;: &quot;&quot;,  &quot;storeurl&quot;: &quot;&quot;,  &quot;realip&quot;: &quot;182.92.196.236&quot;,  &quot;isqualityapp&quot;: 0,  &quot;bidfloor&quot;: 0.0,  &quot;aw&quot;: 0,  &quot;ah&quot;: 0,  &quot;imeimd5&quot;: &quot;&quot;,  &quot;macmd5&quot;: &quot;&quot;,  &quot;idfamd5&quot;: &quot;&quot;,  &quot;openudidmd5&quot;: &quot;&quot;,  &quot;androididmd5&quot;: &quot;&quot;,  &quot;imeisha1&quot;: &quot;&quot;,  &quot;macsha1&quot;: &quot;&quot;,  &quot;idfasha1&quot;: &quot;&quot;,  &quot;openudidsha1&quot;: &quot;&quot;,  &quot;androididsha1&quot;: &quot;&quot;,  &quot;uuidunknow&quot;: &quot;&quot;,  &quot;userid&quot;: &quot;vtUO8pPXfwdsPnvo6ttNGhAAnHi8NVbA&quot;,  &quot;reqdate&quot;: null,  &quot;reqhour&quot;: null,  &quot;iptype&quot;: 1,  &quot;initbidprice&quot;: 0.0,  &quot;adpayment&quot;: 175547.0,  &quot;agentrate&quot;: 0.0,  &quot;lomarkrate&quot;: 0.0,  &quot;adxrate&quot;: 0.0,  &quot;title&quot;: &quot;中信建投首次公开发行股票发行结果 本次发行价格为5.42元/股&quot;,  &quot;keywords&quot;: &quot;IPO,中信建投证券,股票,投资,财经&quot;,  &quot;tagid&quot;: &quot;rBRbAEQhkcAaeZ6XlTrGXOxyw6w9JQ7x&quot;,  &quot;callbackdate&quot;: &quot;2018-10-07&quot;,  &quot;channelid&quot;: &quot;123528&quot;,  &quot;mediatype&quot;: 2,  &quot;email&quot;: &quot;e4aqd67bo@263.net&quot;,  &quot;tel&quot;: &quot;13105823726&quot;,  &quot;age&quot;: &quot;29&quot;,  &quot;sex&quot;: &quot;0&quot;&#125;</code></pre><h3 id="5-IP规则库解析"><a href="#5-IP规则库解析" class="headerlink" title="5.IP规则库解析"></a>5.IP规则库解析</h3><p><code>本项目利用IP规则库进行解析，在生产中应该需要专门的公司提供的IP服务。IP规则库中的一条如下：</code></p><pre><code class="python">1.0.1.0|1.0.3.255|16777472|16778239|亚洲|中国|福建|福州||电信|350100|China|CN|119.306239|26.075302其中第三列是该段ip起始地址（十进制），第四列是ip终止地址（十进制）。</code></pre><h3 id="6-代码实现"><a href="#6-代码实现" class="headerlink" title="6.代码实现"></a>6.代码实现</h3><p><code>LogETLApp.scala代码</code></p><pre><code class="scala">import com.imooc.bigdata.cp08.utils.IPUtilsimport org.apache.spark.sql.SparkSessionobject LogETLApp &#123;  def main(args: Array[String]): Unit = &#123;    //启动本地模式的spark    val spark = SparkSession.builder()      .master(&quot;local[2]&quot;)      .appName(&quot;LogETLApp&quot;)      .getOrCreate()    //使用DataSourceAPI直接加载json数据    var jsonDF = spark.read.json(&quot;data-test.json&quot;)    //jsonDF.printSchema()    //jsonDF.show(false)    //导入隐式转换    import spark.implicits._    //加载IP库,建议将RDD转成DF    val ipRowRDD = spark.sparkContext.textFile(&quot;ip.txt&quot;)    val ipRuleDF = ipRowRDD.map(x =&gt; &#123;      val splits = x.split(&quot;\\|&quot;)      val startIP = splits(2).toLong      val endIP = splits(3).toLong      val province = splits(6)      val city = splits(7)      val isp = splits(9)      (startIP, endIP, province, city, isp)    &#125;).toDF(&quot;start_ip&quot;, &quot;end_ip&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)    //ipRuleDF.show(false)    //利用Spark SQL UDF转换json中的ip    import org.apache.spark.sql.functions._    def getLongIp() = udf((ip:String)=&gt;&#123;      IPUtils.ip2Long(ip)    &#125;)    //添加字段传入十进制IP    jsonDF = jsonDF.withColumn(&quot;ip_long&quot;,      getLongIp()($&quot;ip&quot;))    //将日志每一行的ip对应省份、城市、运行商进行解析    //两个DF进行join，条件是：json中的ip在规则ip中的范围内    val result =jsonDF.join(ipRuleDF,jsonDF(&quot;ip_long&quot;)      .between(ipRuleDF(&quot;start_ip&quot;),ipRuleDF(&quot;end_ip&quot;)))   // 等价于sql   //  &quot; select * from logs left join &quot; +   // &quot;ips on logs.ip_long between ips.start_ip and ips.end_ip &quot;          val tableName = &quot;ods&quot;    val partitionId = &quot;ip&quot;    val schema = SchemaUtils.ODSSchema    KuduUtils.sink(result,tableName,masterAddresses,schema,partitionId)    spark.stop()  &#125;&#125;</code></pre><p><code>工具类中将字符串转成十进制的IPUtils.scala：</code></p><pre><code class="scala">object IPUtils &#123;  //字符串-&gt;十进制  def ip2Long(ip:String)=&#123;    val splits = ip.split(&quot;[.]&quot;)    var ipNum = 0L    for(i&lt;-0 until(splits.length))&#123;      //“|”是按位或操作，有1即1，全0则0      //“&lt;&lt;”是整体左移      //也就是说每一个数字算完向前移动8位接下一个数字      ipNum = splits(i).toLong | ipNum &lt;&lt; 8L    &#125;    ipNum  &#125;  def main(args: Array[String]): Unit = &#123;    println(ip2Long(&quot;1.1.1.1&quot;))  &#125;&#125;</code></pre><p><code>SchemaUtils.scala</code></p><pre><code class="scala">lazy val ODSSchema: Schema = &#123;    val columns = List(      new ColumnSchemaBuilder(&quot;ip&quot;, Type.STRING).nullable(false).key(true).build(),      new ColumnSchemaBuilder(&quot;sessionid&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;advertisersid&quot;,Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;adorderid&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;adcreativeid&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;adplatformproviderid&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;sdkversion&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;adplatformkey&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;putinmodeltype&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;requestmode&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;adprice&quot;, Type.DOUBLE).nullable(false).build(),      new ColumnSchemaBuilder(&quot;adppprice&quot;, Type.DOUBLE).nullable(false).build(),      new ColumnSchemaBuilder(&quot;requestdate&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;appid&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;appname&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;uuid&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;device&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;client&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;osversion&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;density&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;pw&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;ph&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;provincename&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;cityname&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;ispid&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;ispname&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;isp&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;networkmannerid&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;networkmannername&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;iseffective&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;isbilling&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;adspacetype&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;adspacetypename&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;devicetype&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;processnode&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;apptype&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;district&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;paymode&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;isbid&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;bidprice&quot;, Type.DOUBLE).nullable(false).build(),      new ColumnSchemaBuilder(&quot;winprice&quot;, Type.DOUBLE).nullable(false).build(),      new ColumnSchemaBuilder(&quot;iswin&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;cur&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;rate&quot;, Type.DOUBLE).nullable(false).build(),      new ColumnSchemaBuilder(&quot;cnywinprice&quot;, Type.DOUBLE).nullable(false).build(),      new ColumnSchemaBuilder(&quot;imei&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;mac&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;idfa&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;openudid&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;androidid&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;rtbprovince&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;rtbcity&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;rtbdistrict&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;rtbstreet&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;storeurl&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;realip&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;isqualityapp&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;bidfloor&quot;, Type.DOUBLE).nullable(false).build(),      new ColumnSchemaBuilder(&quot;aw&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;ah&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;imeimd5&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;macmd5&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;idfamd5&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;openudidmd5&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;androididmd5&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;imeisha1&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;macsha1&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;idfasha1&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;openudidsha1&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;androididsha1&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;uuidunknow&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;userid&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;iptype&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;initbidprice&quot;, Type.DOUBLE).nullable(false).build(),      new ColumnSchemaBuilder(&quot;adpayment&quot;, Type.DOUBLE).nullable(false).build(),      new ColumnSchemaBuilder(&quot;agentrate&quot;, Type.DOUBLE).nullable(false).build(),      new ColumnSchemaBuilder(&quot;lomarkrate&quot;, Type.DOUBLE).nullable(false).build(),      new ColumnSchemaBuilder(&quot;adxrate&quot;, Type.DOUBLE).nullable(false).build(),      new ColumnSchemaBuilder(&quot;title&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;keywords&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;tagid&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;callbackdate&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;channelid&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;mediatype&quot;, Type.INT64).nullable(false).build(),      new ColumnSchemaBuilder(&quot;email&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;tel&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;sex&quot;, Type.STRING).nullable(false).build(),      new ColumnSchemaBuilder(&quot;age&quot;, Type.STRING).nullable(false).build()    ).asJava    new Schema(columns)  &#125;lazy val APPSchema: Schema = &#123;    val columns = List(new ColumnSchemaBuilder(&quot;appid&quot;, Type.STRING).nullable(false).key(true).build(),new ColumnSchemaBuilder(&quot;appname&quot;, Type.STRING).nullable(false).key(true).build(),new ColumnSchemaBuilder(&quot;origin_request&quot;, Type.INT64).nullable(false).build(),new ColumnSchemaBuilder(&quot;valid_request&quot;, Type.INT64).nullable(false).build(),new ColumnSchemaBuilder(&quot;ad_request&quot;, Type.INT64).nullable(false).build(),new ColumnSchemaBuilder(&quot;bid_cnt&quot;, Type.INT64).nullable(false).build(),new ColumnSchemaBuilder(&quot;bid_success_cnt&quot;, Type.INT64).nullable(false).build(),new ColumnSchemaBuilder(&quot;bid_success_rate&quot;, Type.DOUBLE).nullable(false).build(),new ColumnSchemaBuilder(&quot;ad_display_cnt&quot;, Type.INT64).nullable(false).build(),new ColumnSchemaBuilder(&quot;ad_click_cnt&quot;, Type.INT64).nullable(false).build(),new ColumnSchemaBuilder(&quot;ad_click_rate&quot;, Type.DOUBLE).nullable(false).build(),new ColumnSchemaBuilder(&quot;ad_consumption&quot;, Type.DOUBLE).nullable(false).build(),new ColumnSchemaBuilder(&quot;ad_cost&quot;, Type.DOUBLE).nullable(false).build()    ).asJavanew Schema(columns)  &#125;</code></pre><p><code>KuduUtils.scala</code></p><pre><code class="scala">import java.utilimport com.imooc.bigdata.chapter08.utils.SchemaUtilsimport org.apache.kudu.Schemaimport org.apache.kudu.client.&#123;CreateTableOptions, KuduClient&#125;import org.apache.kudu.client.KuduClient.KuduClientBuilderimport org.apache.spark.sql.&#123;DataFrame, SaveMode&#125;  object KuduUtils &#123;  /**    * 将DF数据落地到Kudu    * @param data DF结果集    * @param tableName  Kudu目标表    * @param master Kudu的Master地址    * @param schema Kudu的schema信息    * @param partitionId  Kudu表的分区字段    */  def sink(data:DataFrame,           tableName:String,           master:String,           schema:Schema,           partitionId:String)=&#123;    val client = new KuduClientBuilder(master).build()    if(client.tableExists(tableName))&#123;      client.deleteTable(tableName)    &#125;    val options = new CreateTableOptions()    options.setNumReplicas(1)    val parcols = new util.LinkedList[String]()    parcols.add(partitionId)    options.addHashPartitions(parcols,3)    client.createTable(tableName,schema,options)    //数据写入Kudu    data.write.mode(SaveMode.Append)      .format(&quot;org.apache.kudu.spark.kudu&quot;)      .option(&quot;kudu.table&quot;,tableName)      .option(&quot;kudu.master&quot;,master)      .save() //    spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)//      .option(&quot;kudu.master&quot;,master)//      .option(&quot;kudu.table&quot;,tableName)//      .load().show()  &#125;&#125;</code></pre><h3 id="7-功能二开发"><a href="#7-功能二开发" class="headerlink" title="7.功能二开发"></a><strong>7.功能二开发</strong></h3><h3 id="统计省份、城市数量分布情况，按照provincename与cityname分组统计"><a href="#统计省份、城市数量分布情况，按照provincename与cityname分组统计" class="headerlink" title="统计省份、城市数量分布情况，按照provincename与cityname分组统计"></a><code>统计省份、城市数量分布情况，按照provincename与cityname分组统计</code></h3><p>ProvinceCityStatApp。scala</p><pre><code class="scala">import com.imooc.bigdata.cp08.utils.SQLUtilsimport org.apache.spark.sql.SparkSessionobject ProvinceCityStatApp &#123;  def main(args: Array[String]): Unit = &#123;    val spark = SparkSession.builder()      .master(&quot;local[2]&quot;)      .appName(&quot;ProvinceCityStatApp&quot;)      .getOrCreate()    //从Kudu的ods表中读取数据，然后按照省份和城市分组即可    val sourceTableName = &quot;ods&quot;    val masterAddress = &quot;hadoop000&quot;    val odsDF = spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)      .option(&quot;kudu.table&quot;, sourceTableName)      .option(&quot;kudu.master&quot;, masterAddress)      .load()    //odsDF.show(false)    odsDF.createOrReplaceTempView(&quot;ods&quot;)    val result = spark.sql(SQLUtils.PROVINCE_CITY_SQL)    result.show(false)    spark.stop()  &#125;&#125;</code></pre><p>SQLUtils.scala</p><pre><code class="scala">lazy val PROVINCE_CITY_SQL = &quot;select provincename,cityname,count(1) as cnt from ods group by provincename,cityname&quot; lazy val PROVINCE_CITY_SQL = &quot;select provincename,cityname,count(1) as cnt from ods group by provincename,cityname&quot;</code></pre><h3 id="8-代码重构"><a href="#8-代码重构" class="headerlink" title="**8:代码重构 **"></a>**8:代码重构 **</h3><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2O2R2sWRGyQHreSAGqmpRwtXN0gP4jqCyFr7X6ibkI7EMqhv399ADbBv6aHnMnKMicELEjGpPWuwDhA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img" style="zoom:50%;"><h3 id="trait"><a href="#trait" class="headerlink" title="trait"></a><code>trait</code></h3><!--Scala Trait(特征) 相当于 Java 的接口，实际上它比接口还功能强大。与接口不同的是，它还可以定义属性和方法的实现。一般情况下Scala的类只能够继承单一父类，但是如果是 Trait(特征) 的话就可以继承多个，从结果来看就是实现了多重继承。--><pre><code class="scala">import org.apache.spark.sql.SparkSession//顶层数据处理接口trait DataProcess &#123;  def process(spark:SparkSession)&#125;</code></pre><h3 id="Processor"><a href="#Processor" class="headerlink" title="Processor"></a><code>Processor</code></h3><h3 id="3-1-需求一：ETL的Processor"><a href="#3-1-需求一：ETL的Processor" class="headerlink" title="3.1 需求一：ETL的Processor"></a><code>3.1 需求一：ETL的Processor</code></h3><pre><code class="scala">import com.imooc.bigdata.cp08.`trait`.DataProcessimport com.imooc.bigdata.cp08.utils.&#123;IPUtils, KuduUtils, SQLUtils, SchemaUtils&#125;import org.apache.spark.sql.SparkSessionobject LogETLProcessor extends DataProcess&#123;  override def process(spark: SparkSession): Unit = &#123;    //使用DataSourceAPI直接加载json数据    var jsonDF = spark.read.json(&quot;D:\\Hadoop基础与电商行为日志分析\\spark\\coding385\\sparksql-train\\data\\data-test.json&quot;)    //jsonDF.printSchema()    //jsonDF.show(false)    //导入隐式转换    import spark.implicits._    //加载IP库,建议将RDD转成DF    val ipRowRDD = spark.sparkContext.textFile(&quot;D:\\Hadoop基础与电商行为日志分析\\spark\\coding385\\sparksql-train\\data\\ip.txt&quot;)    val ipRuleDF = ipRowRDD.map(x =&gt; &#123;      val splits = x.split(&quot;\\|&quot;)      val startIP = splits(2).toLong      val endIP = splits(3).toLong      val province = splits(6)      val city = splits(7)      val isp = splits(9)      (startIP, endIP, province, city, isp)    &#125;).toDF(&quot;start_ip&quot;, &quot;end_ip&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)    //ipRuleDF.show(false)    //利用Spark SQL UDF转换json中的ip    import org.apache.spark.sql.functions._    def getLongIp() = udf((ip:String)=&gt;&#123;      IPUtils.ip2Long(ip)    &#125;)    //添加字段传入十进制IP    jsonDF = jsonDF.withColumn(&quot;ip_long&quot;,      getLongIp()($&quot;ip&quot;))    //将日志每一行的ip对应省份、城市、运行商进行解析    //两个DF进行join，条件是：json中的ip在规则ip中的范围内    //    val result = jsonDF.join(ipRuleDF, jsonDF(&quot;ip_long&quot;)    //      .between(ipRuleDF(&quot;start_ip&quot;), ipRuleDF(&quot;end_ip&quot;)))    //      //.show(false)    //用SQL的方式完成    jsonDF.createOrReplaceTempView(&quot;logs&quot;)    ipRuleDF.createOrReplaceTempView(&quot;ips&quot;)    val sql = SQLUtils.SQL    val result = spark.sql(sql)    //.show(false)    //Kudu    val masterAddresses = &quot;hadoop000&quot;    val tableName = &quot;ods&quot;    val partitionId = &quot;ip&quot;    val schema = SchemaUtils.ODSSchema    KuduUtils.sink(result,tableName,masterAddresses,schema,partitionId)    spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)      .option(&quot;kudu.master&quot;,masterAddresses)     .option(&quot;kudu.table&quot;,tableName)      .load().show()  &#125;&#125;</code></pre><h3 id="3-2-需求二：ProvinceCityStatProcessor"><a href="#3-2-需求二：ProvinceCityStatProcessor" class="headerlink" title="3.2 需求二：ProvinceCityStatProcessor"></a><code>3.2 需求二：ProvinceCityStatProcessor</code></h3><pre><code class="scala">import com.imooc.bigdata.cp08.`trait`.DataProcessimport com.imooc.bigdata.cp08.utils.&#123;KuduUtils, SQLUtils, SchemaUtils&#125;import org.apache.spark.sql.SparkSessionobject ProvinceCityStatProcessor extends DataProcess&#123;  override def process(spark: SparkSession): Unit = &#123;    //从Kudu的ods表中读取数据，然后按照省份和城市分组即可    val sourceTableName = &quot;ods&quot;    val masterAddress = &quot;hadoop000&quot;    val odsDF = spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)      .option(&quot;kudu.table&quot;, sourceTableName)      .option(&quot;kudu.master&quot;, masterAddress)      .load()    //odsDF.show(false)    odsDF.createOrReplaceTempView(&quot;ods&quot;)    val result = spark.sql(SQLUtils.PROVINCE_CITY_SQL)    //result.show(false)    //Kudu    val sinkTableName = &quot;province_city_stat&quot;    val partitionId = &quot;provincename&quot;    val schema = SchemaUtils.ProvinceCitySchema    KuduUtils.sink(result,sinkTableName,masterAddress,schema,partitionId)    spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)      .option(&quot;kudu.master&quot;,masterAddress)      .option(&quot;kudu.table&quot;,sinkTableName)      .load().show()  &#125;&#125;</code></pre><h3 id="项目入口"><a href="#项目入口" class="headerlink" title="项目入口"></a>项目入口</h3><pre><code class="scala">import com.imooc.bigdata.cp08.business.&#123;LogETLProcessor, ProvinceCityStatProcessor&#125;import org.apache.spark.sql.SparkSession//整个项目的入口object SparkApp &#123;  def main(args: Array[String]): Unit = &#123;    val spark = SparkSession.builder()      .master(&quot;local[2]&quot;)      .appName(&quot;SparkApp&quot;)      .getOrCreate()    //ETL    LogETLProcessor.process(spark)    //省份    ProvinceCityStatProcessor.process(spark)    spark.stop()  &#125;&#125;</code></pre><p><code>9 、实现需求四：APP统计。需求如下：</code></p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2O2R2sWRGyQHreSAGqmpRwta1qiaJBqc9MolImXDQRB6dTMfWgD5yv6Dfl9qQ4iaonWUsfv4g6ibpDOg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img" style="zoom:50%;"><p>AreaStatProcessor.scala</p><pre><code class="scala">import com.imooc.bigdata.cp08.`trait`.DataProcessimport com.imooc.bigdata.cp08.utils.&#123;KuduUtils, SQLUtils, SchemaUtils&#125;import org.apache.spark.sql.SparkSession object AreaStatProcessor extends DataProcess&#123;  override def process(spark: SparkSession): Unit = &#123;    val sourceTableName = &quot;ods&quot;    val masterAddresses = &quot;hadoop000&quot;     val odsDF = spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)      .option(&quot;kudu.table&quot;,sourceTableName)      .option(&quot;kudu.master&quot;,masterAddresses)      .load()     odsDF.createOrReplaceTempView(&quot;ods&quot;)     val resultTmp = spark.sql(SQLUtils.AREA_SQL_STEP1)    resultTmp.show()          val sinkTableName = &quot;app_stat&quot;    val partitionId = &quot;appid&quot;    val schema = SchemaUtils.APPSchema    KuduUtils.sink(result,sinkTableName,masterAddresses,schema,partitionId)    spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)      .option(&quot;kudu.master&quot;,masterAddresses)      .option(&quot;kudu.table&quot;,sinkTableName)      .load().show()   &#125;&#125;</code></pre><p>SQLUtils.scala</p><pre><code class="scala">lazy val AREA_SQL_STEP1 = &quot;select provincename,cityname, &quot; +    &quot;sum(case when requestmode=1 and processnode &gt;=1 then 1 else 0 end) origin_request,&quot; +    &quot;sum(case when requestmode=1 and processnode &gt;=2 then 1 else 0 end) valid_request,&quot; +    &quot;sum(case when requestmode=1 and processnode =3 then 1 else 0 end) ad_request,&quot; +    &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and isbid=1 and adorderid!=0 then 1 else 0 end) bid_cnt,&quot; +    &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 then 1 else 0 end) bid_success_cnt,&quot; +    &quot;sum(case when requestmode=2 and iseffective=1 then 1 else 0 end) ad_display_cnt,&quot; +    &quot;sum(case when requestmode=3 and processnode=1 then 1 else 0 end) ad_click_cnt,&quot; +    &quot;sum(case when requestmode=2 and iseffective=1 and isbilling=1 then 1 else 0 end) medium_display_cnt,&quot; +    &quot;sum(case when requestmode=3 and iseffective=1 and isbilling=1 then 1 else 0 end) medium_click_cnt,&quot; +    &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 and adorderid&gt;20000  then 1*winprice/1000 else 0 end) ad_consumption,&quot; +    &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 and adorderid&gt;20000  then 1*adpayment/1000 else 0 end) ad_cost &quot; +    &quot;from ods group by provincename,cityname&quot;lazy val APP_SQL_STEP2 = &quot;select appid,appname, &quot; +    &quot;origin_request,&quot; +    &quot;valid_request,&quot; +    &quot;ad_request,&quot; +    &quot;bid_cnt,&quot; +    &quot;bid_success_cnt,&quot; +    &quot;bid_success_cnt/bid_cnt bid_success_rate,&quot; +    &quot;ad_display_cnt,&quot; +    &quot;ad_click_cnt,&quot; +    &quot;ad_click_cnt/ad_display_cnt ad_click_rate,&quot; +    &quot;ad_consumption,&quot; +    &quot;ad_cost from app_tmp &quot; +    &quot;where bid_cnt!=0 and ad_display_cnt!=0&quot;</code></pre><h2 id="Flink-CEP-原理和案例详解"><a href="#Flink-CEP-原理和案例详解" class="headerlink" title="Flink CEP 原理和案例详解"></a>Flink CEP 原理和案例详解</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1 概念"></a>1 概念</h3><p><em><strong><code>（1）定义</code></strong></em></p><!--复合事件处理（Complex Event Processing，CEP）是一种基于动态环境中事件流的分析技术，事件在这里通常是有意义的状态变化，通过分析事件间的关系，利用过滤、关联、聚合等技术，根据事件间的时序关系和聚合关系制定检测规则，持续地从事件流中查询出符合要求的事件序列，最终分析得到更复杂的复合事件。--><p><em><strong><code>(2）特征</code></strong></em><br>CEP的特征如下:<br>   <!--目标：从有序的简单事件流中发现一些高阶特征；--><br>   <!--输入：一个或多个简单事件构成的事件流；--><br>   <!--处理：识别简单事件之间的内在联系，多个符合一定规则的简单事件构成复杂事件；--><br>  <!--输出：满足规则的复杂事件。--></p><h2 id><a href="#" class="headerlink" title></a></h2><h2 id="Spark-封装代码："><a href="#Spark-封装代码：" class="headerlink" title="Spark 封装代码："></a><strong>Spark 封装代码：</strong></h2><p><em><strong>1：单例模式：</strong></em></p><pre><code class="scala">object SQLContextSingleton &#123;  @transient  private var instance: SQLContext = _  def getInstance(sparkContext: SparkContext): SQLContext = &#123;    if (instance == null) &#123;      instance = new SQLContext(sparkContext)    &#125;    instance  &#125;&#125;</code></pre><h2 id="Hive数仓"><a href="#Hive数仓" class="headerlink" title="Hive数仓"></a><strong>Hive数仓</strong></h2><p><em><strong>建模理论</strong></em></p><pre><code class="python">/**ER模型*/        此建模方法是从全企业的高度设计一个符合第三范式(3NF)模型，用实体关系(ER)模型描述企业业务。数据仓库中3NF和OLTP系统中的3NF的区别是它站在全企业角度面向主题的抽象，而不是针对某个具体的业务流程的实体对象关系的抽象。换句话说，设计数据仓库既不面向功能也不面向应用，是为了满足全企业的数据需求，而不是为了满足某个部门的特定分析需求。        采用ER模型设计数据仓库模型的出发点是为了整合或集成数据，将各个系统中的数据，以整个企业角度按照主题进行相似性组合和合并，并进行一致性处理，为数据分析决策服务，但不能直接用于分析决策（因为分析决策的功能不在数仓层）。数据建模分为三个层次：高层建模（称为实体关系图，或ERD），中间层建模（数据项集或DIS）和底层建模（物理模型）。高层模型：是实体的最高抽象层模型，描述主要的主题或主题域的关系，用于描述企业的业务总体概况。中间层模型：为高层模型标示出的每个主要主题或主题域再进一步细化数据项，是每个主题域的进一步扩展。底层模型：在中间层模型的基础上，考虑物理存储和物理属性设计，可做一些表合并、分区设计，使模型中包含关键字(主键)和物理特性。        ER模型的最佳实践是Teradata基于金融行业发布的FS-LDM逻辑数据模型，它通过对金融业务高度抽象和总结，将金融业务划分为10大金融主题（当事人、产品、协议、事件、资产、财务、机构、地域、营销和渠道），并以设计面向金融数仓模型的核心为基础，适当调整和扩展即可快速落地实施。    /**维度模型*/        此建模方法是数据仓库大师Ralph Kimball所倡导的。是数据仓库工程领域最流行的数据仓库建模经典。        维度建模从数据分析决策的需求出发构建模型，为分析需求服务，因此其重点是关注业务用户如何更快地完成需求分析，同时具有较好的查询性能选择过程：识别出主要的业务过程。它们会充当事实表的源，用数值的、可累加的事实来填充事实表。选择粒度：粒度的选择意味着准确确定事实表中的记录代表什么，记录细分程度。只有选择了维度的情况下，才能进行业务过程对应维度有条理探讨。识别和一致化维度：选择了粒度后进行维度表设计。维度是事实表入口，是用户分析进行分组、筛选和检索的关键，需符合企业数据仓库总线前提下，一致化维度和一致化事实设计。选择事实：确定分析需要衡量的指标。事实尽可能具有累加性。        这些只是描述建模设计主要步骤，当然还有其他的步骤，如在事实表中存储预处理算法（事实表可累加事实之间的预处理显示存储），缓慢变化维度设计和物理设计等。    /**常见分层架构*/## 操作数据层（ODS）：把源系统数据几乎无处理地存放到数据仓库中。同步：结构化数据增量或全量同步到数据仓库Hive结构化：把流式、批式半结构或非结构化数据经过结构化处理存储数据仓库Hive## 公共维度模型层（CDM）：存放明细事实数据、维度数据及公共统一指标汇总数据，其中明细事实数据、维度表数据一般根据ODS层数据加工生成，公共统一指标汇总数据一般根据维度表和明细事实表加工生成。CDM层又细分为DWD层和DWS层，分别是明细数据层和总汇层，用维度建模理论为基础，将维度退化到事实表中，减少了事实表和维度表的关联提高查询性能；同时汇总数据层，加强指标维度退化，采取构建大宽表构建公共统一指标数据层 ，提升指标的易用性和查询性能。主要功能如下：组合和合并相似数据：采用明细宽表，复用关联计算，减少表之间的关联和数据扫描，合并不同业务为统一过程统一指标加工：统一命名规范、统一数据类型、统一计算口径，为数据产品、应用和服务提供统一指标体系；建立逻辑汇总宽表。建立一致性维度和事实：降低了粒度、计算口径、筛选和分组不一致的风险。数据清洗：去重、去噪、空值转化、格式统一等数据清洗## 数据标签层（TDM）：通过客户信息标签化，存放客户属性、行为、消费、社交和风险控制维度刻画客户的全貌信息。主要功能如下：客户ID打通：ID-MApping，即把客户不同来源等身份标识通过数据手段识别为同一主体。完成对客户信息的全面刻画。统一标签加工：统一标签分类、命名规范、数据类型、计算口径等，为数据分析与应用服务。数据标签体系按照标签类型可分为统计类、规则类和机器学习挖掘类三大类型；亦可从用户属性、用户行为、用户消费、风险控制和社交属性共五大维度划分归类。如下：</code></pre><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200924234141.jpg" alt="微信图片_20200924234141" style="zoom:50%;"><pre><code class="python"> 数据标签体系建设涉及标签分类、命名规范和计算口径等元数据管理、标签开发、标签存储、标签应用、数据质量管理和数据价值管理内容较多，笔者会单独写文章详细分享。## 应用数据层（ADS）：存放数据产品个性化的统计指标数据，CDM层和ODS层加工生成。个性化指标加工：不公用型，复杂性（指数型、比值型、排名指标等）基于应用数据组织：大宽表数据集市、横表转纵表等。建模：1：确定主题域2：确定总线架构3：划分主题域，构建总线矩阵4：dw模型设计5：维度建模 选择业务过程--&gt;声明粒度--&gt;标识维度--&gt;标识事实</code></pre><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200927230650.png" alt="微信图片_20200927230650" style="zoom:50%;"><h4 id="事实和维度"><a href="#事实和维度" class="headerlink" title="事实和维度"></a><em><strong>事实和维度</strong></em></h4><pre><code class="python">##事实表：我们可以简单地将事实理解为现实中发生的一次操作型事件。比如订单表，我们就可以理解为一张事实表，我们每完成一个订单，就会在订单事实表中增加一条记录。事实表产生于业务过程，存储了业务活动或事件提炼出来的性能度量。从最低的粒度级别来看，事实表行对应一个度量事件。事实表根据粒度的角色划分不同，可分为事务事实表、周期快照事实表、累积快照事实表。（1）事务事实表，用于承载事务数据，通常粒度比较低，它是面向事务的，其粒度是每一行对应一个事务，它是最细粒度的事实表，例如产品交易事务事实、ATM交易事务事实。（2）周期快照事实表，按照一定的时间周期间隔(每天，每月)来捕捉业务活动的执行情况，一旦装入事实表就不会再去更新，它是事务事实表的补充。用来记录有规律的、固定时间间隔的业务累计数据，通常粒度比较高，例如账户月平均余额事实表。（3）累积快照事实表，用来记录具有时间跨度的业务处理过程的整个过程的信息，每个生命周期一行，通常这类事实表比较少见。注意：这里需要值得注意的是，在事实表的设计时，一定要注意一个事实表只能有一个粒度，不能将不同粒度的事实建立在同一张事实表中。## 维度表：我们可以简单地理解维度表包含了事实表中指定属性的相关详细信息。比如商品维度表表和用户维度表。维度表，一致性维度，业务过程的发生或分析角度，我们主要关注下退化维度和缓慢变化维。（1）退化维度（DegenerateDimension）在维度类型中，有一种重要的维度称作为退化维度，亦维度退化一说。这种维度指的是直接把一些简单的维度放在事实表中。退化维度是维度建模领域中的一个非常重要的概念，它对理解维度建模有着非常重要的作用，退化维度一般在分析中可以用来做分组使用。（2）缓慢变化维（Slowly Changing Dimensions）维度的属性并不是始终不变的，它会随着时间的流逝发生缓慢的变化，这种随时间发生变化的维度我们一般称之为缓慢变化维（SCD）。</code></pre><p>SCD常用的三种处理方式：</p><p>① <strong>TYPE1</strong> 直接覆盖原值</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/1OYP1AZw0W1UwPYvStthXYOIVIJMRPHgR0hXEic2ooibCQcliandOomZCkS90JesBrE3XfK9FT0L4mzloqeiaBLGiaQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p>② <strong>TYPE2</strong> 增加维度行</p><p>   <em>在为维度成员增加新行时，需为其分配新的主代理键。<strong>并且，至少需要在维度行再增加三列：</strong>有效日期、截止日期、行标识。**这个地方可联想拉链表设计。</em></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/1OYP1AZw0W1UwPYvStthXYOIVIJMRPHgJQjR5Q6icbWznia9JUHr1DA4I6nNictPvXmgNayXhgcBcL96FOJGmY8kw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p>③ <strong>TYPE3</strong> 增加属性列 </p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/1OYP1AZw0W1UwPYvStthXYOIVIJMRPHgB52FtYIJ0gs20XVBtSFsiaMFCtqsDZjAEkR03vFibnATtpNEBzkn3hnA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p>④ 混合方式</p><p>可根据实际业务场景，混合或选择使用以上三种方式，以快速方便而又准确的分析历史变化情况。</p><p><em><strong>星形模型，雪花模型，星座模型的详解</strong></em></p><pre><code class="python"></code></pre><p><em><strong>元数据</strong></em></p><p> 元数据是对潜在信息的信息，是关于数据的更高层次抽象，是对数据的描述。 </p><pre><code class="python">##  业务元数据：指标名称、计算口径、业务术语解释、衍生指标等数据概念模型和逻辑模型业务规则引擎的规则、数据质量检测规则、数据挖掘算法等数据血缘和影响分析数据的安全或敏感级别等## 技术元数据：物理数据库表名称、列名称、列属性、备注、约束信息等数据存储类型、位置、数据存储文件格式或数据压缩类型等数据访问权限、组和角色字段级血缘关系、ETL抽取加载转换信息调度依赖关系、进度和数据更新频率## 操作元数据：系统执行日志访问模式、访问频率和执行时间程序名称和描述版本维护等备份、归档时间、归档存储信息        上述只是大致的分为三类，简单地列举常用的元数据信息，其实还包括结构性元数据、保存性和权限元数据等等这里就不一一列举了。</code></pre><p> <strong>元数据管理</strong> </p><p> 元数据也是数据，同样适用数据生命周期管理。元数据生命周期可分为采集、整合、存储、分析、应用、价值和服务几个阶段。 </p><pre><code class="python">## 元数据架构         元数据战略是关于企业元数据管理目标的说明，也是开发团队的参考框架。元数据战略决定了企业元数据架构。元数据架构可分为三类：集中式元数据架构、分布式元数据架构和混合元数据架构。## 集中式元数据架构：        集中式架构包括一个集中的元数据存储，在这里保存了来自各个元数据来源的元数据最新副本。保证了其独立于源系统的元数据高可用性；加强了元数据存储的统一性和一致性；通过结构化、标准化元数据及其附件的元数据信息，提升了元数据数据质量。集中式元数据架构有利于元数据标准化统一管理与应用。分布式元数据架构：        分布式架构包括一个完整的分布式系统架构只维护一个单一访问点，元数据获取引擎响应用户的需求，从元数据来源系统实时获取元数据，而不存在统一集中元数据存储。虽然此架构保证了元数据始终是最新且有效的，但是源系统的元数据没有经过标准化或附加元数据的整合，且查询能力直接受限于相关元数据来源系统的可用性。        ## 混合式元数据架构：        这是一种折中的架构方案，元数据依然从元数据来源系统进入存储库。但是存储库的设计只考虑用户增加的元数据、高度标准化的元数据以及手工获取的元数据。        这三类各有千秋，但为了更好发挥数据价值，就需要对元数据标准化、集中整合化、统一化管理。如果企业做功能较为完善的数据资产管理平台可采用集中式元数据架构。</code></pre><p> <strong>元数据生命周期</strong> </p><p>  笔者这里以集中式元数据架构为例讲解，通过对数据源系统的元数据信息采集，发送Kafka消息系统进行解耦合，再使用Antlr4开发各版SQL解析器，对元数据信息新增、修改和删除操作进行标准化集中整合存储。在元数据集中存储的基础上或过程中，可提供元数据服务与应用，如数据资产目录、数据地图、集成IDE、统一SQL多处理引擎、字段级血缘关系、影响度分析、下线分析、版本管理和数据价值分析等（这些元数据应用可根据产品经理设计理念进行优化组合，笔者这里拉平排列各功能应用，为了方便讲解各元数据应用模块）。这里就包括了元数据采集、整合、存储、分析、应用等阶段的生命周期。 </p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200924235456.jpg" alt="微信图片_20200924235456" style="zoom:50%;"><p>例：</p><p>技术元数据</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200924235712.png" alt="微信图片_20200924235712" style="zoom:50%;"><p>业务元数据</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200924235755.jpg" alt="微信图片_20200924235755" style="zoom: 80%;"><p><strong>数据质量监控系统的设计与实现</strong></p><p><strong>总体介绍</strong></p><p>​    此数据质量监控系统是基于Spark计算引擎，通过界面配置对Hive数据仓库各层表，离线批数据质量监控系统（流式数据质量监模块近期实现后再做分享）。用户可通过前端界面选择哪个数据源（哪个集群），数据库、表或表中字段、配置监控规则，存放到Mysql库（下文有表结构设计），程序通过规则大类，监控规则等元数据信息，动态生成SQL片段集合，在进行优化组合，尽量减少对表读取次数，将执行结果存放监控结果表。调度根据当前任务执行结果判断是否熔断告警。再根据执行结果形成数据质量报告。</p><p><strong>功能</strong></p><ul><li>丰富可扩展数据质量监控规则库</li><li>自定义数据质量监控规则及语法检查</li><li>任务熔断、电话、短信、邮件多级告警</li><li>清晰定位质量问题业务和技术数据Owner</li><li>数据质量问题汇总与明细展示</li><li>监控对象表结构变更动态感知</li><li>数据质量问题订阅</li></ul><p><strong>设计实现</strong></p><p>系统框架图</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Users\14352\AppData\Roaming\Typora\typora-user-images\1600965150300.png" alt="1600965150300" style="zoom:50%;"><p> <strong>数据质量监控执行流程图</strong> </p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Users\14352\AppData\Roaming\Typora\typora-user-images\1600965581908.png" alt="1600965581908" style="zoom: 33%;"><p> <strong>表结构设计</strong> </p><pre><code class="python">## 1、监控任务与监控对象表的对应关系表    说明：source_id:制定对哪个集群数据进行质量监控databaseName：指定数据库名称table_name:指定表名称owner：数据owner，表创建人，是从HiveMeta元数据中取得## 2、监控规则配置表 说明：task_id:即上一张表id，调度时使用databaseName：指定数据库名称table_name: 指定表名称part_flag: 是否为分区表标志part_format: 分区格式task_desc: 用户对此条监控规则的描述rule_name: 监控规则大分类，如有效性、唯一性、准确性等等term：具体的监控指标名称field_name: 监控字段，如果是表级别，则填写“表级规则”或自定义SQL规则warn_grad：告警级别 1:熔断电话告警；2:电话告警；3：邮件告警等等rule_logic_monitor:监控规则，需要配置逻辑信息，是个json字符串，程序执行对对json的key进行解析data_owner: 数据owneris_usable: 此条监控规则是否可用，前端配置人员可停启规则    ## 3、监控规则库表    说明：rule_name: 监控规则大分类，如有效性、唯一性、准确性等等rule_desc: 规则大类的说明term：具体的监控指标名称term_desc: 规则说明logic_remark: 监控规则的详细说明term_level: 规则级别，字段级别、表级别或自定义SQL规则is_useDefine: 是否为自定义规则datatype_scope: 此条监控规则的是否范围，比如：同环比波动监测，只能数值型字段stats: 状态，是否可用## 4、监控规则元数据表    说明：rule_name: 监控规则大分类，如有效性、唯一性、准确性等等term：具体的监控指标名称param_name:  参数名称，为rule_logic_monitor监控规则，需要配置逻辑信息，是个json字符串，程序执行对对json的key进行解析param_desc: 界面展示value_type: 界面输入框还是下拉框等说明constrints: 界面输入框输入内容限制，正则表达式param_weight: 界面参数展示顺序 ## 5、监控结果表    说明：task_id：任务iduuid: 调用的批次databaseName：指定数据库名称table_name: 指定表名称rule_name: 监控规则大分类，如有效性、唯一性、准确性等等term：具体的监控指标名称field_name: 监控字段，如果是表级别，则填写“表级规则”或自定义SQL规则term_value：监控结果值result_code：结果返回码，配合告警级别一起使用warn_grad：告警级别 1:熔断电话告警；2:电话告警；3：邮件告警等等data_owner: 数据ownerstats_date: 统计日期    ## 6、监测结果码表    说明：    对监控结果表的term_value结果值说明</code></pre><p> <strong>规则指标</strong> </p><pre><code class="python">数据质量监控指标从有效性、唯一性、完整性、准确性、一致性、有效性、时效性和自定义监控规则八大类，约20条监控规则指标。以下对各个监控指标做出解释。## 有效性 字段长度有效对字段内容长度是否在有效性范围的监控指标，可配置[最小长度，最        大长度]。如mobile手机号11位；身份证18位或15位是否满足配置监控长度，不等于配置长度范围，视为脏数据。 字段内容有效对字段内容是否在满足正则表达式指定内容格式的监控指标。如对name姓名含有中英文结合；身份证号含有中文；手机号11111111111等异常数据监控。 字段数值范围有效对数值类型字段是否在有效性值范围的监控指标，可根据业务场景配置该字段值范围[最小值，最大值]。如age年龄，超过1000岁等。 枚举值个数有效对枚举值字段的可枚举值种类个数的监控指标，可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]与期望值的比较。如银行储值卡在“消费、转账、提现”三种业务类型，枚举值个数某天少了一种或多种业务类型，可能是上游业务系统出现问题，或数据采集时丢失数据。 枚举值集合有效对枚举值字段的可枚举值种类内容集合的监控指标，可配置“包含、相等、不包含”与期望值集合的比较。如银行储值卡在“消费、转账、提现”三种业务类型，出现了“消费、转账、贷款”三种业务类型，虽然枚举值个数也是3种，但是枚举值内容有误。## 唯一性 是否重复对主键是否存在重复数据的监控指标。出现重复数据导致重复计算等问题，也支持联合主键唯一性监控。完整性 字段是否为空或NULL对字段内容是否存NULL的监控指标。 记录数是否丢失表级别质量监控指标，判断是否记录是否丢失或无数据，可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]与期望值的比较。 记录数环比波动表级别质量监控指标，判断记录数环比波动范围的指标监控，如可配置[最小波动值,最大波动值]可接受记录数波动范围。 记录数方差检验表级别质量监控指标，判断记录数方差检验波动范围的指标监控，如可配置[最小波动值,最大波动值]可接受记录数波动范围。## 准确性 数值同比波动监测对数值类型字段值同比年、季度或月度值是否在波动范围内的指标监控。可配置[最小波动值,最大波动值]可接受记录数波动范围。一般用于报表指标等监控。 数值环比波动监测对数值类型字段值环比[1-30]天值是否在波动范围内的指标监控。可配置[最小波动值,最大波动值]可接受记录数波动范围。一般用于报表指标等监控。 数值方差波动检验对数值类型字段值方差检验是否在波动范围内的指标监控。可配置[最小波动值,最大波动值]可接受记录数波动范围。一般用于报表指标等监控。 表逻辑检查表级别质量监控指标，对表两个字段存在逻辑关系是否准确的监控指标。如信用卡当前剩余可用额度&lt;=此次消费金额；还如贷款，起息日应早于贷款放款日期等异常监控。## 一致性 表级别一致性检查表级别质量监控指标，根据提前定义的数据标准，基础元数据字段命名规范，术语命名规则、字段comennt规范、数据类型规范；指标元数据字段命名规范，术语命名规范、字段comennt规范、数据类型规范，计算口径是否统一等规范。对表结构字段、字段comment、数据类型等的是否一致的监控检查。 交叉验证表级别质量监控指标，判断两张表的主体对象是否一致。## 时效性数据是否准时产出表级别质量监控指标，数据是否按时产出。如用于决策的报表是否领导们上班前准时看到准确无误的报表。## 数据剖析 最大值检查对数值类型字段的最大值与期望值可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]比较的监控指标。支持Where条件的自定义谓词条件限制。最小值检查对数值类型字段的最小值与期望值可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]比较的监控指标。支持Where条件的自定义谓词条件限制。平均值检查对数值类型字段的平均值与期望值可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]比较的监控指标。支持Where条件的自定义谓词条件限制。汇总值检查对数值类型字段的汇总值与期望值可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]比较的监控指标。支持Where条件的自定义谓词条件限制。## 自定义规则检查 自定义SQL规则用户写自定义SQL实现的监控规则，但这段SQL结果必须一行一列值，即监测结果是一个值。可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]与期望值的比较，判断监测结果是否异常。</code></pre><p> 常见数据质量监控规则如下图：</p><img src="https://img-blog.csdnimg.cn/20190528154230375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mjg5MzY1MA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 50%;"><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200927231751.jpg" alt="微信图片_20200927231751" style="zoom:50%;"><p>表结构变更动态感知** </p><pre><code class="python">1:</code></pre><p><strong>告警及通知</strong></p><pre><code>1:</code></pre><h2 id="Hive插入数据的几种方式"><a href="#Hive插入数据的几种方式" class="headerlink" title="Hive插入数据的几种方式"></a><strong>Hive插入数据的几种方式</strong></h2><pre><code class="python">##1 将查询结果保存到一张新的hive表中create table t_tmpasselect * from t_p;##2 将查询结果保存到一张已经存在的hive表中 (overwrite 覆盖 into 追加)insert into/overwrite table t_tmp            select * from t_p;##3 将查询结果保存到指定的文件目录insert overwrite local directory &#39;/home/hadoop/test&#39;select * from t_p;##4 多重插入from xxx_table_name PARTITION (ds = &#39;20190827&#39;)insert overwrite table table1 PARTITION (ds = &#39;0&#39;) select name where crc32(cust_id)%4 == 0insert overwrite table table1 PARTITION (ds = &#39;1&#39;) select name where crc32(cust_id)%4 == 1insert overwrite table table1 PARTITION (ds = &#39;2&#39;) select name where crc32(cust_id)%4 == 2insert overwrite table table1 PARTITION (ds = &#39;3&#39;) select name where crc32(cust_id)%4 == 3##5 with创建临时表 CTE后面必须直接跟使用CTE的SQL语句（如select、insert、update等），否则，CTE将失效with cte1 as (     select * from table1 where name like &#39;abc%&#39; ), cte2 as (     select * from table2 where id &gt; 20 ), cte3 as (     select * from table3 where price &lt; 100 ) select a.* from cte1 a, cte2 b, cte3 c where a.id = b.id and a.id = c.id 或者：WITH Generation (ID) AS(-- First anchor member returns Bonnie&#39;s mother.    SELECT Mother     FROM Person    WHERE Name = &#39;Bonnie&#39;UNION-- Second anchor member returns Bonnie&#39;s father.    SELECT Father     FROM Person    WHERE Name = &#39;Bonnie&#39;UNION ALL-- First recursive member returns male ancestors of the previous generation.    SELECT Person.Father    FROM Generation, Person    WHERE Generation.ID=Person.IDUNION ALL-- Second recursive member returns female ancestors of the previous generation.    SELECT Person.Mother    FROM Generation, Person    WHERE Generation.ID=Person.ID)SELECT Person.ID, Person.Name, Person.Mother, Person.FatherFROM Generation, PersonWHERE Generation.ID = Person.ID;</code></pre><h2 id="hive对于连续多天未出现的时间节点补零操作"><a href="#hive对于连续多天未出现的时间节点补零操作" class="headerlink" title="hive对于连续多天未出现的时间节点补零操作"></a>hive对于连续多天未出现的时间节点补零操作</h2><h2 id="postgresql与hive的炸裂函数"><a href="#postgresql与hive的炸裂函数" class="headerlink" title="postgresql与hive的炸裂函数"></a>postgresql与hive的炸裂函数</h2><pre><code class="sql">------------------------------------------------ |                                             |            postgresql炸裂详解   |                                             |------------------------------------------------ |select regexp_split_to_table(tr,&#39;,&#39;) field from (select array_to_string(tiff_ids,&#39;,&#39;) trfrom order_table_test)  o炸裂函数regexp_split_to_tableselect regexp_split_to_table(&#39;飞机，火车，地铁，汽车&#39;,  &#39;，&#39; )                    以逗号切分，转为数据集select regexp_split_to_array(&#39;飞机，火车，地铁，汽车&#39;, &#39;，&#39; )                     转为数组select (regexp_split_to_array(&#39;飞机，火车，地铁，汽车&#39;,  &#39;，&#39; ))[1]               取数组的第二个元素select regexp_split_to_table(&#39;F:\QH本部文件\一套表部署相关\test.sh&#39;,&#39;\\&#39;)         正则匹配array_agg(expression)       把表达式变成一个数组 一般配合 array_to_string() 函数使用select nameid, array_agg(traffic ) from dbscheme.test0001 group by nameid order by nameid ;   变为数组string_agg(expression, delimiter)    直接把一个表达式变成字符串select nameid, string_agg(traffic,&#39;,&#39;) , update_time from dbscheme.test0001   //相同id 的连接到一起，逗号分隔group by nameid,update_time order by  nameid,update_time;    select nameid, array_to_string(array_agg(traffic),&#39;,&#39;) from dbscheme.test0001 group by nameid order by nameid ; .数组转字符串------------------------------------------------ |                                             |            hive炸裂详解   |                                             |------------------------------------------------ |+----------+----------------------+--+|   a.id   |        a.tim         |+----------+----------------------+--+| a,b,c,d  | 2:00,3:00,4:00,5:00  || f,b,c,d  | 1:10,2:20,3:30,4:40  |+----------+----------------------+--+explode 炸裂函数，一列变多行。select id,tim,single_tim from atlasdemo.a lateral view explode(split(tim,&#39;,&#39;)) t as single_tim+----------+----------------------+-------------+--+|    id    |         tim          | single_tim  |+----------+----------------------+-------------+--+| a,b,c,d  | 2:00,3:00,4:00,5:00  | 2:00        || a,b,c,d  | 2:00,3:00,4:00,5:00  | 3:00        || a,b,c,d  | 2:00,3:00,4:00,5:00  | 4:00        || a,b,c,d  | 2:00,3:00,4:00,5:00  | 5:00        || f,b,c,d  | 1:10,2:20,3:30,4:40  | 1:10        || f,b,c,d  | 1:10,2:20,3:30,4:40  | 2:20        || f,b,c,d  | 1:10,2:20,3:30,4:40  | 3:30        || f,b,c,d  | 1:10,2:20,3:30,4:40  | 4:40        |+----------+----------------------+-------------+--+select id,tim,single_id_index,single_id from atlasdemo.a lateral view posexplode(split(id,&#39;,&#39;)) t as single_id_index, single_id;posexplode炸裂除了会炸开数组/map，还会对应生成索引下标。+----------+----------------------+------------------+------------+--+|    id    |         tim          | single_id_index  | single_id  |+----------+----------------------+------------------+------------+--+| a,b,c,d  | 2:00,3:00,4:00,5:00  | 0                | a          || a,b,c,d  | 2:00,3:00,4:00,5:00  | 1                | b          || a,b,c,d  | 2:00,3:00,4:00,5:00  | 2                | c          || a,b,c,d  | 2:00,3:00,4:00,5:00  | 3                | d          || f,b,c,d  | 1:10,2:20,3:30,4:40  | 0                | f          || f,b,c,d  | 1:10,2:20,3:30,4:40  | 1                | b          || f,b,c,d  | 1:10,2:20,3:30,4:40  | 2                | c          || f,b,c,d  | 1:10,2:20,3:30,4:40  | 3                | d          |+----------+----------------------+------------------+------------+--+如果想实现对两列听同事进行多行转换，那么用explode()函数就不能实现了，但可以用posexplode()函数，因为该函数可以将index和数据都取出来，使用两次posexplode并令两次取到的index相等就行了。select id,tim,single_id,single_tim from atlasdemo.a lateral view posexplode(split(id,&#39;,&#39;)) t as single_id_index, single_idlateral view posexplode(split(tim,&#39;,&#39;)) t as single_yim_index, single_timwhere single_id_index = single_yim_index;+----------+----------------------+------------+-------------+--+|    id    |         tim          | single_id  | single_tim  |+----------+----------------------+------------+-------------+--+| a,b,c,d  | 2:00,3:00,4:00,5:00  | a          | 2:00        || a,b,c,d  | 2:00,3:00,4:00,5:00  | b          | 3:00        || a,b,c,d  | 2:00,3:00,4:00,5:00  | c          | 4:00        || a,b,c,d  | 2:00,3:00,4:00,5:00  | d          | 5:00        || f,b,c,d  | 1:10,2:20,3:30,4:40  | f          | 1:10        || f,b,c,d  | 1:10,2:20,3:30,4:40  | b          | 2:20        || f,b,c,d  | 1:10,2:20,3:30,4:40  | c          | 3:30        || f,b,c,d  | 1:10,2:20,3:30,4:40  | d          | 4:40        |+----------+----------------------+------------+-------------+--+</code></pre><h2 id="美团-数据质量平台-设计与实践"><a href="#美团-数据质量平台-设计与实践" class="headerlink" title="美团 数据质量平台 设计与实践"></a>美团 数据质量平台 设计与实践</h2><p><em>*<em>*</em>*挑战****</em>*</p><p>美旅数据中心日均处理的离线和实时作业高达数万量级， 如何更加合理、高效的监控每类作业的运行状态，并将原本分散、孤岛式的监控日志信息通过规则引擎集中共享、关联、处理；洞察关键信息，形成事前预判、事中监控、事后跟踪的质量管理闭环流程；沉淀故障问题，搭建解决方案的知识库体系。在数据质量监管平台的规划建设中，面临如下挑战：</p><ul><li>缺乏统一监控视图，离线和实时作业监控分散，影响性、关联性不足。</li><li>数据质量的衡量标准缺失，数据校验滞后，数据口径不统一。</li><li>问题故障处理流程未闭环，“点”式解决现象常在；缺乏统一归档，没有形成体系的知识库。</li><li>数据模型质量监控缺失，模型重复，基础模型与应用模型的关联度不足，形成信息孤岛。</li><li>数据存储资源增长过快，不能监控细粒度资源内容。</li></ul><p>DataMan质量监管平台研发正基于此，以下为具体建设方案</p><p><em>*<em>*</em>*解决思路****</em>*</p><p><strong>整体框架</strong></p><p>构建美旅大数据质量监控平台，从可实践运用的视角出发，整合平台资源、技术流程核心要点，重点着力平台支持、技术控制、流程制度、知识体系形成等方向建设，确保质量监控平台敏捷推进落地的可行性。数据质量监控平台整体框架如图1所示：</p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200927223431.jpg" alt="微信图片_20200927223431" style="zoom:50%;">]]></content>
      
      
      <categories>
          
          <category> 面试题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据组件版本</title>
      <link href="/da-shu-ju/fu-wu-qi-zu-jian-ban-ben/"/>
      <url>/da-shu-ju/fu-wu-qi-zu-jian-ban-ben/</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th>系统</th><th>CentOS Linux release 7.6.1810</th></tr></thead><tbody><tr><td>JDK</td><td>JDK_1.8</td></tr><tr><td>CM</td><td>CM_5.16.2</td></tr><tr><td>CDH</td><td>CDH_5.16.2</td></tr><tr><td>Hadoop</td><td>2.6.0</td></tr><tr><td>Yarn</td><td>2.6.0</td></tr><tr><td>MapReduce</td><td>2.6.0</td></tr><tr><td>Kafka</td><td>2.2.1</td></tr><tr><td>Oozie</td><td>4.1.0</td></tr><tr><td>Spark2</td><td>2.4.0</td></tr><tr><td>Sqoop</td><td>1.4.6</td></tr><tr><td>Zookeeper</td><td>3.4.5</td></tr><tr><td>Canal</td><td>1.1.4</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><table><thead><tr><th>hadoop1</th><th>hadoop2</th><th>hadoop3</th></tr></thead><tbody><tr><td>HDFS配置</td><td></td><td></td></tr><tr><td>HDFS.namenode</td><td>HDFS.SNN</td><td>HDFS.HttpFS</td></tr><tr><td>HDFS.blancer</td><td>HDFS.Gateway</td><td>HDFS.Gateway</td></tr><tr><td>HDFS.Gateway</td><td>HDFS.datanode</td><td>HDFS.datanode</td></tr><tr><td>HDFS.datanode</td><td>HDFS.ZKfailover controller</td><td>HDFS.journalnode</td></tr><tr><td>HDFS.https</td><td>HDFS.journalnode</td><td></td></tr><tr><td>HDFS.journalnode</td><td>HDFS.namenode</td><td></td></tr><tr><td>HDFS.ZKfailover controller</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>Hive配置</td><td></td><td></td></tr><tr><td>Hive.Gateway</td><td>Hive.Gateway</td><td>Hive.Gateway</td></tr><tr><td>Hive.Metastore Server</td><td>Hive.Server2</td><td>Hive.Server2</td></tr><tr><td>Hive.Server2</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>Kafka配置</td><td></td><td></td></tr><tr><td>Kafka.Gateway</td><td>Kafka.Gateway</td><td>Kafka.Gateway</td></tr><tr><td>Kafka.Broker</td><td>Kafka.Broker</td><td>Kafka.Broker</td></tr><tr><td>Kafka.MirrorMaker</td><td>Kafka.MirrorMaker</td><td>Kafka.MirrorMaker</td></tr><tr><td></td><td></td><td></td></tr><tr><td>CM配置</td><td></td><td></td></tr><tr><td>Service Monitor</td><td></td><td></td></tr><tr><td>Alert Publisher</td><td></td><td></td></tr><tr><td>Event Server</td><td></td><td></td></tr><tr><td>Host Monitor</td><td></td><td></td></tr><tr><td>Alert Monitor</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>Oozie配置</td><td></td><td></td></tr><tr><td>Oozie.server</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>Spark2</td><td></td><td></td></tr><tr><td>Spark2.Gateway</td><td>Spark2.Gateway</td><td>Spark2.Gateway</td></tr><tr><td></td><td>Spark2.History Server</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>Sqoop</td><td></td><td></td></tr><tr><td>Sqoop.Client Gateway</td><td>Sqoop.Client Gateway</td><td>Sqoop.Client Gateway</td></tr><tr><td></td><td></td><td></td></tr><tr><td>Yarn</td><td></td><td></td></tr><tr><td>Yarn.NodeManager</td><td>Yarn.JobHistory Server</td><td>Yarn.NodeManager</td></tr><tr><td></td><td>Yarn.NodeManager</td><td></td></tr><tr><td></td><td>Yarn.ResourceManager</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>Zookeeper配置</td><td></td><td></td></tr><tr><td>ZK.server</td><td>ZK.server</td><td>ZK.server</td></tr><tr><td></td><td></td><td></td></tr><tr><td>Canal配置</td><td></td><td></td></tr><tr><td>Canal.Server</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSql笔记</title>
      <link href="/cdh/spark/sparksql/"/>
      <url>/cdh/spark/sparksql/</url>
      
        <content type="html"><![CDATA[<h1 id="SparkSql"><a href="#SparkSql" class="headerlink" title="SparkSql"></a>SparkSql</h1><h3 id="什么是SparkSQL"><a href="#什么是SparkSQL" class="headerlink" title="什么是SparkSQL"></a>什么是SparkSQL</h3><p>Spark是基于sparkcore的一个上层框架，处理结构化的数据</p><h3 id="SparkSQL的4个特点"><a href="#SparkSQL的4个特点" class="headerlink" title="SparkSQL的4个特点"></a>SparkSQL的4个特点</h3><p>1.易整合</p><p>将sql查询与spark程序无缝混合，可以使用java、scala、python、R等语言的API操作。</p><p>2.统一的数据访问</p><p>以相同的方式连接到任何数据源。</p><p>3.兼容Hive</p><p>支持hiveSQL的语法。无缝衔接hive</p><p>4.标准的数据连接</p><p>可以使用行业标准的JDBC或ODBC连接。</p><h3 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h3><p>使用 Dataset 或者 Datafram 编写 Spark SQL 应用的时候，第一个要创建的对象就是 SparkSession。</p><h4 id="SparkSession创建"><a href="#SparkSession创建" class="headerlink" title="SparkSession创建"></a>SparkSession创建</h4><p>1、SparkSession在Spark1.x 版本的创建语法，叫做SQLContext</p><pre><code class="scala">val conf = new SparkConf().setAppName(&quot;sql&quot;).(&quot;local[*]&quot;)val sc = new SparkContext(conf)//方法已过期val sqlSC: SQLContext = new SQLContext(sc)</code></pre><p>2、SparkSession在Spark2.0开始全新Spark接口</p><p><strong>Builder 是 SparkSession 的构造器。 通过 Builder, 可以添加各种配置。</strong></p><pre><code class="scala">val session: SparkSession = SparkSession.builder().appName(&quot;sql&quot;).master(&quot;local[*]&quot;).getOrCreate()</code></pre><p><strong>Builder 是 SparkSession 的构造器。 通过 Builder, 可以添加各种配置。</strong></p><p>Builder 的方法如下：</p><table><thead><tr><th><strong>Method</strong></th><th></th></tr></thead><tbody><tr><td>getOrCreate</td><td>获取或者新建一个 sparkSession，单例模式</td></tr><tr><td>enableHiveSupport</td><td>增加支持 hive Support</td></tr><tr><td>appName</td><td>设置 application 的名字</td></tr><tr><td>config</td><td>设置各种配置</td></tr><tr><td>master</td><td>local 本地单线程<br>local[K] 本地<a href="https://so.csdn.net/so/search?from=pc_blog_highlight&q=%E5%A4%9A%E7%BA%BF%E7%A8%8B">多线程</a>（指定K个内核）<br>local[*] 本地多线程（指定所有可用内核）</td></tr></tbody></table><h4 id="SparkSession的使用"><a href="#SparkSession的使用" class="headerlink" title="SparkSession的使用"></a>SparkSession的使用</h4><p>1、读取文件</p><pre><code class="scala">import spark.implicits._val session: SparkSession = SparkSession.builder().appName(&quot;sql&quot;).master(&quot;local[*]&quot;).getOrCreate()//读取文件val df2: Dataset[Long] = session.read.textFile(&quot;access.log&quot;).map(t =&gt; &#123;    val strs: Array[String] = t.split(&quot;\\|&quot;)    ip2Long(strs(1))    &#125;)</code></pre><p>2、使用sparkSql</p><pre><code class="scala">val resultsDF: DataFrame = spark.sql(&quot;SELECT city, pop, state, zip FROM zips_table&quot;)resultsDF.show()</code></pre><h4 id="Sparksql的并行度设置"><a href="#Sparksql的并行度设置" class="headerlink" title="Sparksql的并行度设置"></a>Sparksql的并行度设置</h4><p><strong>SparkSQL并行度是SparkSQL的第一个调优点，默认的并行度是200，需要根据实际情况进行设置，它有有两种设置方法，</strong></p><p>1：代码的方式指定</p><pre><code class="scala">val spark = SparkSession.builder()      .config(&quot;spark.sql.shuffle.partitions&quot;,100)//设置并行度100      .getOrCreate()</code></pre><p>2.命令行的方式指定</p><pre><code class="shell">./bin/spark-submit \--class com.imooc.log.TopNStatJobYARN \--name TopNStatJobYARN \--master yarn \--executor-memory 1G \--num-executors 1 \--conf spark.sql.shuffle.partitions=100 \/home/hadoop/lib/sql-1.0-jar-with-dependencies.jar \hdfs://hadoop001:8020/imooc/clean 20170511 </code></pre><p>如果都配置了，代码的优先级更高</p><h2 id="数据源的不同方式"><a href="#数据源的不同方式" class="headerlink" title="数据源的不同方式"></a>数据源的不同方式</h2><p>这里先讲了6种</p><h3 id="1、text"><a href="#1、text" class="headerlink" title="1、text"></a>1、text</h3><pre><code class="scala">//读取数据val frame1: DataFrame = session.read.text(&quot;stu.txt&quot;)//写出数据 ！ text只能一行的写出，要写出多行只能拼接，而且保存的数据不带有列信息session.read.parquet(&quot;parquet_res&quot;).map(t=&gt;&#123;    val address = t.getAs[String](&quot;address&quot;)    val count = t.getAs[Long](&quot;count&quot;)    val id = t.getAs[Long](&quot;id&quot;)    address + &quot; &quot;+count +&quot; &quot; + id&#125;).write.text(&quot;text_res&quot;)</code></pre><h3 id="2、textFile"><a href="#2、textFile" class="headerlink" title="2、textFile"></a>2、textFile</h3><pre><code class="scala">//读取数据val frame2: Dataset[String] = session.read.textFile(&quot;stu.txt&quot;)//写出数据?session.write.textFile(&quot;parquet_res&quot;)</code></pre><h3 id="3、JDBC"><a href="#3、JDBC" class="headerlink" title="3、JDBC"></a>3、JDBC</h3><pre><code class="scala">//读取数据val url = &quot;jdbc:mysql://Linux03:3306/spakr?characterEncoding=utf-8&quot;    val table = &quot;visit&quot;    val props = new Properties()    props.put(&quot;user&quot;, &quot;root&quot;)    props.put(&quot;password&quot;, &quot;123456&quot;)    val frame3: DataFrame = session.read.jdbc(url, table, props)    frame3.show()    //jdbc底层是    //    session.read.format(&quot;jdbc&quot;)    //      .option(&quot;url&quot;, url)    //      .option(&quot;datable&quot;, table)    //      .option(&quot;user&quot;, &quot;root&quot;)    //      .option(&quot;password&quot;, &quot;123456&quot;)    //      .load()//dataframe的过滤//dataframe还可以过滤,底层都是使用的row.getxxxframe3.where(&quot;count&gt;1000&quot;).show()frame3.filter(row =&gt; row.getAs[Int](&quot;count&quot;)&gt;1000).show()///jdbc写出，写出的表必须是不存在的frame3.filter(row =&gt; row.getAs[Int](&quot;count&quot;)&gt;1000).write.jdbc(url,table,props)///模式//mode模式frame3.filter(row =&gt; row.getAs[Int](&quot;count&quot;)&gt;1000).write.mode(SaveMode.Append).jdbc(url,table,props)</code></pre><h4 id="JDBC的模式"><a href="#JDBC的模式" class="headerlink" title="JDBC的模式"></a>JDBC的模式</h4><pre><code class="scala">//mode模式类型//save方法保存数据的时候需要用到mode模式//Append表存在则拼接//Overwrite进行覆盖//ErrorIFExists默认级别，如果存在就报错//Ignore如果存在数据就忽略frame3.filter(row =&gt; row.getAs[Int](&quot;count&quot;)&gt;1000).write.mode(SaveMode.Append).jdbc(url,table,props)</code></pre><h3 id="4、JSON"><a href="#4、JSON" class="headerlink" title="4、JSON"></a>4、JSON</h3><pre><code class="scala">//读取数据val frame4: DataFrame = session.read.json(&quot;json_res&quot;)frame4.show()//写出数据frame3.write.json(&quot;parquet_res&quot;)</code></pre><h3 id="5、parquet"><a href="#5、parquet" class="headerlink" title="5、parquet"></a>5、parquet</h3><p>列式存储文件，保存的文件默认snappy压缩格式</p><pre><code class="scala">//读取数据val frame5: DataFrame = session.read.parquet(&quot;parquet_res&quot;)//写出数据frame3.write.parquet(&quot;parquet_res&quot;)</code></pre><h3 id="6、csv"><a href="#6、csv" class="headerlink" title="6、csv"></a>6、csv</h3><p>保存的文件可以被text和excel打开，是一个特殊的数据源，有一个效验和，单文件发生变化，效验和会发生改变，跟原来效验和对比如果不同就会失效</p><p>csv和text一样，<strong>保存的数据不带有列信息</strong></p><pre><code class="scala">//读取数据session.read.csv(&quot;csv_res&quot;)//写出数据frame5.write.csv(&quot;csv_res&quot;)</code></pre><h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><h3 id="RDD和DataFrame的区别"><a href="#RDD和DataFrame的区别" class="headerlink" title="RDD和DataFrame的区别"></a>RDD和DataFrame的区别</h3><p>dataFrame装入的数据和rdd是一样的，只不过存在很多scheme信息（表头），<strong>除了数据以外，还掌握数据的结构信息，即</strong> <strong>schema</strong></p><h3 id="创建Dataframe两种方式"><a href="#创建Dataframe两种方式" class="headerlink" title="创建Dataframe两种方式"></a>创建Dataframe两种方式</h3><p>1、rdd.toDF()</p><pre><code class="scala">   val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;sql&quot;)    val sc = new SparkContext(conf)    //sqlContext是一个包装对象    //这是1.0版本，2.0是sparksession    //1、创建sparksql对象    val sqlSC = new SQLContext(sc)    val rdd1: RDD[Student] = sc.textFile(&quot;student.txt&quot;).map(t =&gt; &#123;      val strs: Array[String] = t.split(&quot; &quot;)      new Student(strs(0).toInt, strs(1), strs(2).toInt)    &#125;)    import sqlSC.implicits._    //2、转换成DF    val frame: DataFrame = rdd1.toDF()</code></pre><p>2、Row,sqlSC.createDataFrame(rdd, types)，更加灵活的创建</p><p>Row只能由系统来指定，自己指定会报错</p><pre><code class="scala">val conf = new SparkConf().setAppName(&quot;sql&quot;).setMaster(&quot;local[*]&quot;)    val sc = new SparkContext(conf)    val sqlSC = new SQLContext(sc)    //Row  相当于一行内容    val rdd: RDD[Row] = sc.textFile(&quot;student.txt&quot;).map(t =&gt; &#123;      val strs: Array[String] = t.split(&quot; &quot;)        //Row相当于一行内容的对象      Row(strs(0).toInt, strs(1), strs(2).toInt)    &#125;)    val types: StructType = StructType(      List(        //字段名称，字段类型，是否可以为空        StructField(&quot;id&quot;, IntegerType, false),        StructField(&quot;name&quot;, StringType, true),        StructField(&quot;age&quot;, IntegerType, true)      )    )    val df: DataFrame = sqlSC.createDataFrame(rdd, types)    df.show()    df.registerTempTable(&quot;student&quot;)    //show默认打印20条    val frame: DataFrame = sqlSC.sql(&quot;select * from student where age &gt; 30&quot;)    frame.show()</code></pre><h3 id="dataframe的使用"><a href="#dataframe的使用" class="headerlink" title="dataframe的使用"></a>dataframe的使用</h3><ol><li><pre><code>引入sql的依赖</code></pre><ol start="2"><li><pre><code>创建一个sqlContext(spark1.x) sparkSession(spark2.x)</code></pre><ol start="3"><li><pre><code>创建sparkcontext对象</code></pre><ol start="4"><li><pre><code>通过包装类的形式创建sqlContext</code></pre><ol start="5"><li><pre><code>使用sc对象读取外部文件</code></pre><ol start="6"><li><pre><code>将文件转换为一个rdd</code></pre><ol start="7"><li><pre><code>将rdd.toDF转换为一个dataFrame(必须引入import sqlSc.implicit._)</code></pre><ol start="8"><li><pre><code>需要将df转换为一个表 sqlsc.registTempTable()</code></pre><ol start="9"><li><pre><code>sqlSc.sql()转换为一个新的dataFrame</code></pre><ol start="10"><li><pre><code>printScheme打印表的信息</code></pre><ol start="11"><li><pre><code>show方法是一个action 打印表中的数据</code></pre></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol><h3 id="字段添加别名的两种方式"><a href="#字段添加别名的两种方式" class="headerlink" title="字段添加别名的两种方式"></a>字段添加别名的两种方式</h3><p>1、直接在rdd.toDF()方法中添加</p><pre><code class="scala">val frame: DataFrame = rdd1.toDF(&quot;id&quot;, &quot;name&quot;, &quot;age&quot;)</code></pre><p>2、使用样例类，字段名称就是别名</p><pre><code class="scala">case class Student(val id: Int, val name: String, val age: Int)new Student(strs(0).toInt, strs(1), strs(2).toInt)</code></pre><h3 id="DSL风格操作方式"><a href="#DSL风格操作方式" class="headerlink" title="DSL风格操作方式"></a>DSL风格操作方式</h3><p>DSL：<strong>领域特定语言</strong>，以极其高效的方式描述特定领域的对象、规则和运行方式的语言。</p><p>需要有特定的解释器与其配合。</p><p>高效简洁的领域语言，与通用语言相比能极大降级理解和使用难度，同时极大提高开发效率的语言。</p><p>能够描述特定领域的世界观和方法论的语言。</p><p><strong>DSL 通过在表达能力上做的妥协换取在某一领域内的高效</strong>。</p><h4 id="DSL操作方式案例"><a href="#DSL操作方式案例" class="headerlink" title="DSL操作方式案例"></a>DSL操作方式案例</h4><pre><code class="scala">//DSL方式val df = sqlSc.createDataFrame(rdd,types)    import sqlSc.implicits._df.select(&quot;id&quot;,&quot;name&quot;,&quot;age&quot;).orderBy($&quot;age&quot; desc).show()//传统方式//    df.registerTempTable(&quot;stu&quot;)//    //show默认打印数据集中的20条数据//    sqlSc.sql(&quot;select id,name,age from stu where id&gt;1 order by age desc&quot;).show()</code></pre><h4 id="DSL方式加别名"><a href="#DSL方式加别名" class="headerlink" title="DSL方式加别名"></a>DSL方式加别名</h4><p>&#x2F;&#x2F;order by方法的时候如果只是字段可以使用字符串，如果是<strong>列的对象信息使用$进行分装</strong>  desc asc都是列对象中的方法<br>&#x2F;&#x2F;<strong>聚合函数加上别名 agg函数</strong>，专门调用聚合函数并且给聚合函数加别名的</p><p><strong>聚合类算子必须加在relationalGroupedDataset,所以必须使用agg</strong></p><p>如果想要联合的<strong>使用多个聚合类的算子必须使用agg函数</strong></p><pre><code class="scala">//order by方法的时候如果只是字段可以使用字符串，如果是列的对象信息可以使用$进行分装  desc asc都是列对象中的方法//聚合函数加上别名 agg函数，专门调用聚合函数并且给聚合函数加别名的//select gender,count(*) cnt from student group by gender order by cnt desc    import org.apache.spark.sql.functions._//    df.groupBy($&quot;gender&quot; as &quot;性别&quot;).agg(count(&quot;*&quot;) as &quot;数量&quot;).show()//聚合类算子必须加在relationalGroupedDataset,所以必须使用agg    val df1:RelationalGroupedDataset = df.groupBy($&quot;gender&quot; as &quot;性别&quot;)    df1.count().show()    val df2: Long = df.count()    print(df2+&quot;8888888888888888888888888888888888888888888888888&quot;)    session.close()</code></pre><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><h3 id="什么是DataSet"><a href="#什么是DataSet" class="headerlink" title="什么是DataSet"></a>什么是DataSet</h3><p>spark1.6以后产生的一种操作sql的对象，但是存在很多和rdd相似的便捷之处，可以使用lambda表达式，可以使用转换类的算子，例如map flatMap filter等等，并且存在一个优化了的执行引擎（spark2.0的优势）</p><h3 id="DataSet和DataFrame的不同"><a href="#DataSet和DataFrame的不同" class="headerlink" title="DataSet和DataFrame的不同"></a>DataSet和DataFrame的不同</h3><p><strong>DataFrame</strong>：<strong>只能使用sql风格的查询方式</strong>，必须通过sc读取hdfs中的文件，然后将文件处理成列信息，然后才能到达dataFrame中，不能直接用dataframe直接进行数据的格式化，DataFrame没有泛型，他的底层<strong>类型是DataSet[Row]<strong>，</strong>Row不能够自己调用，只能他自己生成</strong>。Row在获取值时使用.getAs[type] (字段名称或者是索引)</p><pre><code class="scala">//创建dataSet的时候，里面能够放入的元素必须是样例类的形势，不能放入rowsession.createDataset(Row())</code></pre><p><strong>DataSet</strong> : DataSet的优点是可以使用sql风格的查询方式也可以使用DSL风格，重点是还可以向RDD调用算子一样进行计算，DataSet在创建完毕后<strong>拥有类型(类型推断)<strong>，DataSet可以放入各种数据类型，但是没办法改别名，</strong>只能通过样例类方式和toDF()方法来修改别名</strong>，如果是只是传统sql查询方面和DataFrame没有什么区别</p><h3 id="创建DataSet"><a href="#创建DataSet" class="headerlink" title="创建DataSet"></a>创建DataSet</h3><p>&#x2F;&#x2F;1、读取文件的方式</p><pre><code class="scala">//session.read...  直接返回值就是DataSet类型val df2: Dataset[Long] = session.read.textFile(&quot;access.log&quot;).map(t =&gt; &#123;      val strs: Array[String] = t.split(&quot;\\|&quot;)      ip2Long(strs(1))    &#125;)</code></pre><p>&#x2F;&#x2F;2、集合并行化</p><pre><code class="scala">val session = SparkSession.builder().appName(&quot;join&quot;).master(&quot;local[*]&quot;).getOrCreate()import session.implicits._val list1 = List((1, &quot;zhangsan&quot;, 200), (2, &quot;lisi&quot;, 300), (3, &quot;wangwu&quot;, 1000), (4, &quot;zhaosi&quot;, 250))val list2 = List((1, 30), (2, 25), (3, 30))//toDF()来修改别名，但变成DataFrame类型val ds1: DataFrame = session.createDataset(list1).toDF(&quot;id&quot;, &quot;name&quot;, &quot;salary&quot;)val ds2: DataFrame = session.createDataset(list2).toDF(&quot;userid&quot;, &quot;day&quot;)</code></pre><h2 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h2><h3 id="Join的使用方式"><a href="#Join的使用方式" class="headerlink" title="Join的使用方式"></a>Join的使用方式</h3><p>1、直接join，默认inner，全连接，产生笛卡尔积，一般不用</p><pre><code class="scala">val session = SparkSession.builder().appName(&quot;join&quot;).master(&quot;local[*]&quot;).getOrCreate()import session.implicits._val list1 = List((1, &quot;zhangsan&quot;, 200), (2, &quot;lisi&quot;, 300), (3, &quot;wangwu&quot;, 1000), (4, &quot;zhaosi&quot;, 250))val list2 = List((1, 30), (2, 25), (3, 30))val ds1: DataFrame = session.createDataset(list1)val ds2: DataFrame = session.createDataset(list2)//直接joinds1.join(ds2).show()</code></pre><p>2、关联指定字段，适用两表关联字段名称相同</p><pre><code class="scala">val ds1: Dataset[(String, String)] = session.createDataset(list1)val ds2: Dataset[(String, String)] = session.createDataset(list2)//这里关联的是各自的第一个字段(id)ds1.join(ds2,&quot;_1&quot;).show()</code></pre><p>3、两种不同名称字段指定,放入一个Seq()中，使用两表关联字段名称不一样</p><p>但是不知道为什么不能用</p><pre><code class="scala">val ds1: DataFrame = session.createDataset(list1).toDF(&quot;id&quot;, &quot;name&quot;, &quot;salary&quot;)val ds2: DataFrame = session.createDataset(list2).toDF(&quot;mid&quot;, &quot;day&quot;)//指定ds1.join(ds2,Seq(&quot;uid&quot;,&quot;mid&quot;)).show()</code></pre><p>4、可以<strong>添加连接方式</strong></p><pre><code class="scala">//可以使用的添加方式//`inner`, `cross`, `outer`, `full`,`full_outer`,`left`,`left_outer`,`right`,`right_outer`, `left_semi`, `left_anti`</code></pre><pre><code class="scala">val ds1: DataFrame = session.createDataset(list1).toDF(&quot;id&quot;, &quot;name&quot;, &quot;salary&quot;)val ds2: DataFrame = session.createDataset(list2).toDF(&quot;id&quot;, &quot;day&quot;)//左连接，显示左侧所有结果，没有匹配的为Nullds1.join(ds2,Seq(&quot;id&quot;),&quot;left_outer&quot;).show</code></pre><p>5、where 过滤  需要把列信息转换成列对象</p><pre><code class="scala">val ds1: DataFrame = session.createDataset(list1).toDF(&quot;id&quot;, &quot;name&quot;, &quot;salary&quot;)val ds2: DataFrame = session.createDataset(list2).toDF(&quot;userid&quot;, &quot;day&quot;)//下面三条语句都是把列信息转换成列对象ds1.join(ds2, $&quot;id&quot; === $&quot;userid&quot;).show()import org.apache.spark.sql.functions._//这里使用的col需要导包ds1.join(ds2, col(&quot;id&quot;) === col(&quot;userid&quot;)).show()ds1.join(ds2, ds1(&quot;id&quot;) === ds2(&quot;userid&quot;)).show()</code></pre><h3 id="Join的三种算法"><a href="#Join的三种算法" class="headerlink" title="Join的三种算法"></a>Join的三种算法</h3><p><a href="https://www.cnblogs.com/0xcafedaddy/p/7614299.html">https://www.cnblogs.com/0xcafedaddy/p/7614299.html</a></p><p>Hash   Join</p><h4 id="1、broadcast-hash-join"><a href="#1、broadcast-hash-join" class="headerlink" title="1、broadcast hash join"></a>1、broadcast hash join</h4><p><strong>适用情景</strong>：一张小表，一张大表，小表小到可以放入到内存中（广播的表大小必须小于spark.sql.autoBroadcastJoinThreshold，默认是10M）</p><p><strong>广播变量</strong>形式的join ，<strong>小表广播分发到大表所在的分区节点上</strong>，效率最高</p><p><strong>broadcast hash join可以分为两步：</strong></p><p>1、broadcast阶段：将小表广播分发到大表所在的所有主机。广播算法可以有很多，最简单的是先发给driver，driver再统一分发给所有executor；要不就是基于bittorrete的p2p思路；</p><p>2、hash join阶段：在每个executor上执行单机版hash join，小表映射，大表试探；</p><h4 id="2、shuffle-hash-join"><a href="#2、shuffle-hash-join" class="headerlink" title="2、shuffle hash join"></a>2、shuffle hash join</h4><p><strong>适用情景</strong>：一张小表，一张大表，但是小表不够小，不能放入到内存中，不适合广播分发</p><p>先shuffle流程两张表重新组织分配，hashpartition分区后，一个executor变成一个桶，executor再按照hashcode进行处理数据，每个executor处理自己的，单机形式的</p><p>shuffle hash join也可以分为两步：</p><p>1、shuffle阶段：分别将两个表按照join key进行分区，将相同join key的记录重分布到同一节点，两张表的数据会被重分布到集群中所有节点。这个过程称为shuffle</p><p>2、hash join阶段：每个分区节点上的数据单独执行单机hash join算法。</p><h4 id="3、sort-merge-join"><a href="#3、sort-merge-join" class="headerlink" title="3、sort merge join"></a>3、sort merge join</h4><p><strong>适用情景</strong>：两张大表，不能放入到内存中，不适合广播分发</p><p>1、shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式并行处理</p><p>2、sort阶段：对单个分区节点的两表数据，分别进行排序</p><p>3、merge阶段：对排好序的两张分区表数据执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同join key就merge输出，否则取更小一边</p><h4 id="查看执行计划"><a href="#查看执行计划" class="headerlink" title="查看执行计划"></a>查看执行计划</h4><p>逻辑执行计划就是按照步骤一步步的形成计划<br>逻辑执行计划转为物理执行计划，物理执行计划写死的东西，最简单的形式</p><pre><code class="scala">ds1.join(ds2, $&quot;id&quot; === $&quot;userid&quot;).explain()//== Physical Plan ==//*BroadcastHashJoin [id#7], [userid#19], Inner, BuildRight//:- LocalTableScan [id#7, name#8, salary#9]//+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))//   +- LocalTableScan [userid#19, day#20]</code></pre><h2 id="UDAF自定义聚合函数"><a href="#UDAF自定义聚合函数" class="headerlink" title="UDAF自定义聚合函数"></a>UDAF自定义聚合函数</h2><h3 id="自定义函数概念"><a href="#自定义函数概念" class="headerlink" title="自定义函数概念"></a>自定义函数概念</h3><p>三种</p><p>UDAF自定义聚合函数</p><p>UDF炸裂函数</p><p>UDTF自定义聚合函数</p><h3 id="UDAF使用解析"><a href="#UDAF使用解析" class="headerlink" title="UDAF使用解析"></a>UDAF使用解析</h3><p>继承 <strong>UserDefinedAggregateFunction</strong>{}<br>重写八个方法<br>session使用时注册udf</p><h3 id="UDAF使用案例"><a href="#UDAF使用案例" class="headerlink" title="UDAF使用案例"></a>UDAF使用案例</h3><h4 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h4><pre><code class="scala">//编写自定义UDAFclass MyUDAF extends UserDefinedAggregateFunction&#123;  //输入数据类型  override def inputSchema: StructType = StructType(    List(      StructField(&quot;id&quot;,LongType)    )  )  //缓存结果的类型  override def bufferSchema: StructType = StructType(    List(      StructField(&quot;sum&quot;,LongType),      StructField(&quot;count&quot;,IntegerType)    )  )  //最终返回值类型  override def dataType: DataType = DoubleType  //保持一致性的开关 默认给true  override def deterministic: Boolean = true  //初始化值，这个值得顺序和上面的设定的值类型保持一致  override def initialize(buffer: MutableAggregationBuffer): Unit = &#123;    buffer(0)=1L    buffer(1)=0  &#125;  //每次接受到一个数据的时候要进行的计算操作  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123;    buffer(0)=buffer.getLong(0)* input.getAs[Long](0)    buffer(1) = buffer.getInt(1)+1  &#125;  //捏合 每个分区中的结果合并在一起  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123;    buffer1(0) = buffer1.getLong(0)*buffer2.getLong(0)    buffer1(1) = buffer1.getInt(1)+buffer2.getInt(1)  &#125;  //最终返回值  override def evaluate(buffer: Row): Any = &#123;    math.pow(buffer.getLong(0),1.0/buffer.getInt(1))  &#125;&#125;</code></pre><p>使用UDAF</p><pre><code class="scala">object TestUDAF &#123;  def main(args: Array[String]): Unit = &#123;    val session = SparkSession.builder().appName(&quot;sql&quot;).master(&quot;local[*]&quot;).getOrCreate()    //    session.createDataset(List(1,2,3,4,5,6,7,8,9))    //agg    val ds: Dataset[lang.Long] = session.range(1,3)    val udf = new MyUDAF    //dsl特定领域语言    ds.agg(udf(ds(&quot;id&quot;))).show()    //    ds.registerTempTable(&quot;numbers&quot;)    //    session.udf.register(&quot;xx&quot;,new MyUDAF)    //    session.sql(&quot;select xx(id) from numbers&quot;).show()    session.close()  &#125;&#125;</code></pre><h4 id="案例2"><a href="#案例2" class="headerlink" title="案例2"></a>案例2</h4><pre><code class="scala">课堂练习id name day salaryDay1 zhangsan 2 2002 zhangsan 5 3003 lisi 4 5004 wangwu 3 10005 lisi 3 12006 wangwu 4 12007 wangwu 5 1500//数据求出每个人的平均每天工资</code></pre><pre><code class="scala">class salayUDAF extends UserDefinedAggregateFunction &#123;  //输入的值  override def inputSchema: StructType = StructType &#123;    List(      StructField(&quot;daycount&quot;, IntegerType),      StructField(&quot;salarySum&quot;, IntegerType)    )  &#125;  //缓存区的内容  override def bufferSchema: StructType = StructType &#123;    List(      StructField(&quot;daycount&quot;, IntegerType),      StructField(&quot;salarySum&quot;, IntegerType)    )  &#125;  //最终返回类型  override def dataType: DataType = DoubleType  //一致性  override def deterministic: Boolean = true  //初始值  override def initialize(buffer: MutableAggregationBuffer): Unit = &#123;    buffer(0) = 0    buffer(1) = 0  &#125;  //单个分区合并  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123;    buffer(0) = buffer.getInt(0) + input.getInt(0)    buffer(1) = buffer.getInt(1) + input.getInt(0) * input.getInt(1)  &#125;  //大合并  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123;    buffer1(0) = buffer1.getInt(0) + buffer2.getInt(0)    buffer1(1) = buffer2.getInt(1) + buffer2.getInt(1)  &#125;  //最终返回值  override def evaluate(buffer: Row): Any = &#123;    buffer.getInt(1) * 1.0 / buffer.getInt(0)  &#125;&#125;</code></pre><pre><code class="scala">object salayUDAF &#123;  def main(args: Array[String]): Unit = &#123;    val session: SparkSession = SparkSession.builder().master(&quot;local[*]&quot;).appName(&quot;salay&quot;).getOrCreate()    val list: List[(Int, String, Int, Int)] = List(      &quot;1 zhangsan 2 200&quot;,      &quot;2 zhangsan 5 300&quot;,      &quot;3 lisi 4 500&quot;,      &quot;4 wangwu 3 1000&quot;,      &quot;5 lisi 3 1200&quot;,      &quot;6 wangwu 4 1200&quot;,      &quot;7 wangwu 5 1500&quot;    ).map(t=&gt;&#123;      val strs: Array[String] = t.split(&quot; &quot;)      (strs(0).toInt,strs(1),strs(2).toInt,strs(3).toInt)    &#125;)    import session.implicits._    val df = session.createDataset(list).toDF(&quot;id&quot;,&quot;name&quot;,&quot;day&quot;,&quot;daysalary&quot;)    //传统sql方式//    df.registerTempTable(&quot;salary_table&quot;)//    session.udf.register(&quot;avg_salary&quot;,new salayUDAF)//    session.sql(&quot;select name,avg_salary(day,daysalary) avg_salary from salary_table group by name&quot;).show()    //dsl 特定领域语言    val salaryUDAFFuncation = new salayUDAF    //聚合函数使用列对象    df.groupBy(&quot;name&quot;).agg(salaryUDAFFuncation($&quot;day&quot;,$&quot;daysalary&quot;) as &quot;avg_salary&quot;).show()    session.close()  &#125;&#125;</code></pre><h3 id="数据倾斜解决办法"><a href="#数据倾斜解决办法" class="headerlink" title="数据倾斜解决办法"></a>数据倾斜解决办法</h3><p>再Hive中分桶时，根据hashcode进行分桶，如果key的值大量重复，则会造成数据倾斜，一个线程处理过大的任务，可能会让系统宕机</p><p>UDAF解决问题</p><p>在大量重复数据中，加入<strong>随机前缀</strong>把数据打散，多个线程计算这个数据，减少压力</p><pre><code class="scala">import org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;import scala.util.Random//解决数据倾斜,随即前缀object RandomKey &#123;  def main(args: Array[String]): Unit = &#123;    val session = SparkSession.builder().master(&quot;local[3]&quot;).appName(&quot;randomkey&quot;).getOrCreate()    import session.implicits._    //匿名udaf！！！    //加上随机的前缀    session.udf.register(&quot;randomkey&quot;, (t: String, number: Int) =&gt; &#123;      val random = new Random()      val index = random.nextInt(number)      index + &quot;_&quot; + t    &#125;)    val df: DataFrame = session.read.textFile(&quot;stu.txt&quot;).map(t =&gt; &#123;      val strs: Array[String] = t.split(&quot; &quot;)      (strs(0), strs(1), strs(2).toInt, strs(3))    &#125;).toDF(&quot;id&quot;, &quot;name&quot;, &quot;salary&quot;, &quot;subject&quot;)        df.registerTempTable(&quot;stu    //加入1-3的前缀    val df1: DataFrame = session.sql(&quot;select randomkey(subject,3) subject,salary from stu where subject=&#39;wx&#39;&quot; +      &quot; union &quot; +      &quot;select subject,salary from stu where subject != &#39;wx&#39;&quot;)        df1.registerTempTable(&quot;stu1&quot;)    val df2 = session.sql(&quot;select subject,count(*) cnt,sum(salary) sal from stu1 group by subject&quot;)    //匿名udaf！！！    //把前缀去掉    session.udf.register(&quot;cntkey&quot;,(t:String)=&gt;&#123;      t.substring(t.indexOf(&quot;_&quot;) + 1)    &#125;)    df2.registerTempTable(&quot;stu2&quot;)    session.sql(&quot;select cntkey(subject),sum(cnt),sum(sal) subject from stu2 group by cntkey(subject)&quot;).show()    session.close()  &#125;&#125;</code></pre><h1 id="Spark2-0和Spark1-0的区别"><a href="#Spark2-0和Spark1-0的区别" class="headerlink" title="Spark2.0和Spark1.0的区别"></a>Spark2.0和Spark1.0的区别</h1><p>更简单 支持标准sql和简化的API<br>更快spark作为一个编译器<br>更智能 Dataset结构化数据流</p><p>2.0的dataset有了查询优化，排序和shuffle阶段不需要反序列化，Dataset比较是不要要进行反序列化的，在shuffle合并比较时，直接比较序列化完毕的数组就行，比上四位byte</p><h3 id="物理执行计划"><a href="#物理执行计划" class="headerlink" title="物理执行计划"></a>物理执行计划</h3><p>物理执行计划<br>先扫描表，在进行filter，在join<br>先过滤，在进行join。</p><p>对过程进行了简化，</p><h3 id="逻辑执行计划"><a href="#逻辑执行计划" class="headerlink" title="逻辑执行计划"></a>逻辑执行计划</h3><p>适配所有场景<br>先扫描两个表，然后join产生笛卡尔积，再进行filter<br>spark2.0 产物<br>还可以通过更加智能的数据源来进行操作<br>从哪里拉取数据，在那里进行过滤</p><h3 id="SparkSQL-on-Hive"><a href="#SparkSQL-on-Hive" class="headerlink" title="SparkSQL(on Hive)"></a>SparkSQL(on Hive)</h3><p>它是将Spark SQL转换成RDD，然后提交到集群中去运行，执行效率非常快！</p><p>spark完美接入hive，spark可以直接使用hive的元数据</p><p>spark-sql  会启动一个数据仓库，跟hive没有关系，机制是模仿hive的  hdfs + mysql元数据存储（hhive-site.xml）+spark处理</p><h4 id="Spark-on-Hive安装"><a href="#Spark-on-Hive安装" class="headerlink" title="Spark on Hive安装"></a>Spark on Hive安装</h4><p>1、安装Hive和mysql和Spark(看spark1的笔记)</p><p><a href="https://blog.csdn.net/zhwyj1019/article/details/80274269">https://blog.csdn.net/zhwyj1019/article/details/80274269</a></p><p>2、在$SPARK_HOME的conf目录下新建hive-site.xml<strong>(三台都改)</strong></p><pre><code class="xml">&lt;configuration&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;    &lt;!--jdbc连接mysql，连结地址（mysql安装的地址），元数据放置的的数据库（在mysql中必须创建数据库）和数据库字符编码 --&gt;&lt;value&gt;jdbc:mysql://192.168.247.22:3306/hive?characterEncoding=UTF-8&lt;/value&gt;&lt;/property&gt;&lt;property&gt;     &lt;!--mysql驱动程序--&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;!--账号--&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt;     &lt;!--密码--&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;&lt;value&gt;123456&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre><p>3、在jars目录下放一个<strong>MySQL的包</strong></p><p>启动sparksql需要在spark的jars保重放入一个mysql-connector-java-5.1.46的jar包</p><p>4、启动hdfs、mysql和spark</p><pre><code class="scala">//需要启动spark和hdfsstart-dfs.shstart-master.shstart-slaves.sh</code></pre><p>5、启动spark-sql</p><p>启动后用的hive的元数据库，使用的hive的表，spark代替了mr来进行计算</p><pre><code class="scala">spark-sql --master spark://Linux02:7077</code></pre><h4 id="手动修改保存位置"><a href="#手动修改保存位置" class="headerlink" title="手动修改保存位置"></a>手动修改保存位置</h4><p>在刚启动的时候，数据库的文件保存文职可能是本地，而不是hdfs中，可以手动修改他的位置，连接数据库后修改DBS表，是保存的文件到hdfs中</p><p><img src="/cdh/spark/sparksql/image-20200408194923029.png" alt="image-20200408194923029"></p><h3 id="SparkSql面试题"><a href="#SparkSql面试题" class="headerlink" title="SparkSql面试题"></a>SparkSql面试题</h3><h4 id="SparkSql优化"><a href="#SparkSql优化" class="headerlink" title="SparkSql优化"></a>SparkSql优化</h4><p>SparkSql的并行度是一个调优点。默认的并行度是200，可以根据实际情况进行设置</p><p>有两种方式进行修改：</p><ol><li>代码设置</li></ol><pre><code class="scala">val spark = SparkSession.builder()        .config(&quot;spark.sql.shuffle.partitions&quot;,100)//设置并行度100        .getOrCreate()</code></pre><ol start="2"><li>提交任务修改提供参数</li></ol>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习笔记</title>
      <link href="/cdh/spark/spark/"/>
      <url>/cdh/spark/spark/</url>
      
        <content type="html"><![CDATA[<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="Spark是什么？"><a href="#Spark是什么？" class="headerlink" title="Spark是什么？"></a>Spark是什么？</h3><p>Apache Spark 是专为<strong>大规模数据处理而设计的快速通用的计算引擎</strong>，Spark是用Scala编写的</p><h3 id="Spark和mr的区别？"><a href="#Spark和mr的区别？" class="headerlink" title="Spark和mr的区别？"></a>Spark和mr的区别？</h3><p>1、mr读取数据应该是多次,如果运算比较复杂应该是多个mr联合在一起使用的，处理的结果会先落地到磁盘上</p><p>2、mr中的算子太少map reduce spark:list–&gt;map –&gt;flatMap-&gt;filter–&gt;grroup…</p><p>3、mr没有容错。spark存在很优秀的容错概念</p><p>4、spark通用性比较强大 java scala python R</p><h3 id="Spark有几种部署方式"><a href="#Spark有几种部署方式" class="headerlink" title="Spark有几种部署方式?"></a>Spark有几种部署方式?</h3><h4 id="1、Local-模式"><a href="#1、Local-模式" class="headerlink" title="1、Local 模式"></a>1、Local 模式</h4><p>该模式被称为Local[N]单机模式，是用单机的多个线程来模拟Spark分布式计算，通常用来验证开发出来的应用程序逻辑上有没有问题。需要hadoop集群，本地还分为local单线程和local-cluster多线程</p><p><strong>指令：</strong>local单机模式：常用于本地开发测试，</p><pre><code>spark-shell --master local spark-shell --master local[4]//代表4个核数来并发执行程序</code></pre><h4 id="2、Standalone集群模式"><a href="#2、Standalone集群模式" class="headerlink" title="2、Standalone集群模式"></a>2、Standalone集群模式</h4><p>Standalone模式和单机运行的模式不同，这里必须在执行应用程序前，先<strong>启动Spark的Master和Worker</strong>守护进程。不过也能看出Master是有单点故障的；Spark支持ZooKeeper来实现 HA。启动Spark集群，<strong>不用启动Hadoop服务</strong>，除非你用到了HDFS的内容。使用独立的Spark集群模式提交任务</p><p>可以使用Spark的<strong>8080</strong> web ui来<strong>观察资源和应用程序的执行情况</strong>了。</p><h6 id="集群的结构"><a href="#集群的结构" class="headerlink" title="集群的结构"></a>集群的结构</h6><p>spark应用程序有一个<strong>Driver</strong>驱动，<strong>Driver可以运行在Client上也可以运行在master上</strong>。如果你使用spark-shell去提交job的话它会是运行在master上的，如果你使用spark-submit或者IDEA开发工具方式运行，那么它是运行在Client上的。这样我们知道了，<strong>Client的主体作用就是运行Driver</strong>。而<strong>master除了资源调度的作用还可以运行Driver</strong>。</p><p><strong>standalone是一个主从模式，master节点负责资源管理，worker节点负责任务的执行。</strong></p><p><strong>指令：</strong></p><pre><code>spark-shell --master spark://Linux02:7077spark-shell --master spark://Linux02:7077 --deploy-mode client//还可以配置属性</code></pre><h6 id="产生的进程"><a href="#产生的进程" class="headerlink" title="产生的进程"></a>产生的进程</h6><p>1、<strong>Master</strong>进程做为cluster manager，用来对应用程序申请的资源进行管理</p><p>2、worker </p><p>3、<strong>SparkSubmit</strong> 做为Client端和运行driver程序，提交任务的客户端</p><p>4、<strong>CoarseGrainedExecutorBackend</strong> 就是executor，用来并发执行应用程序</p><h4 id="3、Yarn"><a href="#3、Yarn" class="headerlink" title="3、Yarn"></a>3、Yarn</h4><p><a href="https://mp.weixin.qq.com/s/IFksSe-VlegzzblE9WSByg">https://mp.weixin.qq.com/s/IFksSe-VlegzzblE9WSByg</a></p><p><strong>启动spark集群和Hadoop集群</strong></p><p>限于YARN自身的发展，目前仅支持粗粒度模式（Coarse-grained Mode）。这是由于YARN上的Container资源是不可以动态伸缩的，一旦Container启动之后，可使用的资源不能再发生变化，不过这个已经在YARN计划中了。</p><p><strong>yarn 负责资源管理</strong>，<strong>Spark 负责任务调度和计算</strong> </p><h4 id="Spark-on-Yarn的执行过程"><a href="#Spark-on-Yarn的执行过程" class="headerlink" title="Spark on Yarn的执行过程"></a>Spark on Yarn的执行过程</h4><p><img src="/cdh/spark/spark/image-20200904140401244.png" alt="image-20200904140401244"></p><ol><li>client向ResouceManager申请资源，返回一个applicationID</li><li>client上传spark jars下面的jar包，自己写的jar和配置</li><li>ResourceManager随机找一个资源充足的NodeManager</li><li>然后通过RPC让NodeManager从HDFS上下载jar包和配置，启动ApplicationMaster</li><li>ApplicationMaster向ResourceManager申请资源</li><li>ResourceManager中的ResourceScheduler找到符合条件的NodeManager，将NodeManager的信息返回给ApplicationMaster</li><li>ApplicationMaster和返回的NodeManager进行通信</li><li>NodeManager从HDFS下载依赖</li><li>NodeManager启动Executor</li><li>Executor启动之后反向向ApplicationMaster（Driver）注册</li></ol><h5 id="spark-on-yarn-的支持两种模式："><a href="#spark-on-yarn-的支持两种模式：" class="headerlink" title="spark on yarn 的支持两种模式："></a><strong>spark on yarn 的支持两种模式：</strong></h5><p>(1) <strong>yarn-cluster</strong>：适用于生产环境；</p><p>(2) <strong>yarn-client</strong>：适用于交互、调试，希望立即看到app的输出</p><p><strong>yarn-cluster和yarn-client的区别</strong>在于yarn appMaster，每个yarn app实例有一个appMaster进程，是为app启动的第一个container；负责从ResourceManager请求资源，获取到资源后，告诉NodeManager为其启动container。yarn-cluster和yarn-client模式内部实现还是有很大的区别。如果你需要用于生产环境，那么请选择yarn-cluster；而如果你仅仅是Debug程序，可以选择yarn-client。</p><h4 id="4、mesos"><a href="#4、mesos" class="headerlink" title="4、mesos"></a>4、mesos</h4><p>一个强大的分布式资源管理框架，它允许多种不同的框架部署在其上</p><h3 id="Spark的standalone集群的安装"><a href="#Spark的standalone集群的安装" class="headerlink" title="Spark的standalone集群的安装"></a>Spark的standalone集群的安装</h3><p>spark-2.2.0-bin-hadoop2.7获取jar包</p><p>1、解压完毕后，修改&#x2F;etc&#x2F;profile文件</p><pre><code>export JAVA_HOME=/root/Downloads/jdk1.8.0_161export PATH=$PATH:$JAVA_HOME/binexport HADOOP_HOME=/root/Downloads/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport ZOOKEEPER_HOME=/root/Downloads/zookeeper-3.4.5export PATH=$PATH:$ZOOKEEPER_HOME/binexport HIVE_HOME=/root/Downloads/apache-hive-2.1.1-binexport PATH=$PATH:$HIVE_HOME/binexport FLUME_HOME=/root/Downloads/apache-flume-1.6.0-binexport PATH=$PATH:$FLUME_HOME/bin export HBASE_HOME=/root/Downloads/hbase-1.2.6export PATH=$PATH:$HBASE_HOME/bin export SQOOP_HOME=/root/Downloads/sqoop-1.4.6.bin__hadoop-2.0.4-alphaexport PATH=$PATH:$SQOOP_HOME/binexport SCALA_HOME=/install/scala/scala-2.11.8export PATH=$PATH:$SCALA_HOME/binexport SPARK_HOME=/install/spark-2.2.0-bin-hadoop2.7export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin</code></pre><p>source profile  刷新配置文件</p><p>2、修改conf&#x2F;spark-env.sh</p><pre><code>#主节点位置SPARK_MASTER_HOST=Linux02#端口，不配置默认7077SPARK_MASTER_PORT=7077#jdk配置JAVA_HOME=/root/Downloads/jdk1.8.0_161</code></pre><p>3、修改conf&#x2F;slaves</p><p>cp slaves.template slaves </p><pre><code>Linux01Linux02Linux03</code></pre><p>4、发送到其他节点上</p><pre><code>scp -r spark-2.2.0-bin-hadoop2.7 root@Linux02:/install/</code></pre><p>5、启动Spark</p><pre><code>start-master.sh  //在主节点机器上启动masterstart-slaves.sh//启动其他机器的worker</code></pre><h4 id="standalone集群搭建HA"><a href="#standalone集群搭建HA" class="headerlink" title="standalone集群搭建HA"></a>standalone集群搭建HA</h4><p>standalone模式中存在两个角色 master worker</p><p>zookeeper 进行masters节点的监控，并且切换主从</p><p>1、修改配置文件spark-env.sh  三台都改</p><pre><code class="scala">#SPARK_MASTER_HOST=univers02#YARN_CONF_DIR=/home/ghostwowo/Downloads/hadoop-2.6.4/etc/hadoopexport SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=Linux01:2181,Linux02:2181,Linux03:2181-Dspark.deploy.zookeeper.dir=/spark&quot;SPARK_MASTER_PORT=7077JAVA_HOME=/home/ghostwowo/Downloads/jdk1.8.0_73</code></pre><p>2、启动zookeeper，三台启动</p><pre><code>ZKServer.sh start </code></pre><p>3、自动master和worker</p><pre><code>start-master.shstart-slaves.sh</code></pre><p>4、在另一台机器上再启动一个master</p><pre><code>start-master.sh</code></pre><h3 id="Spark集群Yarn模式"><a href="#Spark集群Yarn模式" class="headerlink" title="Spark集群Yarn模式"></a>Spark集群Yarn模式</h3><p>1、修改hadoop下面的<strong>yarn-site.xml</strong></p><p>提交任务给yarn的时候，需要spark配置文件中指定yarn的配置地址，才能找到yarn的提交端口和地址</p><p>yarn会将磁盘中的一块区域当作内存使用，避免产生内存溢出</p><pre><code class="xml">&lt;property&gt;&lt;name&gt;yarn.nodemanager.pmen-check-enabled&lt;/name&gt;&lt;value&gt;false&lt;value&gt;&lt;/property&gt;&lt;!--关闭资源检测--&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.vmen-check-enabled&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;vim capacity-scheduler.xml  &lt;property&gt;    &lt;name&gt;yarn.scheduler.capacity.resource-calculator&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.yarn.util.resource.DominantResourceCalculator&lt;/value&gt;    &lt;description&gt;      The ResourceCalculator implementation to be used to compare      Resources in the scheduler.      The default i.e. DefaultResourceCalculator only uses Memory while      DominantResourceCalculator uses dominant-resource to compare      multi-dimensional resources such as Memory, CPU etc.    &lt;/description&gt;  &lt;/property&gt;</code></pre><h4 id="yarn集群两种部署模式"><a href="#yarn集群两种部署模式" class="headerlink" title="yarn集群两种部署模式"></a>yarn集群两种部署模式</h4><p>client&#x2F;cluster<br>client任务退出后，application断开，任务就死了<br>cluster的driver在container中，在终止任务后，任务继续在集群中运行，application没有断开</p><p>1、–deploy-mode部署模式，提交任务到集群中的时候，client模式下我们能够一直存在客户端，进行一直监听任务，driver在client端，可以以交互式的形式一直对任务进行管理</p><p>client模式中  appmaster默认占用的是512M</p><p>client模式下，driver在客户端，并且没有使用集群中的核数</p><pre><code class="scala">spark-submit --master yarn --deploy-mode client --class org.apache.spark.examples.SparkPi spark-examples_2.11-2.2.0.jar  10</code></pre><p>2、–deploy-mode cluster :集群模式，driver在集群中，driver和applicationMaster,driver不在客户端，提交完毕的任务全部托管到集群</p><p>cluster模式，driver端在集群中，第1个containe是给application使用的，结果就在第一个container中</p><pre><code class="scala">spark-submit --master yarn --deploy-mode cluster --class org.apache.spark.examples.SparkPi spark-examples_2.11-2.2.0.jar  10//下面的是错误的//spark-shell跟driver进行交互，client模式，不能是clusterspark-shell --master yarn --deploy-mode cluster --class org.apache.spark.examples.SparkPi spark-examples_2.11-2.2.0.jar  10</code></pre><h4 id="hdfs和yarn的关系"><a href="#hdfs和yarn的关系" class="headerlink" title="hdfs和yarn的关系"></a>hdfs和yarn的关系</h4><p>yarn运行的时候需要在hdfs中创建一个临时空间，工作空间需要放入的数据是任务的代码包，描述文件，所以需要启动hdfs</p><h4 id="yarn模式进程"><a href="#yarn模式进程" class="headerlink" title="yarn模式进程"></a>yarn模式进程</h4><p>CoarseGrainedExecutorBackend     executor进程<br>SparkSubmit提交任务的<br>ExecutorLanuncher（applicationMaster）Yarn client 模式中独有，appmaster是cluster模式中的名称</p><h4 id="集群资源分配情况"><a href="#集群资源分配情况" class="headerlink" title="集群资源分配情况"></a>集群资源分配情况</h4><p><strong>cluster模式下的资源</strong></p><p>在第一台机器上启动第一个container就是appMaster 1cores 2G</p><p><img src="/cdh/spark/spark/image-20200401134900711.png" alt="image-20200401134900711"></p><p>其实executor是默认启动两个，executor默认使用1core 2G</p><p>因为<strong>在yarn中的分配资源最小单位1G</strong></p><p>在默认配置文件中的资源是1G+1cores,但是资源的使用情况是2G+1core</p><p><img src="/cdh/spark/spark/image-20200401135109494.png" alt="image-20200401135109494"></p><p>默认会增加driver中的一个缓冲区内容，防止oom错误，增加384M</p><p>1024+384 &#x3D; 1408M</p><p><strong>client模式下的资源</strong></p><p>appMaster 1G+1cores</p><p>每个executor启动的时候占用2G+1cores</p><p><img src="/cdh/spark/spark/image-20200401135338355.png" alt="image-20200401135338355"></p><p>在client模式中am占用的资源是512M+384&#x3D;896M –&gt;1G</p><h4 id="yarn运行流程"><a href="#yarn运行流程" class="headerlink" title="yarn运行流程"></a>yarn运行流程</h4><p>1、client提交一个任务给RM，RM把任务加入到队列queue(FIFO)，并返回一个APPId和一个hdfs路径（临时目录）。</p><p>2、client根据hdfs路径，把jar包配置文件、处理的文件描述信息发送到hdfs中（临时文件）。默认备份3个</p><p>3、rm节点开始启动第一个container，运行appMaster，将appMaster启动</p><p>4、appMaster反向向RM注册，</p><p>5、appMaster向RM申请资源</p><p>6、RM向appMaster返回一个List形式的container，这些container只是标识，不存在</p><p>7、appMaster拿到集合后，让NM启动container</p><p>8、driver开始解析任务<br>9、container中启动executor<br>10、executor方向和driver进行注册<br>………</p><h3 id="spark集群中的任务资源分配"><a href="#spark集群中的任务资源分配" class="headerlink" title="spark集群中的任务资源分配"></a>spark集群中的任务资源分配</h3><p>client通过submit或shell向executor发送任务，worker默认占有所有的资源(cores,)，内存默认当前大小减1G，如果只有一个G，则就使用一个G</p><p><strong>worker</strong>:集群启动的时候每个worker使用的默认一个8 cores  和 1G内存</p><p><strong>executor</strong>:默认占用1G，worker中就是1G，executor默认占用worker所有核数和内存</p><h3 id="Spark两种提交方式"><a href="#Spark两种提交方式" class="headerlink" title="Spark两种提交方式"></a>Spark两种提交方式</h3><h4 id="1、spark-submit提交"><a href="#1、spark-submit提交" class="headerlink" title="1、spark-submit提交"></a>1、spark-submit提交</h4><p>spark-submit模式，提交jar包任务运行，运行完毕直接停止，短应用</p><p><strong>指令：</strong></p><pre><code>spark-submit --master spark://Linux02:7077 --class com.star.spark.WordCount spark1902-1.0-SNAPSHOT.jar hdfs://Linux01:9000/aa.txt hdfs://Linux01:9000/wcres01//spark-submit --master spark://Linux02:7077 --class  运行类的路径 jar包名  输入路径  输出路径</code></pre><h5 id="常用的spark-submit参数"><a href="#常用的spark-submit参数" class="headerlink" title="常用的spark-submit参数"></a>常用的spark-submit参数</h5><p>–master：提交的集群主机</p><p>–class:运行jar包中哪个类</p><p>–name：任务在集群中的名称(单机必须指定)</p><p>–jars：一个任务中需要其他的jar，参数方式指定</p><p>–deploy-mode：部署模式(yarn模式使用)</p><p>–executor-memory 每个executor默认使用内存的大小</p><p>–executor-cores:每个executor默认使用的cores</p><p>–total-executor-cores:允许所有的executor公用的核数</p><pre><code>spark-submit --class org.apache.spark.examples.SparkP1 --executor-memory 512M --executor-cores 4 /spark-2.2.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.2.0.jar 10//这里不用指定total,这个任务和上面任务共存，使用剩余核数</code></pre><h4 id="2、spark-shell提交"><a href="#2、spark-shell提交" class="headerlink" title="2、spark-shell提交"></a>2、spark-shell提交</h4><p><strong>spark-shell交互式命令行</strong>，<strong>长应用</strong>，当时提交任务的时候<strong>底层也是用的spark-submit</strong></p><pre><code>//指定executor使用核数，需要指定total核数，否则自动用全部核数创建executorspark-shell --master spark://Linux02:7077 --executor-memory 512M --executor-cores 4 --total-executor-cores 12</code></pre><h4 id="spark-shell和spark-submit区别"><a href="#spark-shell和spark-submit区别" class="headerlink" title="spark-shell和spark-submit区别"></a>spark-shell和spark-submit区别</h4><p>①客户端的SparkSubmit进程会在应用程序提交给集群之后就退出</p><p>②Master会在集群中选择一个Worker进程生成一个子进程DriverWrapper来启动driver程序</p><p>③而该DriverWrapper 进程会占用Worker进程的一个core，所以同样的资源下配置下，会比第3种运行模式，少用1个core来参与计算</p><p>④应用程序的结果，会在执行driver程序的节点的stdout中输出，而不是打印在屏幕上</p><h3 id="Spark组件"><a href="#Spark组件" class="headerlink" title="Spark组件"></a>Spark组件</h3><p>client 提交任务</p><p>driverdriver进程运行在client端，对应用进行管理监控。Master节点指定某个Worker节点启动Driver进程，负责监控整个应用的执行。</p><p><strong>master+worker负责任务的资源调用的</strong></p><p>master控制整个集群，监控worker。在YARN模式中为资源管理器</p><p>worker从节点，负责控制计算节点，启动Executor或者Driver。</p><p>executor一个应用程序拥有一系列进程，叫做executors，他们在集群上运行，即使没有job在运行，这些executor仍然被这个应用程序占有。这种方式让数据存储在内存中，以支持快速访问，同时让task快速启动成为现实。</p><p><strong>driver+executors  负责任务执行，（一次性的，和每个app相关</strong>）<br>app：spark–submit spark-shel提交的任务就是一个ap</p><p>DAG图在执行前生成执行规划图，优化的作用</p><h3 id="Spark的特点"><a href="#Spark的特点" class="headerlink" title="Spark的特点"></a>Spark的特点</h3><p>1、<strong>速度快</strong></p><p>Spark 使用DAG 调度器、查询优化器和物理执行引擎，能够在批处理和流数据获得很高的性能</p><p>2、<strong>使用简单</strong></p><p>Spark的易用性主要体现在两个方面。一方面，我们可以用较多的编程语言来写我们的应用程序，比如说Java,Scala,Python,R 和 SQL;另一方面，Spark 为我们提供了超过80个高阶操作，这使得我们十分容易地创建并行应用，除此之外，我们也可以使用Scala,Python,R和SQL shells,以实现对Spark的交互。</p><p>3、 <strong>通用性强</strong></p><p>以Spark为基础建立起来的模块(库)有**Spark SQL,Spark Streaming,MLlib(machine learning)和GraphX(graph)**。我们可以很容易地在同一个应用中将这些库结合起来使用，以满足我们的实际需求。</p><p>4、<strong>到处运行</strong></p><p>Spark应用程度可以运行十分多的框架之上。它可以运行在Hadoop,Mesos,Kubernetes,standalone,或者云服务器上。它有多种多种访问源数据的方式。可以用standalone cluster模式来运行Spark应用程序，并且其应用程序跑在Hadoop,EC2,YARN,Mesos,或者Kubernates。对于访问的数据源，我们可以通过使用Spark访问HDFS,Alluxio,Apache Cassandra,HBase,Hive等多种数据源。</p><h3 id="Spark端口号"><a href="#Spark端口号" class="headerlink" title="Spark端口号"></a>Spark端口号</h3><p>8080tomcat端口，用于网页监控，底层tcp协议</p><p>8081worker的端口号tcp协议的链接</p><p>7077master主节点默认通信端口号（管理心跳等，心跳传输，命令接受）</p><p>6066Spark外部服务端口</p><p>8081可以监控workers的信息</p><p>4040监控任务执行情况（主节点上使用）</p><h3 id="Spark中的shuffle流程"><a href="#Spark中的shuffle流程" class="headerlink" title="Spark中的shuffle流程"></a>Spark中的shuffle流程</h3><p>将数据放入到上一个rdd分区所对应的那个机器上，进行本地化，第二个rdd对应的task任务进行拉取数据</p><h3 id="Spark的数据本地化策略"><a href="#Spark的数据本地化策略" class="headerlink" title="Spark的数据本地化策略"></a>Spark的数据本地化策略</h3><p><a href="https://www.cnblogs.com/jxhd1/p/6702224.html?utm_source=itdadao&amp;utm_medium=referral">https://www.cnblogs.com/jxhd1/p/6702224.html?utm_source=itdadao&amp;utm_medium=referral</a></p><p>1.Spark数据的本地化：移动计算，而不是移动数据</p><p>2.Spark中的数据本地化级别： </p><pre><code class="scala">//五个级别PROCESS_LOCALNODE_LOCALNO_PREFRACK_LOCALANY</code></pre><p><strong>1、PROCESS_LOCAL</strong>  进程本地化：task要计算的数据在同一个Executor中</p><p><strong>2、NODE_LOCAL</strong>   节点本地化：速度比 PROCESS_LOCAL 稍慢，<strong>因为数据需要在不同进程之间传递或从文件中读取</strong></p><p>情况一：task要计算的数据是在同一个Worker的不同Executor进程中</p><p> 情况二：task要计算的数据是在同一个Worker的磁盘上，或在 HDFS 上，恰好有 block 在同一个节点上。</p><p><img src="/cdh/spark/spark/1008304-20170310232213498-2042397906.jpg" alt="img"></p><p><strong>Spark计算数据来源于HDFS，那么最好的数据本地化级别就是NODE_LOCAL</strong></p><p><strong>3、NODE_PREF</strong>   没有最佳位置这一说，数据从哪里访问都一样快，不需要位置优先。比如说SparkSQL读取MySql中的数据</p><p><strong>4、RACK_LOCAL</strong> 机架本地化，数据在同一机架的不同节点上。需要通过网络传输数据及文件 IO，比 NODE_LOCAL 慢</p><p>情况一：task计算的数据在Worker2的Executor中</p><p>情况二：task计算的数据在Worker2的磁盘上</p><p><img src="/cdh/spark/spark/1008304-20170310232213967-209773010.png" alt="img"></p><p>5、<strong>ANY</strong>  跨机架，数据在非同一机架的网络上，速度最慢</p><h3 id="Spark的基本概念"><a href="#Spark的基本概念" class="headerlink" title="Spark的基本概念"></a>Spark的基本概念</h3><p><strong>application</strong>&#x3D;&#x3D;spark-submit spark-shell代码块组成的jar就是一个应用</p><p><strong>Job</strong>:遇见多少个action算子就有多少个job，包含很多的task进行并行计算。Spark 采用惰性机制，对 RDD 的创建和转换并不会立即执行，只有在遇到第一个 Action 时才会生成一个 Job，然后统一调度执行。一个 Job 包含 N 个Transformation 和 1 个 Action。</p><p><strong>stage阶段</strong>：job中存在几个阶段，是根据遇见shuffle当前任务就会切分阶段，<strong>stage&#x3D;shuffle+1</strong>。由于 Shuffle 的存在，不同的Stage 是不能并行计算的，如果是shuffle类型的，因为后面 Stage 的计算需要前面 ShuffleStage  的结果，所以shufflemapStage保存到本地</p><p>shuffleMapStage，结果数据保存到本地，供下一个rdd的shuffle拉取数据</p><p>resultStage，在其他的介质中存储或者打印</p><p><strong>RDD</strong></p><p>一个RDD就是你的数据的一个不可变的分布式元素集合，在集群中跨节点分布，可以通过若干提供了转换和处理的底层API进行并行处理。每个RDD都被分为多个分区，这些分区运行在集群不同的节点上。</p><p><strong>Task</strong>:具体执行任务。一个 Job 在每个 Stage 内都会<strong>按照Stage中的最后一个RDD 的 Partition 数量，一个分区对应一个task，创建对应数量的task</strong>。每个 Stage 内多个并发的 Task 执行逻辑完全相同，只是在不同的Partition，<strong>一个 Stage 的总 Task 的个数由 Stage 中最后的一个 RDD 的 Partition 的个数决定</strong></p><p>Spark 中有<strong>两种类型 task:</strong></p><p>ShuffleMapTask：输出是 shuffle 所需数据， stage 的划分也以此为依据， shuffle 之前的所有变换是一个 stage，shuffle 之后的操作是另一个 stage 。</p><p>ResultTask：输出是 result，比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有 shuffle，直接就输出了，那么它的 task 是 resultTask，stage 也只有一个；如果是 rdd.map(x &#x3D;&gt; (x, 1)).reduceByKey(_ + _).foreach(println)， 这个 job 因为有 reduce，所以有一个 shuffle 过程，那么 reduceByKey 之前的是一个 stage，执行 ShuffleMapTask，输出 shuffle 所需的数据，reduceByKey 到最后是一个 stage，直接就输出结果了。如果 job 中有多次 shuffle，那么每个 shuffle 之前都是一个stage。</p><p><strong>Master</strong></p><p>主节点，管理从节点，接受心跳</p><p><strong>Worker</strong></p><p>从节点，主要任务是接收master请求，启动executor，运行任务</p><p><strong>executor</strong>：本质是一个包装后的线程池，里面执行task任务，线程池只能执行多线程任务，但是task不是多线程的，task外层包装成了taskRunner，这个类是多线程的，运行它的Run方法。</p><p><strong>Partition</strong>：分区。一个 RDD 在物理上被切分为多个 Partition，即数据分区，这些 Partition 可以分布在不同的节点上。<strong>Partition 是 Spark 计算任务的基本处理单位</strong>，决定了并行计算的粒度，而 Partition 中的每一条 Record 为基本处理对象。例如对某个 RDD 进行 map 操作，在具体执行时是由多个并行的 Task 对各自分区的每一条记录进行 map 映射。</p><h4 id="Driver端的运行组件和运行流程"><a href="#Driver端的运行组件和运行流程" class="headerlink" title="Driver端的运行组件和运行流程"></a>Driver端的运行组件和运行流程</h4><p>提交任务到master时，driver通过spark-submit方法，一反射机制调用用户自己定义的main方法，driver端开始初始化组件（DAGScheduler，TaskScheduler，schedulerBackend）</p><p><strong>DAG</strong>(Directed Acyclic Graph)：有向无环图。在图论中，边没有方向的图称为无向图，如果边有方向称为有向图。在无向图的基础上，任何顶点都无法经过若干条边回到该点，则这个图就没有环路，称为有向无环图( DAG 图)。Spark 中使用 DAG 对 RDD 的关系进行建模，<strong>描述了 RDD 的依赖关系</strong>，这种关系也被称之为 lineage。</p><p>在看DAG图时：（粉色的大的代表阶段 蓝色的小的部分算子 点rdd 连线代表依赖关系）</p><p><strong>RDD object</strong>：生成DAG有向无环图</p><p>将这个DAG图交给DAGScheduler进行切分阶段</p><p><strong>DAGScheduler</strong>：根据Job构建基于Stage的DAG图（把生成的图进行切分），并提交Stage给TaskScheduler，其划分Stage的依据是RDD之间的依赖关系。（从最后一个阶段往前找，找到这个阶段的父阶段，继续判断父阶段是不是存在父阶段，如果依赖关系是宽依赖，则stage数量+1，是窄依赖，则这个stage中的rdd数量+1）最后一个rdd分区数量对应个数的task任务每个阶段都放入到taskSet中（每个taskset对应一个stage），如果组成task任务的个数不等于0，将生成的task放入到TaskSet中。在父阶段都全部查找完毕，开始提交任务给TaksScheduler</p><p><strong>Shuffle：</strong>产生宽依赖就会有一个shuffle</p><p><strong>TaskScheduler</strong>：将TaskSet提交给Worker（集群）运行，每个Executor运行什么Task就是在此处分配的，对任务的执行和监控的组件。首先初始化调度模式FIFO模式，通过定时器的形式不断检测有没有资源，如果资源足够通过schedulerBackend组件提交任务给executor执行，</p><p><strong>schedulerBackend</strong>：通信组件，负责driver和executor之间的通信，在提交任务到集群之前，先确定资源足够，将任务进行序列化，使用的是netty（类似akka）框架，每个任务发送给那个executor，发送完毕后的executor减去相应的资源</p><h4 id="Spark提交流程图"><a href="#Spark提交流程图" class="headerlink" title="Spark提交流程图"></a>Spark提交流程图</h4><p><img src="/cdh/spark/spark/20180627143538839" alt="img"></p><h4 id="Driver端工作组件"><a href="#Driver端工作组件" class="headerlink" title="Driver端工作组件"></a>Driver端工作组件</h4><p><img src="/cdh/spark/spark/307536-20190626225303718-2048524998.png" alt="img"></p><h4 id="Spark执行流程"><a href="#Spark执行流程" class="headerlink" title="Spark执行流程"></a>Spark执行流程</h4><p>1、driver进程启动，初始化后，发送请求给Master，进行注册</p><p>2、Master是一个消息队列（FIFIO），Master接收到Driver请求，把任务加入到master队列中，反馈任务已经加入到队列中并返回一个任务编号（id）</p><p>3、driver 调用DAGScheduler准备DAG图</p><pre><code class="scala">//driver组件工作流程//1、RDD Object //解析代码，生成DAG图，构建一个简单的图解//2、DAGScheduler  //根据依赖关系进行任务的划分和构建，shuffle算子划分成不同的阶段，每个阶段stage的最后一个rdd的分区数量就是本阶段中task最终的数量，将一个stage中的所有task包装成taskSet进行提交，一个taskset中有多个task，task 会被分发到指定的 Executor 去执行，在任务执行的过程中，Executor 也会不断与 Driver 进行通信，报告任务运行情况//3、taskScheduler//task调度器，接收到数据taskSet，通过DagScheduler拿到任务对象，将任务通过集群管理器的形式进行提交给master//master开始进行任务资源的划分，开始执行提交的任务，如果没有资源，进入消息队列中等待//4、executor开始执行任务，执行任务的时候，按照阶段执行，stage分为两种，shuffleMapStage resultStage//shuffleMapStage执行完毕后，结果保存到本地，//resultStage将这个任务存储到其他介质，或者打印出来（这期间driver和master工作是并行执行的，master通知worker，worker开始按照master分配的资源开始启动executor，executor启动后反向向driver注册，）</code></pre><p>3、master发送请求给worker要求启动executor，worker接收到Master请求后为任务启动executor，负载均衡的调配</p><p>4、executor启动后，向Driver反注册，注册完毕driver给executor远程协议发送任务（task）</p><p>5、executor对RDD的partition进行并行计算，按照阶段执行，形成新的RDD</p><p><img src="/cdh/spark/spark/1775767-20200320164621730-1944918839.png" alt="img"></p><p>stage分为两种，shuffleMapStage resultStage<br>shuffleMapStage执行完毕后，结果保存到本地，<br>resultStage将这个任务存储到其他介质，或者打印出来</p><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><h4 id="什么是RDD？"><a href="#什么是RDD？" class="headerlink" title="什么是RDD？"></a>什么是RDD？</h4><p><strong>RDD</strong>：Spark提供的<strong>主要抽象是一个弹性分布式数据集(RDD)<strong>，它是一个</strong>跨集群节点分区的元素集合</strong>，可以<strong>并行操作</strong>。只读分区记录的集合，Spark 对所处理数据的基本抽象。<strong>RDD 是 Spark 分发数据和计算的基础抽象类</strong>。一个 RDD 是一个<strong>不可改变的分布式集合对象</strong>，因此在使用 scala 编写时，**前面加修饰符 val **。Spark 中 的计算可以简单抽象为对 RDD 的创建、转换和返回操作结果的过程。(RDD是一个装有很多数据的集合，这个集合分布在多个节点上面，允许集群进行并行化的操作，rdd是一个主抽象（rdd其实是不存在的，里面也没有数据），只读的，不可变的，默认被分区的数据集)</p><p><strong>RDD在driver端</strong>，负责代理数据的流动节点，告诉代码逻辑处理的时候将要受到什么样的处理</p><p>RDD代表处理数据的节点，真正的数据在原来的位置</p><p>RDD里面的操作方法叫做算子，算子才是处理数据的方法</p><h4 id="创建RDD的三种方式"><a href="#创建RDD的三种方式" class="headerlink" title="创建RDD的三种方式"></a>创建RDD的三种方式</h4><h5 id="1、读取hdfs这样的分布式文件系统的文件"><a href="#1、读取hdfs这样的分布式文件系统的文件" class="headerlink" title="1、读取hdfs这样的分布式文件系统的文件"></a>1、读取hdfs这样的分布式文件系统的文件</h5><p>&#x2F;&#x2F;block块数量就是rdd分区数量，如果是文件默认是2</p><p>这个方法产生rdd的时候，它的分区数量和block的数量一样，并且分区的数量最小是2</p><pre><code class="scala">scala&gt; sc.textFile(&quot;hdfs://Linux01:9000/aa.txt&quot;)res0: org.apache.spark.rdd.RDD[String] = hdfs://Linux01:9000/aa.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:25//这个是在集群中执行的</code></pre><h5 id="2、集合并行化-在driver端生成一个本地集合，通过集合创建这个rdd"><a href="#2、集合并行化-在driver端生成一个本地集合，通过集合创建这个rdd" class="headerlink" title="2、集合并行化(在driver端生成一个本地集合，通过集合创建这个rdd)"></a>2、集合并行化(在driver端生成一个本地集合，通过集合创建这个rdd)</h5><p>如果是集合并行化的方式，那么<strong>产生的分区数量就是集群的核数</strong>，集群中的核数意味着能够并行的处理任务数量，rdd分区和core对应，才能让集群的处理能力最大化</p><p><img src="/cdh/spark/spark/image-20200325182509432.png" alt="image-20200325182509432"></p><p>makeRDD底层使用的是parallelize方法</p><pre><code class="scala">val arr = Array(1,2,3,4,5)//这个是在driver本地执行的sc.makeRDD(arr)//将集合变成RDDscala&gt; sc.parallelize(arr)res3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:27//也是将集合变成RDD</code></pre><p><strong>如果是集合并行化的方式，那么产生的分区数量就是集群的核数，集群中的核数意味着能够并行的处理任务数量，rdd分区和core对应，才能让集群的处理能力最大化</strong></p><pre><code>scala&gt; var arr = Array(1,2,3,4,5,67)arr: Array[Int] = Array(1, 2, 3, 4, 5, 67)scala&gt; sc.makeRDD(arr)res11: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at makeRDD at &lt;console&gt;:27scala&gt; res11.partitions.sizeres12: Int = 24</code></pre><h5 id="3、转换类算子生成新的RDD"><a href="#3、转换类算子生成新的RDD" class="headerlink" title="3、转换类算子生成新的RDD"></a>3、转换类算子生成新的RDD</h5><p><strong>转换类算子的形势形成的新的RDD的分区数量和原来的rdd的分区数量一致（默认）</strong></p><p>转换类算子存在两种：1、可以修改分区的 2、不可以修改分区的，只要不是刻意的去修改分区，那么分区数量就不会发生任何改变</p><pre><code class="scala">scala&gt; res0.flatMap(_.split(&quot; &quot;))res4: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at flatMap at &lt;console&gt;:27</code></pre><h4 id="RDD特点"><a href="#RDD特点" class="headerlink" title="RDD特点"></a>RDD特点</h4><p>1、容错，数据能够自动恢复</p><p>2、分布式，自动分区，每个分区被一个executor处理</p><p>3、弹性的，可大可小，容错</p><p>4、数据集，在使用的时候可以当作存储数据的集合</p><h4 id="RDD的五大特性"><a href="#RDD的五大特性" class="headerlink" title="RDD的五大特性"></a>RDD的五大特性</h4><p><img src="/cdh/spark/spark/image-20200318205222799.png" alt="image-20200318205222799"></p><ol><li>每个rdd上面都存在着一系列的分区列表</li><li>存在一个函数，这个函数用于计算每一个分片</li><li>每个rdd都和其他的rdd存在一系列的依赖关系（算子）</li><li>在rdd上会存在一个可选择的分区器，必须放在k-v这样的rdd上面</li><li>优先位置进行计算(移动计算比移动数据本身更划算，本地化策略)</li></ol><h5 id="特性1，获取分区列表"><a href="#特性1，获取分区列表" class="headerlink" title="特性1，获取分区列表"></a>特性1，获取分区列表</h5><pre><code class="scala">protected def getPartitions: Array[Partition]</code></pre><p>每个rdd上面都存在这样的一个方法，能够获取rdd上面的所有分区列表</p><p><img src="/cdh/spark/spark/image-20200318210518165.png" alt="image-20200318210518165"></p><p>对于代码来说，分区就是一个特质</p><p><img src="/cdh/spark/spark/image-20200318210606694.png" alt="image-20200318210606694"></p><p>分区的作用，每个分区以后再处理的数据每个分区对应一个线程，分区越多人物的并行化就越高，在没有运行任务的时候，首先解析代码，记录每个分区和blk的对应关系，以后这个分区中的数据将会从blk中读取数据（block在集群上备份3份）</p><p>blockRddPartition每个分区对应一个blk，计算的时候首先获取这个blk的位置</p><h5 id="特性2，computer方法"><a href="#特性2，computer方法" class="headerlink" title="特性2，computer方法"></a>特性2，computer方法</h5><p>spark中在处理rdd的时候，是一个写好的，完整的，成熟的处理框架，能够适配几乎所有的用户行为,面对于用户的不同业务需求和逻辑，spark中提供了一个<strong>compute的方法可以进行几乎任何逻辑的处理</strong>，compute方法可以将任何用户的函数进行总的计算</p><pre><code class="scala">override def compute(split: Partition, context: TaskContext): Iterator[U] =    f(context, split.index, firstParent[T].iterator(split, context))</code></pre><p>compute函数，这个函数通过用户自定义的函数逻辑，将<strong>rdd中每个分区的数据进行遍历迭代，然后获取每个分区中的每条数据，使用函数进行统一处理</strong></p><p>处理完毕以后将数据封装到一个iterator迭代器中返回</p><p><img src="/cdh/spark/spark/image-20200319102342436.png" alt="image-20200319102342436"></p><h5 id="特性3，依赖关系"><a href="#特性3，依赖关系" class="headerlink" title="特性3，依赖关系"></a>特性3，依赖关系</h5><p>RDD之间存在一系列的依赖关系：其实依赖关系就是rdd之间的算子</p><p>精髓：在一个任务中，一旦遇见宽依赖肯定切分阶段，宽依赖&#x3D;&#x3D;shuffle</p><pre><code class="scala">protected def getDependencies: Seq[Dependency[_]] = deps//每个rdd上面都存在这样一个方法能够获取rdd上面的依赖关系</code></pre><p>依赖关系是一个抽象类，存在两个大的实现类，<strong>shuffleDependency</strong>,<strong>narrowDependency</strong></p><p><img src="/cdh/spark/spark/image-20200319102712878.png" alt="image-20200319102712878"></p><p><strong>shuffleDependency</strong>：宽依赖，rdd之间存在shuffle流程</p><p><strong>narrowDependency</strong>：窄依赖，rdd和rdd之间的关系是一对一管道形式的</p><p><strong>narrowDependency窄依赖中又存在两种：</strong></p><p><strong>OneToOneDependency</strong></p><p>几乎所有的窄依赖都是OneToOneDependency的</p><p><strong>RangeDependency</strong></p><p><strong>union</strong>：rangeDependency（union比较特殊，是rangeDependency，可以合并分区，没有shuffle）</p><p><strong>宽依赖算子：reduceBykey groupByKey sortBy sortBykey distinct repartition</strong></p><p><strong>窄依赖算子：map flatMap filter mapPartitions coalesce …</strong></p><p><strong>map窄依赖案例</strong></p><img src="file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.jpg" alt="img"><p><strong>reduceByKey宽依赖案例</strong></p><p><img src="/cdh/spark/spark/image-20200319104127633.png" alt="image-20200319104127633"></p><h5 id="特性4，分区器"><a href="#特性4，分区器" class="headerlink" title="特性4，分区器"></a>特性4，分区器</h5><p>在一个k-v 的rdd上面存在一个可选的分区器</p><p>一般的rdd上面不存在分区器，rdd数据从上一个到下一个rdd之间，数据相互移动，重新分配数据，分区器其实是算子，将上一个rdd的数据按照一定的规定给下一个rdd的不同分区进行分配数据，一般会在shuffle中产生，必须加在k-v类型的rdd上面</p><p>有两种Partitioner</p><h5 id="hashPartitionerrangePartitioner"><a href="#hashPartitionerrangePartitioner" class="headerlink" title="hashPartitionerrangePartitioner"></a><strong>hashPartitionerrangePartitioner</strong></h5><p>这两种是自带的分区器，一旦<strong>产生shuffle才会有分区器</strong></p><pre><code class="scala">//hash分区器的功能int类型的hashcode就是自己本身如果key是空就放入到0号分区中，如果不是空的就按照key的hashcode值进行按照分区编号取余数，放入对应分区//range分区器的功能rangePartition让rdd到下一个rdd之间尽量均匀的适配，但是分区的数量和元素数量一致或者小于元素数量（如果数据不够自定义分区的数量，分区数量自适应）在rangePartitioner存在一个抽样方式，进行数据的适配，让数据在下一个rdd中尽量的分配均匀，</code></pre><p><img src="/cdh/spark/spark/image-20200726162959005.png" alt="image-20200726162959005"></p><p>reduceByKeygroupByKey…</p><p><strong>几乎所有的shuffle算子使用的分区器都是hashPartitioner</strong></p><p><strong>只有sortByKey使用的是rangePartitioner</strong></p><p><strong>sortByKey不可以随便改分区，自适应</strong></p><pre><code class="scala">sortByKey(false,10)partition.size=4</code></pre><p><strong>源码：</strong></p><pre><code class="scala">@transient val partitioner: Option[Partitioner] = None</code></pre><pre><code class="scala">abstract class Partitioner extends Serializable &#123;  def numPartitions: Int  def getPartition(key: Any): Int&#125;</code></pre><p><img src="/cdh/spark/spark/image-20200319104759983.png" alt="image-20200319104759983"></p><h5 id="特性5，移动计算"><a href="#特性5，移动计算" class="headerlink" title="特性5，移动计算"></a>特性5，移动计算</h5><p>优先位置进行计算，减少数据的移动，几乎不产生远程的io，让计算速度更快</p><h3 id="SparkSQL-–-Join-的三种方式"><a href="#SparkSQL-–-Join-的三种方式" class="headerlink" title="SparkSQL – Join 的三种方式"></a>SparkSQL – Join 的三种方式</h3><p>参考文章：<a href="https://www.cnblogs.com/0xcafedaddy/p/7614299.html">https://www.cnblogs.com/0xcafedaddy/p/7614299.html</a></p><p>参考文章：<a href="https://juejin.cn/post/6844903998734991374">https://juejin.cn/post/6844903998734991374</a></p><h5 id="1、Shuffle-Hash-Join"><a href="#1、Shuffle-Hash-Join" class="headerlink" title="1、Shuffle Hash Join"></a>1、<strong>Shuffle Hash Join</strong></h5><p>一张小表，一张大表</p><p>过程：</p><ol><li>确定build table和probe table 两个概念。build table 会根据join key来进行hash table ，而probe table 使用join key进行匹配。通常情况，比较小的表当作build table ，大表当作probe table。</li><li>hash table 确定之后，读取build table 数据，每一条数据进行join key 进行hash 处理，hash到对应的bucket中。生成一张hashtable，hashtable缓存到内存中，内存放不下就放到对应的磁盘中。</li><li>扫描 probe table 数据，数据在hash table中进行匹配，匹配成功就join在一起。</li></ol><h5 id="2、Broadcast-Hash-Join"><a href="#2、Broadcast-Hash-Join" class="headerlink" title="2、Broadcast Hash Join"></a>2、Broadcast Hash Join</h5><p>broadcast hash join 就是 shuffle hash join 的小表广播版。</p><p>使用条件：</p><ol><li>被广播的表需要小于spark.sql.autoBroadcastJoinThreshold所配置的信息，默认是10M；</li><li>基表不能被广播，比如left outer join时，只能广播右表。</li></ol><p>一张小表（大于10M）和一张大表，小于10M的进行广播</p><p>将其中一张小表广播分发到另一张大表所在的所有主机</p><p>在每个executor上执行单机版hash join，小表映射，大表试探</p><p><strong>缺点</strong>：</p><p>这个方案只能广播较小的表，否则数据的冗余传输就是远大于shuffle的开销；</p><p>另外，广播时需要被广播的表collect到driver端，当频繁的广播出现时，对driver端的内存也是一个考验。</p><h5 id="3、Sort-Merge-join"><a href="#3、Sort-Merge-join" class="headerlink" title="3、Sort Merge join"></a>3、Sort Merge join</h5><p>shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式并行处理</p><p>sort阶段：对单个分区节点的两表数据，分别进行排序</p><p>merge阶段：对排好序的两张分区表数据执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同join key就merge输出，否则取更小一边。全部结束后 union all 全部的结果</p><p>见下图示意：</p><p><img src="/cdh/spark/spark/0" alt="img"></p><h3 id="自定义分区器"><a href="#自定义分区器" class="headerlink" title="自定义分区器"></a>自定义分区器</h3><p>在数据倾斜的时候可以使用这种方式</p><p>通过继承Partitioner类，重写（numPartitions、getPartition）两个方法，实现自定义分区器</p><pre><code class="scala">coalesce//自定义分区器    hello分一个  其他的分一个class MyPartitions extends Partitioner &#123;  //几个分区  override def numPartitions: Int = 2  //分区的规则  override def getPartition(key: Any): Int = &#123;      //hello字符串的单独分一组    if ((&quot;hello&quot;).equals(key.asInstanceOf[String])) &#123;      0      //其他的单独分一组    &#125; else &#123;      1    &#125;  &#125;&#125;</code></pre><pre><code class="scala">//调用自定义分区器  def main(args: Array[String]): Unit = &#123;    val conf = new SparkConf()    //本机器所有核    conf.setMaster(&quot;local[*]&quot;) //默认生成    conf.setAppName(&quot;wc&quot;) //系统指定的    val sc = new SparkContext(conf)    //单机版读取本地文件    val strs: RDD[String] = sc.textFile(&quot;aa.txt&quot;)    val rdd1: RDD[String] = strs.flatMap(t =&gt; t.split(&quot; &quot;))    val rdd2: RDD[(String, Int)] = rdd1.map((_, 1)).reduceByKey(_ + _)    val rdd3: RDD[(String, Int)] = rdd2.partitionBy(new MyPartitions)    //    val rdd3: RDD[(String, Iterable[(String, Int)])] = rdd2.groupBy(_._1)    //    val rdd4: RDD[(String, Int)] = rdd3.mapValues(_.foldLeft(0)((a,b)=&gt;a+b._2))    //    rdd4.foreach(println)//    rdd3.saveAsTextFile(&quot;res&quot;)  println(rdd3.toDebugString)    rdd3.groupByKey()  &#125;</code></pre><h3 id="DAG有向无环图"><a href="#DAG有向无环图" class="headerlink" title="DAG有向无环图"></a>DAG有向无环图</h3><h4 id="DAG有向无环图查看方法"><a href="#DAG有向无环图查看方法" class="headerlink" title="DAG有向无环图查看方法"></a>DAG有向无环图查看方法</h4><p><img src="/cdh/spark/spark/image-20200726164017855.png" alt="image-20200726164017855"></p><p>DAG有向无环图 </p><p>粉色的大的代表阶段stage </p><p>蓝色的小代表一个算子 </p><p>黑色的点rdd的个数 </p><p>连线代表依赖关系</p><p>起点：整个任务中的第一个rdd</p><p>终点：action算子</p><h3 id="RDD依赖关系"><a href="#RDD依赖关系" class="headerlink" title="RDD依赖关系"></a>RDD依赖关系</h3><p>依赖关系叫做dependency,<strong>宽依赖shuffleDependency</strong> 和<strong>窄依赖 narrowDependency</strong></p><p><strong>RDD和RDD之间的依赖关系就是算子</strong></p><h4 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a><strong>窄依赖</strong></h4><p><strong>窄依赖不划分阶段</strong>，窄依赖是一个抽象类，实现类有两个（<strong>rangeDependency</strong>，<strong>OneToOneDependency</strong>）</p><p><strong>大部分窄依赖算子的依赖都是OneToOneDependency，不改变分区，不产生shuffle</strong></p><p><strong>rangDependency的算子只有union</strong></p><h4 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a><strong>宽依赖</strong></h4><p>宽依赖划分阶段</p><p><strong>带有shuffle流程的算子就是宽依赖</strong></p><h4 id="改变分区而不产生shuffle的特殊的算子"><a href="#改变分区而不产生shuffle的特殊的算子" class="headerlink" title="改变分区而不产生shuffle的特殊的算子"></a>改变分区而不产生shuffle的特殊的算子</h4><p><strong>join</strong></p><p>join算子既可以是窄依赖也可以是宽依赖，hash分区器，分区数量一样就是窄依赖</p><p>两个都是hashPartitioner，但是分区数量不一样，就是宽依赖</p><p>如果两个join的rdd之间分区器一样，那么就是窄依赖，不然就是宽依赖</p><p><strong>union</strong></p><p>union算子可以改变分区数量，但是不会产生阶段的划分，因为没有shuffle</p><p><strong>coalesce</strong></p><p>可以修改分区数量（缩小），不产生阶段的划分不存在shuffle，增大分区数会有shuffle过程</p><h4 id="依赖和容错："><a href="#依赖和容错：" class="headerlink" title="依赖和容错："></a>依赖和容错：</h4><p>1、数据的恢复，rdd中的数据在运行的时候丢失了，可以从上一个rdd对应的分区中查找数据，窄依赖数据恢复比较简单，宽依赖需要从上一个rdd对应的所有分区中查找</p><p>2、如果数据丢失可以从driver端进行查找数据的关系，进行数据恢复</p><p>3、  driver端除了进行数据的恢复记载，还有如果executor执行任务期间宕机了，driver端会进行任务的重试，重试3次</p><p>本地化策略 node_local process_local</p><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><ol><li><pre><code>计算过程中查看依赖关系，窄依赖是以管道方式进行运算的，都是在一个机器节点的上面处理数据，比如filter map flatMap,shuffle算子才会产生数据的节点间相互移动（mr中的shuffle一样）</code></pre><ol start="2"><li><pre><code>失败回复来看，窄依赖的数据恢复，只需要去上一个rdd的对应分区中去看，宽依赖需要找到上一个rdd中的所有分区，复杂程度比较高</code></pre><ol start="3"><li><pre><code>综上所述，引入一个新的概念stage,在一个job中通过shuffle算子切分的每个部分就是一个stage，一个stage其实一组一对一方式的rdd组成的，一个stage中的rdd对应关系都是管道的关系，执行任务的时候也会按照stage进行执行</code></pre></li></ol></li></ol></li></ol><h3 id="RDD分区规则"><a href="#RDD分区规则" class="headerlink" title="RDD分区规则"></a>RDD分区规则</h3><p>读取HDFS文件创建RDD，有几个block块RDD几个分区（RDD分区数量和block数量一致），默认最小的分区数量最小是2，可以自己指定分区数量（不要轻易改变分区数量），保证一个线程处理一个分区的数据，一个exexurot可以多个线程</p><p>如果是集合并行化产生的RDD，那么分区数量就是集群的核数，因为集群中的核数意味能够并行处理的任务数量，RDD分区和core对应，才能让集群能力最大化</p><pre><code>scala&gt; val arr = Array(1,2,3,4,5,6)arr: Array[Int] = Array(1, 2, 3, 4, 5, 6)scala&gt; sc.makeRDD(arr)res7: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at makeRDD at &lt;console&gt;:27scala&gt; res7.partitions.sizeres8: Int = 24</code></pre><pre><code>sc.textFile(&quot;hdfs://Linux01:9000/aa.txt&quot;,1)//指定分区数量是1</code></pre><p>转化类算子形成新的RDD的分区数量和原来RDD分区的数量一致</p><p>转换类算子存在两种：</p><p>1、可以修改分区的</p><pre><code>groupBy(_._1)  会修改分区</code></pre><p>2、不可以修改分区的</p><pre><code>map就修改不了分区</code></pre><h3 id="RDD的算子"><a href="#RDD的算子" class="headerlink" title="RDD的算子"></a>RDD的算子</h3><p><strong>算子是一种规范，只是规定了怎么进行rdd中数据的处理，而真正执行的逻辑是在算子的函数中，并且算子永远只在driver端，算子中的函数才是在executor端执行的</strong></p><p><strong>有几个action算子就有几个job，一个job对应多个task任务，task任务随机分配在各个executor中。</strong><br><strong>job任务之间分区没有关系，每个job重新进行分区</strong></p><p>RDD中所有的算子都分为两类</p><p>1、<strong>action算子（行动类算子）</strong></p><p>会触发 SparkContext 提交 Job 作业</p><p><strong>在调用时不会产生新的RDD</strong></p><p>2、<strong>transformation算子（转换类算子）</strong></p><p><strong>调用后产生新的RDD</strong></p><pre><code>sc.textFile(&quot;hdfs://Linux01:9000/aa.txt&quot;)val rdd = rdd.map(_*10)rdd1.collect //sc.textFile 运行在driver端//hdfs://Linux01:9000/aa.txt 运行在executor端</code></pre><h5 id="map："><a href="#map：" class="headerlink" title="map："></a><strong>map</strong>：</h5><p>算子遍历的次数是rdd中元素的个数 rdd.map(_*10)</p><ol><li>一一映射的关系</li><li>返回值的集合类型和原来的类型一样，元素个数不发生改变</li><li>没有修改分区的功能</li></ol><h5 id="mapValues："><a href="#mapValues：" class="headerlink" title="mapValues："></a><strong>mapValues：</strong></h5><ol><li>scala中只能作用在map集合的value上面</li><li>不能修改分区数量</li></ol><pre><code class="scala">scala&gt; res7.mapValues(t=&gt;t._1*t._2)res9: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[10] at mapValues at &lt;console&gt;:35scala&gt; res9.collectres10: Array[(String, Int)] = Array((zhangsan,6000), (wangwu,10500), (lisi,8100))</code></pre><h5 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions:"></a><strong>mapPartitions:</strong></h5><p>转换类算子</p><p>每次遍历一个分区中的内容，并且返回</p><p>如果存在一个业务逻辑，这个逻辑一旦在mapPartitions中进行运行，那么就会减少很多次数</p><pre><code class="scala">sc.makeRDD(arr)res37.makeRDD(arr)res38.mapPartitions(t=&gt;t.map(_*10))res41.collect</code></pre><h5 id="case-when"><a href="#case-when" class="headerlink" title="case when"></a>case when</h5><p>条件函数</p><pre><code class="scala">case when 条件1 then 结果1 else 结果2 end列入：case when aa==1 then 1  else 0 end</code></pre><h5 id="zip"><a href="#zip" class="headerlink" title="zip:"></a><strong>zip:</strong></h5><p>转换类算子</p><p>拉链操作</p><pre><code class="scala">var arr1 = Array(1,2,3,4,5,6,7,8)var arr = Array(1,2,3,4,5,6)arr zip arr1sc.makeRDD(arr,3)sc.makeRDD(arr1,4)res50 zip res51res52.collect//会报错，因为他们的分区数量不相同sc.makeRDD(arr,3)sc.makeRDD(arr1,3)res50 zip res54res52.collect//会报错，虽然分区数量相同，但是每个分区的元素并不相同</code></pre><h5 id="zipWithIndex"><a href="#zipWithIndex" class="headerlink" title="zipWithIndex:"></a><strong>zipWithIndex:</strong></h5><p>跟索引进行拉链，索引类型是long</p><pre><code>res50.zipWithIndexres58.collect//res59: Array[(Int, Long)] = Array((1,0), (2,1), (3,2), (4,3), (5,4), (6,5))</code></pre><h5 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a><strong>mapPartitionsWithIndex</strong></h5><p>遍历一个分区并且带有分区下标（哪个分区的，里面数据）</p><p>遍历每一个分区，把分区下表和数据进行拉链</p><pre><code class="scala">scala&gt; var arr = Array(1,2,3,4,5,6,7,8,9)arr: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)scala&gt; sc.makeRDD(arr,3)res60: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[46] at makeRDD at &lt;console&gt;:27scala&gt; res60.mapPartitionsWithIndex((part:Int,data:Iterator[Int])=&gt;data.map((part,_)))res61: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[47] at mapPartitionsWithIndex at &lt;console&gt;:29scala&gt; res61.collectres62: Array[(Int, Int)] = Array((0,1), (0,2), (0,3), (1,4), (1,5), (1,6), (2,7), (2,8), (2,9))//（索引，数据）</code></pre><h5 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a><strong>flatMap</strong></h5><p>map+flatten,<strong>在spark中没有flatten</strong></p><p>filter算子</p><p>元素的个数会发生改变，集合类型不变，数据类型不变</p><p>不能修改分区</p><pre><code class="scala">scala&gt; var arr = Array(1,2,3,4,5,6,7,8,9)arr: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)scala&gt; sc.makeRDD(arr,3)res71: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[57] at makeRDD at &lt;console&gt;:27scala&gt; res71.filter(_&gt;3)res72: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[58] at filter at &lt;console&gt;:29scala&gt; res72.mapPartitionsWithIndex((a,b)=&gt;b.map((a,_)))res73: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[59] at mapPartitionsWithIndex at &lt;console&gt;:31scala&gt; res73.collectres74: Array[(Int, Int)] = Array((1,4), (1,5), (1,6), (2,7), (2,8), (2,9))//filter算子如果将一个分区中的数据都过滤掉，那么这个分区依旧存在</code></pre><h5 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a><strong>groupBy</strong></h5><p>groupBy的区别就是元素的<strong>value是一个iterable的类型</strong></p><p><strong>groupBy可以修改分区的数量</strong></p><pre><code class="scala">scala&gt; var arr = Array(1,2,3,4,5,6,7,8,9)arr: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)//这里分区是3scala&gt; sc.makeRDD(arr,3)res71: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[57] at makeRDD at &lt;console&gt;:27scala&gt; res71.groupBy(_&gt;5)res75: org.apache.spark.rdd.RDD[(Boolean, Iterable[Int])] = ShuffledRDD[61] at groupBy at &lt;console&gt;:29scala&gt; res75.mapPartitionsWithIndex((a,b)=&gt;b.map((a,_)))res76: org.apache.spark.rdd.RDD[(Int, (Boolean, Iterable[Int]))] = MapPartitionsRDD[62] at mapPartitionsWithIndex at &lt;console&gt;:31//这里结果分区变成了1scala&gt; res76.collectres77: Array[(Int, (Boolean, Iterable[Int]))] = Array((1,(false,CompactBuffer(4, 5, 1, 2, 3))), (1,(true,CompactBuffer(6, 7, 8, 9))))</code></pre><h5 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a><strong>groupByKey</strong></h5><p>转换类算子</p><p>groupByKey默认就按照key进行分组，所以<strong>操作的rdd必须是（k,v）类型的</strong></p><p><strong>可以修改分区数量</strong>，返回值类型 <strong>RDD[k,Iterable[v]]</strong></p><pre><code class="scala">scala&gt; var arr = Array((&quot;A&quot;,1),(&quot;A&quot;,1),(&quot;B&quot;,1),(&quot;C&quot;,1))arr: Array[(String, Int)] = Array((A,1), (A,1), (B,1), (C,1))scala&gt; sc.makeRDD(arr,3)res78: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[63] at makeRDD at &lt;console&gt;:27//这是groupBy结果，v是[(k,v)]scala&gt; res78.groupBy(_._1)res79: org.apache.spark.rdd.RDD[(String, Iterable[(String, Int)])] = ShuffledRDD[65] at groupBy at &lt;console&gt;:29//这是groupByKey结果，v是[(v)]scala&gt; res78.groupByKey()res80: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[66] at groupByKey at &lt;console&gt;:29//聚合一下vscala&gt; res80.mapValues(_.sum)res81: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[67] at mapValues at &lt;console&gt;:31scala&gt; res81.collectres82: Array[(String, Int)] = Array((B,1), (C,1), (A,2))</code></pre><h5 id="reduceByKey："><a href="#reduceByKey：" class="headerlink" title="reduceByKey："></a>reduceByKey：</h5><p>转换类算子</p><p>带有局部的聚合combiner，带有聚合函数</p><p>取出key对应的value来聚合，可以修改分区</p><p>自动的分组和聚合</p><pre><code class="scala">scala&gt; var arr = Array((&quot;A&quot;,1),(&quot;A&quot;,1),(&quot;B&quot;,1),(&quot;C&quot;,1))arr: Array[(String, Int)] = Array((A,1), (A,1), (B,1), (C,1))scala&gt; sc.makeRDD(arr,3)res78: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[63] at makeRDD at &lt;console&gt;:27//加入聚合函数scala&gt; res78.reduceByKey(_+_)res87: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[69] at reduceByKey at &lt;console&gt;:29scala&gt; res87.collectres88: Array[(String, Int)] = Array((B,1), (C,1), (A,2))</code></pre><h5 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy()"></a>sortBy()</h5><p>转换类算子，可以改变分区</p><p>对数据进行排序（指定排序字段）</p><pre><code>res0.sortBy(-_._2)//倒序res0.sortBy(_._2)//正序res0.sortBy(_._2,false,3)//倒序，分区数量是4</code></pre><h5 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey()"></a>sortByKey()</h5><p>转换类算子</p><p>按照key进行排序，底层使用的分区器是rangepartition，</p><p>在rangePartitioner中存在一个抽样方式，进行数据的适配，让数据在下一个rdd中尽量的分配均匀，所以虽然sortByKey是转换类算子，但是<strong>依旧存在job的生成</strong></p><pre><code>var arr = Array((&quot;a&quot;,20),(&quot;a&quot;,34),(&quot;b&quot;,32))sc.textFile(arr)res0.reduceBykey(_+_)res0.sortByKey(true,3)//正序，分区数量是3，按照key排序scala&gt; res12.sortByKey(false,2)res15: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[14] at sortByKey at &lt;console&gt;:29scala&gt; res15.partitions.sizeres16: Int = 2scala&gt; res12.sortByKey(false,10)res17: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[17] at sortByKey at &lt;console&gt;:29scala&gt; res17.partitions.sizeres18: Int = 4scala&gt; res12.reduceByKey(_+_,10)res19: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[18] at reduceByKey at &lt;console&gt;:29scala&gt; res19.partitions.sizeres20: Int = 10</code></pre><h5 id="union"><a href="#union" class="headerlink" title="union"></a>union</h5><p>并集，将两个RDD数据和分区进行组合</p><pre><code class="scala">scala&gt; var arr = Array(1,2,3,4,5,6)arr: Array[Int] = Array(1, 2, 3, 4, 5, 6)scala&gt; var arr1 = Array(4,5,6,7)arr1: Array[Int] = Array(4, 5, 6, 7)scala&gt; sc.makeRDD(arr,2)res1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at &lt;console&gt;:27scala&gt; sc.makeRDD(arr1,3)res2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:27scala&gt; res1 union res2res3: org.apache.spark.rdd.RDD[Int] = UnionRDD[2] at union at &lt;console&gt;:33scala&gt; res3.collect                                                               res4: Array[Int] = Array(1, 2, 3, 4, 5, 6, 4, 5, 6, 7)scala&gt; res3.partitions.sizeres5: Int = 5//通过mapPartitionsWithIndex看出元素没有发生变化scala&gt; res3.mapPartitionsWithIndex((a,b)=&gt;b.map((a,_)))res7: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[3] at mapPartitionsWithIndex at &lt;console&gt;:35scala&gt; res7.collectres8: Array[(Int, Int)] = Array((0,1), (0,2), (0,3), (1,4), (1,5), (1,6), (2,4), (3,5), (4,6), (4,7))//五个分区</code></pre><h5 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h5><p>转换类算子  <strong>交集</strong></p><p>对数据进行交集，分区数量根据分区数量大的那个来分区，改变分区数量</p><pre><code class="scala">scala&gt; var arr = Array(1,2,3,4,5,6)arr: Array[Int] = Array(1, 2, 3, 4, 5, 6)scala&gt; var arr1 = Array(4,5,6,7)arr1: Array[Int] = Array(4, 5, 6, 7)scala&gt; sc.makeRDD(arr,2)res1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at &lt;console&gt;:27scala&gt; sc.makeRDD(arr1,3)res2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:27scala&gt; res1 intersection res2res9: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at intersection at &lt;console&gt;:33scala&gt; res9.partitions.sizeres10: Int = 3</code></pre><h5 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h5><p>转换类算子</p><p>数据进行差集，分区是谁进行的，分区数就是谁的</p><pre><code class="scala">scala&gt; res1 subtract res2res12: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at subtract at &lt;console&gt;:33scala&gt; res12.partitions.sizeres13: Int = 2scala&gt; res2 subtract res1res14: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[17] at subtract at &lt;console&gt;:33scala&gt; res14.partitions.sizeres15: Int = 3</code></pre><h5 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h5><p>转换类算子</p><p>对数据进行去重</p><p><strong>distinct算法原理</strong></p><pre><code class="scala">//distinct算法原理def distinct(numPartitions:Int)(implicit ord:Ordering[T] = null):RDD[T]= withScope&#123;map(x=&gt;(x,null)).reduceByKey((x,y))=&gt;x.numPartitions).map(_._1)&#125;//使用的是reduceByKey，//reduceByKey后变成(k,null),然后map(._1),获取key</code></pre><p><img src="/cdh/spark/spark/image-20200316173534151.png" alt="image-20200316173534151"></p><h5 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h5><p><strong>aggregateByKey中的初始化值每个分区参与一次运算，整体聚合的时候不参与</strong></p><p><strong>aggreagate源码</strong></p><pre><code class="scala">def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U = withScope &#123;    // Clone the zero value since we will also be serializing it as part of tasks    var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance())    val cleanSeqOp = sc.clean(seqOp)    val cleanCombOp = sc.clean(combOp)    val aggregatePartition = (it: Iterator[T]) =&gt; it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)    val mergeResult = (index: Int, taskResult: U) =&gt; jobResult = combOp(jobResult, taskResult)    sc.runJob(this, aggregatePartition, mergeResult)    jobResult  &#125;</code></pre><p><strong>案例</strong></p><p><strong>每个分区参与一次运算，整体聚合的时候不参与</strong></p><pre><code class="scala">def main(args: Array[String]): Unit = &#123;    val agg =  Array((&quot;A&quot;,1), (&quot;B&quot;,1), (&quot;A&quot;,1), (&quot;B&quot;,1), (&quot;C&quot;,1), (&quot;A&quot;,10))    val conf = new SparkConf()    conf.setMaster(&quot;local[*]&quot;)    conf.setAppName(&quot;wc&quot;)    val sc = new SparkContext(conf)    val value: RDD[(String, Int)] = sc.makeRDD(agg,3)    val value1: RDD[(String, Int)] = value.aggregateByKey(0)(_ + _, _ + _)    value1.collect.foreach(println)    val value2: RDD[(String, Int)] = value.aggregateByKey(10)(_ + _, _ + _)    value2.collect.foreach(println)  &#125;//(B,22)(C,11)(A,42)</code></pre><h5 id="join"><a href="#join" class="headerlink" title="join"></a>join</h5><p>join算子是关联算子，<strong>只能作用在k-v类型的数据</strong>上面，<strong>两个rdd之间做join的时候是按照k1&#x3D;&#x3D;k2进行的join</strong></p><pre><code class="scala">var arr = Array((&quot;zhangsan&quot;,200),(&quot;lisi&quot;,300),(&quot;wangwu&quot;,350),(&quot;zhaosi&quot;,320))var arr1 = Array((&quot;zhangsan&quot;,30),(&quot;lisi&quot;,27),(&quot;wangwu&quot;,30),(&quot;liuneng&quot;,1))sc.makeRDD(arr,3)sc.makeRDD(arr1,3)res5 join res6res7.collect//没有对应的k则丢弃//Array[(String, (Int, Int))] = Array((zhangsan,(200,30)), (wangwu,(350,30)), (lisi,(300,27)))res7.mapValues(t=&gt;t._1*t._2)res9.collect</code></pre><h5 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h5><p><strong>只能作用在k-v类型的数据</strong>，显示左侧的全部数据,没有对应数据的v是0，出来后v._2的数据类型是Option</p><p>Option两个子类： Some,None</p><pre><code class="scala">var arr = Array((&quot;zhangsan&quot;,200),(&quot;lisi&quot;,300),(&quot;wangwu&quot;,350),(&quot;zhaosi&quot;,320))var arr1 = Array((&quot;zhangsan&quot;,30),(&quot;lisi&quot;,27),(&quot;wangwu&quot;,30),(&quot;liuneng&quot;,1))sc.makeRDD(arr,3)sc.makeRDD(arr1,3)res5 leftOuterJoin res6//org.apache.spark.rdd.RDD[(String, (Int, Option[Int]))] = MapPartitionsRDD[17] at leftOuterJoin at &lt;console&gt;:33res16.mapValues(t=&gt;t._1*t._2.getOrElse(0))res17.collect//Array[(String, Int)] = Array((zhangsan,6000), (wangwu,10500), (lisi,8100), (zhaosi,0))</code></pre><h5 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h5><p><strong>只能作用在k-v类型的数据</strong>，显示左侧的全部数据,没有对应数据的v是0，出来后v._1的数据类型是Option</p><pre><code class="scala">var arr = Array((&quot;zhangsan&quot;,200),(&quot;lisi&quot;,300),(&quot;wangwu&quot;,350),(&quot;zhaosi&quot;,320))var arr1 = Array((&quot;zhangsan&quot;,30),(&quot;lisi&quot;,27),(&quot;wangwu&quot;,30),(&quot;liuneng&quot;,1))sc.makeRDD(arr,3)sc.makeRDD(arr1,3)res5 rightOuterJoin  res6//org.apache.spark.rdd.RDD[(String, (Option[Int], Int))] = MapPartitionsRDD[21] at rightOuterJoin at &lt;console&gt;:33res16.mapValues(t=&gt;t._1*t._2.getOrElse(0))res17.collect//Array[(String, (Option[Int], Int))] = Array((zhangsan,(Some(200),30)), (wangwu,(Some(350),30)), (lisi,(Some(300),27)), (liuneng,(None,1)))</code></pre><h5 id="cogroup-全连接"><a href="#cogroup-全连接" class="headerlink" title="cogroup  全连接"></a>cogroup  全连接</h5><p>全连接，v._1和v._2的类型都是Option</p><p>会将两个集合中k相同的值合并成一个</p><pre><code class="scala">var arr = Array((&quot;zhangsan&quot;,200),(&quot;lisi&quot;,300),(&quot;wangwu&quot;,350),(&quot;zhaosi&quot;,320))var arr1 = Array((&quot;zhangsan&quot;,30),(&quot;lisi&quot;,27),(&quot;wangwu&quot;,30),(&quot;liuneng&quot;,1))sc.makeRDD(arr,3)sc.makeRDD(arr1,3)res5 cogroup res6//org.apache.spark.rdd.RDD[(String, (Iterable[Int], Iterable[Int]))] = MapPartitionsRDD[24] at cogroup at &lt;console&gt;:33res24.collect//Array[(String, (Iterable[Int], Iterable[Int]))] = //Array((zhangsan,(CompactBuffer(200),CompactBuffer(30))), //(wangwu,(CompactBuffer(350),CompactBuffer(30))), //(lisi,(CompactBuffer(300),CompactBuffer(27))), //(zhaosi,(CompactBuffer(320),CompactBuffer())), //(liuneng,(CompactBuffer(),CompactBuffer(1)))</code></pre><h5 id="fulljoin（cogroup）"><a href="#fulljoin（cogroup）" class="headerlink" title="fulljoin（cogroup）"></a>fulljoin（cogroup）</h5><p>也是全连接</p><pre><code class="scala">var arr = Array((&quot;zhangsan&quot;,200),(&quot;lisi&quot;,300),(&quot;wangwu&quot;,350),(&quot;zhaosi&quot;,320))var arr1 = Array((&quot;zhangsan&quot;,30),(&quot;lisi&quot;,27),(&quot;wangwu&quot;,30),(&quot;liuneng&quot;,1))sc.makeRDD(arr,3)sc.makeRDD(arr1,3)res5 cogroup res6//org.apache.spark.rdd.RDD[(String, (Iterable[Int], Iterable[Int]))] = MapPartitionsRDD[24] at cogroup at &lt;console&gt;:33res24.collect//Array[(String, (Iterable[Int], Iterable[Int]))] = //Array((zhangsan,(CompactBuffer(200),CompactBuffer(30))),//(wangwu,(CompactBuffer(350),CompactBuffer(30))), //(lisi,(CompactBuffer(300),CompactBuffer(27))), //(zhaosi,(CompactBuffer(320),CompactBuffer())), //(liuneng,(CompactBuffer(),CompactBuffer(1))))res24.mapValues(t=&gt;t._1.sum * t._2.sum)scala&gt; res26.collect//res27: Array[(String, Int)] = Array((zhangsan,6000), (wangwu,10500), (lisi,8100), (zhaosi,0), (liuneng,0))</code></pre><h5 id="fullOuterJoin"><a href="#fullOuterJoin" class="headerlink" title="fullOuterJoin"></a>fullOuterJoin</h5><p>和fulljoin效果一样</p><h5 id="cartesian笛卡尔积"><a href="#cartesian笛卡尔积" class="headerlink" title="cartesian笛卡尔积"></a>cartesian笛卡尔积</h5><p>算子会将所有的分区数量都相乘，然后元素个数也会相乘</p><pre><code class="scala">var arr = Array(1,2,3,4,5,6,7,8,9)sc.makeRDD(arr)res29 cartesian res29res30.collect// Array[(Int, Int)] = Array((1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (1,7), (1,8), (1,9), (2,1), (2,2)res30.count//res32: Long = 81 //9*9=81个job</code></pre><h5 id="repartition-算子"><a href="#repartition-算子" class="headerlink" title="repartition 算子"></a>repartition 算子</h5><p>转换类算子</p><p>专门修改分区</p><p>repartition算子可以增加和减小分区数量</p><p><strong>repartition带有shuffle</strong></p><p> repartition<strong>底层使用的是coalesce算子</strong></p><pre><code class="scala">var arr = Array(1,2,3,4,5,6,7,8,9)sc.makeRDD(arr,3)res33.repartition(2)res34.partitions.size// Int = 2//能增能减</code></pre><h5 id="coalesce算子"><a href="#coalesce算子" class="headerlink" title="coalesce算子"></a>coalesce算子</h5><p>专门修改分区</p><p>coalesce算子<strong>只能减小分区的数量</strong>但是不能增加，<strong>coalesce算子不带有shuffle</strong></p><pre><code class="scala">var arr = Array(1,2,3,4,5,6,7,8,9)sc.makeRDD(arr,3)res33.coalesce(2)res34.partitions.size// Int = 2res33.coalesce(6)// Int = 3//只能减不能增coalesce(number,boolear)//是否带有shuffle</code></pre><h5 id="aggregateByKey-1"><a href="#aggregateByKey-1" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h5><p>转换类算子</p><p>按照key进行聚合</p><p>aggregateByKey中的初始化值<strong>每个分区参与一次运算，整体聚合的时候不参与</strong></p><pre><code class="scala">var arr = Array((&quot;A&quot;,1),(&quot;B&quot;,1),(&quot;A&quot;,1),(&quot;B&quot;,1),(&quot;C&quot;,1),(&quot;A&quot;,10))sc.makeRDD(arr,3)res4.aggregateByKey(0)(_+_,_+_)//org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[2] at aggregateByKey at &lt;console&gt;:29res5.collect// Array[(String, Int)] = Array((B,2), (C,1), (A,12))   res4.aggregateByKey(10)(_+_,_+_)//res7: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[3] at aggregateByKey at &lt;console&gt;:29res7.collect//res8: Array[(String, Int)] = Array((B,22), (C,11), (A,42))</code></pre><h4 id="行动类算子"><a href="#行动类算子" class="headerlink" title="行动类算子"></a>行动类算子</h4><h5 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h5><p>action行动类算子</p><p>迭代每个元素</p><p>打印的时候<strong>每个分区对应一个executor进行执行，在executor中进行打印数据，driver端是看不见效果的</strong></p><p>foreach算子在打印多次数据的时候，出现每个executor中的数据不停变化，spark-shell是不是一个应用，一个应用中如果使用多次action算子，会产生多个job，这些job都会使用一套driver+executor，每个job在运行的时候分区和executor的对应关系是不断变化的</p><h5 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h5><p>遍历每个分区对象，是iterator类型的</p><p>每次遍历一个分区的数据进行打印，但是每次运行的结果不一定在一个executor上面</p><pre><code>res0.foreachPartition(t=&gt;t.foreach(println))</code></pre><pre><code>每个分区单独使用一个connection对象res.foreachPartition(t=&gt;&#123;val connection = DriverManager.getconnetion&#125;)</code></pre><h5 id="take"><a href="#take" class="headerlink" title="take"></a>take</h5><p>行动类算子</p><p>是一个特殊的算子，截取rdd上面对应个数的元素，组成一个新的本地集合返回，如果<strong>元素相隔分区会产生多个的job，take算子的job任务个数和扫描分区的个数相关</strong></p><pre><code>var arr = Array(1,2,3,4,5,6,7,8,9)sc.makeRDD(arr,3)arr.take(3)//一个jobres27.take(4)//跨分区扫描，会有两个job</code></pre><h5 id="first"><a href="#first" class="headerlink" title="first"></a>first</h5><p>行动类算子</p><p>获取第一个元素返回</p><pre><code class="scala">sc.makeRDD(Array(4,2,1,3,8,7),2)res19.first//Int = 4</code></pre><h5 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h5><p>收集各个executor计算的结果形成一个集合，返回driver端</p><h5 id="collectAsMap"><a href="#collectAsMap" class="headerlink" title="collectAsMap"></a>collectAsMap</h5><p>将收集各个executor计算的结果形成一个Map，返回driver端</p><p>收集的时候数据类型必须得是对偶元组类型的，否则报错</p><h5 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h5><p>按照key求value的个数</p><pre><code>var arr = Array((&quot;a&quot;,20),(&quot;a&quot;,34))sc.makeRDD(arr,2)res0.countByKey()//Map(a-&gt;2)</code></pre><h5 id="aggreagate"><a href="#aggreagate" class="headerlink" title="aggreagate"></a>aggreagate</h5><p>行动类算子，初始化值参与四次运算，<strong>每个分区参与一次，整体聚合参与一次</strong></p><p><img src="/cdh/spark/spark/image-20200318203010014.png" alt="image-20200318203010014"></p><pre><code class="scala">aggregate(initial_value)(function1,function2)</code></pre><pre><code class="scala">var arr = Array(1,2,3,4,5,6,7,8,9)sc.makeRDD(arr,3)res0.aggregate(0)(_+_,_+_)//res1:Int = 45  res0.aggregate(0)(_+_,_*_)//res2: Int = 0  res0.aggregate(10)(_+_,_+_)//res3: Int = 85//每个分区参与一次，整体聚合参与一次//((1+2+3+10),(4+5+6+10),(7+8+9+10)+10)</code></pre><h5 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered()"></a>takeOrdered()</h5><p>行动类算子</p><p>正序取索引个数</p><pre><code>var arr = Array(6,4,2,3,1)res0.takeOrdered(2)//1,2</code></pre><h5 id="top"><a href="#top" class="headerlink" title="top()"></a>top()</h5><p>行动类算子</p><p>倒序之后的索引个数</p><pre><code>var arr = Array(6,4,2,3,1)res0.top(2)//6,4</code></pre><h5 id="reduce"><a href="#reduce" class="headerlink" title="reduce()"></a>reduce()</h5><p>行动类算子</p><p>从左到又进行计算，然后根据传入的函数进行计算</p><pre><code class="scala">scala&gt; var arr = Array((&quot;zhangsan&quot;,2000),(&quot;hello&quot;,3000),(&quot;word&quot;,4000))arr: Array[(String, Int)] = Array((zhangsan,2000), (hello,3000), (word,4000))scala&gt; sc.makeRDD(arr,2)res13: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[1] at makeRDD at &lt;console&gt;:27scala&gt; res13.reducedef reduce(f: ((String, Int), (String, Int)) =&gt; (String, Int)): (String, Int)scala&gt; res13.reduce((a,b)=&gt;(&quot;&quot;,a._2+b._2))res14: (String, Int) = (&quot;&quot;,9000)</code></pre><h5 id="count"><a href="#count" class="headerlink" title="count"></a>count</h5><p>行动类算子</p><p>把符合条件的元素进行统计计数,也能计算所有元素的个数</p><p>RDD带有分区的，rdd中不存在数据，所以<strong>必须将所有的元素都遍历一遍然后得出总的个数</strong></p><pre><code>res0.count(_._2&gt;2000)</code></pre><p>把RDD的元素个数进行返回</p><pre><code>res0.count//会遍历一遍所有分区的元素</code></pre><h5 id="countByKey-1"><a href="#countByKey-1" class="headerlink" title="countByKey"></a>countByKey</h5><p>行动类算子</p><p>countByKey:按照key求出个数，</p><h5 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h5><p>行动类算子</p><h4 id="Shuffle算子"><a href="#Shuffle算子" class="headerlink" title="Shuffle算子"></a>Shuffle算子</h4><h5 id="distinct-1"><a href="#distinct-1" class="headerlink" title="distinct"></a>distinct</h5><h5 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h5><h5 id="groupByKey-1"><a href="#groupByKey-1" class="headerlink" title="groupByKey"></a>groupByKey</h5><h5 id="aggregateByKey-2"><a href="#aggregateByKey-2" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h5><h5 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h5><h5 id="SortByKey"><a href="#SortByKey" class="headerlink" title="SortByKey"></a>SortByKey</h5><h4 id="特殊算子"><a href="#特殊算子" class="headerlink" title="特殊算子"></a>特殊算子</h4><h5 id="textFile"><a href="#textFile" class="headerlink" title="textFile()"></a>textFile()</h5><p>不属于RDD的算子，是创建RDD的方法，是属于SparkContext，产生rdd的时候，<strong>它的分区数量和block的数量一样，并且分区的数量最小是2</strong></p><p>读取文件数据，会产生两个RDD</p><p>一个HadoopRDD一个mapPartitionRDD</p><pre><code>sc.textFile(&quot;aa.txt&quot;)</code></pre><h5 id="makeRDD"><a href="#makeRDD" class="headerlink" title="makeRDD()"></a>makeRDD()</h5><p><img src="/cdh/spark/spark/image-20200726161427744.png" alt="image-20200726161427744"></p><p>不属于RDD的算子，是创建RDD的方法，通过集合并行化的方式（底层用的是parallelize方法）</p><pre><code>scala&gt; val arr = Array(1,2,3,4,5,6)arr: Array[Int] = Array(1, 2, 3, 4, 5, 6)scala&gt; sc.makeRDD(arr)res2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at makeRDD at &lt;console&gt;:27</code></pre><h4 id="mapPartition-foreachPartition不同点"><a href="#mapPartition-foreachPartition不同点" class="headerlink" title="mapPartition+foreachPartition不同点"></a>mapPartition+foreachPartition不同点</h4><p>他们的优点<br>map和foreach算子遍历次数是rdd中元素的个数<br>mapPartitoins每个分区遍历一次,在使用资源的情况下，每个分区使用一个资源，而map和foreach是每个元素一个资源</p><h4 id="groupByKey-reduceByKey-区别"><a href="#groupByKey-reduceByKey-区别" class="headerlink" title="groupByKey reduceByKey:区别"></a>groupByKey reduceByKey:区别</h4><p><strong>相同点：</strong></p><p>他们都应该作用在k-v的类型数据上面</p><p>都是根据key进行的数据聚合</p><p>都可以修改分区的数量，但是默认是不改变的，都会产生shuffle</p><p><strong>不同点：</strong></p><p>groupBykey就是简单的分类，不存在聚合函数</p><p>reduceBykey合并，带有聚合函数，并且带有combiner</p><h3 id="自定义排序"><a href="#自定义排序" class="headerlink" title="自定义排序"></a>自定义排序</h3><p><strong>两种方法</strong></p><p>1、通过继承<strong>Ordered</strong>类重写compare方法</p><p>2、也可以通过隐式转换来自动compare方法</p><p><strong>二次排序</strong></p><pre><code class="scala">object SecondarySort &#123;  def main(args: Array[String]): Unit = &#123;    val conf = new SparkConf()    conf.setAppName(&quot;secondSort&quot;)    conf.setMaster(&quot;local[1]&quot;)    val sc = new SparkContext(conf)//    var arr = Array(9,1,8,2,7,3,6,4,5)    var arr = Array((&quot;zhangsan&quot;,90,89),(&quot;lisi&quot;,90,85),(&quot;yangkai&quot;,100,95),(&quot;changjian&quot;,90,95),(&quot;liting&quot;,97,98),(&quot;shuaishuai&quot;,97,99))    val rdd = sc.makeRDD(arr,3)//    val rdd1 = rdd.map(t=&gt;new Student(t._1,t._2,t._3))    //shuffle//    val rdd1 = rdd.sortBy(t=&gt;new Student(t._1,t._2,t._3))//    val rdd1 = rdd.sortBy(t=&gt;(t._2,t._3))    //证明元组可以排序，但是这个元组没有比较器的接口    //将自己的类也变成样例类就带有了序列化的功能，写一个隐式的转换方法，可以将没有比较器的类转换为带有比较器的类val rdd1 =  rdd.sortBy(t=&gt;Student(t._1,t._2,t._3))    rdd1.foreach(println)  &#125;//隐式转换完成二次排序  implicit def noOrdered2Ordered(s:Student):Ordered[Student]=&#123;    new Ordered[Student] &#123;      override def compare(that: Student): Int = &#123;        if(that.math == s.math)&#123;          that.chinese-s.chinese        &#125;else&#123;          that.math-s.math        &#125;      &#125;    &#125;  &#125;&#125;case class Student(val name:String,val math:Int,val chinese:Int)</code></pre><h3 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h3><h4 id="为什么要持久化"><a href="#为什么要持久化" class="headerlink" title="为什么要持久化"></a>为什么要持久化</h4><p>在某种情况下，一个rdd中的数据要经过很多步骤处理才能得到的，以后大家都要使用这个数据，也就是这个rdd中的数据会被多次复用，所以我们<strong>将这个rdd中的数据缓存起来</strong>，以后使用直接从缓存中读取就可以了</p><h4 id="持久化的两个算子"><a href="#持久化的两个算子" class="headerlink" title="持久化的两个算子"></a>持久化的两个算子</h4><p><strong>缓存类的算子都是懒加载的，必须调用行动类算子才可以执行</strong></p><p><strong>cache 算子和 checkpoint 算子</strong></p><p><strong>他们底层都是persist算子，只是存储级别不同</strong></p><h4 id="cache算子说明"><a href="#cache算子说明" class="headerlink" title="cache算子说明"></a>cache算子说明</h4><p>rdd并不是存储在driver端，<strong>每个分区(task)单独存储自己的数据到executor的storage区域。内存比例查看spark的内存管理方案</strong></p><p>[spark内存管理方案]: Spark&#x2F;spark2.x内存管理方案pdf“Spark&#x2F;spark2.x内存管理方案pdf”</p><p>，每个execuotr存在一个单独的blockManager存储自己的数据</p><p><strong>缓存类的算子都是懒加载的，必须调用行动类算子才可以执行</strong></p><h5 id="cache的使用"><a href="#cache的使用" class="headerlink" title="cache的使用"></a>cache的使用</h5><pre><code class="scala">rdd.cache()  rdd.persist()//持久化的两个算子//cache底层用的persist方法//默认的持久化级别是MEMORY_ONLY</code></pre><p><img src="/cdh/spark/spark/image-20200323201828924.png" alt="image-20200323201828924"></p><p><strong>cache&#x3D;&#x3D;&gt;persist&#x3D;&#x3D;&gt;persist(存储级别)</strong></p><p>rdd如果缓存了数据，会变成绿色的，在以后使用这个rdd数据时，<strong>不会重新进行处理，依赖关系和流程都不会发生改变</strong></p><img src="/cdh/spark/spark/Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200323202349785.png" alt="image-20200323202349785" style="zoom:67%;"><h5 id="去除缓存数据操作"><a href="#去除缓存数据操作" class="headerlink" title="去除缓存数据操作"></a>去除缓存数据操作</h5><p>rdd.unpersist()进行缓存数据的删除</p><pre><code class="scala">rdd.unpersist()//缓存的数据是每个executor都存储自己处理的分区的数据，所以executor和缓存的数据是绑定的，application杀死，缓存的数据会随之消失</code></pre><h4 id="checkpoint算子说明"><a href="#checkpoint算子说明" class="headerlink" title="checkpoint算子说明"></a>checkpoint算子说明</h4><p><strong>1、把RDD的数据以文本的形式存储到hdfs中，每个数据都是序列化以后的二进制数据，每个分区保存一份单独的数据</strong></p><p><strong>2、多个checkpoint可以使用同一个文件夹</strong></p><p><strong>3、使用checkpoint缓存，之前的依赖关系不再存在，依赖关系会发生改变</strong></p><p><strong>4、checkpint算子会单独启动一个线程进行处理</strong></p><p><strong>缓存类的算子都是懒加载的，必须调用行动类算子才可以执行</strong></p><h5 id="checkpoint的使用"><a href="#checkpoint的使用" class="headerlink" title="checkpoint的使用"></a>checkpoint的使用</h5><pre><code class="scala">sc.setCheckpointDir(&quot;hdfs://Linux01:9000/1902chpt&quot;)res.checkpoint//懒加载//使用时必须先设置缓存路径！</code></pre><h5 id="checkpoint将处理流程重新加载"><a href="#checkpoint将处理流程重新加载" class="headerlink" title="checkpoint将处理流程重新加载"></a>checkpoint将处理流程重新加载</h5><pre><code class="scala">//将checkpoint的rdd进行cache//同一个application，checkpoint使用同一个文件夹的时候，根据应用不同生成的二级文件夹不一样</code></pre><h4 id="persist算子"><a href="#persist算子" class="headerlink" title="persist算子"></a>persist算子</h4><p>cache和checkpoint都是使用的persist算子，它可以指定存储级别</p><h4 id="存储级别"><a href="#存储级别" class="headerlink" title="存储级别"></a>存储级别</h4><p>1.MEMORY_ONLY</p><p>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是<strong>默认的持久化策略</strong>，使用cache()方法时，实际就是使用的这种持久化策略。</p><p>2.MEMORY_AND_DISK</p><p>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</p><p>3.MEMORY_ONLY_SER</p><p>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</p><p>4.MEMORY_AND_DISK_SER</p><p>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</p><p>5.DISK_ONLY</p><p>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</p><p><img src="/cdh/spark/spark/image-20200323201714024.png" alt="image-20200323201714024"></p><pre><code>userDisk=存储rdd数据的时候是不是使用磁盘userMemory:是不是使用内存userOffHeap:是不是使用堆外内存deserialized:是不是不序列化replication:存储rdd的数据的副本数量是多少</code></pre><h4 id="driver端的数据在executor使用的时候究竟传输多少次？"><a href="#driver端的数据在executor使用的时候究竟传输多少次？" class="headerlink" title="driver端的数据在executor使用的时候究竟传输多少次？"></a>driver端的数据在executor使用的时候究竟传输多少次？</h4><p>因为必须经过序列化和反序列化的转换</p><p><img src="/cdh/spark/spark/image-20200323205253359.png" alt="image-20200323205253359"></p><pre><code class="scala">//这样写，每个分区的每条数据都会产生一个数据//有多少数据产生多少次数据</code></pre><p><img src="/cdh/spark/spark/image-20200323204649598.png" alt="image-20200323204649598"></p><pre><code class="scala">//一个分区中不管有多少个数据，引用的都是同一份driver端的变量，有多少分区产生多少数据</code></pre><p><img src="/cdh/spark/spark/image-20200323205616658.png" alt="image-20200323205616658"></p><pre><code class="scala">//使用广播变量，共享数据，一个executor中的所有分区(task)都使用这个数据</code></pre><h2 id="全局累加器Accumulator"><a href="#全局累加器Accumulator" class="headerlink" title="全局累加器Accumulator"></a>全局累加器Accumulator</h2><h4 id="累加器概述"><a href="#累加器概述" class="headerlink" title="累加器概述"></a>累加器概述</h4><p><a href="https://www.cnblogs.com/itboys/p/11056758.html">https://www.cnblogs.com/itboys/p/11056758.html</a></p><p>全局累加器的意思就是将所有的内容全部累加到一起<br>没有分类的概念<br><strong>主要的意义就是跨分区</strong>，所有的内容全部都会加到一起<br>主要操作的rdd的一个整体概念，每个分区执行往后的结果进行聚合，那就是最终结果</p><p>Spark内置了三种类型的Accumulator，分别是LongAccumulator用来累加整数型，DoubleAccumulator用来累加浮点型，CollectionAccumulator用来累加集合元素。</p><p>Accumulator在使用时有两个方法，已经定义好的:</p><pre><code class="scala">//方法已过期，需要手动定义默认值val acc = sc.accumulator(0,&quot;count&quot;)//方法没过期，不需要定义默认值，默认0val acc1 = sc.longAccumulator(&quot;count_1&quot;)</code></pre><h4 id="累加器的使用"><a href="#累加器的使用" class="headerlink" title="累加器的使用"></a>累加器的使用</h4><pre><code class="scala">object TestAcc &#123;  def main(args: Array[String]): Unit = &#123;    val arr = Array(1,2,3,4,5,6,7,8,9)    val conf = new SparkConf()    conf.setAppName(&quot;testAcc&quot;)    conf.setMaster(&quot;local[*]&quot;)    val sc = new SparkContext(conf)    //查看这个rdd中元素的个数    val rdd = sc.makeRDD(arr,3)//    println(rdd.count())    //序列化问题//    var count:Int = 0    val acc = sc.accumulator(0,&quot;count&quot;)    val acc1 = sc.longAccumulator(&quot;count_1&quot;)    rdd.foreach(t=&gt;&#123;      //这个count对象是一个executor中一个单独的对象和driver端的count不是一个但是是一个值//      count+=1//      println(count)      acc.add(1)      acc1.add(1)    &#125;)    println(acc.value)    println(acc1.value)//    println(count)    //全局累加器，类似于全局都使用一个变量  &#125;&#125;</code></pre><h4 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h4><p>需要继承AccumulatorV2类，重写六个方法</p><p>写完运行时，自定义累加器必须先注册 sc.register(acc)</p><pre><code>//六个方法isZero   //判断是否为空copy//返回一个新的累加器reset//全局变空add//添加数据marge//全局合并，形成最后的结果value//返回哪个数据使用累加器时需要注意只有Driver能够取到累加器的值，Task端进行的是累加操作</code></pre><pre><code class="scala">class MyConcatAccumulator extends AccumulatorV2[String,Map[String,Int]]&#123;  var map = Map[String,Int]()  //判断初始化的值是不是为空  override def isZero: Boolean = map.size==0 //因为定义的是一个累加器，那么这个累加器就会被大家所是有，这个方法相当于一个  //构造器可以放回一个累加器的对象  override def copy(): AccumulatorV2[String, Map[String,Int]] =    new MyConcatAccumulator  override def reset(): Unit =&#123;    map =  Map[String,Int]()  &#125;  //当前的分区中的累加器的内容，可以不断的进行累加，出现一个值就累加一次  override def add(v: String): Unit = &#123;    map.get(v) match&#123;      case Some(number)=&gt; map+=(v-&gt;(number+1))      case None=&gt; map+=(v-&gt;1)    &#125;  &#125;  //合并的概念，每个分区中的累加结果进行一个整体的最终结果  //Map[car1,12   car2,20] ++ Map[car1,21 car2,23  car4,19]  override def merge(other: AccumulatorV2[String, Map[String,Int]]): Unit = &#123;    val list:List[(String,Int)] =  this.map.toList ++  other.asInstanceOf[MyConcatAccumulator].map.toList    val groupdata:Map[String,List[(String,Int)]] = list.groupBy(_._1)    val res:Map[String,Int] = groupdata.mapValues(_.foldLeft(0)((a,b)=&gt;a+b._2))    map = res  &#125;  //所有累加完毕的结果一次性返回  override def value: Map[String,Int] = map&#125;//调用自定义累加器object TestAccumulator &#123;  def main(args: Array[String]): Unit = &#123;      val conf = new SparkConf()    conf.setAppName(&quot;acc&quot;)    conf.setMaster(&quot;local[*]&quot;)    val sc = new SparkContext(conf)    val myacc = new MyConcatAccumulator      //进行注册    sc.register(myacc)    sc.textFile(&quot;test.txt&quot;).map(_.split(&quot; &quot;)(1)).foreach(t=&gt;&#123;      myacc.add(t)    &#125;)    val res = myacc.value.map(t=&gt;t._1+&quot;:&quot;+t._2).mkString(&quot;|&quot;)    println(res)  &#125;&#125;</code></pre><h2 id="JDBCRDD"><a href="#JDBCRDD" class="headerlink" title="JDBCRDD"></a>JDBCRDD</h2><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>用来读取mysql中的内容，跟普通RDD不同，JDBCRDD需要new</p><pre><code class="scala">new JdbcRDD(sc,  ()=&gt;DriverManager.getConnection(&quot;jdbc:mysql://univers02:3306/1902_db?characterEncoding=utf-8&quot;,&quot;root&quot;,&quot;123456&quot;),  &quot;select * from visit where id&gt;? and id&lt;?&quot;,  1,  5,  2,  t=&gt;(t.getString(&quot;address&quot;),t.getInt(&quot;count&quot;))).foreach(println)//1、连接地址//2、name//3、password//4、sql语句//5、</code></pre><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><h3 id="什么是广播变量"><a href="#什么是广播变量" class="headerlink" title="什么是广播变量"></a>什么是广播变量</h3><p>使用SparkContext对象广播变量，是<strong>每一个executor使用一份广播数据</strong>，好处是1. 优化内容，join数据，尽量的避免join操作，使用广播变量的文件形式替换join</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><pre><code class="scala">sc.broadcast(***)</code></pre><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>1、在driver端进行广播变量，不能再executor端</p><p>2、不能广播RDD</p><p>3、广播数据必须需要实现序列化<br>为了让每一个executor中的所有task使用同一份数据，<br>比特洪流技术，减少driver压力，不管要下载多少数据，driver导出一份，其他机器进行共享</p><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><pre><code class="scala"> val orderRDD = sc.makeRDD(Array(   (1,&quot;京东&quot;,2000),   (2,&quot;京东&quot;,120),   (2,&quot;京东&quot;,300),   (1,&quot;京东&quot;,1200),   (1,&quot;京东&quot;,400),   (3,&quot;淘宝&quot;,4100),   (4,&quot;淘宝&quot;,200),   (3,&quot;淘宝&quot;,900),   (4,&quot;淘宝&quot;,100) )).map(t=&gt;(t._1,(t._2,t._3))) val userRDD = new JdbcRDD[(Int,String)](   sc,   ()=&gt;DriverManager.getConnection(&quot;jdbc:mysql://univers02:3306/1902_db?characterEncoding=utf-8&quot;,&quot;root&quot;,&quot;123456&quot;),           &quot;select * from user where id&gt;=? and id&lt;=?&quot;,   0,   5,           2,           t=&gt;(t.getInt(&quot;id&quot;),t.getString(&quot;name&quot;)) )val map = userRDD.collect().toMap val bd = sc.broadcast(map) orderRDD.map(t=&gt;&#123;  val name =  bd.value(t._1)   (t._1,name,t._2) &#125;).foreach(println)</code></pre><h2 id="Sparkstreaming消费kafka的两种方式，它们之间的区别是什么？"><a href="#Sparkstreaming消费kafka的两种方式，它们之间的区别是什么？" class="headerlink" title="Sparkstreaming消费kafka的两种方式，它们之间的区别是什么？"></a>Sparkstreaming消费kafka的两种方式，它们之间的区别是什么？</h2><h3 id="1、基于Receiver的方式"><a href="#1、基于Receiver的方式" class="headerlink" title="1、基于Receiver的方式"></a>1、基于Receiver的方式</h3><p>Receiver是使用Kafka的高层次Consumer API来实现的。receiver 从Kafka中获取的数据都是存储在Spark Executor的内存中的（如果突然数据暴增，大量batch堆积，很容易出现内存溢出的问题），然后Spark Streaming启动的job会去处理那些数据。 </p><p>然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的预写日志机制（Write Ahead Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。</p><h3 id="2、基于Direct的方式"><a href="#2、基于Direct的方式" class="headerlink" title="2、基于Direct的方式"></a>2、基于Direct的方式</h3><p>替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。</p><p><strong>优点如下：</strong> </p><p><strong>简化并行读取：</strong>如果要读取多个partition，不需要创建多个输入DStream然后对它们进行union操作。Spark会创建跟Kafka partition一样多的RDD partition，并且会并行从Kafka中读取数据。所以在Kafka partition和RDD partition之间，有一个一对一的映射关系。</p><p><strong>高性能：</strong>如果要保证零数据丢失，在基于receiver的方式中，需要开启WAL机制。这种方式其实效率低下，因为数据实际上被复制了两份，Kafka自己本身就有高可靠的机制，会对数据复制一份，而这里又会复制一份到WAL中。而基于direct的方式，不依赖Receiver，不需要开启WAL机制，只要Kafka中作了数据的复制，那么就可以通过Kafka的副本进行恢复。 </p><p><strong>一次且仅一次的事务机制</strong>。</p><h3 id="三、对比："><a href="#三、对比：" class="headerlink" title="三、对比："></a><strong>三、对比：</strong></h3><p>基于receiver的方式，是使用Kafka的高阶API来在<strong>ZooKeeper中保存消费过的offset的</strong>。这是消费Kafka数据的传统方式。这种方式配合着WAL机制可以保证数据零丢失的高可靠性，但是却无法保证数据被处理一次且仅一次，可能会处理两次。因为Spark和ZooKeeper之间可能是不同步的。</p><p>基于direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据是消费一次且仅消费一次。</p><p>在实际生产环境中大都用Direct方式</p><h4 id="Spark的bean类超过20个字段后需要继承Product接口和Serializable接口"><a href="#Spark的bean类超过20个字段后需要继承Product接口和Serializable接口" class="headerlink" title="Spark的bean类超过20个字段后需要继承Product接口和Serializable接口"></a>Spark的bean类超过20个字段后需要继承Product接口和Serializable接口</h4><p>需要重写<strong>productElement</strong>，<strong>productArity</strong>，<strong>canEqual</strong>，<strong>apply</strong>这四个方法</p><pre><code>productElement角标和成员属性的映射关系productArity有多少个数据canEqual返回true就行了</code></pre><h1 id="Spark调优"><a href="#Spark调优" class="headerlink" title="Spark调优"></a>Spark调优</h1><h3 id="1、资源调优"><a href="#1、资源调优" class="headerlink" title="1、资源调优"></a>1、资源调优</h3><p>1）搭建Spark集群的时候要给Spark集群足够的资源</p><pre><code>在spark安装包的conf下spark-env.shSPARK_WORKER_CORESSPARK_WORKER_MEMORYSPARK_WORKER_INSTANCE</code></pre><p>2）在提交Application的时候给Application分配更多的资源。</p><pre><code>提交命令选项：（在提交Application的时候使用选项）    --executor-cores    --executor-memory    --total-executor-cores配置信息：（在Application的代码中设置            在Spark-default.conf中设置）    spark.executor.cores    spark.executor.memory    spark.max.cores</code></pre><h3 id="2、并行度调优"><a href="#2、并行度调优" class="headerlink" title="2、并行度调优"></a>2、并行度调优</h3><p>原则：一个core一般分配2~3个task,每一个task一般处理1G数据</p><pre><code>提高并行度的方式：1).如果读取的数据在HDFS上，降低block块的大小2).sc.textFile(path,numPartitions) 3)sc.parallelize(list,numPartitions) 一般用于测试4)coalesce、repartition可以提高RDD的分区数。5)配置信息：spark.default.parallelism  not set (默认executor core的总个数)spark.sql.shuffle.partitions 200 6)自定义分区器</code></pre><h3 id="3、代码调优"><a href="#3、代码调优" class="headerlink" title="3、代码调优"></a>3、代码调优</h3><h4 id="1）避免创建重复的RDD，复用同一个RDD"><a href="#1）避免创建重复的RDD，复用同一个RDD" class="headerlink" title="1）避免创建重复的RDD，复用同一个RDD"></a>1）避免创建重复的RDD，复用同一个RDD</h4><h4 id="2）对多次使用的RDD进行持久化"><a href="#2）对多次使用的RDD进行持久化" class="headerlink" title="2）对多次使用的RDD进行持久化"></a>2）对多次使用的RDD进行持久化</h4><pre><code>如何选择一种最合适的持久化策略？默认情况下，性能最高的当然是MEMORY_ONLY，相当于调用cache算子。但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</code></pre><h6 id="持久化算子："><a href="#持久化算子：" class="headerlink" title="持久化算子："></a>持久化算子：</h6><p><strong>cache:</strong></p><p>  MEMORY_ONLY</p><p><strong>persist：</strong></p><p>  MEMORY_ONLY</p><p>  MEMORY_ONLY_SER</p><p>  MEMORY_AND_DISK_SER</p><p>  一般不要选择带有_2的持久化级别。</p><p><strong>checkpoint:</strong></p><p>①  如果一个RDD的计算时间比较长或者计算起来比较复杂，一般将这个RDD的计算结果保存到HDFS上，这样数据会更加安全。</p><p>②  如果一个RDD的依赖关系非常长，也会使用checkpoint,会切断依赖关系，提高容错的效率。</p><h4 id="3）避免使用shuffle算子"><a href="#3）避免使用shuffle算子" class="headerlink" title="3）避免使用shuffle算子"></a>3）避免使用shuffle算子</h4><p>使用广播变量来模拟使用join,使用情况：一个RDD比较大，一个RDD比较小。</p><p>join算子&#x3D;广播变量+filter、广播变量+map、广播变量+flatMap</p><h4 id="4）使用map-side预聚合的shuffle算子"><a href="#4）使用map-side预聚合的shuffle算子" class="headerlink" title="4）使用map-side预聚合的shuffle算子"></a>4）使用map-side预聚合的shuffle算子</h4><p>即尽量使用有combiner的shuffle类算子。<br>combiner概念：<br>在map端，每一个map task计算完毕后进行的局部聚合。<br>combiner好处：</p><ol><li><pre><code>降低shuffle write写磁盘的数据量。</code></pre><ol start="2"><li><pre><code>降低shuffle read拉取数据量的大小。</code></pre><ol start="3"><li><pre><code>降低reduce端聚合的次数。</code></pre>有combiner的shuffle类算子：<ol><li><pre><code>reduceByKey:这个算子在map端是有combiner的，在一些场景中可以使用reduceByKey代替groupByKey。</code></pre><ol start="2"><li><pre><code>aggregateByKey（fun1,func2）</code></pre></li></ol></li></ol></li></ol></li></ol></li></ol><h4 id="5）尽量使用高性能的算子"><a href="#5）尽量使用高性能的算子" class="headerlink" title="5）尽量使用高性能的算子"></a>5）尽量使用高性能的算子</h4><p>使用reduceByKey替代groupByKey</p><p>使用mapPartition替代map</p><p>使用foreachPartition替代foreach</p><p>filter后使用coalesce减少分区数</p><p>使用使用repartitionAndSortWithinPartitions替代repartition与sort类操作</p><p>使用repartition和coalesce算子操作分区。</p><h4 id="6）使用广播变量"><a href="#6）使用广播变量" class="headerlink" title="6）使用广播变量"></a>6）使用广播变量</h4><p>使用广播变量可以大大降低集群中变量的副本数。不使用广播变量，变量的副本数和task数一致。使用广播变量变量的副本和Executor数一致。</p><p>使用广播变量可以大大的降低集群中变量的副本数。<br>不使用广播变量：变量的副本数和task数一致。<br>使用广播变量:变量的副本数与Executor数一致。<br>广播变量最大可以是多大?<br>ExecutorMemory*60%*90%*80%    &#x3D; executorMemory *0.42</p><h4 id="7）使用Kryo优化序列化性能"><a href="#7）使用Kryo优化序列化性能" class="headerlink" title="7）使用Kryo优化序列化性能"></a>7）使用Kryo优化序列化性能</h4><p>在Spark中，<strong>主要有三个地方涉及到了序列化：</strong><br>1)在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输。(闭包引用)<br>2)将自定义的类型作为RDD的泛型类型时（比如JavaRDD<SXT>，SXT是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。<br>               (New一个类)<br>3)使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。(持久化)</SXT></p><h6 id="Kryo序列化器介绍："><a href="#Kryo序列化器介绍：" class="headerlink" title="Kryo序列化器介绍："></a>Kryo序列化器介绍：</h6><p>Spark支持使用Kryo序列化机制。Kryo序列化机制，比默认的Java序列化机制，速度要快，序列化后的数据要更小，大概是Java序列化机制的1&#x2F;10。所以Kryo序列化优化以后，可以让网络传输的数据变少；在集群中耗费的内存资源大大减少。<br>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream&#x2F;ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p><h6 id="Spark中使用Kryo："><a href="#Spark中使用Kryo：" class="headerlink" title="Spark中使用Kryo："></a>Spark中使用Kryo：</h6><pre><code class="scala">package cn.doitedu.yiee.serdeimport java.utilimport java.util.&#123;ArrayList, List&#125;import org.apache.spark.SparkConfimport org.apache.spark.serializer.KryoSerializerimport org.apache.spark.sql.SparkSessionobject SparkSerde &#123;  def main(args: Array[String]): Unit = &#123;    // spark中将对象序列化，默认调用都是jdk的objectoutputstream（serializable），效率低    // 所以，我们在spark代码中，一般都要修改序列化器，可以用kryo序列化框架    // kryo序列化框架的序列化结果要比jdk的序列化结果更精简（少了一些类的元信息）    val spark1 = SparkSession.builder.config(&quot;spark.serializer&quot;,classOf[KryoSerializer].getName).appName(&quot;&quot;).master(&quot;local&quot;).getOrCreate    import spark1.implicits._    spark1.createDataset(Seq(new Person(&quot;zz&quot;, 1888.8, 28)));    // 上面的做法，kryo在序列化时，还是会带上一些必要的类元信息，以便于下游task能正确反序列化    // 下面的做法，可以提前将这些可能要被序列化的类型，注册到kryo的映射表中，这样，kryo在序列化时就不需要序列化类元信息了    val conf = new SparkConf    conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)    conf.registerKryoClasses(Array(classOf[Person],classOf[Person2]))    val spark2 = SparkSession.builder()      .config(conf)      .master(&quot;local&quot;)      .appName(&quot;序列化案例&quot;)      .getOrCreate()  &#125;&#125;</code></pre><p><img src="/cdh/spark/spark/image-20210303165400048.png" alt="image-20210303165400048"></p><pre><code class="scala">Sparkconf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;).registerKryoClasses(new Class[]&#123;SpeedSortKey.class&#125;)</code></pre><h4 id="8）优化数据结构"><a href="#8）优化数据结构" class="headerlink" title="8）优化数据结构"></a>8）优化数据结构</h4><p>java中有<strong>三种类型比较消耗内存</strong>：<br>1)<strong>对象</strong>，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。<br>2)<strong>字符串</strong>，每个字符串内部都有一个字符数组以及长度等额外信息。<br>3)<strong>集合类型</strong>，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。<br>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，<strong>尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</strong></p><h4 id="9）使用高性能的库fastutil"><a href="#9）使用高性能的库fastutil" class="headerlink" title="9）使用高性能的库fastutil"></a>9）使用高性能的库fastutil</h4><h6 id="fasteutil介绍："><a href="#fasteutil介绍：" class="headerlink" title="fasteutil介绍："></a>fasteutil介绍：</h6><p>fastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue；fastutil能够提供更小的内存占用，更快的存取速度；<strong>我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set，好处在于，fastutil集合类，可以减小内存的占用</strong>，并且在进行集合的遍历、根据索引（或者key）获取元素的值和设置元素的值的时候，提供更快的存取速度。<strong>fastutil的每一种集合类型，都实现了对应的Java中的标准接口（比如fastutil的map，实现了Java的Map接口），因此可以直接放入已有系统的任何代码中。</strong><br>fastutil最新版本要求Java 7以及以上版本。</p><h6 id="使用："><a href="#使用：" class="headerlink" title="使用："></a>使用：</h6><p>见RandomExtractCars.java类</p><h3 id="4、数据本地化"><a href="#4、数据本地化" class="headerlink" title="4、数据本地化"></a>4、数据本地化</h3><h5 id="数据本地化的级别："><a href="#数据本地化的级别：" class="headerlink" title="数据本地化的级别："></a>数据本地化的级别：</h5><h6 id="1）PROCESS-LOCAL"><a href="#1）PROCESS-LOCAL" class="headerlink" title="1）PROCESS_LOCAL"></a>1）PROCESS_LOCAL</h6><p>task要计算的数据在本进程（Executor）的内存中。</p><p><img src="/cdh/spark/spark/image-20200729163015599.png" alt="image-20200729163015599"></p><h6 id="2）NODE-LOCAL"><a href="#2）NODE-LOCAL" class="headerlink" title="2）NODE_LOCAL"></a>2）NODE_LOCAL</h6><p>task所计算的数据在本节点所在的磁盘上。</p><p>task所计算的数据在本节点其他Executor进程的内存中。</p><p><img src="/cdh/spark/spark/image-20200729163301225.png" alt="image-20200729163301225"></p><h6 id="3）NO-PREF"><a href="#3）NO-PREF" class="headerlink" title="3）NO_PREF"></a>3）NO_PREF</h6><p>在哪里速度都一样，公共的地方，比如mysql</p><p><img src="/cdh/spark/spark/image-20200729163413792.png" alt="image-20200729163413792"></p><h6 id="4）RACK-LOCAL"><a href="#4）RACK-LOCAL" class="headerlink" title="4）RACK_LOCAL"></a>4）RACK_LOCAL</h6><p>task所计算的数据在同机架的不同节点的磁盘或者Executor进程的内存中</p><p><img src="/cdh/spark/spark/image-20200729163530720.png" alt="image-20200729163530720"></p><h6 id="5）ANY"><a href="#5）ANY" class="headerlink" title="5）ANY"></a>5）ANY</h6><p>跨机架。</p><h3 id="5、SparkShuffle类型调优"><a href="#5、SparkShuffle类型调优" class="headerlink" title="5、SparkShuffle类型调优"></a>5、SparkShuffle类型调优</h3><p><img src="/cdh/spark/spark/image-20200729163942925.png" alt="image-20200729163942925"></p><p><strong>Spark中任务调度时，TaskScheduler在分发之前需要依据数据的位置来分发，最好将task分发到数据所在的节点上，如果TaskScheduler分发的task在默认3s依然无法执行的话，TaskScheduler会重新发送这个task到相同的Executor中去执行，会重试5次，如果依然无法执行，那么TaskScheduler会降低一级数据本地化的级别再次发送task。</strong><br><strong>如上图中，会先尝试1,PROCESS_LOCAL数据本地化级别，如果重试5次每次等待3s,会默认这个Executor计算资源满了，那么会降低一级数据本地化级别到2，NODE_LOCAL,如果还是重试5次每次等待3s还是失败，那么还是会降低一级数据本地化级别到3，RACK_LOCAL。这样数据就会有网络传输，降低了执行效率。</strong></p><h5 id="1-如何提高数据本地化的级别？"><a href="#1-如何提高数据本地化的级别？" class="headerlink" title="1)如何提高数据本地化的级别？"></a>1)如何提高数据本地化的级别？</h5><p><strong>可以增加每次发送task的等待时间（默认都是3s），将3s倍数调大，结合WEBUI来调节：</strong><br>    • spark.locality.wait<br>    • spark.locality.wait.process<br>    • spark.locality.wait.node<br>    • spark.locality.wait.rack</p><p><strong>注意：等待时间不能调大很大，调整数据本地化的级别不要本末倒置，虽然每一个task的本地化级别是最高了，但整个Application的执行时间反而加长</strong>。</p><h5 id="2-如何查看数据本地化的级别？"><a href="#2-如何查看数据本地化的级别？" class="headerlink" title="2)    如何查看数据本地化的级别？"></a>2)    如何查看数据本地化的级别？</h5><p>通过日志或者WEBUI</p><h4 id="Spark的Shuffle管理类型"><a href="#Spark的Shuffle管理类型" class="headerlink" title="Spark的Shuffle管理类型"></a>Spark的Shuffle管理类型</h4><p><strong>SparkShuffle：</strong></p><p><strong>spark1.x 中有 两种类型的shuffle （hashShuffleManager  另外一个是sortShuffleManager）</strong></p><p><strong>到spark2.x以后  只有一种shuffle 机制  SortShuffle  管理器叫做SortShuffleManager</strong>  </p><h5 id="SparkShuffle概念"><a href="#SparkShuffle概念" class="headerlink" title="SparkShuffle概念"></a>SparkShuffle概念</h5><p>reduceByKey会将上一个RDD中的每一个key对应的所有value聚合成一个value，然后生成一个新的RDD，元素类型是&lt;key,value&gt;对的形式，这样每一个key对应一个聚合起来的value。<br>问题：聚合之前，每一个key对应的value不一定都是在一个partition中，也不太可能在同一个节点上，因为RDD是分布式的弹性的数据集，RDD的partition极有可能分布在各个节点上。</p><p><strong>如何聚合？</strong></p><p><strong>– Shuffle Write：</strong>上一个stage的每个map task就必须保证将自己处理的当前分区的数据相同的key写入一个分区文件中，可能会写入多个不同的分区文件中。</p><p> <strong>– Shuffle Read：</strong>reduce task就会从上一个stage的所有task所在的机器上寻找属于己的那些分区文件，这样就可以保证每一个key所对应的value都会汇聚到同一个节点上去处理和聚合。</p><p>Spark中有两种Shuffle管理类型，HashShufflManager和SortShuffleManager，Spark1.2之前是HashShuffleManager， Spark1.2引入SortShuffleManager,在Spark 2.0+版本中已经将HashShuffleManager丢弃。</p><h5 id="1、HashShuffleManager"><a href="#1、HashShuffleManager" class="headerlink" title="1、HashShuffleManager"></a>1、HashShuffleManager</h5><h6 id="1）普通机制"><a href="#1）普通机制" class="headerlink" title="1）普通机制"></a>1）普通机制</h6><p><img src="/cdh/spark/spark/image-20200729165321639.png" alt="image-20200729165321639"></p><p><strong>执行流程</strong><br>a)每一个map task将不同结果写到不同的buffer中，每个buffer的大小为32K。buffer起到数据缓存的作用。<br>b)每个buffer文件最后对应一个磁盘小文件。<br>c)reduce task来拉取对应的磁盘小文件。</p><p><strong>总结</strong></p><p>①.map task的计算结果会根据分区器（默认是hashPartitioner）来决定写入到哪一个磁盘小文件中去。ReduceTask会去Map端拉取相应的磁盘小文件。<br>②.产生的磁盘小文件的个数：<br>M（map task的个数）*R（reduce task的个数）</p><p><strong>存在的问题</strong><br>产生的磁盘小文件过多，会导致以下问题：<br>a)在Shuffle Write过程中会产生很多写磁盘小文件的对象。<br>b)在Shuffle Read过程中会产生很多读取磁盘小文件的对象。<br>c)在JVM堆内存中对象过多会造成频繁的gc,gc还无法解决运行所需要的内存 的话，就会OOM。<br>d)在数据传输过程中会有频繁的网络通信，频繁的网络通信出现通信故障的可能性大大增加，一旦网络通信出现了故障会导致shuffle file cannot find 由于这个错误导致的task失败，TaskScheduler不负责重试，由DAGScheduler负责重试Stage。</p><h6 id="2）合并机制-considation机制"><a href="#2）合并机制-considation机制" class="headerlink" title="2）合并机制(considation机制)"></a>2）合并机制(considation机制)</h6><p><img src="/cdh/spark/spark/image-20200729165925527.png" alt="image-20200729165925527"></p><p><strong>总结</strong></p><p>产生磁盘小文件的个数：C(core的个数)*R（reduce的个数)</p><h5 id="2、-SortShuffleManager"><a href="#2、-SortShuffleManager" class="headerlink" title="2、 SortShuffleManager"></a>2、 SortShuffleManager</h5><h6 id="1）普通机制-1"><a href="#1）普通机制-1" class="headerlink" title="1）普通机制"></a>1）普通机制</h6><p><img src="/cdh/spark/spark/image-20200729170342023.png" alt="image-20200729170342023"></p><p><strong>执行流程</strong><br>a)map task 的计算结果会写入到一个内存数据结构里面，内存数据结构默认是5M<br>b)在shuffle的时候会有一个定时器，不定期的去估算这个内存结构的大小，当内存结构中的数据超过5M时，比如现在内存结构中的数据为5.01M，那么他会申请5.01*2-5&#x3D;5.02M内存给内存数据结构。<br>c)如果申请成功不会进行溢写，如果申请不成功，这时候会发生溢写磁盘。<br>d)在溢写之前内存结构中的数据会进行排序分区<br>e)然后开始溢写磁盘，写磁盘是以batch的形式去写，一个batch是1万条数据，<br>f)map task执行完成后，会将这些磁盘小文件合并成一个大的磁盘文件，同时生成一个索引文件。<br>g)reduce task去map端拉取数据的时候，首先解析索引文件，根据索引文件再去拉取对应的数据。<br><strong>总结</strong><br>产生磁盘小文件的个数： 2*M（map task的个数）</p><h6 id="2）bypass机制"><a href="#2）bypass机制" class="headerlink" title="2）bypass机制"></a>2）bypass机制</h6><p><img src="/cdh/spark/spark/image-20200729170502993.png" alt="image-20200729170502993"></p><p><strong>总结</strong><br>①.bypass运行机制的触发条件如下：<br>shuffle reduce task的数量小于spark.shuffle.sort.bypassMergeThreshold的参数值。这个值默认是200。<br>②.产生的磁盘小文件为：2*M（map task的个数）</p><h5 id="Shuffle文件寻址"><a href="#Shuffle文件寻址" class="headerlink" title="Shuffle文件寻址"></a>Shuffle文件寻址</h5><h6 id="1-MapOutputTracker"><a href="#1-MapOutputTracker" class="headerlink" title="1)   MapOutputTracker"></a>1)   MapOutputTracker</h6><p>MapOutputTracker是Spark架构中的一个模块，是一个主从架构。管理磁盘小文件的地址。</p><p>MapOutputTrackerMaster是主对象，存在于Driver中。</p><p>MapOutputTrackerWorker是从对象，存在于Excutor中。</p><h6 id="2-BlockManager"><a href="#2-BlockManager" class="headerlink" title="2)BlockManager"></a>2)BlockManager</h6><p>BlockManager块管理者，是Spark架构中的一个模块，也是一个主从架构。<br>BlockManagerMaster,主对象，存在于Driver中。<br>BlockManagerMaster会在集群中有用到广播变量和缓存数据或者删除缓存数据的时候，通知BlockManagerSlave传输或者删除数据。<br>BlockManagerSlave，从对象，存在于Excutor中。<br>BlockManagerSlave会与BlockManagerSlave之间通信。<br>无论在Driver端的BlockManager还是在Excutor端的BlockManager都含有三个对象：<br>①<strong>DiskStore</strong>:负责磁盘的管理。<br>②<strong>MemoryStore</strong>：负责内存的管理。<br>③<strong>BlockTransferService</strong>:负责数据的传输。</p><h3 id="6、Spark内存调优"><a href="#6、Spark内存调优" class="headerlink" title="6、Spark内存调优"></a>6、Spark内存调优</h3><h5 id="两种内存管理方案"><a href="#两种内存管理方案" class="headerlink" title="两种内存管理方案"></a>两种内存管理方案</h5><p>静态内存管理</p><p>动态内存管理</p><p>C:\Users\Administrator\Desktop\大数据总结\面试文件夹\spark2.x内存管理方案.pdf</p><h6 id="reduce中OOM如何处理？"><a href="#reduce中OOM如何处理？" class="headerlink" title="reduce中OOM如何处理？"></a><strong>reduce中OOM如何处理？</strong></h6><ol><li><pre><code>减少每次拉取的数据量</code></pre><ol start="2"><li><pre><code>提高shuffle聚合的内存比例</code></pre><ol start="3"><li><pre><code>提高Excutor的总内存</code></pre></li></ol></li></ol></li></ol><h6 id="调节Executor的堆外内存"><a href="#调节Executor的堆外内存" class="headerlink" title="调节Executor的堆外内存"></a>调节Executor的堆外内存</h6><p>Spark底层shuffle的传输方式是使用netty传输，netty在进行网络传输的过程会申请堆外内存（netty是零拷贝），所以使用了堆外内存。默认情况下，这个堆外内存上限默认是每一个executor的内存大小的10%；真正处理大数据的时候，这里都会出现问题，导致spark作业反复崩溃，无法运行；此时就会去调节这个参数，到至少1G（1024M），甚至说2G、4G。</p><p>executor在进行shuffle write，优先从自己本地关联的mapOutPutWorker中获取某份数据，如果本地block manager没有的话，那么会通过TransferService，去远程连接其他节点上executor的block manager去获取，尝试建立远程的网络连接，并且去拉取数据。频繁创建对象让JVM堆内存满溢，进行垃圾回收。正好碰到那个exeuctor的JVM在垃圾回收。处于垃圾回过程中，所有的工作线程全部停止；相当于只要一旦进行垃圾回收，spark &#x2F; executor停止工作，无法提供响应，spark默认的网络连接的超时时长是60s；如果卡住60s都无法建立连接的话，那么这个task就失败了。<strong>task失败了就会出现shuffle file cannot find的错误。</strong></p><h3 id="7、数据倾斜"><a href="#7、数据倾斜" class="headerlink" title="7、数据倾斜"></a>7、数据倾斜</h3><h6 id="1）提高shuffle的read-task的并行度"><a href="#1）提高shuffle的read-task的并行度" class="headerlink" title="1）提高shuffle的read task的并行度"></a>1）提高shuffle的read task的并行度</h6><p>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p><p><strong>方案实现原理：</strong></p><p>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个不同的key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。</p><h6 id="2-双重聚合-双MR的道理"><a href="#2-双重聚合-双MR的道理" class="headerlink" title="2)双重聚合(双MR的道理)"></a>2)双重聚合(双MR的道理)</h6><p><strong>方案适用场景</strong>：<br>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。<br><strong>方案实现思路：</strong><br>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。<br><strong>方案实现原理：</strong><br>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。</p><h6 id="3-mapjoin"><a href="#3-mapjoin" class="headerlink" title="3)mapjoin"></a>3)mapjoin</h6><p><strong>方案适用场景</strong>：</p><p>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><p><strong>方案实现思路：</strong></p><p>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p><p><strong>方案实现原理：</strong></p><p>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。</p><h6 id="4-采样倾斜key并分拆join操作"><a href="#4-采样倾斜key并分拆join操作" class="headerlink" title="4)采样倾斜key并分拆join操作"></a>4)采样倾斜key并分拆join操作</h6><p><strong>方案适用场景：</strong></p><p>两个RDD&#x2F;Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD&#x2F;Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD&#x2F;Hive表中的少数几个key的数据量过大，而另一个RDD&#x2F;Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p><p><strong>方案实现思路：</strong></p><p>对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。而另外两个普通的RDD就照常join即可。最后将两次join的结果使用union算子合并起来即可，就是最终的join结果</p><p><img src="/cdh/spark/spark/image-20200801163326016.png" alt="image-20200801163326016"></p><h6 id="5-一个随机前缀一个扩容RDD进行join"><a href="#5-一个随机前缀一个扩容RDD进行join" class="headerlink" title="5)一个随机前缀一个扩容RDD进行join"></a>5)一个随机前缀一个扩容RDD进行join</h6><p><strong>方案适用场景：</strong><br>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。<br><strong>方案实现思路：</strong><br>首先查看RDD&#x2F;Hive表中的数据分布情况，找到那个造成数据倾斜的RDD&#x2F;Hive表，比如有多个key都对应了超过1万条数据。然后将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。</p><p><img src="/cdh/spark/spark/image-20200801163730390.png" alt="image-20200801163730390"></p><h3 id="8、算子调优"><a href="#8、算子调优" class="headerlink" title="8、算子调优"></a>8、算子调优</h3><h5 id="1-mapPartitions算子"><a href="#1-mapPartitions算子" class="headerlink" title="1)mapPartitions算子"></a>1)mapPartitions算子</h5><p>mapPartitions算子中的数据是iterator<br>尽量使用mapPartitions而不是map算子  比如按照数据中的某个字段查询mysql中的信息，尽量使用这个算子减少connection的创建次数</p><p>比如批次处理数据数据量会很大，那么这个数据很可能产生内存溢出，map算子随便遍历次数比较多，但是能够一个一个处理</p><h5 id="2-foreachPartition算子"><a href="#2-foreachPartition算子" class="headerlink" title="2)foreachPartition算子"></a>2)foreachPartition算子</h5><p>使用场景一般都是在链接数据库并且写出数据的时候</p><h5 id="3-filter算子在使用的时候最好加上coalesce算子"><a href="#3-filter算子在使用的时候最好加上coalesce算子" class="headerlink" title="3)filter算子在使用的时候最好加上coalesce算子"></a>3)filter算子在使用的时候最好加上coalesce算子</h5><p>因为数据减少很多，分区中的数据不需要一个单独的线程进行处理会浪费资源，coalsec算子进行分区的合并，数据合并的时候可能出现数据倾斜<br>1.可以先找出数据的一个分部情况<br>2.如果倾斜的比例比较多 repartition使用shuffle流程操作<br>分区合并的情况<br>多数分区合并为少数分区，合并的比例比较大 coalsce算子<br>数据的量的差别比较大 shuffle–&gt;repartition<br>某一个单独的分区数据量特别大，其他的都比较小<br>扩容分区repartition</p><h5 id="4-sparksql的并行度设置和其他普通阶段的并行度不关联"><a href="#4-sparksql的并行度设置和其他普通阶段的并行度不关联" class="headerlink" title="4)sparksql的并行度设置和其他普通阶段的并行度不关联"></a>4)sparksql的并行度设置和其他普通阶段的并行度不关联</h5><p>SparkSQL并行度是SparkSQL的第一个调优点，默认的并行度是200，需要根据实际情况进行设置，它有有两种设置方法</p><pre><code>val spark = SparkSession.builder()      .config(&quot;spark.sql.shuffle.partitions&quot;,100)//设置并行度100      .getOrCreate()2.提交任务时指定--conf spark.sql.shuffle.partitions=100 \</code></pre><h5 id="5-reduceByKey算子，相当于存在一个map端的reduce也就是combiner"><a href="#5-reduceByKey算子，相当于存在一个map端的reduce也就是combiner" class="headerlink" title="5)reduceByKey算子，相当于存在一个map端的reduce也就是combiner"></a>5)reduceByKey算子，相当于存在一个map端的reduce也就是combiner</h5><p>1.本地聚合后在map端的数据量减少很多<br>2.shuffle read阶段复制的数据要减少<br>3.shuffleRead端的内存使用减少<br>4.map端存在聚合所有reduce端聚合数据减少<br>groupByKey和reduceBykey的区别</p><h3 id="9、故障处理"><a href="#9、故障处理" class="headerlink" title="9、故障处理"></a>9、故障处理</h3><h5 id="一、shuffle-file-cannot-find：磁盘小文件找不到"><a href="#一、shuffle-file-cannot-find：磁盘小文件找不到" class="headerlink" title="一、shuffle file cannot find：磁盘小文件找不到"></a>一、shuffle file cannot find：磁盘小文件找不到</h5><h6 id="1-connection-timeout-—-shuffle-file-cannot-find"><a href="#1-connection-timeout-—-shuffle-file-cannot-find" class="headerlink" title="1)  connection timeout —-shuffle file cannot find"></a>1)  connection timeout —-shuffle file cannot find</h6><p>提高建立连接的超时时间，或者降低gc，降低gc了那么spark不能堆外提供服务的时间就少了，那么超时的可能就会降低。</p><h6 id="2-fetch-data-fail-—-shuffle-file-cannot-find"><a href="#2-fetch-data-fail-—-shuffle-file-cannot-find" class="headerlink" title="2)  fetch data fail —- shuffle file cannot find"></a>2)  fetch data fail —- shuffle file cannot find</h6><p>提高拉取数据的重试次数以及间隔时间。</p><h6 id="3-OOM-x2F-executor-lost-—-shuffle-file-cannot-find"><a href="#3-OOM-x2F-executor-lost-—-shuffle-file-cannot-find" class="headerlink" title="3)  OOM&#x2F;executor lost —- shuffle file cannot find"></a>3)  OOM&#x2F;executor lost —- shuffle file cannot find</h6><p>提高堆外内存大小，提高堆内内存大小。</p><h5 id="二、reduce-OOM"><a href="#二、reduce-OOM" class="headerlink" title="二、reduce OOM"></a>二、reduce OOM</h5><p>BlockManager拉取的数据量大，reduce task处理的数据量小</p><p>解决方法：</p><ol><li><p>降低每次拉取的数据量</p></li><li><p>提高shuffle聚合的内存比例</p></li><li><p>提高Executor的内存比例</p></li></ol><h5 id="三、序列化问题"><a href="#三、序列化问题" class="headerlink" title="三、序列化问题"></a>三、序列化问题</h5><h5 id="四、Null值问题"><a href="#四、Null值问题" class="headerlink" title="四、Null值问题"></a>四、Null值问题</h5><pre><code class="scala">val rdd = rdd.map&#123;x=&gt;&#123;    x+”~”;&#125;&#125;rdd.foreach&#123;x=&gt;&#123;    System.out.println(x.getName())&#125;&#125;</code></pre><h2 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h2><h3 id="1、wordcount中产生几个RDD？"><a href="#1、wordcount中产生几个RDD？" class="headerlink" title="1、wordcount中产生几个RDD？"></a>1、wordcount中产生几个RDD？</h3><p>产生6个RDD</p><pre><code>val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;aaa&quot;)val sc = new SparkContext(conf)val rdd = sc.textfile(&quot;tt.log&quot;).map(_._2,1).flatMap((a,b)=&gt;(a._1,a._2+b._2)).reducebykey()rdd.saveTextFile()</code></pre><p><img src="/cdh/spark/spark/image-20200726164017855.png" alt="image-20200726164017855"></p><p>textFile产生两个rdd：</p><p><img src="/cdh/spark/spark/image-20200726165402164.png" alt="image-20200726165402164"></p><p><img src="/cdh/spark/spark/image-20200726165419690.png" alt="image-20200726165419690"></p><p>在hadoopRDD上面去除掉key,RDD.map(_._2)</p><p><img src="/cdh/spark/spark/image-20200726165529217.png" alt="image-20200726165529217"></p><p>flatMap算子中产生一个RDD</p><p><img src="/cdh/spark/spark/image-20200726165545560.png" alt="image-20200726165545560"></p><p>map方法产生一个rdd</p><p><img src="/cdh/spark/spark/image-20200726165559770.png" alt="image-20200726165559770"></p><p>reducebykey产生shuffledRDD</p><p><img src="/cdh/spark/spark/image-20200726165616080.png" alt="image-20200726165616080"></p><p>saveAsTextFile保存数据的算子产生一个新的RDD</p><p><img src="/cdh/spark/spark/image-20200726165626808.png" alt="image-20200726165626808"></p><h3 id="2、dag在哪里构建的？"><a href="#2、dag在哪里构建的？" class="headerlink" title="2、dag在哪里构建的？"></a>2、dag在哪里构建的？</h3><p>driver</p><h3 id="3、rdd在哪段生成的？"><a href="#3、rdd在哪段生成的？" class="headerlink" title="3、rdd在哪段生成的？"></a>3、rdd在哪段生成的？</h3><p>drive</p><h3 id="4、调用rdd的算子在哪里"><a href="#4、调用rdd的算子在哪里" class="headerlink" title="4、调用rdd的算子在哪里?"></a>4、调用rdd的算子在哪里?</h3><p>driver</p><h3 id="5、rdd中调用的算子中的函数在哪里"><a href="#5、rdd中调用的算子中的函数在哪里" class="headerlink" title="5、rdd中调用的算子中的函数在哪里"></a>5、rdd中调用的算子中的函数在哪里</h3><p>executor</p><h3 id="6、dag的切分"><a href="#6、dag的切分" class="headerlink" title="6、dag的切分"></a>6、dag的切分</h3><p>dagschecutor</p><h3 id="7、自定义的分区器，在哪里实例化的"><a href="#7、自定义的分区器，在哪里实例化的" class="headerlink" title="7、自定义的分区器，在哪里实例化的"></a>7、自定义的分区器，在哪里实例化的</h3><p>drive</p><h3 id="8、简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系-（笔试重点）"><a href="#8、简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系-（笔试重点）" class="headerlink" title="8、简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系? （笔试重点）"></a>8、简述SparkSQL中RDD、DataFrame、DataSet三者的区别与联系? （笔试重点）</h3><p>1）RDD</p><p>优点:</p><p>编译时类型安全 </p><p>编译时就能检查出类型错误</p><p>面向对象的编程风格 </p><p>直接通过类名点的方式来操作数据</p><p>缺点:</p><p>序列化和反序列化的性能开销 </p><p>无论是集群间的通信, 还是IO操作都需要对对象的结构和数据进行序列化和反序列化。</p><p>GC的性能开销，频繁的创建和销毁对象, 势必会增加GC</p><p>2）DataFrame</p><p>DataFrame引入了schema和off-heap</p><p>schema : RDD每一行的数据, 结构都是一样的，这个结构就存储在schema中。 Spark通过schema就能够读懂数据, 因此在通信和IO时就只需要序列化和反序列化数据, 而结构的部分就可以省略了。</p><p>3）DataSet</p><p>DataSet结合了RDD和DataFrame的优点，并带来的一个新的概念Encoder。</p><p>当序列化数据时，Encoder产生字节码与off-heap进行交互，能够达到按需访问数据的效果，而不用反序列化整个对象。Spark还没有提供自定义Encoder的API，但是未来会加入。</p><p>DataSet &lt; Row&gt;就是DataFrame</p><h3 id="9、Spark消费kafka数据如何保证exactly-once语义？"><a href="#9、Spark消费kafka数据如何保证exactly-once语义？" class="headerlink" title="9、Spark消费kafka数据如何保证exactly-once语义？"></a>9、Spark消费kafka数据如何保证exactly-once语义？</h3><p>可以在处理完毕数据后在进行提交offset</p><p>可以保存checkpoint，再失败的时候，读取这个文件中的offset，再次进行消费</p><h3 id="10、spark和mr的区别"><a href="#10、spark和mr的区别" class="headerlink" title="10、spark和mr的区别"></a>10、spark和mr的区别</h3><p>spark最核心的概念是RDD（弹性分布式数据集），它的所有rdd在并行运算过程程中，可以做到数据共享，也就是可以重复使用mr在计算过程中</p><p>1、spark把运算的中间数据存放在内存，迭代计算效率更高，Spark中除了基于内存计算外，还有DAG有向无环图来切分任务的执行先后顺序；mapreduce的中间结果需要落地，需要保存到磁盘，这样必然会有磁盘io操做，影响性能。</p><p>2、spark容错性高，它通过弹性分布式数据集RDD来实现高效容错，RDD是一组分布式的存储在节点内存中的只读性质的数据集，这些集合是弹性的，某一部分丢失或者出错，可以通过整个数据集的计算流程的血缘关系来实现重建；mapreduce的话容错可能只能重新计算了，成本较高。</p><p> 3、spark更加通用，spark提供了transformation和action这两大类的多个功能api，另外还有流式处理sparkstreaming模块、图计算GraphX等等；mapreduce只提供了map和reduce两种操作，流计算以及其他模块的支持比较缺乏。</p><p> 4、spark框架和生态更为复杂，首先有RDD、血缘lineage、执行时的有向无环图DAG、stage划分等等，很多时候spark作业都需要根据不同业务场景的需要进行调优已达到性能要求；mapreduce框架及其生态相对较为简单，对性能的要求也相对较弱，但是运行较为稳定，适合长期后台运行。</p><h3 id="11、SparkStreaming-on-Kafka-Direct与Receiver-的对比："><a href="#11、SparkStreaming-on-Kafka-Direct与Receiver-的对比：" class="headerlink" title="11、SparkStreaming on Kafka Direct与Receiver 的对比："></a>11、SparkStreaming on Kafka Direct与Receiver 的对比：</h3><p>Receiver 是通过Kafka中高层次的消费者API连续不断地从Kafka中读取数据</p><p>接收到的数据被存储在Spark workers&#x2F;executors中的内存，同时也被写入到WAL中，在处理完毕后，才会发送偏移量到zookeeper中更新offset</p><p>接收到的数据和WAL存储位置信息被可靠地存储，如果期间出现故障，这些信息被用来从错误中恢复，并继续处理数据。 </p><p>这个方法可以保证从Kafka接收的数据不被丢失。但是在失败的情况下，有些数据很有<strong>可能会被处理不止一次</strong>！这种情况在一些接收到的数据被可靠地保存到WAL中，但是还没有来得及更新Zookeeper中Kafka偏移量，系统出现故障的情况下发生。这导致数据出现不一致性：Spark Streaming知道数据被接收，但是Kafka那边认为数据还没有被接收，这样在系统恢复正常时，Kafka会再一次发送这些数据。</p><p>Direct</p><p>0.10之后只有Direct没有Receiver</p><p>简单地给出每个batch区间需要读取的偏移量位置，最后，每个batch的Job被运行，那些对应偏移量的数据在Kafka中已经准备好了。这些偏移量信息也被可靠地存储（checkpoint），在从失败中恢复可以直接读取这些偏移量信息。</p><h3 id="12、spark的部署模式"><a href="#12、spark的部署模式" class="headerlink" title="12、spark的部署模式"></a>12、spark的部署模式</h3><p>1）Local:运行在一台机器上，通常是练手或者测试环境。</p><p>2）Standalone:构建一个基于Mster+Slaves的资源调度集群，Spark任务提交给Master运行。是Spark自身的一个调度系统。</p><p>3）Yarn: Spark客户端直接连接Yarn，不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。</p><p>4）Mesos：国内大环境比较少用。</p><h3 id="13、Spark的架构与作业提交流程"><a href="#13、Spark的架构与作业提交流程" class="headerlink" title="13、Spark的架构与作业提交流程"></a>13、Spark的架构与作业提交流程</h3><h3 id="14、宽窄依赖的算子每个说5个"><a href="#14、宽窄依赖的算子每个说5个" class="headerlink" title="14、宽窄依赖的算子每个说5个"></a>14、宽窄依赖的算子每个说5个</h3><p>宽依赖：groupby，groupbykey，reducebykey，take，first等等</p><p>窄依赖：map，mappartitoin，filter，flatmap，union</p><h3 id="15、Spark-Shuffle原理？"><a href="#15、Spark-Shuffle原理？" class="headerlink" title="15、Spark Shuffle原理？"></a>15、Spark Shuffle原理？</h3><p>不同RDD触发数据重新分发，map task 将数据中相同的key写入一个分区文件中(会写入多个分区文件中)，reduce task 从上一个stage阶段的task 机器上找到对应的分区文件，然后汇聚在同一个节点执行。</p><p>不同的管理机制看上面的<code>Spark的Shuffle管理类型</code></p><h3 id="16、spark-shuffle-几种？有什么区别-排序不排序有什么区别？"><a href="#16、spark-shuffle-几种？有什么区别-排序不排序有什么区别？" class="headerlink" title="16、spark shuffle 几种？有什么区别?排序不排序有什么区别？"></a>16、spark shuffle 几种？有什么区别?排序不排序有什么区别？</h3><p>三种：hash join、boastcast hashjoin、sortmerge join</p><h3 id="17、Shuffle产生文件个数有什么关系？倍数什么关系？"><a href="#17、Shuffle产生文件个数有什么关系？倍数什么关系？" class="headerlink" title="17、Shuffle产生文件个数有什么关系？倍数什么关系？"></a>17、Shuffle产生文件个数有什么关系？倍数什么关系？</h3><p>hashshufflemanager：Map*Reduce  个文件数</p><p>优化后的：                     Core*Reduce   个文件数</p><p>这里的2是：索引文件+磁盘文件</p><p>SortShuffleManager：    2*Reduce  个文件数</p><p>bypass：                            2*Reduce  个文件数    （他只是不需要排序了）</p><h3 id="18、Spark-Sql数据导入产生大量小文件？"><a href="#18、Spark-Sql数据导入产生大量小文件？" class="headerlink" title="18、Spark Sql数据导入产生大量小文件？"></a>18、Spark Sql数据导入产生大量小文件？</h3><p>对原始数据按照分区进行shuffle，但是可能数据倾斜。</p><p>可以distribute by 指定分发字段或者 rand() 均匀分发数据。高版本spark3.0以上可以自动合并小文件。也可以使用repartition 和其他改变分区的算子。</p><h3 id="19、Spark数据倾斜"><a href="#19、Spark数据倾斜" class="headerlink" title="19、Spark数据倾斜"></a>19、Spark数据倾斜</h3><p>增大分区个数，相当于增加reduce个数。</p><p>可以通过repartition、coalesce 等算子增加分区数。</p><p>自定义分区规则</p><p>对数据添加随机值，实现均匀分配。</p><p>通过distribute by 定义分发的规则。distribute by rand()</p><p>单个key数据过多，通过样例采集，获取倾斜的key进行单独处理。然后union 结果。</p><h3 id="sparksql-的执行流程和-spark的执行流程有什么不同"><a href="#sparksql-的执行流程和-spark的执行流程有什么不同" class="headerlink" title="sparksql 的执行流程和 spark的执行流程有什么不同"></a>sparksql 的执行流程和 spark的执行流程有什么不同</h3>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDH </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala学习笔记</title>
      <link href="/scala/scala-suan-zi/"/>
      <url>/scala/scala-suan-zi/</url>
      
        <content type="html"><![CDATA[<p>sorted,sortWith,sortBy</p><h4 id="sorted"><a href="#sorted" class="headerlink" title="sorted"></a>sorted</h4><p>sorted方法真正排序的逻辑是调用的java.util.Arrays.sort。</p><h4 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h4><p>提供一个比较的属性</p><pre><code>def sortBy[B](f: A =&gt; B)(implicit ord: Ordering[B]): Repr = sorted(ord on f)</code></pre><h4 id="sortWith"><a href="#sortWith" class="headerlink" title="sortWith"></a>sortWith</h4><p>传入一个比较方法来比较（A,A） &#x3D;&gt; Boolean</p><h4 id="map和tuple都可以使用-1-2来进行获取值"><a href="#map和tuple都可以使用-1-2来进行获取值" class="headerlink" title="map和tuple都可以使用._1,._2来进行获取值"></a><strong>map和tuple都可以使用._1,._2来进行获取值</strong></h4><h4 id="union并集-intersect交集-diff差集"><a href="#union并集-intersect交集-diff差集" class="headerlink" title="union并集 intersect交集 diff差集"></a><strong>union并集 intersect交集 diff差集</strong></h4><p>并集 ，是集合的全部内容</p><p>交集   是集合的共有元素</p><p>差集  var arr &#x3D; (1,2,3) var arr1 &#x3D; (4,5,6)</p><p>arr diff arr1  &#x3D; (1,2)</p><h4 id="count方法"><a href="#count方法" class="headerlink" title="count方法"></a>count方法</h4><p>count是计数，需要传入条件，把符合条件的元素进行计数返回Int</p><pre><code>scala&gt; arr.count(_%2==0)res7: Int = 1</code></pre><h4 id="find方法"><a href="#find方法" class="headerlink" title="find方法"></a>find方法</h4><p>find方法查询返回符合条件的第一个元素，并放入some中</p><pre><code>Array[Int] = Array(1, 2, 3)arr.find(_&gt;1)//2arr.find(_&gt;2)//3</code></pre><h4 id="Range步长控制"><a href="#Range步长控制" class="headerlink" title="Range步长控制"></a>Range步长控制</h4><pre><code>0 to (10,2)0 until (10,2)0 until 10 by 2</code></pre><h4 id="mapValues方法"><a href="#mapValues方法" class="headerlink" title="mapValues方法"></a>mapValues方法</h4><p>这个方法<strong>在scala中只能作用于map集合的value上</strong></p><p>只要涉及到分组，那么一般我们都不会改key，一般只能操作value</p><pre><code>res32.mapValues(t=&gt;t.map(_._2).sum/t.length)res34: scala.collection.immutable.Map[String,Double] = Map(gz -&gt; 32.43333333333333, bj -&gt; 27.86666666666667, sz -&gt; 31.8, sh -&gt; 29.3)</code></pre><h4 id="reduce方法"><a href="#reduce方法" class="headerlink" title="reduce方法"></a>reduce方法</h4><p>reduce聚合函数 </p><p>传入的处理方法必须是两个参数，参数类型一致，返回值类型和传入的参数类型一致，递归调用</p><h4 id="groupBy-方法"><a href="#groupBy-方法" class="headerlink" title="groupBy()方法"></a>groupBy()方法</h4><p>按照什么分组，返回map集合</p><h4 id="asInstanceOf方法"><a href="#asInstanceOf方法" class="headerlink" title="asInstanceOf方法"></a>asInstanceOf方法</h4><p>类型转换</p><h4 id="isInstanceOf方法"><a href="#isInstanceOf方法" class="headerlink" title="isInstanceOf方法"></a>isInstanceOf方法</h4><p>判断类型</p><h4 id="样例类"><a href="#样例类" class="headerlink" title="样例类"></a>样例类</h4><p>case class xxx{}</p><p>实现类序列化的接口，重写了一些方法，模式匹配更加方便，此外和其他的类一样</p><h4 id="偏函数"><a href="#偏函数" class="headerlink" title="偏函数"></a>偏函数</h4><p>使用只把符合条件的执行语句,没有返回值</p><p><strong>PartialFunction</strong>[输入类型，输出类型]&#x3D;{</p><p>​case i:Int &#x3D;&gt; i+1</p><p>}</p><h4 id="AKKA"><a href="#AKKA" class="headerlink" title="AKKA"></a>AKKA</h4><p>是一个邮件系统</p><p>有中央系统，注册系统，</p><h4 id="项目开发流程"><a href="#项目开发流程" class="headerlink" title="项目开发流程"></a>项目开发流程</h4><p>0、可行性分析(干什么，谁来干？)</p><p>1、需求分析（需求分析师，分析报告）</p><p>2、设计阶段（架构师，什么架构，什么计数，什么系统，设计文档，界面）</p><p>3、实现阶段（软件工程师，看懂文档，实现模块）</p><p>4、测试阶段(测试工程师，一边开发，一边测试，白盒，黑盒，灰盒)</p><p>5、实施阶段（实施工程师，部署到系统上，正确运行）</p><p>6、维护阶段（不一定有专人，没人找经理）</p><h5 id="设计流程"><a href="#设计流程" class="headerlink" title="设计流程"></a>设计流程</h5><p>设计从上高下，编写从下到上</p><p><img src="/scala/scala-suan-zi/image-20200310162230014.png" alt="image-20200310162230014"></p>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala学习笔记</title>
      <link href="/scala/scala/"/>
      <url>/scala/scala/</url>
      
        <content type="html"><![CDATA[<h3 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h3><p>Scala是一门以<strong>java虚拟机为运行环境</strong>，并将面向对象和函数式编程最佳特性结合在一起的<strong>静态类型编程语言</strong></p><p>在大数据中</p><p>海量数据的采集、存储、计算</p><p>存储hbase</p><p>计算mapreduce</p><p>java编写完成后，由javac编译成.class文件，在jvm上运行，在java的类库中(jdk)找到对应源码，然后运行</p><p><img src="/scala/scala/image-20200214154841642.png" alt="image-20200214154841642"></p><p>Scala可以使用java的部分语法 </p><p>scala有自己的特有语法</p><p>val map &#x3D; Map((“”,””),(“”,””))</p><p>同时还增强了一些功能，函数式编程</p><p>偏函数</p><p>函数的柯里化</p><p>将函数作为参数传递</p><p>而Scala在拥有特有的类库，还对java的类进行了包装 </p><p>在运行时，通过scalac编译器运行</p><h4 id="Scala的安装"><a href="#Scala的安装" class="headerlink" title="Scala的安装"></a>Scala的安装</h4><p>下载好tgz包，然后传到虚拟机端，</p><p>修改&#x2F;etc&#x2F;profile文件</p><pre><code>export JAVA_HOME=/root/Downloads/jdk1.8.0_161export PATH=$PATH:$JAVA_HOME/binexport HADOOP_HOME=/root/Downloads/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport ZOOKEEPER_HOME=/root/Downloads/zookeeper-3.4.5export PATH=$PATH:$ZOOKEEPER_HOME/binexport HIVE_HOME=/root/Downloads/apache-hive-2.1.1-binexport PATH=$PATH:$HIVE_HOME/binexport FLUME_HOME=/root/Downloads/apache-flume-1.6.0-binexport PATH=$PATH:$FLUME_HOME/bin export HBASE_HOME=/root/Downloads/hbase-1.2.6export PATH=$PATH:$HBASE_HOME/binexport SQOOP_HOME=/root/Downloads/sqoop-1.4.6.bin__hadoop-2.0.4-alphaexport PATH=$PATH:$SQOOP_HOME/binexport SCALA_HOME=/install/scala/scala-2.11.8export PATH=$PATH:$SCALA_HOME/bin</code></pre><p>修改完成后，记得source 一下</p><h4 id="Scala的执行流程"><a href="#Scala的执行流程" class="headerlink" title="Scala的执行流程"></a>Scala的执行流程</h4><p>在写好Scala类时，scalac先进行编译成两个class文件，类名.class和类名$.class文件，然后加载到jvm上运行</p><p>先从类名.class中执行，执行main方法，调用类名$.class中的main方法，调用module方法执行主体</p><h4 id="Scala的注意事项"><a href="#Scala的注意事项" class="headerlink" title="Scala的注意事项"></a>Scala的注意事项</h4><p>Scala源文件以“.scala”为扩展名。</p><p>Scala程序的执行入口是main()函数</p><p>Scala语言严格区分大小写</p><p>Scala方法由一条条语句构成，不需要加分号</p><p>如果一行有多条语句，除了最后一行其他语句都要加分号</p><h4 id="Scala语言输出的三种方式"><a href="#Scala语言输出的三种方式" class="headerlink" title="Scala语言输出的三种方式"></a>Scala语言输出的三种方式</h4><p>1、字符串通过+号连接</p><p>2、printf用法  用%号引用变量（格式化输出）</p><pre><code class="scala">printf(&quot;%d,%s,%h&quot;,name,age,height)</code></pre><p>3、 字符串通过$引用</p><p>使用$直接引用变量</p><pre><code class="scala">println(&quot;个人信息如下 $name&quot;)//这个还可以运算 &#123;&#125; 代表了一个表达式println(&quot;个人信息如下 $&#123;sal+1*10&#125;&quot;)</code></pre><h4 id="IDEA快捷键"><a href="#IDEA快捷键" class="headerlink" title="IDEA快捷键"></a>IDEA快捷键</h4><p>Ctrl+Alt+L   快速格式化 </p><p>Ctrl+B   查看源码</p><p>Tab    可以区域代码向后移动一个制表符位置</p><p>shift+Tab   可以向前移动</p><h4 id="Scala查看源码的方法"><a href="#Scala查看源码的方法" class="headerlink" title="Scala查看源码的方法"></a>Scala查看源码的方法</h4><p>将scala-sources-2.12.4.tar拷贝到scala的lib目录，解压完进行关联源码，就可以查看了 </p><h4 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h4><pre><code>@deprecated  过期标签@example例子标签n1=10 n2=20 return 30@param    参数标签@return   返回值标签</code></pre><h4 id="生成注释文档"><a href="#生成注释文档" class="headerlink" title="生成注释文档"></a>生成注释文档</h4><p>scaladoc -d d:&#x2F;mydoc 文件名.scala</p><h4 id="Scala的语法"><a href="#Scala的语法" class="headerlink" title="Scala的语法"></a>Scala的语法</h4><pre><code>def   方法定义 Unit  没有返回值 val(var)   定义变量</code></pre><h5 id="数据类型的主要内容"><a href="#数据类型的主要内容" class="headerlink" title="数据类型的主要内容"></a>数据类型的主要内容</h5><p>1、Scala与Java有着相同的数据类型，Scala数据类型都是对象，也就是说scala没有Java中原生类型</p><p>2、Scala数据类型分为两大类AnyVal(值类型)和AnyRef(引用类型)，它们都是对象</p><p>3、Null类型是scala的特别类型，他只有一个值，他是bottom class，是所有AnyRef类型的子类</p><p>4、Nothing也是bottom class，它是所有类的子类，在开发中可以将Noting的值返回给任意变量或者函数，在抛出异常使用的多</p><p>5、在scala中仍然遵守，低精度得值，像高精度的值自动转换（隐式转换）</p><p><img src="/scala/scala/image-20200214130544337.png" alt="image-20200214130544337"></p><h5 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h5><p><img src="/scala/scala/image-20200214132607246.png" alt="image-20200214132607246"></p><h6 id="Char："><a href="#Char：" class="headerlink" title="Char："></a><strong>Char：</strong></h6><p><strong>在使用Char类型时，255内的数字会自动查询unicode码，输出这个unicode码</strong></p><p><strong>Unicode码值表包含了ascii表</strong></p><p>超过Char数字范围，报错   var char1 : Char &#x3D; 999999</p><p>自动转码后进行计算，判定类型是Int，报错  var char : Char &#x3D; ‘a’ + 1</p><h6 id="Boolean："><a href="#Boolean：" class="headerlink" title="Boolean："></a>Boolean：</h6><p>Boolean占一个字节，<strong>Scala中不能使用1或者0来代替true和false</strong></p><h6 id="Null"><a href="#Null" class="headerlink" title="Null:"></a>Null:</h6><p>Null只有一个实例，null，类似java中null的引用，<strong>null可以赋值给任意的引用类型（AnyRef），不能赋值给（AnyVal）</strong></p><pre><code>var dog : Dog = null     //okvar char : Char = null   //错误</code></pre><h6 id="Unit："><a href="#Unit：" class="headerlink" title="Unit："></a>Unit：</h6><p>Unit等价于java的void，只有一个实例值：()</p><h6 id="String"><a href="#String" class="headerlink" title="String"></a>String</h6><p>Scala中String必须赋默认值，没有值是错的 </p><p>var  ss : String &#x3D; _ &#x2F;&#x2F;<strong>默认值是null</strong></p><h5 id="自动类型转换细节"><a href="#自动类型转换细节" class="headerlink" title="自动类型转换细节"></a>自动类型转换细节</h5><p>1、多种类型的数据混合运算时，系统首先将所有数据转换成容量最大的那种数据类型，然后再进行计算</p><p>2、当我们把进度大的数据类型赋值给精度小的数据类型时，就会报错，反之就会自动类型转换</p><p>3、(byte,short)和char之间不会相互自动转换</p><p>4、byte、short、char 他们三者可以计算，在计算时首先转换为int类型</p><p>5、自动提升原则，表达式结果的类型自动提升为操作数中最大的类型</p><p>注：Byte和Short类型在进行运算时，当作Int类型处理</p><h5 id="高级隐式转换和隐式函数（后面讲）"><a href="#高级隐式转换和隐式函数（后面讲）" class="headerlink" title="高级隐式转换和隐式函数（后面讲）"></a>高级隐式转换和隐式函数（后面讲）</h5><p>（还没有将，放到高级讲）</p><h5 id="强制类型转换"><a href="#强制类型转换" class="headerlink" title="强制类型转换"></a>强制类型转换</h5><p>自动类型转换的逆过程，将容量大的转换为容量小的数据类型，使用时要加上强制类型转换，但可能造成精度降低或溢出</p><pre><code>java：int num = (int)2.5scala ；var num : Int = 2.7.toInt//toInt直接保留整数位，不进行四舍五入</code></pre><h6 id="强制类型转换细节说明"><a href="#强制类型转换细节说明" class="headerlink" title="强制类型转换细节说明"></a>强制类型转换细节说明</h6><p>1、但进行数据从大——小，就需要用到强制转换</p><p>2、强转符号只针对最近的操作数有效，往往使用小括号提升优先级</p><p>Char类型可以保存Int的常量值，但不能保存Int的变量值，需要强转</p><h4 id="变量使用讲解"><a href="#变量使用讲解" class="headerlink" title="变量使用讲解"></a>变量使用讲解</h4><pre><code>val | val 变量名 [:变量类型] = 变量值var 修饰的值可以修改val 修饰的值不可以修改，(final),没有线程安全问题，效率较高，推荐使用valval修饰的变量在编译后，等同于加上final!!变量声明时需要初始值，&quot;_&quot;是默认值!!//名称在前  类型在后 值在最后声明变量是声明数据类型定义变量是开辟内存空间大部分两种一起</code></pre><p><img src="/scala/scala/image-20200214123156457.png" alt="image-20200214123156457"></p><h4 id="内存分析"><a href="#内存分析" class="headerlink" title="内存分析"></a>内存分析</h4><p>全局变量放在堆 </p><p>局部变量放在栈</p><p>现在许多编译器加了（逃逸分析），也不一定就放在一个里面是不变的</p><h4 id="进制运算"><a href="#进制运算" class="headerlink" title="进制运算"></a>进制运算</h4><p>最高位是符号位，用来表示正负的</p><p>0为正，1为负</p><h4 id="标识符"><a href="#标识符" class="headerlink" title="标识符"></a>标识符</h4><p>1、Scala对各种变量，方法、函数等命名时使用的字符序列称为标识符</p><p>2、凡是自己可以起名字的地方叫做标识符</p><h5 id="标识符命名规范"><a href="#标识符命名规范" class="headerlink" title="标识符命名规范"></a>标识符命名规范</h5><p>1、首字符为字母，后续字符任意字母和数字，美元符号，可后接下划线_</p><p>2、数字不可以开头</p><p>3、<strong>首字符为操作符(比如+-*&#x2F;)，后续字符也需跟操作符，至少一个</strong></p><pre><code>    val ++ = 90    val +-*/ = 29</code></pre><p>4、操作符(比如+-*&#x2F;)不能在标识符中间或者最后</p><p>5、用反引号<code>....</code>包括的任意字符串，即使是关键字也可以true</p><pre><code class="scala">    val `true` : String = &quot;sss&quot;</code></pre><h4 id="运算符介绍"><a href="#运算符介绍" class="headerlink" title="运算符介绍"></a>运算符介绍</h4><p>运算符是一种特殊的符号，用以表示数据的运算，赋值和比较等</p><h5 id="1、算术运算符"><a href="#1、算术运算符" class="headerlink" title="1、算术运算符"></a>1、算术运算符</h5><p><img src="/scala/scala/image-20200214180750699.png" alt="image-20200214180750699"></p><h6 id="的运算原则："><a href="#的运算原则：" class="headerlink" title="%的运算原则："></a>%的运算原则：</h6><p>a % b &#x3D; a - a&#x2F;b *b</p><p>例：10 % 3 &#x3D; 10 - 10 &#x2F; 3 * 3 &#x3D; 10 - 9 &#x3D; 1</p><h6 id="和-–"><a href="#和-–" class="headerlink" title="++ 和 –"></a>++ 和 –</h6><p>在scala中没有++和–，而使用了+&#x3D;和-&#x3D;来代替</p><h6 id="2、赋值运算符"><a href="#2、赋值运算符" class="headerlink" title="2、赋值运算符"></a>2、赋值运算符</h6><p>赋值运算符就是将某个运算后的值，赋给指定的变量</p><p><img src="/scala/scala/image-20200214183242300.png" alt="image-20200214183242300"></p><p><strong>这部分包含了进制运算</strong></p><p><img src="/scala/scala/image-20200214183423340.png" alt="image-20200214183423340"></p><pre><code>var num = 2num &lt;&lt;= 2   //num = 8 2*2*2num &gt;&gt;= 1   //num = 4   8/2 </code></pre><p><strong>Scala中还支持代码块，返回值</strong></p><pre><code>var res = &#123;    if(num &gt; 1) &quot;1&quot; else 2 &#125;</code></pre><h6 id="3、比较运算符（关系运算符）"><a href="#3、比较运算符（关系运算符）" class="headerlink" title="3、比较运算符（关系运算符）"></a>3、比较运算符（关系运算符）</h6><p>1、关系运算符的结果都是boolean型，也就是要么是true，要不就是false</p><p>2、关系表达式经常使用if结果和循环结构条件中</p><p><img src="/scala/scala/image-20200214182946847.png" alt="image-20200214182946847"></p><h6 id="4、逻辑运算符"><a href="#4、逻辑运算符" class="headerlink" title="4、逻辑运算符"></a>4、逻辑运算符</h6><p>用于连接多个条件，最终结果也是一个boolean值</p><p><img src="/scala/scala/image-20200214183138391.png" alt="image-20200214183138391"></p><h6 id="5、位运算符"><a href="#5、位运算符" class="headerlink" title="5、位运算符"></a>5、位运算符</h6><p><img src="/scala/scala/image-20200214185855402.png" alt="image-20200214185855402"></p><h3 id="程序流程控制"><a href="#程序流程控制" class="headerlink" title="程序流程控制"></a>程序流程控制</h3><h4 id="1、顺序控制"><a href="#1、顺序控制" class="headerlink" title="1、顺序控制"></a>1、顺序控制</h4><h4 id="2、分支控制（嵌套分支不要超过三层）"><a href="#2、分支控制（嵌套分支不要超过三层）" class="headerlink" title="2、分支控制（嵌套分支不要超过三层）"></a>2、分支控制（<strong>嵌套分支不要超过三层</strong>）</h4><p>Scala中任意表达式都是有返回值的，代表着ifelse是有返回结果的，具体返回结果的值取决于满足条件的代码体的最后一行内容</p><pre><code class="scala">val age = 60val res = if(age &gt; 20)&#123;    println(&quot;hello age &gt; 20&quot;)    9 + 20    &quot;yes ok&quot;&#125;else&#123;    7&#125;//结果是res=yes ok</code></pre><h5 id="单分支"><a href="#单分支" class="headerlink" title="单分支"></a>单分支</h5><h5 id="双分支"><a href="#双分支" class="headerlink" title="双分支"></a>双分支</h5><h5 id="多分支"><a href="#多分支" class="headerlink" title="多分支"></a>多分支</h5><h6 id="Scala中没有Switch（后面讲）"><a href="#Scala中没有Switch（后面讲）" class="headerlink" title="Scala中没有Switch（后面讲）"></a>Scala中没有Switch（后面讲）</h6><p>Scala没有switch，而是使用模式匹配来处理</p><p>match-case</p><h5 id="嵌套分支"><a href="#嵌套分支" class="headerlink" title="嵌套分支"></a>嵌套分支</h5><h4 id="3、循环控制"><a href="#3、循环控制" class="headerlink" title="3、循环控制"></a>3、循环控制</h4><p>Scala的for循环常见的控制结构提供了非常多的特性，for循环也被称为for推导式或for表达式</p><pre><code class="scala">for(i &lt;- 1 to 3)&#123;    println(i)&#125;//&lt;-把右边的值赋值给i//1 to 3 就是 1到3//to 是关键字 var list= List(&quot;hello&quot;,30)//scala的集合可以存放任意对象for(item &lt;- list)&#123;    println(item)&#125;</code></pre><pre><code class="scala">//until循环范围是start到end-1for(item &lt;- 1 until 5)&#123;    //就是1 - 4 (5-1)    println(item)&#125;</code></pre><h6 id="循环守卫"><a href="#循环守卫" class="headerlink" title="循环守卫"></a>循环守卫</h6><p>循环保护(也是条件判断式守卫)，<strong>保护式为true则进入循环体内部，为false则跳过，类似于continue</strong></p><pre><code class="scala">for(i &lt;- 1 to 3 if i !=2)&#123;    println(i)&#125;</code></pre><h6 id="循环控制引入变量"><a href="#循环控制引入变量" class="headerlink" title="循环控制引入变量"></a>循环控制引入变量</h6><pre><code class="scala">for(i &lt;- 1 to 3 ; j = 4 - i)&#123;    print(j)&#125;//这里没有关键字，所以要加上&quot;;&quot;来隔断逻辑</code></pre><h6 id="for循环返回值"><a href="#for循环返回值" class="headerlink" title="for循环返回值"></a>for循环返回值</h6><pre><code class="scala">val res = for(i &lt;- 1 to 10) yield iprintln(res)//将遍历过程中处理的结果返回到一个新的Vector集合中，使用yield关键字//在i可以是一个代码块例：val res = for(i &lt;- 1 to 10) yield i*2println(res)//还可以添加代码块例：val res = for(i &lt;- 1 to 10) yield &#123;if(i % 2 == 0)&#123;    i&#125;else&#123;    &quot;不是偶数&quot;&#125;&#125;println(res)</code></pre><h6 id="for循环花括号-代替小括号"><a href="#for循环花括号-代替小括号" class="headerlink" title="for循环花括号{}代替小括号()"></a>for循环花括号{}代替小括号()</h6><pre><code class="scala">for(i &lt;- 1 to 3 ; j = 4 - i)&#123;    print(j)&#125;//可以写成for&#123;i &lt;- 1 to 3     j = 4 - i&#125;&#123;    print(j)&#125;//当for推导式仅包含单一表达式时使用圆括号，但其中包含多个表时使用大括号//使用&#123;&#125;来换行写表达式时，分号就不用写了</code></pre><h6 id="for循环的步长控制"><a href="#for循环的步长控制" class="headerlink" title="for循环的步长控制"></a>for循环的步长控制</h6><pre><code class="scala">//步长控制为2for(i &lt;- Range(1,10,2))&#123;    print(j)    //1，3，5，7，9&#125;//使用循环守卫控制步长for(i &lt;- 1 to 10 if i % 2 == 0)&#123;    print(j)    //1，3，5，7，9&#125;//在不使用Range的情况下也能进行步长控制</code></pre><h6 id="while循环"><a href="#while循环" class="headerlink" title="while循环"></a>while循环</h6><p>Scala中有while循环，<strong>while语句的返回值是Unit类型()，</strong>但是while中没有返回值，当用该语句计算返回结果时吗，就不可避免地使用了变量，变量需要声明在while循环的外部，那么等于循环的内部对外部的变量造成了影响，所以不推荐使用，推荐使用for循环</p><h5 id="Scala内置控制结构特地去除了break"><a href="#Scala内置控制结构特地去除了break" class="headerlink" title="Scala内置控制结构特地去除了break"></a>Scala内置控制结构特地去除了break</h5><p>使用了函数式的风格解决了break和continue功能，而不是一个关键字</p><pre><code class="scala">//要导入包import util.control.Breaks._var n = 1    while (n &lt;= 20) &#123;      n += 1      if(n==18)&#123;        break()          //这里会报一个异常breakException      &#125;&#125;</code></pre><pre><code class="scala">//这个break()方法做的事就是抛出了一个异常def break(): Nothing = &#123; throw breakException &#125;//这是一个高阶函数，可以接受函数的函数//op: =&gt;Unit    表示接受的参数是一个没有输入，也灭有返回值的函数//就相当于一个代码块 //将你的代码块在try中执行，同时还对break()抛出的异常做了处理def breakable(op: =&gt;Unit)&#123;try &#123;      op    &#125; catch &#123;      case ex: BreakControl =&gt;        if (ex ne breakException) throw ex    &#125;&#125;</code></pre><pre><code class="scala">//所以上面的两个方法要联合起来使用    breakable &#123;      var n = 1      while (n &lt;= 20) &#123;        n += 1        println(n+&quot;ddddd&quot;)        if (n == 18) &#123;          break()        &#125;      &#125;    &#125;</code></pre><h5 id="也可以使用循环守卫来实现中断效果"><a href="#也可以使用循环守卫来实现中断效果" class="headerlink" title="也可以使用循环守卫来实现中断效果"></a>也可以使用循环守卫来实现中断效果</h5><pre><code class="scala">    var loop = true    var sum2 = 0    for (i &lt;- 1 to 100 if loop==true) &#123;      sum2 += i      if (sum2 &gt; 20) &#123;        println(i)        loop=false      &#125;    &#125;</code></pre><h5 id="Scala内置控制结构特地去除了continue"><a href="#Scala内置控制结构特地去除了continue" class="headerlink" title="Scala内置控制结构特地去除了continue"></a>Scala内置控制结构特地去除了continue</h5><p>Scala内置控制结构特地去除了continue，为了更好地适应函数话编程，<strong>可以使用if-else 或是循环守卫的方式来实现continue</strong></p><pre><code class="scala">for (i &lt;- 1 to 10 if (i != 2 &amp;&amp; i!=3)) &#123;    printf(&quot;%d&quot;,i)&#125;//效果一样    for (i &lt;- 1 to 10 ) &#123;      if (i != 2 &amp;&amp; i!=3)&#123;        printf(&quot;%d\n&quot;,i)      &#125;    &#125;</code></pre><h4 id="键盘输入语句"><a href="#键盘输入语句" class="headerlink" title="键盘输入语句"></a>键盘输入语句</h4><p>在编程中，需要接收用户输入的数据，就可以使用键盘输入语句来获取。InputDemo.scala</p><p><strong>Scala的实现 【 import scala.io.StdIn】</strong></p><pre><code></code></pre><h4 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h4><h5 id="概念介绍"><a href="#概念介绍" class="headerlink" title="概念介绍"></a>概念介绍</h5><p>方法、函数、函数式编程、面向对象编程的概念：</p><p>1、在Scala中，方法和函数几乎等同（他们的定义、使用、运行机制都一样），只是函数的使用方式更加灵活</p><p>2、函数式编程是从编程方式角度来谈的，在Scala中，函数既可以当作函数的变量使用，也可以将函数赋值给一个变量，函数的创建也不用依赖于类或者对象，在java中，函数的创建则依赖于类、抽象类或者接口</p><p>3、面向对象编程是以对象为基础的编程方式</p><p>4、在Scala中函数式编程和面向对象编程融合在一起</p><pre><code class="scala">//将对象方法转化为函数val f1 = dog.sum _println(f1)println(f1(1,2))//函数val f2 = (t1:Int,t2:Int) =&gt; t1+t2println(f2)println(f2(1,2))</code></pre><h4 id="函数的定义"><a href="#函数的定义" class="headerlink" title="函数的定义"></a>函数的定义</h4><p>def 函数名 ([参数名：参数类型]，…)[:返回值类型&#x3D;]{</p><p>语句…</p><p>return 返回值</p><p>}</p><p>1、函数声明关键字为def（definiton）</p><p>2、[参数名：参数类型],…..，表示函数的输入（就是参数列表），可以没有。如果有，多个参数使用逗号间隔</p><p>3、函数中的语句，表示未来实现某一功能代码块</p><p>4、函数可以有返回值，也可以没有返回值</p><p>5、返回值形式1:返回值类型&#x3D;</p><p>6、返回值形式2：&#x3D; 表示返回值类型不确定，适用类型推导完成</p><p>7、表示没有返回值，return 不生效 </p><p>8、如果没有return，默认以执行到最后一行作为返回值</p><h4 id="函数递归调用"><a href="#函数递归调用" class="headerlink" title="函数递归调用"></a>函数递归调用</h4><p>1、执行一个函数时，创建新的受保护的独立空间 </p><p>2、函数的局部变量是独立的 ，不会相互影响</p><p>3、递归必须先推出递归的条件逼近，否则就是无限递归</p><p>4、但一个函数执行完毕，或者遇到return，就会返回，遵守谁调用，就将结果返回给谁</p><h4 id="递归练习题"><a href="#递归练习题" class="headerlink" title="递归练习题"></a>递归练习题</h4><p><img src="/scala/scala/image-20200215144730070.png" alt="image-20200215144730070"></p><p><img src="/scala/scala/image-20200215144716788.png" alt="image-20200215144716788"></p><h4 id="函数注意事项"><a href="#函数注意事项" class="headerlink" title="函数注意事项"></a>函数注意事项</h4><p>1、函数的形参列表可以是多个，如果参数没有形参，<strong>调用时可以不带()</strong></p><p>2、形参列表和返回值列表的数据类型可以是值类型和引用类型</p><p>3、Scala中的函数可以根据函数体的最后一行代码自行推断函数返回值类型，在这种情况下，return关键字可以省略</p><p>4、Scala可以自行推断，在省略return关键字的情况下，返回值类型也可以省略</p><p>5、如果函数明确使用return关键字，那么函数类型就不能使用自行推断了，要明确写成:返回类型，如果什么也不写，返回值是()</p><p>6、如果函数声明无返回值(Unit),那么函数中即使使用return关键字也不会有返回值</p><pre><code class="scala">//返回值什么都没有写，表示该函数没有返回值//这里的return无效def getSum2(n1 :Int,n2:Int)&#123;return n1 + n2&#125;</code></pre><p>7、如果明确函数无返回值或者不确定返回值类型，那么返回值类型可以省略</p><p>8、Scala语法中任何语法结构都可以嵌套其他语法结构，即：函数中可以在声明&#x2F;定义函数，类种可以在声明类，方法中可以在声明&#x2F;定义方法</p><p>9、<strong>Scala函数的形参，再声明参数时，直接赋初始值，这时调用函数时，如果没有指定实参，则会使用默认值。如果制定了实参，则是参会覆盖默认值</strong></p><pre><code class="scala">println(sayOk())//jackok!println(sayOk(&quot;asdf&quot;))    //asdfok!def sayOk(name :String = &quot;jack&quot;):String = &#123;    return name + &quot;ok!&quot;&#125;</code></pre><p>10、如果函数存在多个参数，每一个参数都可以设定默认值，那么这个时候，传递的参数到底是覆盖默认值，还是赋值给没有默认值的参数，就不确定了（默认按照声明顺序<strong>从左到右</strong>）。在这种情况下，可以使用带名参数</p><pre><code class="scala">//只修改user参数mysqlCon(user = &quot;tom&quot;)def mysqlCon(add:String = &quot;locathost&quot;,port : Int = 3306,user:String =&quot;root&quot;,pwd : String = &quot;root&quot;)=&#123;&#125;</code></pre><p>11、scala的函数形参默认时val的，不能再函数中进行修改</p><p>12、递归函数为执行之前是无返回值结果类型的，在使用时必须有明确的返回值类型</p><p>13、Scala函数支持可变参数</p><pre><code class="scala">def sum(n1:Int,args:Int*) : Int =&#123;&#125;//args是集合，通过for循环可以访问到各个值//可变参数需要放在形参列表的最后</code></pre><p><img src="/scala/scala/image-20200215160417071.png" alt="image-20200215160417071"></p><h4 id="惰性函数"><a href="#惰性函数" class="headerlink" title="惰性函数"></a>惰性函数</h4><p>惰性计算(尽可能延迟表达式求值)，是许多函数式编程语言的特性，惰性集合在需要时提供其元素，无需预先计算他们。首先你可以<strong>将耗时的计算推迟到绝对需要的时候</strong>。其次，你可以创建无限个集合，只要他们继续接受请求，就会继续提供元素。函数的个性能够得到更高效的代码</p><p>单行数返回值被声明为<strong>lazy</strong>时，函数的执行将被推迟，直到我们首次对此取值，该函数才会执行。这种函数我们称为惰性函数</p><pre><code class="scala">lazy val res = sum(10,20)println(&quot;_____________&quot;)println(&quot;res=&quot;+res)def sum(n1: Int,n2:Int): Int = &#123;    println(&quot;sum()执行了&quot;)    n1+n2&#125;//结果:_____________sum()执行了res=30</code></pre><h5 id="惰性函数注意事项"><a href="#惰性函数注意事项" class="headerlink" title="惰性函数注意事项"></a>惰性函数注意事项</h5><p>1、<strong>lazy</strong>不能修饰var类型的变量</p><p>2、在调用函数时，加了lazy，会导致函数的执行被推迟，在声明变量时，如果给声明了lazy，那么变量的分配也会变得推迟</p><h4 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h4><p>Scala提供try和catch块来处理异常。<strong>try块用于包含可能出错的代码，catch块用于处理try块中发生的异常。</strong>可以根据需要在程序中任意数量的try…catch块。</p><h5 id="Java异常处理的注意点"><a href="#Java异常处理的注意点" class="headerlink" title="Java异常处理的注意点"></a>Java异常处理的注意点</h5><p>1、java语言按照try-catch-catch-finally的方式处理异常</p><p>2、不管有没有异常捕获，都会执行finally，finally通常用来释放资源</p><p>3、可以有多个catch，分别捕获对应的异常，<strong>需要把范围小的异常类写在前面，把范围大的异常类写在后面，否则编译错误</strong>。但是Scala中可以大的在前，小的在后</p><h5 id="Scala异常处理"><a href="#Scala异常处理" class="headerlink" title="Scala异常处理"></a>Scala异常处理</h5><p>1、在scala中只有一个catch</p><p>2、在catch中有多个case，每个case可以匹配一种异常case ex : ArithmeticException</p><p>3、&#x3D;&gt;关键符号，表示后面是对异常处理的代码块</p><p>4、finally最重要执行</p><pre><code class="scala">try&#123;    val r = 10 /0&#125;catch &#123;    case ex:ArithmeticException =&gt; println(&quot;捕获除数为0的算数异常&quot;)    case ex:Exception =&gt; println(&quot;捕获了异常&quot;)&#125;finally&#123;println(&quot;scala finally&quot;)&#125;</code></pre><h5 id="Scala异常处理小节"><a href="#Scala异常处理小节" class="headerlink" title="Scala异常处理小节"></a>Scala异常处理小节</h5><p>1、在try之后使用一个catch处理程序来捕获异常，如果发生任何异常，catch处理它，程序继续执行</p><p>2、Scala的异常的工作机制和java一样，但是Scala没有“checked(编译期)”异常， Scala没有编译异常这个概念，异常都是在运行时候捕获处理</p><p>3、用throw关键字，抛出一个异常对象。<strong>所有的一场都是Throwable的字类型。throw表达式是有类型的，就是Nothing，因为Nothing时所有类的字类，所有throw表达式可以写在任意地方</strong></p><p>4、在Scala里，借用了模式匹配的思想来做异常的匹配，因此，在catch的代码里，是一系列case子句来匹配异常。当匹配上后&#x3D;&gt;有多条语句可以换行写，类似java的switch case x：代码块~<br>5、异常捕捉的机制与其他语言中一样，如果有异常发生，catch子句是按次序捕捉的。因此，在catch子句中，越具体的异常越要靠前，越普遍的异常越靠后，如果把越普遍的异常写在前，把具体的异常写在后，在Scala也不会报错，但这样是非常不好的编程风格。</p><pre><code class="scala">//如果我们希望在test()抛出异常后，代码可以继续执行，则我们需要处理  try &#123;    test()  &#125; catch &#123;    case ex: Exception =&gt; &#123;        //捕获到异常算数异常      println(&quot;捕获到异常&quot; + ex.getMessage)      println(&quot;xxxx&quot;)    &#125;    case ex: ArithmeticException =&gt; println(&quot;得到一个算数异常&quot;)  &#125; finally &#123;  &#125;    def test():Nothing&#123;  throw new ArithmeticException(&quot;算数异常&quot;)  &#125;</code></pre><p>7、finally子句用于执行不管是正常处理还是有异常发生时都需要执行的步骤，一般用于对象的清理工作，这点和Java一样。</p><p>8、Scala提供了throws关键字来声明异常。可以使用方法定义声明异常。 它向调用者函数提供了此方法可能引发此异常的信息。 它有助于调用函数处理并将该代码包含在try-catch块中，<strong>以避免程序异常终止</strong>。在scala中，可以使用throws注释来声明异常</p><pre><code class="scala">//throws注释@throws(classOf[NumberFormatException])//等同于NumberFormatException.class  def f11()  = &#123;    &quot;abc&quot;.toInt  &#125;</code></pre><h4 id="类和对象"><a href="#类和对象" class="headerlink" title="类和对象"></a>类和对象</h4><h5 id="定义类的注意事项"><a href="#定义类的注意事项" class="headerlink" title="定义类的注意事项"></a>定义类的注意事项</h5><p>1、scala语法中，类并不声明为public，所有这些类都是具有共有可见性（即默认就是public）</p><p>2、一个Scala源文件可以包含多个类，而且默认都是public</p><h5 id="类和对象的区别"><a href="#类和对象的区别" class="headerlink" title="类和对象的区别"></a>类和对象的区别</h5><p>1、类是抽象的，概念的，代表<strong>一类事物</strong>,比如人类,猫类..</p><p>2、对象是具体的，实际的，代表<strong>一个具体</strong>事物</p><p>3、类是对象的模板，对象是类的一个个体，对应一个实例</p><p>4、Scala中类和对象的区别和联系 和 Java是一样的。</p><h5 id="属性和类的关系"><a href="#属性和类的关系" class="headerlink" title="属性和类的关系"></a>属性和类的关系</h5><p>1、属性是类的一个组成部分，一般是<strong>值数据类型</strong>,也可是<strong>引用类型</strong>。比如我们前面定义猫类 的 age 就是属性</p><h5 id="属性-x2F-成员变量"><a href="#属性-x2F-成员变量" class="headerlink" title="属性&#x2F;成员变量"></a>属性&#x2F;成员变量</h5><p>1、属性的定义语法同变量，示例：var 属性名称 [:类型] &#x3D; 属性值</p><p>2、属性的定义类型可以为任意类型，包含值类型或引用类型</p><p>3、Scala声明一个属性，必须显式的初始化，然后根据初始化数据的类型自动判断，属性类型可以省略</p><p>4、如果赋值null，<strong>一定要加类型</strong>，因为不加类型，该属性的类型就是Null类型</p><p>5、在定义属性时，暂时不赋值，也可以使用符号_(下划线)，让系统分配默认值</p><h4 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h4><p>val | var 对象名 [:类型] &#x3D; new 类型()</p><p>1、如果我们不希望改变对象的引用，应该是val性质的，否则声明var，scala设计者推荐使用val，因为一般来说，在程序中，我们只是改变对象属性的值，而不是改变对象的引用</p><p>2、scala在声明对象变量时，可以根据创建对象的类型自动推断，所以类型声明可以省略，<strong>但当类型和后面new 对象类型有继承关系即多态时，就必须写了</strong></p><h5 id="类和对象的内存分配机制"><a href="#类和对象的内存分配机制" class="headerlink" title="类和对象的内存分配机制"></a>类和对象的内存分配机制</h5><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>Scala中的方法其实就是函数</p><h5 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h5><pre><code>def 方法名(参数列表) [：返回值类型] = &#123;     方法体&#125;</code></pre><h5 id="方法的调用机制原理"><a href="#方法的调用机制原理" class="headerlink" title="方法的调用机制原理"></a>方法的调用机制原理</h5><ol><li><p>当我们scala开始执行时，先在栈区开辟一个main栈。main栈是最后被销毁</p></li><li><p>当scala程序在执行到一个方法时，总会开一个新的栈。</p></li><li><p>每个栈是独立的空间，变量（基本数据类型）是独立的，相互不影响</p></li><li><p>当方法执行完毕后，该方法开辟的栈就会被jvm机回收。</p></li></ol><h4 id="构造器"><a href="#构造器" class="headerlink" title="构造器"></a>构造器</h4><p>主要作用是完成<strong>对象的初始化</strong></p><h5 id="Scala构造器的介绍"><a href="#Scala构造器的介绍" class="headerlink" title="Scala构造器的介绍"></a>Scala构造器的介绍</h5><p>和Java一样，Scala构造对象也需要调用构造方法，并且可以有任意多个构造方法（即scala中构造器也支持重载）。<br>Scala类的构造器包括： <strong>主构造器 和 辅助构造器</strong></p><h5 id="Scala构造器的基本语法"><a href="#Scala构造器的基本语法" class="headerlink" title="Scala构造器的基本语法"></a>Scala构造器的基本语法</h5><pre><code class="scala">class 类名(形参列表) &#123;  // 主构造器   // 类体   def  this(形参列表) &#123;  // 辅助构造器   &#125;   def  this(形参列表) &#123;  //辅助构造器可以有多个...   &#125;&#125; //辅助构造器可以有多个，通过参数或者类型来区分</code></pre><h5 id="构造器的私有化"><a href="#构造器的私有化" class="headerlink" title="构造器的私有化"></a>构造器的私有化</h5><pre><code class="scala">class 类名 private(形参列表) &#123; &#125;</code></pre><h5 id="Scala构造器注意事项和细节"><a href="#Scala构造器注意事项和细节" class="headerlink" title="Scala构造器注意事项和细节"></a>Scala构造器注意事项和细节</h5><p>1、Scala的构造器作用是完成<strong>对新对象的初始化，构造器没有返回值</strong></p><p>2、<strong>主构造器的声明直接放置于类名之后</strong></p><p>3、<strong>主构造器会执行类定义中除方法外的所有语句</strong>，构造器也是方法，这里可以体会到Scala的函数式编程和面向对象编程融合在一起</p><p>4、如果<strong>主构造器无参数，小括号可省略</strong>，构建对象时调用的构造方法小括号也可以省略</p><p>5、<strong>辅助构造器名称为this</strong>（这个和Java是不一样的），多个辅助构造器通过不同参数列表进行区分， 在底层就是构造器重载。</p><p>6、如果想<strong>让主构造器变成私有的，可以在()之前加上private</strong>，这样用户只能通过辅助构造器来构造对象了</p><pre><code class="scala">//私有化主构造器class Person2 private()&#123;&#125;//私有化辅助构造器private this()&#123;    &#125;</code></pre><p>7、辅助构造器的声明不能和主构造器的声明一致,会发生错误(即构造器名重复 )</p><h6 id="快速入门案例"><a href="#快速入门案例" class="headerlink" title="快速入门案例"></a>快速入门案例</h6><pre><code class="scala">//主构造器class Person(inName : String,inage :Int) &#123;  var name: String = inName  var age: Int = inage  //辅助构造器  def this(name: String) &#123;    this()    this.name = name  &#125;  //辅助构造器  def this(name: String, age: Int) &#123;    this()    this.name = name    this.age=age  &#125;  println(&quot;_________&quot;)&#125;</code></pre><h5 id="Scala构造器参数"><a href="#Scala构造器参数" class="headerlink" title="Scala构造器参数"></a>Scala构造器参数</h5><p>1、Scala类的主构造器的形参未用任何修饰符修饰，那么这个参数是局部变量</p><pre><code class="scala">class Car(inname : String)&#123;    //就是一个局部变量的形参&#125;</code></pre><p>2、如果<strong>参数使用val关键字声明，</strong>那么Scala会将参数<strong>作为类的私有的只读属性</strong></p><pre><code class="scala">class Car(val inname : String)&#123;&#125;  def main(args: Array[String]): Unit = &#123;    val car = new Car    println(car.inname)//ok    car.inname=&quot;&quot;//error  &#125;</code></pre><p>3、如果参数使用<strong>var关键字</strong>声明，那么Scala会将参数作为类的<strong>成员属性</strong>使用，即这时的成员属性是私有的，但是可读写。</p><pre><code class="scala">class Car(var inname : String)&#123;&#125;  def main(args: Array[String]): Unit = &#123;    val car = new Car    car.inname=&quot;&quot;    println(car.inname)  &#125;</code></pre><p>4、JavaBeans规范定义了Java的属性是像getXxx（）和setXxx（）的方法。为了Java的互操作性。将Scala<strong>字段加@BeanProperty</strong>时，这样会<strong>自动生成规范的 setXxx&#x2F;getXxx 方法</strong>。这时可以使用 对象.setXxx() 和 对象.getXxx() 来调用属性。</p><pre><code class="scala">class Car(val inname : String)&#123;  @BeanProperty  var name :String = null  def this()&#123;    this(&quot;&quot;)  &#125;&#125;  def main(args: Array[String]): Unit = &#123; val car = new Car car.setName(&quot;宝马&quot;) car.getName  &#125;//加了注解之后的生成的getset方法和原来的car.name方法共存</code></pre><h4 id="Scala对象创建流程？（面试题）"><a href="#Scala对象创建流程？（面试题）" class="headerlink" title="Scala对象创建流程？（面试题）"></a>Scala对象创建流程？（面试题）</h4><pre><code>var p : Person = new Person(&quot;小强&quot;,20)</code></pre><p>1、加载类得信息(属性信息，方法信息)</p><p>2、在内存中(堆)开辟空间，开辟的大小根据属性多大</p><p>3、调用父类的构造器(主和辅助)进行初始化</p><p>4、使用主构造器对属性进行初始化</p><p>5、使用辅助构造器对属性进行初始化</p><p>6、开辟的对象的地址赋值给p引用</p><h2 id="包"><a href="#包" class="headerlink" title="包"></a>包</h2><h4 id="Java包的三大作用"><a href="#Java包的三大作用" class="headerlink" title="Java包的三大作用"></a>Java包的三大作用</h4><p>1、区分相同名字的类</p><p>2、当类很多时，可以很好的管理类</p><p>3、控制访问范围</p><h4 id="Java对包的要求"><a href="#Java对包的要求" class="headerlink" title="Java对包的要求"></a>Java对包的要求</h4><p>类的源文件，需要和包对应的文件路径匹配</p><p>当我们编译Dog类，后生成对应的.class文件，编译器会将该.class文件也放在和包一样的路径里</p><h4 id="Scala包的基本介绍"><a href="#Scala包的基本介绍" class="headerlink" title="Scala包的基本介绍"></a>Scala包的基本介绍</h4><p>和Java一样，Scala中管理项目可以使用包，但Scala中的包的功能更加强大，使用也相对复杂些</p><h5 id="Scala基本语法"><a href="#Scala基本语法" class="headerlink" title="Scala基本语法"></a>Scala基本语法</h5><pre><code>package 包名</code></pre><h5 id="Scala包的特点和Java的一样"><a href="#Scala包的特点和Java的一样" class="headerlink" title="Scala包的特点和Java的一样"></a>Scala包的特点和Java的一样</h5><p>1、区分相同名字的类</p><p>2、当类很多时，可以很好的管理类</p><p>3、控制访问范围</p><p>4、<strong>可以对类的功能进行扩展</strong></p><p>Scala中包名和源码所在的系统文件目录结构要可以不一致，但是编译后的字节码文件路径和包名会保持一致(这个工作由编译器完成)</p><p>Scala中包名和源码所在的系统文件目录结构<strong>可以不一致</strong>，但是编译后的字节码文件路径和包名会保持一致(就是源码路径和.class文件路径可以不一致，但是他们必须在同一个包的目录下)</p><p>例：</p><pre><code class="Scala">//源文件目录package com.star.chapter06.constuctor//.class字节码目录package com.star.chapter06.hello</code></pre><h5 id="Scala包的命名"><a href="#Scala包的命名" class="headerlink" title="Scala包的命名"></a>Scala包的命名</h5><p>只能包含数字、字母、下划线、小圆点.,但不能用数字开头, 也不要使用关键字。</p><h6 id="命名规范"><a href="#命名规范" class="headerlink" title="命名规范"></a>命名规范</h6><p>一般是小写字母+小圆点一般是 </p><p><strong>com.公司名.项目名.业务模块名</strong></p><h5 id="Scala会自动引入的常用包"><a href="#Scala会自动引入的常用包" class="headerlink" title="Scala会自动引入的常用包"></a>Scala会自动引入的常用包</h5><pre><code class="scala">java.lang.*  scala包Predef包//预定义包</code></pre><h5 id="Scala包注意事项和使用细节"><a href="#Scala包注意事项和使用细节" class="headerlink" title="Scala包注意事项和使用细节"></a>Scala包注意事项和使用细节</h5><h6 id="一、Scala打包的方式"><a href="#一、Scala打包的方式" class="headerlink" title="一、Scala打包的方式"></a>一、Scala打包的方式</h6><h6 id="注：包名不会影响源码，只会影响-class字节码文件的位置"><a href="#注：包名不会影响源码，只会影响-class字节码文件的位置" class="headerlink" title="注：包名不会影响源码，只会影响.class字节码文件的位置"></a>注：<strong>包名不会影响源码，只会影响.class字节码文件的位置</strong></h6><p>1、传统的方式</p><pre><code>package com.atguigu.scalaclass Person&#123;&#125;</code></pre><p>2、和第一种完全等价</p><pre><code>package com.atguigupackage class Person&#123;&#125;</code></pre><p>3、这种方式来指定.class文件到哪个包中</p><pre><code class="scala">//package+包名&#123;&#125;package com.atguigu &#123;  class User&#123;  &#125;  package scala2&#123;    object User&#123;    &#125;  &#125;  package scala &#123;    class Person &#123;      val name = &quot;Nick&quot;      def play(message: String): Unit = &#123;        println(this.name + &quot; &quot; + message)      &#125;    &#125;    object Test100 &#123;      def main(args: Array[String]): Unit = &#123;        println(&quot;ok&quot;)      &#125;    &#125;  &#125;</code></pre><h6 id="二、包中有包"><a href="#二、包中有包" class="headerlink" title="二、包中有包"></a>二、包中有包</h6><p>包也可以像嵌套类那样嵌套使用（包中有包）, 这个在前面的第三种打包方式已经讲过了，在使用第三种方式时的好处是：<strong>程序员可以在同一个文件中，将类(class &#x2F; object)、trait 创建在不同的包中</strong>，这样就非常灵活了。</p><pre><code class="scala">package com.atguigu &#123;    //这个类的字节码文件在com.atguigu包下  class User&#123;  &#125;  package scala2&#123;    //这个类的字节码文件在com.atguigu.scala2包下    class User&#123;    &#125;&#125;</code></pre><h6 id="三、作用域原则"><a href="#三、作用域原则" class="headerlink" title="三、作用域原则"></a>三、作用域原则</h6><p>作用域原则：<strong>可以直接向上访问</strong>。即: Scala中子包中直接访问父包中的内容, 大括号体现作用域。(提示：Java中子包使用父包的类，需要import)。<strong>在子包和父包 类重名时，默认采用就近原则，如果希望指定使用某个类，则带上包名即可。</strong></p><h6 id="四、父包访问子包"><a href="#四、父包访问子包" class="headerlink" title="四、父包访问子包"></a>四、父包访问子包</h6><p>父包要访问子包的内容时，需要import对应的类等</p><p>&#x2F;&#x2F;和java不同，位置在哪里都可以</p><pre><code class="scala">package com.atguigu &#123;import com.atguigu.scala.Tigerclass User&#123;    var tiger = new Tiger()&#125;  package scala&#123;    class Tiger&#123;    &#125;  &#125;&#125;</code></pre><h6 id="五、嵌套的package不超过三层"><a href="#五、嵌套的package不超过三层" class="headerlink" title="五、嵌套的package不超过三层"></a>五、嵌套的package不超过三层</h6><p>可以在同一个.scala文件中，声明多个并列的package(建议嵌套的pakage不要超过3层) </p><h6 id="六、包名的相对路径引入和绝对路径引入"><a href="#六、包名的相对路径引入和绝对路径引入" class="headerlink" title="六、包名的相对路径引入和绝对路径引入"></a>六、包名的相对路径引入和绝对路径引入</h6><p>包名可以相对也可以绝对，比如，访问BeanProperty的绝对路径是：<em>root</em>. scala.beans.BeanProperty ，在一般情况下：我们使用相对路径来引入包，<strong>只有当包名冲突时，使用绝对路径来处理。</strong></p><pre><code class="scala">//第一种形式，相对路径@BeanProperty//第二种和第一种一样，也是相对路径@scala.beans.BeanProperty//第三种，绝对路径@_root_.scala.beans.BeanProperty</code></pre><h4 id="包对象"><a href="#包对象" class="headerlink" title="包对象"></a>包对象</h4><p><strong>包可以包含类、对象和特质trait，但不能包含函数&#x2F;方法或变量的定义</strong>。这是Java虚拟机的局限。为了<strong>弥补这一点不足，scala提供了包对象的概念来解决这个问题</strong>。</p><pre><code class="scala">package com.atguigu &#123;    //包对象  package object scala &#123;    var name = &quot;kk&quot;    def sayOk(): Unit = &#123;      println(&quot;package object&quot;)    &#125;  &#125;    //包范围  package scala &#123;    object TestObj &#123;      def main(args: Array[String]): Unit = &#123;        println(name)        sayOk()      &#125;    &#125;  &#125;&#125;</code></pre><h5 id="包对象实现的机制"><a href="#包对象实现的机制" class="headerlink" title="包对象实现的机制"></a>包对象实现的机制</h5><p>一个包对象会生成两个字节码文件</p><p>Package.class和package$.class</p><p>运行类调用包对象的方法时，是调用package$.class中的方法来输出结果</p><h5 id="包对象的应用"><a href="#包对象的应用" class="headerlink" title="包对象的应用"></a>包对象的应用</h5><h6 id="1、语法"><a href="#1、语法" class="headerlink" title="1、语法"></a>1、语法</h6><pre><code>package object 包名//表示创建一个包对象</code></pre><h6 id="2、包对象的名字需要和子包一样"><a href="#2、包对象的名字需要和子包一样" class="headerlink" title="2、包对象的名字需要和子包一样"></a>2、包对象的名字需要和子包一样</h6><p>(放在外面，包对象和包同级的，不是被包包含的)</p><h6 id="3、每一个包都可以有一个包对象"><a href="#3、每一个包都可以有一个包对象" class="headerlink" title="3、每一个包都可以有一个包对象"></a>3、每一个包都可以有一个包对象</h6><h6 id="4、包对象中可以定义变量、方法"><a href="#4、包对象中可以定义变量、方法" class="headerlink" title="4、包对象中可以定义变量、方法"></a>4、包对象中可以定义变量、方法</h6><h6 id="5、包对象中定义的变量和方法，可以在对应的包中使用"><a href="#5、包对象中定义的变量和方法，可以在对应的包中使用" class="headerlink" title="5、包对象中定义的变量和方法，可以在对应的包中使用"></a>5、包对象中定义的变量和方法，可以在对应的包中使用</h6><h4 id="包的可见性"><a href="#包的可见性" class="headerlink" title="包的可见性"></a>包的可见性</h4><h5 id="Java的修饰符范围"><a href="#Java的修饰符范围" class="headerlink" title="Java的修饰符范围"></a>Java的修饰符范围</h5><p>Java的四种访问控制修饰符号控制方法和变量的访问权限(范围)</p><p>1、公开级别:用public 修饰,对外公开<br>2、受保护级别:用protected修饰,对子类和同一个包中的类公开<br>3、默认级别:没有修饰符号,向同一个包的类公开.<br>4、私有级别:用private修饰,只有类本身可以访问,不对外公开</p><h5 id="Scala的可见性"><a href="#Scala的可见性" class="headerlink" title="Scala的可见性"></a>Scala的可见性</h5><p>在Java中，访问权限分为: public，private，protected和默认。在Scala中，你可以通过类似的修饰符达到同样的效果。但是使用上有区别。</p><h5 id="Scala的修饰符范围"><a href="#Scala的修饰符范围" class="headerlink" title="Scala的修饰符范围"></a>Scala的修饰符范围</h5><p><strong>private</strong>   外部不能访问，连方法都是private的，只能在本类或者伴生对象中使用</p><p><strong>protected</strong>(编译器控制)只能够子类访问，权限控制严格</p><p>如果不写默认也是private，但方法是public的，可以在任意地方使用</p><p>但是在解码后，不管你什么修饰符，不是能够让外部访问的，就是private</p><p>能够被外部访问的就是public  就这两种，但是</p><h5 id="伴生类和伴生对象"><a href="#伴生类和伴生对象" class="headerlink" title="伴生类和伴生对象"></a>伴生类和伴生对象</h5><p>伴生对象可以使用伴生对象的私有属性和私有方法</p><p>当一个文件中同时出现了名字相同的class和object</p><pre><code class="scala">object Testvisit &#123;  def main(args: Array[String]): Unit = &#123;    val  c = new Clerk()    c.showInfo()    Clerk.test(c)  &#125;&#125;class Clerk &#123;  var name : String = &quot;jack&quot;  private var sal : Double = 9999.9  def showInfo(): Unit = &#123;    println(&quot; name &quot; + name + &quot; sal= &quot; + sal)  &#125;&#125;//伴生对象可以访问伴生类的私有属性object Clerk&#123;  def test(c : Clerk): Unit = &#123;    //这里体现出在伴生对象中，可以访问c.sal    println(&quot;test() name=&quot; + c.name + &quot; sal= &quot; + c.sal)  &#125;&#125;</code></pre><p>因为Scala设计者将static拿掉，他设计了<strong>伴生类和伴生对象的概念</strong></p><p><strong>class称作伴生类，存放非静态内容的地方</strong></p><p><strong>object称为伴生对象，存放静态内容的地方</strong></p><h5 id="Scala中包的可见性和访问修饰符"><a href="#Scala中包的可见性和访问修饰符" class="headerlink" title="Scala中包的可见性和访问修饰符"></a>Scala中包的可见性和访问修饰符</h5><p>1、当属性访问权限为<strong>默认时</strong>，从底层看<strong>属性是private的</strong>，方法是public的，因为提供了xxx_$eq()[类似setter]&#x2F;xxx()[类似getter] 方法，因此从使用效果看是任何地方都可以访问)</p><p>2、当方法访问权限为<strong>默认时</strong>，默认为public访问权限</p><p>3、<strong>private</strong>为私有权限，<strong>只在类的内部和伴生对象中可用</strong></p><p>4、<strong>protected</strong>为受保护权限，scala中受保护权限比Java中更严格，<strong>只能子类访问，同包无法访问</strong> (编译器)</p><p>5、在scala中<strong>没有public关键字</strong>,即不能用public显式的修饰属性和方法</p><p>6、包访问权限（表示属性有了限制。同时包也有了限制），这点和Java不一样，体现出Scala包使用的灵活性。</p><p>将非静态的内容放在伴生类中</p><p>将静态内容放在伴生对象中</p><pre><code class="scala">class Person &#123;  private[scala] val pname=&quot;hello&quot;   // 增加包访问权限后，1.private同时起作用。不仅同类可以使用   //2. 同时com.atguigu.scala中包下其他类也可以使用&#125;//也可以延展到上层private[atguigu] val description=&quot;zhangsan&quot;</code></pre><h5 id="Scala引入包的细节和注意事项"><a href="#Scala引入包的细节和注意事项" class="headerlink" title="Scala引入包的细节和注意事项"></a>Scala引入包的细节和注意事项</h5><p>1、在Scala中，improt语句可以出现在任何的地方，并不仅限于文件顶端，import语句的作用一直延伸到包含该语句的块末尾。这种语法的好处是：<strong>在需要时在引入包，缩小import 包的作用范围，提高效率</strong>。</p><p>2、Java中如果想要导入包中所有的类，可以通过通配符<em>，*<em>Scala中采用下 “_”</em></em>  </p><pre><code>import scala.beans._</code></pre><p>3、如果不想要某个包中全部的类，而是其中的几个类，可以采用选取器(大括号)</p><pre><code>def test(): Unit = &#123;    import scala.collection.mutable.&#123;HashMap, HashSet&#125;    var map = new HashMap()    var set = new HashSet()&#125;</code></pre><p>4、如果引入的多个包中含有相同的类，那么可以<strong>将不需要的类进行重命名</strong>进行区分，这个就是重命名</p><pre><code>import java.util.&#123; HashMap=&gt;JavaHashMap, List&#125;import scala.collection.mutable._var map = new HashMap() // 此时的HashMap指向的是scala中的HashMapvar map1 = new JavaHashMap(); // 此时使用的java中hashMap的别名</code></pre><p>5、如果某个冲突的类根本就不会用到，那么这个类可以直接隐藏掉</p><pre><code>import java.util.&#123; HashMap=&gt;_, _&#125; // 含义为 引入java.util包的所有类，但是忽略 HahsMap类.var map = new HashMap() // 此时的HashMap指向的是scala中的HashMap, 而且idea工具的提示也不会显示java.util的HashMaple </code></pre><h3 id="面向对象编程三大特征"><a href="#面向对象编程三大特征" class="headerlink" title="面向对象编程三大特征"></a>面向对象编程三大特征</h3><p>面向对象编程有三大特征：封装、继承和多态</p><h4 id="封装介绍"><a href="#封装介绍" class="headerlink" title="封装介绍"></a>封装介绍</h4><p>封装(encapsulation)就是把<strong>抽象出的数据和对数据的操作封装在一起</strong>,数据被保护在内部,<strong>程序的其它部分只有通过被授权的操作(成员方法),才能对数据进行操作。</strong></p><h6 id="封装的好处"><a href="#封装的好处" class="headerlink" title="封装的好处"></a>封装的好处</h6><p>隐藏实现细节<br>提可以对数据进行验证，保证安全合理</p><h6 id="封装的步骤"><a href="#封装的步骤" class="headerlink" title="封装的步骤"></a>封装的步骤</h6><p>1、私有化属性</p><p>2、提供getset方法</p><h5 id="Scala封装的注意事项和细节"><a href="#Scala封装的注意事项和细节" class="headerlink" title="Scala封装的注意事项和细节"></a>Scala封装的注意事项和细节</h5><p>1、Scala中为了简化代码的开发，当声明属性时，本身就自动提供了对应setter&#x2F;getter方法，如果<strong>属性声明为private的，那么自动生成的setter&#x2F;getter方法也是private的</strong>，如果<strong>属性省略访问权限修饰符，那么自动生成的setter&#x2F;getter方法是public</strong>的</p><p>2、因此我们如果只是**对一个属性进行简单的set和get ，只要声明一下该属性(**属性使用默认访问修饰符) 不用写专门的getset，默认会创建，访问时，直接对象.变量。这样也是为了保持访问一致性</p><p>3、从形式上看 dog.food 直接访问属性，其实底层仍然是访问的方法,  看一下反编译的代码就明白<br>4、有了上面的特性，目前很多新的框架，在进行反射时，也支持对属性的直接反射(<strong>支持框架</strong>)</p><h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><h5 id="继承基本介绍"><a href="#继承基本介绍" class="headerlink" title="继承基本介绍"></a>继承基本介绍</h5><p><strong>继承可以解决代码复用</strong>,让我们的编程更加靠近人类思维.当多个类存在相同的属性(变量)和方法时,可以从这些类中抽象出父类(比如Student),在父类中定义这些相同的属性和方法，所有的子类不需要重新定义这些属性和方法，只需要通过extends语句来声明继承父类即可。</p><h5 id="Scala继承的基本语法"><a href="#Scala继承的基本语法" class="headerlink" title="Scala继承的基本语法"></a>Scala继承的基本语法</h5><p>class 子类名 extends 父类名  { 类体 }</p><h5 id="Scala继承给编程带来的便利"><a href="#Scala继承给编程带来的便利" class="headerlink" title="Scala继承给编程带来的便利"></a>Scala继承给编程带来的便利</h5><p>代码的复用性提高了<br>代码的扩展性和维护性提高了</p><p><img src="/scala/scala/image-20200217095200063.png" alt="image-20200217095200063"></p><h5 id="Scala继承了什么？"><a href="#Scala继承了什么？" class="headerlink" title="Scala继承了什么？"></a>Scala继承了什么？</h5><p>子类继承了<strong>所有的属性</strong>，只是私有的属性不能直接访问，<br> 需要<strong>通过公共的方法</strong>去访问</p><h4 id="重写方法"><a href="#重写方法" class="headerlink" title="重写方法"></a>重写方法</h4><p>scala明确规定，重写一个非抽象方法需要用<strong>override</strong>修饰符，调用超类被覆盖的方法使用super关键字  </p><pre><code class="scala">class Person &#123;  var name : String = &quot;tom&quot;  def printName() &#123;    println(&quot;Person printName() &quot; + name)  &#125;&#125;class Emp extends Person &#123;  //这里需要显式的使用override  override def printName() &#123;    println(&quot;Emp printName() &quot; + name)    //父类方法被重写，要调用父类方法使用supper关键字    super.printName()  &#125;&#125;</code></pre><h4 id="Scala中类型检查和转换"><a href="#Scala中类型检查和转换" class="headerlink" title="Scala中类型检查和转换"></a>Scala中类型检查和转换</h4><p>要测试某个对象是否属于某个给定的类，可以用<strong>isInstanceOf</strong>方法。用<strong>asInstanceOf</strong>方法将引用转换为子类的引用。<strong>classOf</strong>获取对象的类名</p><p>1、classOf[String]就如同Java的 String.class 。会输出class java.lang.String</p><pre><code class="scala">classOf[String]//两种等价val s = &quot;kk&quot;//这种是利用反射机制println(s.getClass.getName)</code></pre><p>2、obj.isInstanceOf[T]就如同Java的obj instanceof T <strong>判断obj是不是T类型</strong>。</p><pre><code class="scala">print(&quot;s&quot;.isInstanceOf[String])</code></pre><p>3、obj.asInstanceOf[T]就如同Java的(T)obj <strong>将obj强转成T类型。</strong></p><pre><code class="scala">var p1 = new Personvar emp = new Emp//自动向上转型,还是Person类型p1 = empvar emp2 = p1.isInstanceOf[Emp]</code></pre><h4 id="Scala中超类的构造"><a href="#Scala中超类的构造" class="headerlink" title="Scala中超类的构造"></a>Scala中超类的构造</h4><p>1、类有一个主构器和任意数量的辅助构造器，而<strong>每个辅助构造器都必须先调用主构造器</strong>(也可以是间接调用.)</p><pre><code class="scala">object SealaBaseConstrator &#123;defmain(args: Array[String]): Unit * &#123;//分析一下他的执行流程//1.因为scala 遵循先构建父类部分extends Pcrson700//2.Person...//3.默认的名字//4.Emp....val emp = new Emp700（）//分析一下他的执行流程//1.因为scala 遵循先构建父类部分extends Pcrson700//Person...//默认的名字//3.Emp...//4.Emp 辅助构造器phntln(&quot;========================&quot;)val emp2 = new Emp700(&quot;mary&quot;）print ln(&quot;***********************&quot;)//分析执行的顺序//1.Person...//2.默认的名字//3.Emp...//4.Emp 辅助构造器val emp3 = new Emp700(&quot;smith&quot;)    &#125;&#125;                      //父类 Personclass Person700(pName:String)&#123;    var name = pName    println(&quot;Person...&quot;)    def this()&#123;        this(&quot;默认的名字&quot;)        println(&quot;默认的名字&quot;)    &#125;&#125;                      //子类Emp继承Persoclass Emp700() extends Person700()&#123;    println(&quot;Emp...&quot;)    def this(name:String)&#123;        this        this.name=name        println(&quot;Emp 辅助构造器~&quot;);    &#125;&#125;</code></pre><p>2、<strong>只有主构造器可以调用父类的构造器</strong>。辅助构造器不能直接调用父类的构造器。在Scala的构造器中，你不能调用super(params) ，没有super的语法</p><h4 id="覆写字段"><a href="#覆写字段" class="headerlink" title="覆写字段"></a>覆写字段</h4><p>在Scala中，<strong>子类改写父类的字段</strong>，我们称为<strong>覆写&#x2F;重写字段</strong>。覆写字段需使用 <strong>override</strong>修饰。<strong>是通过重写方法来实现的</strong></p><p>Java中只有方法的重写，没有属性&#x2F;字段的重写，准确的讲，是隐藏字段代替了重写</p><h5 id="Java动态绑定机制"><a href="#Java动态绑定机制" class="headerlink" title="Java动态绑定机制"></a>Java动态绑定机制</h5><p>将一个子类的对象地址交给了一个AA(父类的)引用</p><h6 id="java的动态绑定机制的小结"><a href="#java的动态绑定机制的小结" class="headerlink" title="java的动态绑定机制的小结"></a>java的动态绑定机制的小结</h6><p>1、<strong>如果调用的是方法，则，jvm机将该方法和对象的内存地址绑定</strong></p><p>2、<strong>如果调用的是一个属性，则没有动态绑定机制，在哪里调用，就返回对应值</strong></p><pre><code class="java">class A &#123;    public int i = 10;    public int sum() &#123;        return getI() + 10;    &#125;    public int sum1() &#123;        return i + 10;    &#125;    public int getI() &#123;        return i;    &#125;&#125;class B extends A &#123;    public int i = 20;    public int sum() &#123;        return i + 20;    &#125;    public int getI() &#123;        return i;    &#125;    public int sum1() &#123;        return i + 10;    &#125;&#125;A a = new B();//这个等于30System.out.println(a.sum());  //?//30System.out.println(a.sum1()); //?//如果把B类的sum方法注释掉呢？?System.out.println(a.sum());//这个等于30//因为动态绑定，执行A类的sum方法时执行B的getI()进行计算</code></pre><h5 id="Scala的覆写字段案例"><a href="#Scala的覆写字段案例" class="headerlink" title="Scala的覆写字段案例"></a>Scala的覆写字段案例</h5><p>其实和java动态绑定机制一样，在取值时，调用age()方法，绑定内存地址就调用B类的age()方法</p><pre><code class="scala">class A&#123;val age :Int = 10&#125;class B extends A&#123;    //override重写override val age : Int = 20&#125;def main(args: Array[String]):Unit = &#123;    val obj1 = new B    val obj2 = new B    //都等于20    println(obj1.age+obj2.age)&#125;</code></pre><h5 id="覆写字段的注意事项和细节"><a href="#覆写字段的注意事项和细节" class="headerlink" title="覆写字段的注意事项和细节"></a>覆写字段的注意事项和细节</h5><p>1、<strong>def只能重写另一个def</strong>(即：方法只能重写另一个方法)</p><p>2、<strong>val只能重写另一个val</strong> 属性 或 <strong>重写不带参数的def</strong>（带了参数就是重载了）</p><p>3、var只能重写另一个<strong>抽象的var属性</strong> </p><h5 id="抽象属性-字段"><a href="#抽象属性-字段" class="headerlink" title="抽象属性(字段)"></a>抽象属性(字段)</h5><p><strong>抽象属性</strong>：声明但未初始化的变量就是抽象的属性,抽象属性在抽象类</p><p>1、一个属性没有初始化，那么这个属性就是抽象属性</p><p>2、抽象属性在编译成字节码文件时，属性并不会声明，但是会自动生成抽象方法，所以类必须声明为抽象类</p><p>3、如果是覆写一个父类的抽象属性，那么override 关键字可省略 [原因：父类的抽象属性，生成的是抽象方法，因此就不涉及到方法重写的概念，因此override可省略]</p><pre><code class="scala">//抽象类abstract class A03&#123;  var name : String//抽象&#125;class Sub_A03 extends A03&#123;  override var name: String = &quot;&quot;&#125;</code></pre><p>反编译后的java文件</p><pre><code class="java">public abstract class A03&#123;  public abstract String name();  public abstract void name_$eq(String paramString);&#125;</code></pre><h5 id="覆写案例"><a href="#覆写案例" class="headerlink" title="覆写案例"></a>覆写案例</h5><pre><code class="scala">//代码正确吗?class AAAA &#123;  var name: String = &quot;&quot;&#125;class BBBB extends AAAA &#123;  override  val name: String = &quot;jj&quot;&#125;//这段代码错误的</code></pre><p>这要是成了，就是用BBBB类的方法读，用AAAA类的方法设值了</p><pre><code class="scala">//代码正确吗?class A &#123;   def sal(): Int = &#123;      return 10  &#125;&#125;class B extends A &#123; override val sal : Int = 0&#125;//这段代码正确</code></pre><p>val可以重写不带参数的def</p><h4 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h4><p>在Scala中，通过<strong>abstract</strong>关键字标记不能被实例化的类。<strong>方法不用标记abstract，只要省掉方法体即可</strong>。抽象类可以拥有<strong>抽象字段</strong>，抽象字段&#x2F;属性就是没有初始值的字段</p><pre><code class="scala">abstract class Animal&#123;  var name : String //抽象的字段  var age : Int // 抽象的字段  var color : String = &quot;black&quot;  def cry()//抽象方法&#125;</code></pre><h5 id="抽象类基本语法"><a href="#抽象类基本语法" class="headerlink" title="抽象类基本语法"></a>抽象类基本语法</h5><pre><code>abstract class Person() &#123; // 抽象类   var name: String // 抽象字段, 没有初始化  def printName // 抽象方法, 没有方法体&#125;</code></pre><h5 id="抽象类有什么用？"><a href="#抽象类有什么用？" class="headerlink" title="抽象类有什么用？"></a>抽象类有什么用？</h5><p>抽象类的价值更多是在于设计，是设计者设计好后，让子类继承并实现抽象类(即：实现抽象类的抽象方法)</p><h5 id="Scala抽象类使用的注意事项和细节讨论"><a href="#Scala抽象类使用的注意事项和细节讨论" class="headerlink" title="Scala抽象类使用的注意事项和细节讨论"></a>Scala抽象类使用的注意事项和细节讨论</h5><p>1、抽象类不能被实例，但可以通过匿名子类，在实例化时同时实现它的抽象方法</p><p>2、抽象类不一定要包含abstract方法。也就是说,<strong>抽象类可以没有abstract方法</strong></p><p>3、一旦类包含了抽象方法或者抽象属性,则这个类必须声明为abstract</p><p>4、<strong>抽象方法不能有主体，不允许使用abstract修饰。</strong><br>5、如果一个类继承了抽象类，则它<strong>必须实现抽象类的所有抽象方法和抽象属性，除非它自己也声明为abstract类。</strong><br>6、<strong>抽象方法和抽象属性不能使用private、final 来修饰</strong>，因为这些关键字都是和重写&#x2F;实现相违背的。<br>7、抽象类中可以有实现的方法.<br>8、子类重写抽象方法不需要override，写上也不会错.</p><h4 id="匿名子类"><a href="#匿名子类" class="headerlink" title="匿名子类"></a>匿名子类</h4><p>和Java一样，可以通过包含带有定义或重写的代码块的方式创建一个匿名的子类.</p><h5 id="Java的匿名子类"><a href="#Java的匿名子类" class="headerlink" title="Java的匿名子类"></a>Java的匿名子类</h5><pre><code class="java">abstract class A2&#123;    abstract public   void cry();&#125;A2 obj = new A2() &#123;        @Override        public void cry() &#123;           System.out.println(&quot;okook!&quot;);        &#125;&#125;;</code></pre><h5 id="Scala的匿名子类"><a href="#Scala的匿名子类" class="headerlink" title="Scala的匿名子类"></a>Scala的匿名子类</h5><pre><code class="scala">abstract class Monster&#123;  var name : String  def cry()  &#125;var monster = new Monster &#123;      override var name: String = &quot;牛魔王&quot;      override def cry(): Unit = &#123;        println(&quot;牛魔王哼哼叫唤..&quot;)      &#125;    &#125;</code></pre><h5 id="继承层级关系"><a href="#继承层级关系" class="headerlink" title="继承层级关系"></a>继承层级关系</h5><p>1、在scala中，所有其他类都是AnyRef的子类，类似Java的Object。<br>2、AnyVal和AnyRef都扩展自Any类。Any类是根节点<br>3、<strong>Any中定义了isInstanceOf、asInstanceOf方法，以及哈希方法等。</strong></p><p>4、Null类型的唯一实例就是null对象。可以将null赋值给任何引用，但不能赋值给值类型的变量[案例演示]。<br>5、Nothing类型没有实例。它对于泛型结构是有用处的，举例：空列表Nil的类型是List[Nothing]，它是List[T]的子类型，T可以是任何类。</p><p><img src="/scala/scala/image-20200214130544337.png" alt="image-20200214130544337"></p><h4 id="静态属性和静态方法"><a href="#静态属性和静态方法" class="headerlink" title="静态属性和静态方法"></a>静态属性和静态方法</h4><h5 id="Java的静态概念"><a href="#Java的静态概念" class="headerlink" title="Java的静态概念"></a>Java的静态概念</h5><pre><code>public static 返回值类型  方法名(参数列表) &#123;方法体&#125;      静态属性...</code></pre><p><strong>说明：</strong>: Java中静态方法并不是通过对象调用的，而<strong>是通过类对象调用</strong>的，所以静态操作并不是面向对象的。</p><h5 id="Scala中静态的概念"><a href="#Scala中静态的概念" class="headerlink" title="Scala中静态的概念"></a>Scala中静态的概念</h5><p>Scala语言是完全面向对象(万物皆对象)的语言，所以并没有静态的操作(<strong>即在Scala中没有静态的概念</strong>)。但是为了能够和Java语言交互(因为Java中有静态概念)，就产生了一种特殊的对象来<strong>模拟类对象</strong>，我们称之为类的<strong>伴生对象</strong>。这个类的所有<strong>静态内容</strong>都可以<strong>放置在它的伴生对象</strong>中声明和调用</p><h5 id="伴生对象的快速入门"><a href="#伴生对象的快速入门" class="headerlink" title="伴生对象的快速入门"></a>伴生对象的快速入门</h5><pre><code class="scala">//class是伴生类class ScalaPerson &#123;  var name : String = _&#125;//object是伴生对象object ScalaPerson &#123;   var sex : Boolean = true&#125;</code></pre><h5 id="伴生对象的小结"><a href="#伴生对象的小结" class="headerlink" title="伴生对象的小结"></a>伴生对象的小结</h5><p>1、Scala中伴生对象采用object关键字声明，<strong>伴生对象中声明的全是 “静态”内容</strong>，可以通过伴生对象名称直接调用。<br>2、伴生对象对应的类称之为伴生类，<strong>伴生对象的名称应该和伴生类名一致。</strong></p><p>3、伴生对象中的属性和方法都可以通过伴生对象名(类名)直接调用访问<br>4、从语法角度来讲，所谓的<strong>伴生对象其实就是类的静态方法和成员的集合</strong><br>5、从技术角度来讲，scala还是没有生成静态的内容，只不过是将伴生对象生成了一个新的类，实现属性和方法的调用。[反编译看源码]<br>6、从底层原理看，<strong>伴生对象实现静态特性是依赖于 public static final  MODULE$ 实现的。</strong> </p><p>7、伴生对象的声明应该和伴生类的声明在同一个源码文件中(如果不在同一个文件中会运行错误!)，但是如果没有伴生类，也就没有所谓的伴生对象了，所以放在哪里就无所谓了。</p><p>8、如果 class A 独立存在，那么A就是一个类， 如果 object A 独立存在，那么A就是一个”静态”性质的对象[即类对象], 在 object A中声明的属性和方法可以通过 A.属性 和 A.方法 来实现调用</p><p>9、当一个文件中，存在伴生类和伴生对象时，文件的图标会发生变化</p><h6 id="伴生对象的使用实例"><a href="#伴生对象的使用实例" class="headerlink" title="伴生对象的使用实例:"></a>伴生对象的使用实例:</h6><p>设计一个var total Int表示总人数,我们在创建一个小孩时，就把total加1,并且 total是所有对象共享的就ok了!，我们使用伴生对象来解决</p><pre><code class="scala">class Child(cName:String)&#123;var name = cName&#125;object Child02 &#123;var totalChildNum = 0def joinGame(child:Child02):Unit=&#123;printf(&quot;%s 小孩加入了游戏\n&quot;,child.name)totalchildNum+=1&#125;def showNum():Unit=&#123;printf(&quot;当前有%d小孩玩游戏\n&quot;,totalChildNum)&#125;&#125;</code></pre><h5 id="伴生对象apply方法"><a href="#伴生对象apply方法" class="headerlink" title="伴生对象apply方法"></a>伴生对象apply方法</h5><p>在伴生对象中定义apply方法，可以实现： <strong>类名(参数) 方式来创建对象实例.</strong> </p><pre><code class="scala">object ApplyDemo01&#123;    def main(args:Array[String])；Unit&#123;    val list = List(1,2,5)        println(list)        val pig = new Pig(&quot;hh&quot;)                //使用apply方法创建对象        val pig2 = Pig(&quot;小黑猪&quot;)        val pig3 = pig()    &#125;&#125;class Pig(pName:String)&#123;    var name:String = pName&#125;object Pig&#123;    def apply(pName:String):Pig = new Pig(pName)    def apply :Pig = new Pig(&quot;默认&quot;)&#125;</code></pre><h4 id="Trait-接口"><a href="#Trait-接口" class="headerlink" title="Trait(接口)"></a>Trait(接口)</h4><h5 id="Java接口"><a href="#Java接口" class="headerlink" title="Java接口"></a>Java接口</h5><h6 id="声明接口"><a href="#声明接口" class="headerlink" title="声明接口"></a>声明接口</h6><pre><code>interface 接口名</code></pre><h6 id="实现接口"><a href="#实现接口" class="headerlink" title="实现接口"></a>实现接口</h6><pre><code>class 类名 implements 接口名1，接口2</code></pre><p>1、在Java中, 一个类可以实现多个接口。<br>2、在Java中，接口之间支持多继承<br>3、接口中属性都是常量<br>4、接口中的方法都是抽象的</p><h5 id="Scala接口"><a href="#Scala接口" class="headerlink" title="Scala接口"></a>Scala接口</h5><p>Scala是纯面向对象的语言，在Scala中，没有接口。</p><p>Scala语言中，<strong>采用特质trait（特征）来代替接口的概念</strong>，也就是说，多个类具有相同的特征（特征）时，就可以将这个特质（特征）独立出来，采用关键字trait声明。 <strong>理解trait 等价于(interface + abstract class)</strong></p><img src="/scala/scala/Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200217172555849.png" alt="image-20200217172555849" style="zoom:150%;"><h5 id="trait-特质-的声明"><a href="#trait-特质-的声明" class="headerlink" title="trait(特质)的声明"></a>trait(特质)的声明</h5><p>trait命名一般首字母大写</p><p>在Scala种，java中的接口可以当作特质使用</p><pre><code class="scala">trait 特质名&#123;    trait体&#125;</code></pre><h6 id="案例："><a href="#案例：" class="headerlink" title="案例："></a>案例：</h6><pre><code>object T1 extends Serializable&#123;&#125;</code></pre><h5 id="Scala中trait-的语法"><a href="#Scala中trait-的语法" class="headerlink" title="Scala中trait 的语法"></a>Scala中trait 的语法</h5><p>一个类具有某种特质（特征），就意味着这个类满足了这个特质（特征）的所有要素，所以在使用时，也采用了<strong>extends</strong>关键字，如果有多个特质或存在父类，那么需要采用<strong>with</strong>关键字连接</p><pre><code class="scala">没有父类class  类名   extends   特质1   with    特质2   with   特质3 ..有父类，(父类必须写在前面)class  类名   extends   父类   with  特质1   with   特质2   with 特质3</code></pre><p><strong>Scala的继承是单继承</strong>,也就是一个类最多只能有一个父类,这种单继承的机制可保证类<br>的纯洁性,比c++中的多继承机制简洁。但对子类功能的扩展有一定影响.所以<br>我们认为: <strong>Scala引入trait特征 第一可以替代Java的接口,  第二个也是对单继承机制</strong>的一种补充</p><h6 id="特质的快速入门案例"><a href="#特质的快速入门案例" class="headerlink" title="特质的快速入门案例"></a>特质的快速入门案例</h6><p><img src="/scala/scala/image-20200217180834234.png" alt="image-20200217180834234"></p><pre><code class="scala">trait Trait01&#123;    def getConnect()&#125;class A&#123;&#125;class B extends A&#123;&#125;class C extends C with Trait01&#123;    override def getConnect():Unit=&#123;            &#125;&#125;class D &#123;&#125;class E extends D with Trait01&#123;    override def getConnect():Unit=&#123;            &#125;&#125;class F extends D&#123;&#125;</code></pre><h5 id="带有具体实现的特质"><a href="#带有具体实现的特质" class="headerlink" title="带有具体实现的特质"></a>带有具体实现的特质</h5><p>和Java中的接口不太一样的是<strong>特质中的方法并不一定是抽象的，也可以有非抽象方法</strong>(即：实现了的方法)。</p><pre><code class="scala">trait Trait03&#123;  //抽象方法  def sayHi()  //普通方法  def sayHello(): Unit =&#123;    println(&quot;say Hello&quot;)  &#125;&#125;</code></pre><h5 id="trait执行流程"><a href="#trait执行流程" class="headerlink" title="trait执行流程"></a>trait执行流程</h5><p>同时有抽象方法和普通方法的执行流程：trait生成两个文件，Trait03和Trait03$class。Trait03是一个接口，存着trait的所有方法，Trait03$class文件是一个抽象类，存着trait的普通方法，继承了Trait03的类会实现Trait03接口，普通方法调用trait03$class的普通方法，重写接口中的抽象方法</p><pre><code class="scala">object TraitDemo02 &#123;  def main(args: Array[String]): Unit = &#123;    val sheep = new Sheep    sheep.sayHi()    sheep.sayHello()  &#125;&#125;trait Trait03&#123;  //抽象方法  def sayHi()  //普通方法  def sayHello(): Unit =&#123;    println(&quot;say Hello&quot;)  &#125;&#125;class Sheep extends Trait03&#123;  override def sayHi(): Unit = &#123;    println(&quot;Sheep hi&quot;)  &#125;&#125;</code></pre><p><img src="/scala/scala/image-20200217184449697.png" alt="image-20200217184449697"></p><h4 id="动态混入"><a href="#动态混入" class="headerlink" title="动态混入"></a>动态混入</h4><p>1、除了可以在类声明时继承特质以外，还可以在构建对象时混入特质，扩展目标类的功能</p><p>2、此种方式也可以应用于对抽象类功能进行扩展</p><p>3、动态混入是<strong>Scala特有的方式</strong>（java没有动态混入），可在不修改类声明&#x2F;定义的情况下，扩展类的功能，非常的灵活，<strong>耦合性低</strong> 。</p><p>4、动态混入可以在不影响原有的继承关系的基础上，给指定的类扩展功能。</p><p>5、要注意动态混入时，抽象类也有抽象方法，如何混入</p><pre><code class="scala">object MinInDemo01 &#123;  def main(args: Array[String]): Unit = &#123;    //不修改类定义的基础上使用trait的方法    val oracle = new OratleDB with Operate3    oracle.insert(3)    val l = new MySQL3 with Operate3 &#123;&#125;  &#125;&#125;trait Operate3 &#123;  def insert(id:Int): Unit =&#123;    println(&quot;插入数据&quot;+id)  &#125;&#125;class OratleDB&#123;&#125;abstract class MySQL3&#123;&#125;</code></pre><h5 id="Scala中创建对象有几种方式？"><a href="#Scala中创建对象有几种方式？" class="headerlink" title="Scala中创建对象有几种方式？"></a>Scala中创建对象有几种方式？</h5><p>1、new对象</p><p>2、apply创建</p><p>3、匿名子类创建</p><p>4、动态混入</p><h4 id="叠加特质"><a href="#叠加特质" class="headerlink" title="叠加特质"></a>叠加特质</h4><h5 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h5><p><strong>构建对象的同时如果混入多个特质，称之为叠加特质</strong>，那么<strong>特质声明顺序从左到右</strong>，方法执<strong>行顺序从右到左</strong>。</p><p><strong>调用父类方法是调用右边特质的方法，如果右边没有方法才会去父类找</strong></p><h6 id="案例：-1"><a href="#案例：-1" class="headerlink" title="案例："></a>案例：</h6><p>目的：分析叠加特质时，对象的构建顺序，和执行方法的顺序</p><pre><code class="scala">object AddTraits &#123;  def main(args: Array[String]): Unit = &#123;      //初始化构建    val mysql = new MySQL4 with DB4 with File4    println(mysql)    //结果：    //Operate444    //date444    //db4    //file4    //内存地址       println(&quot;________________&quot;)    println(mysql.insert(1))    //执行的顺序从右到左，调用父类方法是调用左边类的方法    //结果：    //文件    //db4    //插入数据1      &#125;&#125;trait Operate4&#123;  //特质里面的打印语句会生成一个构造器  println(&quot;Operate444&quot;)  def insert(id:Int)&#125;trait Date4 extends Operate4  &#123;  println(&quot;date444&quot;)  override def insert(id: Int): Unit = &#123;    println(&quot;插入数据&quot;+id)  &#125;&#125;trait DB4 extends Date4 &#123;  println(&quot;db4&quot;)  override def insert(id: Int): Unit = &#123;    println(&quot;db4&quot;)    super.insert(id)  &#125;&#125;trait File4 extends Date4&#123;  println(&quot;file4&quot;)  override def insert(id: Int): Unit = &#123;    println(&quot;文件&quot;)    super.insert(id)  &#125;&#125;class MySQL4&#123;&#125;</code></pre><h5 id="叠加特质注意事项和细节"><a href="#叠加特质注意事项和细节" class="headerlink" title="叠加特质注意事项和细节"></a>叠加特质注意事项和细节</h5><p>1、<strong>特质声明顺序从左到右。</strong><br>2、Scala在执行叠加对象的方法时，会首<strong>先从后面的特质(从右向左)开始执行</strong><br>3、Scala中特质中如果<strong>调用super</strong>，并不是表示调用父特质的方法，而<strong>是向前面（左边）继续查找特质，如果找不到，才会去父特质查找</strong><br>4、如果想要调用具体特质的方法，可以指定：super[特质].xxx(…).其中的泛型<strong>必须是该特质的直接超类类型</strong></p><pre><code class="scala">trait File4 extends Date4&#123;  println(&quot;file4&quot;)  override def insert(id: Int): Unit = &#123;    println(&quot;文件&quot;)    super[Date4].insert(id)  &#125;&#125;</code></pre><h5 id="在特质中重写抽象方法"><a href="#在特质中重写抽象方法" class="headerlink" title="在特质中重写抽象方法"></a>在特质中重写抽象方法</h5><p>给某个方法增加了abstract override ，重写了父特质的抽象方法</p><pre><code class="scala">object MixInDemo02 &#123;  def main(args: Array[String]): Unit = &#123;    val mySql = new MySql5 with DB5 with File5    mySql.insert(666)    val mySql_ = new MySql5 with File5 with DB5 //error    //File5在父类MySql5找实现方法没找到  &#125;&#125;trait Operate5&#123;  def insert(id:Int)&#125;trait File5 extends Operate5&#123;    //在这里先输出File5，然后找左边父类的insert方法，输出DB5  abstract override def insert(id: Int): Unit = &#123;    println(&quot;File5&quot;)    super.insert(id)  &#125;&#125;trait DB5 extends Operate5&#123;  override def insert(id: Int): Unit = &#123;    println(&quot;DB5&quot;)  &#125;&#125;class MySql5&#123;&#125;</code></pre><h6 id="理解-abstract-override-的小技巧"><a href="#理解-abstract-override-的小技巧" class="headerlink" title="理解 abstract override 的小技巧"></a>理解 abstract override 的小技巧</h6><p>重写抽象方法时需要考虑混入特质的<strong>顺序问题</strong>和<strong>完整性</strong>问题 </p><p>我们给某个方法增加了abstract override 后，就是明确的告诉编译器，该方法确实是重写了父特质的抽象方法，但是重写后，该方法仍然是一个抽象方法（因为没有完全的实现，需要其它特质继续实现[通过混入顺序]）</p><h5 id="富接口使用的特质"><a href="#富接口使用的特质" class="headerlink" title="富接口使用的特质"></a>富接口使用的特质</h5><p>富接口：即该<strong>特质中既有抽象方法，又有非抽象方法</strong></p><h5 id="特质中的具体字段"><a href="#特质中的具体字段" class="headerlink" title="特质中的具体字段"></a>特质中的具体字段</h5><p>特质中可以定义具体字段，如果<strong>初始化了就是具体字段，如果不初始化就是抽象字</strong>段。混入该特质的类就具有了该字段，字段不是继承，而<strong>是直接加入类，成为自己的字段</strong>。</p><p>特质中未被初始化的字段在具体的子类中必须被重写。</p><h5 id="特质构造顺序"><a href="#特质构造顺序" class="headerlink" title="特质构造顺序"></a>特质构造顺序</h5><pre><code class="scala">//从左到右，从Mysql开始初始化，先执行MySql构造器val mysql2 = new MySQL4 with File4 with DB4</code></pre><pre><code class="scala">//从左到右，从EE开始初始化，最后执行FF构造器class FF extends EE with CC with DD &#123;  println(&quot;F....&quot;)&#125;</code></pre><h5 id="分析两种方式对构造顺序的影响"><a href="#分析两种方式对构造顺序的影响" class="headerlink" title="分析两种方式对构造顺序的影响"></a>分析两种方式对构造顺序的影响</h5><p> 第1种方式实际是构造匿名子类，可以理解成在混入特质时，对象已经创建了。 第2种方式实际是构建类对象, 在混入特质时，该对象还没有创建。</p><h5 id="扩展类的特质"><a href="#扩展类的特质" class="headerlink" title="扩展类的特质"></a>扩展类的特质</h5><p>特质可以继承类，以用来扩展该特质类的一些功能</p><pre><code class="scala">trait LoggedException extends Exception&#123;  def log(): Unit =&#123;    println(getMessage)  &#125;&#125;</code></pre><p>所有混入该特质的类，<strong>会自动成为那个特质所继承的超类的子类</strong>（UnhappyException2也是Exception的子类了）</p><pre><code class="scala">class UnhappyException2 extends IndexOutOfBoundsException with LoggedException&#123;&#125;</code></pre><p>如果混<strong>入该特质的类，已经继承了另一个类(A类)，则要求A类是特质超类的子类</strong>，否则就会出现了多继承现象，发生错误。（要有相同的父类）</p><pre><code class="scala">//他们的父类都是Exceptionclass UnhappyException2 extends IndexOutOfBoundsException with LoggedException&#123;&#125;//错误案例：class CCC&#123;&#125;class UnhappyException3 extends CCC with LoggedException&#123;&#125;</code></pre><h5 id="自身类型"><a href="#自身类型" class="headerlink" title="自身类型"></a>自身类型</h5><p>自身类型：主要是为了<strong>解决特质的循环依赖问题</strong>，同时可以确保特质在不扩展某个类的情况下，依然可以做到限制混入该特质的类的类型。</p><pre><code class="scala">//Logger就是自身类型特质trait Logger &#123;  // 明确告诉编译器，我就是Exception,如果没有这句话，下面的getMessage不能调用  this: Exception =&gt;  def log(): Unit =&#123;    // 既然我就是Exception, 那么就可以调用其中的方法    println(getMessage)  &#125;&#125;</code></pre><pre><code class="scala">class Console extends  Logger &#123;&#125; //errorclass Console extends Exception with Logger//ok</code></pre><h5 id="面试题：Java中，类共有五大成员，请说明是哪五大成员"><a href="#面试题：Java中，类共有五大成员，请说明是哪五大成员" class="headerlink" title="面试题：Java中，类共有五大成员，请说明是哪五大成员"></a>面试题：Java中，类共有五大成员，请说明是哪五大成员</h5><p>1.属性<br>2.方法<br>3.内部类<br>4.构造器<br>5.代码块</p><h4 id="嵌套类"><a href="#嵌套类" class="headerlink" title="嵌套类"></a>嵌套类</h4><h5 id="Java的内部类"><a href="#Java的内部类" class="headerlink" title="Java的内部类"></a>Java的内部类</h5><p>在类中可以再定义一个类，这样的类是嵌套类，其他语法结构也是一样。<br>嵌套类类似于Java中的内部类</p><p>在Java中，一个类的内部又完整的嵌套了另一个完整的类结构。被<strong>嵌套的类称为内部类(inner class)<strong>，嵌套其他类的类称为外部类。内部类最大的特点就是</strong>可以直接访问私有属性</strong>，并且可以体现类与类之间的包含关系 </p><h6 id="Java内部类基本语法"><a href="#Java内部类基本语法" class="headerlink" title="Java内部类基本语法"></a>Java内部类基本语法</h6><pre><code class="java">class Outer&#123;//外部类   class Inner&#123;//内部类&#125;&#125;class Other&#123;//外部其他类&#125;</code></pre><h6 id="Java内部类的分类"><a href="#Java内部类的分类" class="headerlink" title="Java内部类的分类"></a>Java内部类的分类</h6><p>从定义在外部类的成员位置上来看，</p><p><strong>1) 成员内部类（没用static修饰）</strong><br><strong>2) 和静态内部类（使用static修饰），</strong></p><p>定义在外部类局部位置上（比如方法内）来看：<br><strong>分为局部内部类（有类名）</strong><br><strong>匿名内部类（没有类名</strong>）</p><h4 id="Scala嵌套类"><a href="#Scala嵌套类" class="headerlink" title="Scala嵌套类"></a>Scala嵌套类</h4><pre><code class="scala">class ScalaOuterClass &#123;  class ScalaInnerClass &#123; //成员内部类  &#125;&#125;object ScalaOuterClass &#123;  //伴生对象  class ScalaStaticInnerClass &#123; //静态内部类  &#125;&#125;</code></pre><pre><code class="scala"> val outer1 : ScalaOuterClass = new ScalaOuterClass(); val outer2 : ScalaOuterClass = new ScalaOuterClass(); // Scala创建内部类的方式和Java不一样，将new关键字放置在前，使用  对象.内部类  的方式创建 val inner1 = new outer1.ScalaInnerClass() val inner2 = new outer2.ScalaInnerClass() //创建静态内部类对象 val staticInner = new ScalaOuterClass.ScalaStaticInnerClass() println(staticInner)</code></pre><h5 id="在内部类中访问外部类的属性"><a href="#在内部类中访问外部类的属性" class="headerlink" title="在内部类中访问外部类的属性"></a>在内部类中访问外部类的属性</h5><h6 id="方式1"><a href="#方式1" class="headerlink" title="方式1"></a>方式1</h6><p>内部类如果想要访问外部类的属性，可以通过外部类对象访问。</p><p>即：<strong>访问方式：外部类名.this.属性名</strong>  </p><pre><code class="scala">class ScalaOuterClass &#123;  var name : String = &quot;scott&quot;  private var sal : Double = 1.2  class ScalaInnerClass &#123; //成员内部类    def info() = &#123;      // 访问方式：外部类名.this.属性名      // 怎么理解 ScalaOuterClass.this 就相当于是 ScalaOuterClass 这个外部类的一个实例,      // 然后通过 ScalaOuterClass.this 实例对象去访问 name 属性      // 只是这种写法比较特别，学习java的同学可能更容易理解 ScalaOuterClass.class 的写法.      println(&quot;name = &quot; + ScalaOuterClass.this.name        + &quot; age =&quot; + ScalaOuterClass.this.sal)    &#125;&#125;&#125;</code></pre><h6 id="方式2"><a href="#方式2" class="headerlink" title="方式2"></a>方式2</h6><p>内部类如果想要访问外部类的属性，也可以通过外部类别名访问(<strong>推荐</strong>)。</p><p>即：<strong>访问方式：外部类名别名.属性名</strong>   </p><pre><code class="scala">class ScalaOuterClass &#123;  myOuter =&gt;  //这样写，你可以理解成这样写，myOuter就是代表外部类的一个对象.  class ScalaInnerClass &#123; //成员内部类    def info() = &#123;      println(&quot;name = &quot; + ScalaOuterClass.this.name        + &quot; age =&quot; + ScalaOuterClass.this.sal)      println(&quot;-----------------------------------&quot;)      println(&quot;name = &quot; + myOuter.name        + &quot; age =&quot; + myOuter.sal)    &#125;&#125;  // 当给外部指定别名时，需要将外部类的属性放到别名后.  var name : String = &quot;scott&quot;  private var sal : Double = 1.2&#125;</code></pre><h4 id="类型投影"><a href="#类型投影" class="headerlink" title="类型投影"></a>类型投影</h4><p>&#x2F;&#x2F;说明下面调用test 的 正确和错误的原因：<br>&#x2F;&#x2F;1.Java中的内部类从属于外部类,因此在java中 inner.test(inner2) 就可以，因为是按类型来匹配的。<br>&#x2F;&#x2F;2 <strong>Scala中内部类从属于外部类的对象</strong>，所以外部类的对象不一样，创建出来的内部类也不一样，无法互换使用<br>&#x2F;&#x2F;3. 比如你使用idea 看一下在inner1.test()的形参上，它提示的类型是 outer1.ScalaOuterClass, 而不是ScalaOuterClass</p><p>inner1.test(inner1) &#x2F;&#x2F;</p><p> ok inner1.test(inner2) &#x2F;&#x2F; 错误</p><pre><code class="scala">class ScalaOuterClass3 &#123;  myOuter =&gt;  class ScalaInnerClass3 &#123; //成员内部类    def test(ic: ScalaInnerClass3): Unit = &#123;      System.out.println(ic)    &#125;  &#125;&#125;object Scala01_Class &#123;    def main(args: Array[String]): Unit = &#123;        val outer1 : ScalaOuterClass3 = new ScalaOuterClass3();        val outer2 : ScalaOuterClass3 = new ScalaOuterClass3();        val inner1 = new outer1.ScalaInnerClass3()        val inner2 = new outer2.ScalaInnerClass3()        inner1.test(inner1) // ok, 因为 需要outer1.ScalanInner        inner1.test(inner2) // error, 需要outer1.ScalanInnerouter2.ScalanInner    &#125;&#125;</code></pre><h5 id="使用类型投影"><a href="#使用类型投影" class="headerlink" title="使用类型投影"></a>使用类型投影</h5><p>类型投影是指：在方法声明上，如果使用  外部类#内部类  的方式，表示忽略内部类的对象关系，等同于Java中内部类的语法操作，我们将这种方式称之为 类型投影（即：<strong>忽略对象的创建方式，只考虑类型</strong>）</p><pre><code class="scala">class ScalaOuterClass3 &#123;  myOuter =&gt;  class ScalaInnerClass3 &#123; //成员内部类    def test(ic: ScalaOuterClass3#ScalaInnerClass3): Unit = &#123;      System.out.println(ic)    &#125;  &#125;&#125;</code></pre><h4 id="隐式转换和隐式值"><a href="#隐式转换和隐式值" class="headerlink" title="隐式转换和隐式值"></a>隐式转换和隐式值</h4><p>隐式转换函数是以<strong>implicit</strong>关键字声明的带有单个参数的函数。这种函数将会<strong>自动应用</strong>，将值<strong>从一种类型转换为另一种类型</strong></p><pre><code class="scala">//解决数据类型转换,Double转Intimplicit def f1(d: Double): Int = &#123;  d.toInt&#125;</code></pre><h5 id="隐式转换的注意事项和细节"><a href="#隐式转换的注意事项和细节" class="headerlink" title="隐式转换的注意事项和细节"></a>隐式转换的注意事项和细节</h5><p>1、隐式转换函数的函数名可以是任意的，隐式转换与函数名称无关，只与<strong>函数签名</strong>（<strong>函数参数类型和返回值类型</strong>）有关。</p><p>2、隐式函数可以有多个(即：隐式函数列表)，但是需要保证在当前环境下，<strong>只有一个隐式函数能被识别</strong></p><pre><code class="scala">implicit def a(d: Double) = d.toIntimplicit def b(d: Double) = d.toInt val i1 :Int = 3.5//error,编译器不知道该用哪个，所以出错</code></pre><h5 id="隐式转换丰富类库功能"><a href="#隐式转换丰富类库功能" class="headerlink" title="隐式转换丰富类库功能"></a>隐式转换丰富类库功能</h5><p>隐式转换方式动态的给MySQL类增加delete方法。</p><pre><code class="scala">//给mysql添加delete方法，不修改mysqlclass MySQL&#123;  def insert(): Unit = &#123;    println(&quot;insert&quot;)  &#125;&#125;class DB &#123;  def delete(): Unit = &#123;    println(&quot;delete&quot;)  &#125;&#125;//隐式转换implicit def addDelete(mysql:MySql):DB=&#123;new DB&#125;</code></pre><h4 id="隐式值"><a href="#隐式值" class="headerlink" title="隐式值"></a>隐式值</h4><h5 id="基本介绍-1"><a href="#基本介绍-1" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>隐式值也叫隐式变量，将某个形参变量标记为implicit，所以编译器会在方法省略隐式参数的情况下去搜索作用域内的隐式值作为缺省参数</p><pre><code class="scala">implicit val str1: String = &quot;jack&quot;def hello(implicit name: String): Unit = &#123;println(name + &quot; hello&quot;)&#125;hello //调用.不带()//jackhello</code></pre><h5 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h5><pre><code class="scala">//报错，因为两个相似的不知道选哪个(二义性)object ImplicitVal02 &#123;  def main(args: Array[String]): Unit = &#123;      // 隐式变量（值）      implicit val name: String = &quot;Scala&quot;      implicit val name1: String = &quot;World&quot;      def hello(implicit content: String = &quot;jack&quot;): Unit = &#123;        println(&quot;Hello &quot; + content)      &#125; //调用hello      hello    &#125;&#125;</code></pre><pre><code class="scala">//输出HelloScala，隐式值是直接传入方法的，比默认值优先级高object ImplicitVal02 &#123;  def main(args: Array[String]): Unit = &#123;      // 隐式变量（值）      implicit val name: String = &quot;Scala&quot;      def hello(implicit content: String = &quot;jack&quot;): Unit = &#123;        println(&quot;Hello &quot; + content)      &#125; //调用hello      hello    &#125;&#125;</code></pre><pre><code class="scala">//类型不匹配按默认值执行object ImplicitVal02 &#123;  def main(args: Array[String]): Unit = &#123;      // 隐式变量（值）      implicit val Int: Int = &quot;Scala&quot;      def hello(implicit content: String = &quot;jack&quot;): Unit = &#123;        println(&quot;Hello &quot; + content)      &#125; //调用hello      hello    &#125;&#125;</code></pre><pre><code class="scala">//三个(传值，默认值，隐式参数)都没有，会报错object ImplicitVal02 &#123;  def main(args: Array[String]): Unit = &#123;      // 隐式变量（值）      implicit val Int: Int = &quot;Scala&quot;      def hello(implicit content: String): Unit = &#123;        println(&quot;Hello &quot; + content)      &#125; //调用hello      hello    &#125;&#125;</code></pre><h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><p>1、但在程序中，同时有隐式值，默认值，传值</p><p>2、编译器优先级为传值，隐式值，默认值</p><p>3、隐式值匹配时，不能有二义性</p><p>4、如果三个(传值，隐式值，默认值)一个都没有就会报错</p><h4 id="隐式类"><a href="#隐式类" class="headerlink" title="隐式类"></a>隐式类</h4><p>在scala2.10后提供了隐式类，可以使用implicit声明类，隐式类的非常强大，同样可以扩展类的功能，比前面使用隐式转换丰富类库功能更加的方便，在集合中隐式类会发挥重要的作用。</p><h5 id="隐式类使用有如下几个特点："><a href="#隐式类使用有如下几个特点：" class="headerlink" title="隐式类使用有如下几个特点："></a>隐式类使用有如下几个特点：</h5><p>1、其所带的<strong>构造参数有且只能有一个</strong><br>2、隐式类必须被定义在“类”或“伴生对象”或“包对象”里，即<strong>隐式类不能是 顶级的</strong>(top-level  objects)。<br>3、隐式类不能是case class（case class在后续介绍 样例类）<br>4、<strong>作用域内不能有与之相同名称的标识符</strong></p><pre><code class="scala">object ImplicitClassDemo &#123;  def main(args: Array[String]): Unit = &#123;  //隐式类    implicit class DB1(val m: MySQL1) &#123;      def addSuffix(): String = &#123;        m + &quot;scala&quot;      &#125;    &#125;    val mysql = new MySQL1    mysql.addSuffix()  &#125;&#125;class MySQL1&#123;  def sayOk(): Unit =&#123;    println(&quot;sayOk&quot;)  &#125;&#125;</code></pre><pre><code class="scala">object ImplicitClassDemo &#123;  def main(args: Array[String]): Unit = &#123;    val mysql = new MySQL1    mysql.addSuffix()  &#125;&#125;  //隐式类不能放在最外面    implicit class DB1(val m: MySQL1) &#123;      def addSuffix(): String = &#123;        m + &quot;scala&quot;      &#125;    &#125;class MySQL1&#123;  def sayOk(): Unit =&#123;    println(&quot;sayOk&quot;)  &#125;&#125;</code></pre><h5 id="隐式的转换时机"><a href="#隐式的转换时机" class="headerlink" title="隐式的转换时机"></a>隐式的转换时机</h5><p>1、当方法中的参数的类型与目标类型不一致时，或是赋值时<br>2、当对象调用所在类中不存在的方法或成员时，编译器会自动将对象进行隐式转换（根据类型）</p><pre><code class="scala">implicit def (d:Double):Int=&#123;    d.toInt&#125;def test1(n1:Int)&#123;    println(&quot;ok&quot;)&#125;test1(10.1)</code></pre><h6 id="隐式解析机制"><a href="#隐式解析机制" class="headerlink" title="隐式解析机制"></a>隐式解析机制</h6><p>即编译器是如何查找到缺失信息的，解析具有以下两种规则：</p><p>1)首先会在当前代码作用域下查找隐式实体（隐式方法、隐式类、隐式对象）。(<strong>一般是这种情况</strong>)</p><p>2)如果第一条规则查找隐式实体失败，会继续在隐式参数的类型的作用域里查找。类型的作用域是指与该类型相关联的全部伴生模块，一个隐式实体的类型T它的查找范围如下(<strong>第二种情况范围广且复杂在使用时，应当尽量避免出现</strong>)：</p><p>a) 如果T被定义为T with A with B with C,那么A,B,C都是T的部分，在T的隐式解析过程中，它们的伴生对象都会被搜索。</p><p>b) 如果T是参数化类型，那么类型参数和与类型参数相关联的部分都算作T的部分，比如List[String]的隐式搜索会搜索List的伴生对象和String的伴生对象。</p><p>c) 如果T是一个单例类型p.T，即T是属于某个p对象内，那么这个p对象也会被搜索。</p><p>d) 如果T是个类型注入S#T，那么S和T都会被搜索。</p><h5 id="隐式转换的前提"><a href="#隐式转换的前提" class="headerlink" title="隐式转换的前提"></a>隐式转换的前提</h5><p>在进行隐式转换时，需要遵守两个基本的前提：</p><p>1)不能存在二义性</p><p>2)隐式操作不能嵌套使用 &#x2F;&#x2F; [举例：]如:隐式转换函数</p><pre><code class="scala">implicit def (d:Double):Int=&#123;    d.toInt    //error不能嵌套，等于自己调自己形成递归    var num2 = 1.2&#125;</code></pre><p>String是char的一个集合，按照索引来查询的 </p><h4 id="数据结构（应用）"><a href="#数据结构（应用）" class="headerlink" title="数据结构（应用）"></a>数据结构（应用）</h4><h5 id="scala集合基本介绍"><a href="#scala集合基本介绍" class="headerlink" title="scala集合基本介绍"></a>scala集合基本介绍</h5><p>Scala同时支持<strong>不可变集合</strong>和<strong>可变集合</strong>，不可变集合可以安全的并发访问<br>两个主要的包：</p><p><strong>不可变集合：scala.collection.immutable</strong><br><strong>可变集合：  scala.collection.mutable</strong></p><p><strong>Scala默认采用不可变集合</strong>，对于几乎所有的集合类，Scala都同时提供了可变(mutable)和不可变(immutable)的版本<br>Scala的集合有<strong>三大类：序列Seq、集Set、映射Map</strong>，所有的集合都扩展自<strong>Iterable</strong>特质，在Scala中集合有可变（mutable）和不可变（immutable）两种类型。 </p><h6 id="什么时候用什么集合？"><a href="#什么时候用什么集合？" class="headerlink" title="什么时候用什么集合？"></a>什么时候用什么集合？</h6><p>序列(Seq，有序的，Linear Seq)</p><p>Set(去重时用Set)</p><p>Map()</p><h5 id="可变集合和不可变集合举例"><a href="#可变集合和不可变集合举例" class="headerlink" title="可变集合和不可变集合举例"></a>可变集合和不可变集合举例</h5><p><strong>不可变集合</strong>：scala不可变集合，就是这个<strong>集合本身不能动态变化</strong>。(类似java的数组，是不可以动态增长的)<br><strong>可变集合</strong>:可变集合，就是这个<strong>集合本身可以动态变化的</strong>。(比如:ArrayList , 是可以动态增长的) </p><h5 id="Scala不可变集合继承关系一览图"><a href="#Scala不可变集合继承关系一览图" class="headerlink" title="Scala不可变集合继承关系一览图"></a>Scala不可变集合继承关系一览图</h5><p><img src="/scala/scala/image-20200222111236407.png" alt="image-20200222111236407"></p><h6 id="不可变集合小结："><a href="#不可变集合小结：" class="headerlink" title="不可变集合小结："></a>不可变集合小结：</h6><p>1.Set、Map是Java中也有的集合</p><p>2.Seq是Java没有的，我们发现List归属到Seq了,因此这里的List就和java不是同一个概念了</p><p>3.我们前面的for循环有一个 1 to 3 , 就是IndexedSeq 下的Vector<br>4.String也是属于IndexeSeq<br>5.我们发现经典的数据结构比如Queue 和 Stack被归属到LinearSeq<br>6.大家注意Scala中的Map体系有一个SortedMap,说明<strong>Scala的Map可以支持排序</strong><br>7**.IndexSeq 和 LinearSeq 的区别**[IndexSeq是通过索引来查找和定位，因此速度快，比如String就是一个索引集合，通过索引即可定位] [LineaSeq 是线型的，即有头尾的概念，这种数据结构一般是通过遍历来查找，它的价值在于应用到一些具体的应用场景 (电商网站, 大数据推荐系统 :最近浏览的10个商品)</p><p><strong>他返回了一个新的对象，这里面就是指返回了一个新的集合，而老的集合没有改变。</strong></p><h5 id="Scala可变集合继承关系一览图"><a href="#Scala可变集合继承关系一览图" class="headerlink" title="Scala可变集合继承关系一览图"></a>Scala可变集合继承关系一览图</h5><img src="/scala/scala/Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200222111943952.png" alt="image-20200222111943952" style="zoom:150%;"><h6 id="可变集合小结"><a href="#可变集合小结" class="headerlink" title="可变集合小结"></a>可变集合小结</h6><p>1）在可变集合中比不可变集合更加丰富</p><p>2）在Seq集合中，增加了Buffer(缓冲)集合，将来开发我们常用的有ArrayBuffer和ListBuffer</p><p>3）如果涉及到线程安全，使用Syn开头的集合</p><h4 id="定长数组-声明泛型"><a href="#定长数组-声明泛型" class="headerlink" title="定长数组(声明泛型)"></a>定长数组(声明泛型)</h4><p>第一种方式定义数组</p><pre><code class="scala">这里的数组等同于Java中的数组,中括号的类型就是数组的类型val arr1 = new Array[Int](10)//赋值,集合元素采用小括号访问arr1(1) = 7 for (i &lt;- arr01) &#123;println(i)&#125;</code></pre><p>第二种方式定义数组</p><pre><code class="scala">在定义数组时，直接赋值//使用apply方法创建数组对象val arr1 = Array(1, 2)//arr1的类型是Intfor (i &lt;- arr02) &#123;println(i)&#125;val arr2 = Array(1, 2,&quot;String&quot;)//arr2的类型是Anyfor (i &lt;- arr2) &#123;println(i)&#125;</code></pre><h5 id="变长数组-声明泛型"><a href="#变长数组-声明泛型" class="headerlink" title="变长数组(声明泛型)"></a>变长数组(声明泛型)</h5><pre><code class="scala">//定义/声明val arr2 = ArrayBuffer[Int]()//追加值/元素arr2.append(7)//重新赋值arr2(0) = 7//学习集合的流程(创建,查询,修改,删除)</code></pre><h5 id="变长数组分析小结"><a href="#变长数组分析小结" class="headerlink" title="变长数组分析小结"></a>变长数组分析小结</h5><ol><li>ArrayBuffer是变长数组，类似java的ArrayList</li></ol><p>2)val arr2 &#x3D; ArrayBuffer<a href>Int</a> 也是使用的apply方法构建对象</p><p>3)<strong>def</strong> append(elems: A*) { appendAll(elems) } 接收的是可变参数.</p><p>4)<strong>每append一次，arr在底层会重新分配空间，进行扩容，arr2的内存地址会发生变化，也就成为新的ArrayBuffer</strong></p><h5 id="定长数组与变长数组的转换"><a href="#定长数组与变长数组的转换" class="headerlink" title="定长数组与变长数组的转换"></a>定长数组与变长数组的转换</h5><pre><code>arr1.toBuffer  //定长数组转可变数组arr2.toArray  //可变数组转定长数组//！注意：//arr2.toArray 返回结果才是一个定长数组， arr2本身没有变化//arr1.toBuffer返回结果才是一个可变数组， arr1本身没有变化</code></pre><pre><code class="scala">val buffer = ArrayBuffer[Int]()buffer.append(1)val array2 = buffer.toArray</code></pre><pre><code class="scala">val array = Array[Int](3)array(0)=32val buffer1 = array.toBuffer</code></pre><h4 id="多维数组"><a href="#多维数组" class="headerlink" title="多维数组"></a>多维数组</h4><h5 id="多维数组的定义和使用"><a href="#多维数组的定义和使用" class="headerlink" title="多维数组的定义和使用"></a>多维数组的定义和使用</h5><pre><code class="scala">val arr = Array.ofDim[Double](3,4)//说明：二维数组中有三个一维数组，每个一维数组中有四个元素//赋值arr(1)(1) = 11.11</code></pre><pre><code class="scala">    val arr = Array.ofDim[Int](3, 4)    arr(1)(1)=100    //遍历方式1    for (item &lt;- arr) &#123; //取出二维数组的各个元素      for (item2 &lt;- item) &#123; //一维数组元素捋一遍        print(&quot;\t&quot;+item2)      &#125;      println()    &#125;    //遍历方式2    for (item &lt;- 0 to arr.length-1) &#123; //取出二维数组的各个元素      for (item2 &lt;-0 to arr(item).length-1) &#123; //一维数组元素捋一遍        printf(&quot;arr[%s][%h]=%s\t&quot;,item,item2,arr(item)(item2))      &#125;      println()    &#125;</code></pre><h5 id="Scala数组与Java的List的互转"><a href="#Scala数组与Java的List的互转" class="headerlink" title="Scala数组与Java的List的互转"></a>Scala数组与Java的List的互转</h5><h5 id="在项目开发中，有时我们需要将Scala数组转成Java数组，看下面案例："><a href="#在项目开发中，有时我们需要将Scala数组转成Java数组，看下面案例：" class="headerlink" title="在项目开发中，有时我们需要将Scala数组转成Java数组，看下面案例："></a>在项目开发中，有时我们需要将Scala数组转成Java数组，看下面案例：</h5><pre><code class="scala">    // Scala集合和Java集合互相转换    val buffer = ArrayBuffer(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;)    println(buffer)    import scala.collection.JavaConversions.bufferAsJavaList    //这里通过调用隐式函数buffer转成List类型，调用ProcessBuilder构造器    var javaarr = new ProcessBuilder(buffer)    //调用command方法返回arrlist    val arrList = javaarr.command()    println(arrList)//输出[1,2,3]</code></pre><h6 id="补充多态知识点"><a href="#补充多态知识点" class="headerlink" title="补充多态知识点:"></a><strong>补充多态知识点</strong>:</h6><pre><code class="scala">trait MyTrait01 &#123;&#125;class A extends MyTrait01 &#123;&#125;object B &#123;  def test(m: MyTrait01): Unit = &#123;    println(&quot;b ok..&quot;)  &#125;&#125;//明确一个知识点//当一个类继承了一个trait//那么该类的实例，就可以传递给这个trait引用val a01 = new AB.test(a01)</code></pre><h5 id="Scala数组与Java数组的互转"><a href="#Scala数组与Java数组的互转" class="headerlink" title="Scala数组与Java数组的互转"></a>Scala数组与Java数组的互转</h5><p>在项目开发中，有时我们需要将Java的List转成Scala数组，看下面案例：</p><pre><code class="scala"> import scala.collection.JavaConversions.asScalaBuffer    import scala.collection.mutable    //java的list转换成java的ArrayBuffer    val scalaArr :mutable.Buffer[String] = arrList    scalaArr.append(&quot;jack&quot;)    scalaArr.append(&quot;tome&quot;)    scalaArr.remove(0)    scalaArr.set(0,&quot;100&quot;)    println(scalaArr)</code></pre><h4 id="元组Tuple"><a href="#元组Tuple" class="headerlink" title="元组Tuple"></a>元组Tuple</h4><p>元组也是可以理解为一个<strong>容器</strong>，可以存放各种相同或不同类型的数据。</p><p>说的简单点，就是将<strong>多个无关的数据封装为一个整体，称为元组, 最多的特点灵活,对数据没有过多的约束。</strong></p><h5 id="元组中最大只能有22个元素"><a href="#元组中最大只能有22个元素" class="headerlink" title="元组中最大只能有22个元素"></a>元组中最大只能有22个元素</h5><h5 id="元组的创建"><a href="#元组的创建" class="headerlink" title="元组的创建"></a>元组的创建</h5><pre><code class="scala">val tuple1 = (1, 2, 3, &quot;hello&quot;, 4)println(tuple1)//Tuple5</code></pre><p>1)t1 的类型是 Tuple5类 是scala特有的类型</p><p>2)t1 的类型取决于 t1 后面有多少个元素, 有对应关系，比如 4个元素&#x3D;》Tuple4</p><p>3)给大家看一个Tuple5 类的定义,大家就了然了</p><pre><code class="scala">  /*  final case class Tuple5[+T1, +T2, +T3, +T4, +T5](_1: T1, _2: T2, _3: T3, _4: T4, _5: T5) extends Product5[T1, T2, T3, T4, T5] &#123; override def toString() = &quot;(&quot; + _1 + &quot;,&quot; + _2 + &quot;,&quot; + _3 + &quot;,&quot; + _4 + &quot;,&quot; + _5 + &quot;)&quot;  &#125;   */</code></pre><p>4)元组中最大只能有22个元素 即 Tuple1…Tuple22</p><h5 id="元组数据的访问"><a href="#元组数据的访问" class="headerlink" title="元组数据的访问"></a>元组数据的访问</h5><p>访问元组中的数据,可以采用<strong>顺序号（_顺序号</strong>），也可以通过<strong>索引（productElement）</strong>访问。</p><pre><code class="scala">val t1 = (1,2,3,&quot;5&quot;)//访问第一个元素，从1开始println(t1._2) //2//访问第一个元素，从0开始println(t1.productElement(0))//1</code></pre><h5 id="遍历元组"><a href="#遍历元组" class="headerlink" title="遍历元组"></a>遍历元组</h5><p>遍历元组，元组的遍历需要迭代器</p><pre><code class="scala">    val t1 = (1,2,3,&quot;5&quot;)    for (item &lt;- tuple1.productIterator) &#123;      println(item)    &#125;</code></pre><h4 id="创建List"><a href="#创建List" class="headerlink" title="创建List"></a>创建List</h4><p>Scala中的List 和Java List 不一样，在Java中List是一个接口，真正存放数据是ArrayList，而<strong>Scala的List可以直接存放数据，就是一个object，默认情况下Scala的List是不可变的，List属于序列Seq。</strong><br>val List &#x3D; scala.collection.immutable.List<br>object List extends SeqFactory[List]</p><pre><code class="scala">    val list01 = List(1,2,3)    println(list01)    val list02 = Nil//空集合    println(list02)</code></pre><h5 id="List说明"><a href="#List说明" class="headerlink" title="List说明"></a>List说明</h5><p>1、List是scala.collection.immutable.List，既不可变</p><p>2、在scala中，List就是不可变的，<strong>要使用可变的List，使用ListBuffer</strong></p><p>3、List在package object scala做了val List &#x3D; scala.collection.immutable.List</p><p>4、val Nil &#x3D; scala.collection.immutable.Nil&#x2F;&#x2F;List()</p><h5 id="创建List的应用案例小结"><a href="#创建List的应用案例小结" class="headerlink" title="创建List的应用案例小结"></a>创建List的应用案例小结</h5><p>1）List默认为不可变的集合<br>2）List 在 scala包对象声明的,因此不需要引入其它包也可以使用<br>3）val List &#x3D; scala.collection.immutable.List<br>4）List 中可以放任何数据类型，比如 arr1的类型为 List[Any]<br>如果希望得到一个空列表，可以使用Nil对象, 在 scala包对象声明的,因此不需要引入其它包也可以使用val Nil &#x3D; scala.collection.immutable.Nil</p><h5 id="访问List元素"><a href="#访问List元素" class="headerlink" title="访问List元素"></a>访问List元素</h5><pre><code>val value1 = list1(1) // 1是索引，表示取出第2个元素.println(value1)</code></pre><h5 id="元素的追加"><a href="#元素的追加" class="headerlink" title="元素的追加"></a>元素的追加</h5><p>向列表中增加元素, <strong>会返回新的列表&#x2F;集合对象</strong>（没有破坏List不可变的特性）。注意：Scala中List元素的追加形式非常独特，和Java不一样。</p><p><strong>冒号那头是集合，加号那边是元素</strong></p><h6 id="在列表的最后面增加数据"><a href="#在列表的最后面增加数据" class="headerlink" title="在列表的最后面增加数据"></a>在列表的最后面增加数据</h6><pre><code class="scala">var list1 = List(1, 2, 3, &quot;abc&quot;)val list2 = list1 :+ 4// :+运算符表示在列表的最后增加数据println(list1) //list1没有变化println(list2) //新的列表结果是 [1, 2, 3, &quot;abc&quot;, 4]</code></pre><h6 id="在列表的最前面增加数据"><a href="#在列表的最前面增加数据" class="headerlink" title="在列表的最前面增加数据"></a>在列表的最前面增加数据</h6><pre><code class="scala">var list1 = List(1, 2, 3, &quot;abc&quot;)val list2 = 4 +: list1// +:运算符表示在列表的最前面增加数据println(list1) //list1没有变化println(list2) //新的列表结果是 [4，1, 2, 3, &quot;abc&quot;]</code></pre><h6 id="列表的最后增加数据"><a href="#列表的最后增加数据" class="headerlink" title="列表的最后增加数据"></a>列表的最后增加数据</h6><p>1、<strong>符号::表示向集合中  新建集合添加元素。</strong><br>2、运算时，集合对象一定要放置在最右边，<br>3、<strong>运算规则，从右向左。</strong><br>4、**::: 运算符是将集合中的每一个元素加入到空集合中去**</p><pre><code class="scala">val list05 = List(1, 2, 3, &quot;jojo&quot;)val list06 = 4 :: 5 :: 6 :: list05 :: Nilprintln(list06)//输出List(4, 5, 6, List(1, 2, 3, jojo))</code></pre><pre><code class="scala">val list05 = List(1, 2, 3, &quot;jojo&quot;)val list06 = 4 :: 5 :: 6 :: list05 ::: Nilprintln(list06)//输出List(4, 5, 6, List(1, 2, 3, jojo))</code></pre><h6 id="List追加练习题"><a href="#List追加练习题" class="headerlink" title="List追加练习题"></a>List追加练习题</h6><pre><code class="scala">val list1 = List(1, 2, 3, &quot;abc&quot;)val list5 = 4 :: 5 :: 6 :: list1 println(list5) // (4,5,6,1,2,3,&quot;abc&quot;)val list1 = List(1, 2, 3, &quot;abc&quot;)val list5 = 4 :: 5 :: 6 :: list1 :: 9println(list5) //错误，最右边放集合val list1 = List(1, 2, 3, &quot;abc&quot;)val list5 = 4 :: 5 :: 6 ::: list1 ::: Nilprintln(list5) // 错误 ::: 左右边为集合//把6改成一个集合，或者改成&#39;::&#39;val list1 = List(1, 2, 3, &quot;abc&quot;)val list5 = 4 :: 5 :: list1 ::: list1 ::: Nilprintln(list5) // (4,5,1,2,3,&quot;abc&quot;,1,2,3,&quot;abc&quot;)</code></pre><h4 id="ListBuffer"><a href="#ListBuffer" class="headerlink" title="ListBuffer"></a>ListBuffer</h4><h5 id="基本介绍-2"><a href="#基本介绍-2" class="headerlink" title="基本介绍"></a>基本介绍</h5><p><strong>ListBuffer是可变的list集合</strong>，可以添加，删除元素,ListBuffer属于序列<br>&#x2F;&#x2F;追一下继承关系即可<br>Seq var listBuffer &#x3D; ListBuffer(1,2)</p><h5 id="应用实例："><a href="#应用实例：" class="headerlink" title="应用实例："></a>应用实例：</h5><pre><code class="scala">    //listbuffer可变的，在    val listBuffer0 = ListBuffer[Int](1, 2, 3)    println(listBuffer0(2))//3    for (item &lt;- listBuffer0) &#123;      println(item)//1,2,3    &#125;    //不赋值需要new一下进行初始化    val listBuffer3 = new ListBuffer[Int]    listBuffer3 += 4    listBuffer3.append(5)    println(listBuffer3)//4,5        //右边集合元素添加到昨天集合，++一般是集合之间元素相加    listBuffer0 ++= listBuffer3    println(&quot;0=&quot;+listBuffer0)//listBuffer0(1,2,3,4,5)        val list2 = listBuffer0 ++ listBuffer3    println(list2)//list2(1,2,3,4,5,4,5)    val list3 = listBuffer0:+5    println(list3)//list3(1,2,3,4,5,5)    println(&quot;删除&quot;)    list3.remove(5)//删除下标为5的元素    println(list3)</code></pre><h5 id="ListBuffer的head和tail"><a href="#ListBuffer的head和tail" class="headerlink" title="ListBuffer的head和tail"></a>ListBuffer的head和tail</h5><p>head是第一个数据</p><p>tail是除了第一个之外的数据，是拼接起来的</p><h4 id="队列-Queue"><a href="#队列-Queue" class="headerlink" title="队列 Queue"></a>队列 Queue</h4><h5 id="队列的说明"><a href="#队列的说明" class="headerlink" title="队列的说明"></a>队列的说明</h5><p>1、队列是一个<strong>有序列表</strong>，在底层可以用<strong>数组</strong>或是<strong>链表</strong>来实现。<br>2、其输入和输出要遵循<strong>先入先出</strong>的原则。即：先存入队列的数据，要先取出。后存入的要后取出<br>3、在Scala中，由设计者直接给我们提供队列类型使用。<br>4、在scala中, 有 scala.collection.mutable.Queue 和 scala.collection.immutable.Queue , 一般来说，我们在开发中<strong>通常使用可变集合中的队列。</strong> </p><h5 id="应用案例："><a href="#应用案例：" class="headerlink" title="应用案例："></a>应用案例：</h5><pre><code class="scala">    import scala.collection.&#123;mutable&#125;    val queue = new mutable.Queue[Int]()    println(queue)//Queue()    queue+=5    println(queue)//Queue(5)    queue ++= List(1,2,3)    println(queue)//Queue(5,1,2,3)    queue += List(1,2,3)//error,这样相当于添加list集合到队列中</code></pre><h5 id="删除和加入队列元素"><a href="#删除和加入队列元素" class="headerlink" title="删除和加入队列元素"></a>删除和加入队列元素</h5><p>在队列中，严格遵守，入队列的数据放在队尾，出队列的数据是从头部取出</p><pre><code class="scala">  //删除队列元素，从前面拿    import scala.collection.mutable.Queue   //取出队列的第一个元素，queue会变化    val queue2 = mutable.Queue[Int](1, 2, 3)    val firstement = queue2.dequeue()    println(firstement)//1    println(queue)//Queue(2,3)</code></pre><pre><code class="scala">    //加入队列元素，在后面增加    import scala.collection.mutable.Queue   //取出队列的第一个元素，queue会变化    val queue2 = mutable.Queue[Int](1, 2, 3)    queue.enqueue(5,10,15)    println(queue)//Queue(1,2,3,5,10,15)</code></pre><h5 id="返回队列的元素"><a href="#返回队列的元素" class="headerlink" title="返回队列的元素"></a>返回队列的元素</h5><pre><code class="scala">    //获取队列的最后一个元素    println(queue.last)    //获取队列的第一个元素    println(queue.head)    //取出队尾的数据，返回除了第一个以外剩余的元素，可以级联使用    println(queue.tail)    println(queue.tail.tail.tail)</code></pre><h4 id="映射-Map"><a href="#映射-Map" class="headerlink" title="映射 Map"></a>映射 Map</h4><h5 id="Java中的Map回顾"><a href="#Java中的Map回顾" class="headerlink" title="Java中的Map回顾"></a>Java中的Map回顾</h5><p>HashMap 是一个<strong>散列表(数组+链表)<strong>，它存储的内容是</strong>键值对(key-value)映射</strong>，Java中的<strong>HashMap是无序的</strong>，key不能重复。</p><h5 id="Scala中的Map介绍"><a href="#Scala中的Map介绍" class="headerlink" title="Scala中的Map介绍"></a>Scala中的Map介绍</h5><p>Scala中的Map 和Java类似，也是一个散列表，它存储的内容也是键值对(key-value)映射，Scala中<strong>不可变的Map是有序的，可变的Map是无序的。</strong></p><p>Scala中，有可变Map (scala.collection.mutable.Map) 和 不可变Map(scala.collection.immutable.Map) </p><h5 id="构建Map"><a href="#构建Map" class="headerlink" title="构建Map"></a>构建Map</h5><pre><code class="scala">    //1、默认Map是immutable.Map    //2、key-value 类型支持Any    //3、在Map底层，每队key-value是Tuple2    //4、不可变Map输出顺序和声明顺序一致    //构建不可变Map    val map = Map(&quot;Alice&quot; -&gt; 10, 2 -&gt; &quot;two&quot;)    println(map)//Map(2 -&gt; two, Alice -&gt; 10)    //构建可变的Map    val map2 = mutable.Map(&quot;Alice&quot; -&gt; 10, 2 -&gt; &quot;two&quot;)    println(map2)//Map(Alice -&gt; 10, 2 -&gt; two)    //创建空的Map    val nullmap = new mutable.HashMap[String,Int]()    println(nullmap)//Map()    //对偶元组方式创建    val map3 = mutable.Map((&quot;2&quot;,2),(&quot;3&quot;-&gt;3))    println(map3)//Map(2 -&gt; 2, 3 -&gt; 3)</code></pre><h5 id="Map-取值"><a href="#Map-取值" class="headerlink" title="Map-取值"></a>Map-取值</h5><p>方式1-使用map(key)</p><p>如果key存在，则返回对应的值<br>如果key不存在，则抛出异常[java.util.NoSuchElementException]<br>在Java中,如果key不存在则返回null </p><pre><code class="scala">//方式1val map = Map(&quot;Alice&quot; -&gt; 10, 2 -&gt; &quot;two&quot;)println(map(&quot;Alice&quot;))//10println(map(&quot;Alice&quot;))//java.util.NoSuchElementException</code></pre><p>方式2-使用contains方法检查是否存在key </p><p>&#x2F;&#x2F; 返回Boolean<br>&#x2F;&#x2F; 1.如果key存在，则返回true<br>&#x2F;&#x2F; 2.如果key不存在，则返回false</p><pre><code class="scala">//方式2val map = Map(&quot;Alice&quot; -&gt; 10, 2 -&gt; &quot;two&quot;)if(map.contains(&quot;B&quot;))&#123;println(map(&quot;B&quot;))&#125;else&#123;println(&quot;key不存在&quot;)&#125;</code></pre><p>方式3-使用map.get(key).get取值 </p><p>通过 映射.get(键) 这样的调用返回一个Option对象，<strong>要么是Some，要么是None</strong></p><p>map.get方法会将数据进行包装<br>如果 map.get(key) key存在返回some,如果key不存在，则返回None<br> 如果 map.get(key).get  key存在，返回key对应的值,否则，抛出异常 java.util.NoSuchElementException: None.get</p><pre><code class="scala">//方式三val map = Map(&quot;Alice&quot; -&gt; 10, 2 -&gt; &quot;two&quot;)println(map.get(&quot;Alice&quot;))//Some(10)println(map.get(&quot;Alice~~~&quot;))//None</code></pre><p>方式4-使用**map4.getOrElse()**取值 </p><p>如果key存在，返回key对应的值。<br>如果key不存在，返回默认值。在java中底层有很多类似的操作</p><pre><code class="scala">//方式四val map = Map(&quot;Alice&quot; -&gt; 10, 2 -&gt; &quot;two&quot;)println(map.getOrElse(&quot;Alice&quot;,&quot;默认&quot;))//10println(map.getOrElse(&quot;Alice~~~&quot;,&quot;默认&quot;))//默认</code></pre><h6 id="选择取值方式建议"><a href="#选择取值方式建议" class="headerlink" title="选择取值方式建议"></a>选择取值方式建议</h6><p>1、如果我们确定map有这个key ,则应当使用map(key), 速度快<br>2、如果我们不能确定map是否有key ,而且有不同的业务逻辑，使用map.contains() 先判断在加入逻辑<br>3、如果只是简单的希望得到一个值，使用map4.getOrElse(“ip”,”127.0.0.1”)</p><h5 id="对map修改、添加和删除"><a href="#对map修改、添加和删除" class="headerlink" title="对map修改、添加和删除"></a>对map修改、添加和删除</h5><p>map 是可变的，才能修改，否则报错<br>如果<strong>key存在</strong>：<strong>则修改</strong>对应的值,<strong>key不存在,等价于添加</strong>一个key-val</p><pre><code class="scala">    import scala.collection.mutable    val map3 = mutable.Map((&quot;2&quot;,2),(&quot;3&quot;-&gt;3))    println(map3)//Map(2 -&gt; 2, 3 -&gt; 3)    map3(&quot;4&quot;) = 4    println(map3)//Map(2 -&gt; 2, 4 -&gt; 4, 3 -&gt; 3)    map3(&quot;4&quot;) = 20    println(map3)//Map(2 -&gt; 2, 4 -&gt; 20, 3 -&gt; 3)</code></pre><h5 id="添加map元素"><a href="#添加map元素" class="headerlink" title="添加map元素"></a>添加map元素</h5><h6 id="单个元素增加"><a href="#单个元素增加" class="headerlink" title="单个元素增加"></a>单个元素增加</h6><pre><code class="scala">val map4 = mutable.Map((&quot;A&quot;, 1), (&quot;B&quot;, &quot;北京&quot;), (&quot;C&quot;, 3))map4 += ( &quot;D&quot; -&gt; 4 )map4 += ( &quot;B&quot; -&gt; 50 ,&quot;E&quot;-&gt;&quot;e&quot;)//key如果已存在就是更新println(map4)//Map(D -&gt; 4, A -&gt; 1, C -&gt; 3, E -&gt; e, B -&gt; 50)</code></pre><h6 id="多个元素增加"><a href="#多个元素增加" class="headerlink" title="多个元素增加"></a>多个元素增加</h6><pre><code class="scala">val map4 = mutable.Map( (&quot;A&quot;, 1), (&quot;B&quot;, &quot;北京&quot;), (&quot;C&quot;, 3) )val map5 = map4 + (&quot;E&quot;-&gt;1, &quot;F&quot;-&gt;3)map4 += (&quot;EE&quot;-&gt;1, &quot;FF&quot;-&gt;3)  </code></pre><h5 id="删除map元素"><a href="#删除map元素" class="headerlink" title="删除map元素"></a>删除map元素</h5><p>“A”,”B” 就是要删除的key, 可以写多个.<br><strong>如果key存在，就删除，如果key不存在，也不会报错.</strong></p><pre><code class="scala">   val map4 = mutable.Map( (&quot;A&quot;, 1), (&quot;B&quot;, &quot;北京&quot;), (&quot;C&quot;, 3) )   map4 -= (&quot;A&quot;,&quot;B&quot;,&quot;asdfadf&quot;)//不能存在的key不会报错   println(map4)//Map(c -&gt; 3)     map5.remove(&quot;A&quot;)</code></pre><h5 id="map遍历"><a href="#map遍历" class="headerlink" title="map遍历"></a>map遍历</h5><p>对map的元素(元组Tuple2对象 )进行遍历的方式很多，具体如下:</p><pre><code class="scala">val map5 = mutable.Map((&quot;A&quot;, 1), (&quot;B&quot;, &quot;北京&quot;), (&quot;C&quot;, 3))for ((k, v) &lt;- map5) println(k + &quot; is mapped to &quot; + v)//A is mapped to 1//C is mapped to 3//B is mapped to 北京</code></pre><pre><code class="scala">for (k &lt;- map5.keys) println(v//A//C//B</code></pre><pre><code class="scala">for (v &lt;- map5.values) println(v)//1//3//北京</code></pre><pre><code class="scala">//v是Tuple2类型for(v &lt;- map5) println(v)//(A,1)//(C,3)//(B,北京)v._1v._2</code></pre><h4 id="Set-集"><a href="#Set-集" class="headerlink" title="Set 集"></a>Set 集</h4><p>集是不重复元素的结合。集不保留顺序，默认是以哈希集实现</p><h5 id="Java中的Set回顾"><a href="#Java中的Set回顾" class="headerlink" title="Java中的Set回顾"></a>Java中的Set回顾</h5><p>java中，HashSet是实现Set<E>接口的一个实体类，数据是以哈希表的形式存放的，里面的不能包含重复数据。Set接口是一种不包含重复元素的 collection，HashSet中的数据也是没有顺序的。 </E></p><pre><code class="java">HashSet hs = new HashSet&lt;String&gt;();hs.add(&quot;jack&quot;);hs.add(&quot;tom&quot;);hs.add(&quot;jack&quot;);hs.add(&quot;jack2&quot;);System.out.println(hs);</code></pre><h5 id="Scala中Set的说明"><a href="#Scala中Set的说明" class="headerlink" title="Scala中Set的说明"></a>Scala中Set的说明</h5><p>默认情况下，Scala 使用的是不可变集合，如果你想使用可变集合，需要引用 scala.collection.mutable.Set 包</p><pre><code class="scala">    //不可变集合    val set1 = Set(1,2,3)    println(set1)    //可变集合    val set2 = scala.collection.mutable.Set(1,2,3)    set2+=(3)    println(set2)</code></pre><h5 id="Set可变集合的元素添加"><a href="#Set可变集合的元素添加" class="headerlink" title="Set可变集合的元素添加"></a>Set可变集合的元素添加</h5><p>如果添加的对象已经存在，则不会重复添加，也不会报错</p><pre><code class="scala">val set2 = scala.collection.mutable.Set(1,2,3)set2+=(3)set2.add(4)set2 += 0</code></pre><h5 id="Set可变集合的元素删除"><a href="#Set可变集合的元素删除" class="headerlink" title="Set可变集合的元素删除"></a>Set可变集合的元素删除</h5><p>如果删除的对象不存在，则不生效，也不会报错</p><pre><code class="scala">val set02 = mutable.Set(1,2,4,&quot;abc&quot;)set02 -= 2 // 操作符形式set02.remove(&quot;abc&quot;) // 方法的形式，scala的Set可以直接删除值println(set02)//Set(1, 4)</code></pre><h3 id="数据结构-下-集合操作"><a href="#数据结构-下-集合操作" class="headerlink" title="数据结构(下)-集合操作"></a>数据结构(下)-集合操作</h3><h4 id="实际需求"><a href="#实际需求" class="headerlink" title="实际需求"></a>实际需求</h4><p>请将List(3,5,7) 中的所有元素都 * 2 ，将其结果放到一个新的集合中返回，即返回一个新的List(6,10,14), 请编写程序实现</p><h5 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h5><p>没有体现函数式编程特点（集合，函数）</p><p>不够高效，简介</p><pre><code class="scala">val list1 = List(3, 5, 7)var list2 = List[Int]()for (item &lt;- list1) &#123; //遍历list2 = list2 :+ item * 2&#125;println(list2)</code></pre><h5 id="map映射操作"><a href="#map映射操作" class="headerlink" title="map映射操作"></a>map映射操作</h5><p>上面提出的问题，其实就是一个关于<strong>集合元素映射操作</strong>的问题。<br>在Scala中可以通过map映射操作来解决：<strong>将集合中的每一个元素通过指定功能（函数）映射（转换）成新的结果集合</strong>这里其实就是所谓的<strong>将函数作为参数传递给另外一个函数,这是函数式编程的特点</strong></p><p>就是传入函数，在map方法中遍历集合元素，执行传入方法，返回新的集合</p><pre><code class="scala">def map[B](f: (A) ⇒ B): HashSet[B]//map函数的签名//这个就是map映射函数集合类型都有//[B] 是泛型//map 是一个高阶函数(可以接受一个函数的函数，就是高阶函数)，可以接收 函数 f: (A) =&gt; B 后面详解(先简单介绍下.)//HashSet[B] 就是返回的新的集合val list1 = List(3, 5, 7)def f1(n1: Int): Int = &#123;2 * n1&#125;val list2 = list1.map(f1)println(list2)//map方法是系统给定的//这里将map遍历，将参数放入到f1方法中，将返回的结果装入到一个新的Map并返回</code></pre><h4 id="高阶函数的使用"><a href="#高阶函数的使用" class="headerlink" title="高阶函数的使用"></a>高阶函数的使用</h4><p>就是把函数名称当成形参传入方法执行</p><pre><code class="scala">object TestHighOrderDef &#123;  def main(args: Array[String]): Unit = &#123;    val res = test(sum, 6.0)    println(&quot;res=&quot; + res)  &#125;    //输入Double和输出Double的函数  def test(f: Double =&gt; Double, n1: Double) = &#123;    f(n1)  &#125;  def sum(d: Double): Double = &#123;    d + d  &#125;&#125;</code></pre><pre><code class="scala">def main(args: Array[String]): Unit = &#123;test2(sayOK)&#125;def test2(f: () =&gt; Unit) = &#123;f()&#125;def sayOK() = &#123;println(&quot;sayOKKK...&quot;)&#125;</code></pre><h4 id="map映射函数的机制-模拟实现"><a href="#map映射函数的机制-模拟实现" class="headerlink" title="map映射函数的机制-模拟实现"></a>map映射函数的机制-模拟实现</h4><pre><code class="scala">object MapOperatDemo02 &#123;  def main(args: Array[String]): Unit = &#123;    val list = List(3,5,7,9)    val list2 = list.map(multiple)    println(list2)    val myList = MyList()    println(myList)    val myList2 = myList.map(multiple)    println(myList2)  &#125;  def multiple(n:Int):Int=&#123;    println(&quot;diaoyong&quot;)    n*2  &#125;&#125;class MyList &#123;  val list1 = List(3,5,7,9)  var list2 = List[Int]()  def map(f:Int =&gt;Int): List[Int] =&#123;    for(item &lt;- this.list1)&#123;       list2 = list2 :+ f(item)    &#125;    list2  &#125;&#125;object MyList&#123;  def apply(): MyList = new MyList()&#125;</code></pre><h4 id="集合元素的过滤-filter"><a href="#集合元素的过滤-filter" class="headerlink" title="集合元素的过滤-filter"></a>集合元素的过滤-filter</h4><p>filter：将符合要求的数据(筛选)放置到新的集合中</p><p><strong>传入的方法要返回一个boolean类型的</strong></p><h5 id="应用案例"><a href="#应用案例" class="headerlink" title="应用案例"></a>应用案例</h5><p>集合中首字母为’A’的筛选到新的集合</p><pre><code class="scala">object FilterDemo01 &#123;  def main(args: Array[String]): Unit = &#123;    val names = List(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Nick&quot;)    //传入一个string返回boolean    val names2 = names.filter(statA)    println(&quot;names=&quot;+names)    println(&quot;names=&quot;+names2)  &#125;    //！！必须是boolean返回值类型的  def statA(s: String): Boolean = &#123;    s.startsWith(&quot;A&quot;)  &#125;&#125;//names=List(Alice, Bob, Nick)//names=List(Alice)</code></pre><h4 id="flat扁平化映射"><a href="#flat扁平化映射" class="headerlink" title="flat扁平化映射"></a>flat扁平化映射</h4><p>flatmap：flat即压扁，压平，扁平化，效果就是<strong>将集合中的每个元素的子元素映射到某个函数并返回新的集合</strong>。</p><h5 id="应用案例-1"><a href="#应用案例-1" class="headerlink" title="应用案例"></a>应用案例</h5><pre><code class="scala">val names = List(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Nick&quot;)def upper( s : String ) : String = &#123;    s. toUpperCase&#125;//就是先.Map 然后flatten  扁平化println(names.flatMap(upper)) //List(A, L, I, C, E, B, O, B, N, I, C, K)</code></pre><h4 id="化简"><a href="#化简" class="headerlink" title="化简"></a>化简</h4><h5 id="基本介绍-3"><a href="#基本介绍-3" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>化简：<strong>将二元函数引用于集合中的函数。</strong></p><p><strong>二元函数</strong>：方法入参是两个参数</p><h5 id="reduceLeft-机制说明"><a href="#reduceLeft-机制说明" class="headerlink" title="reduceLeft()机制说明"></a>reduceLeft()机制说明</h5><p>def reduceLeft<a href="@deprecatedName('f">B &gt;: A</a> op: (B, A) &#x3D;&gt; B): B<br>reduceLeft(f) 接收的函数需要的形式为 op: (B, A) &#x3D;&gt; B): B<br>reduceleft(f) 的运行规则是 <strong>从左边开始执行将得到的结果返回给第一个参数</strong><br> 然后继续和下一个元素运行，将得到的结果继续返回给第一个参数，继续..<br>即: &#x2F;&#x2F;((((1 + 2)  + 3) + 4) + 5) &#x3D; 15</p><h5 id="应用案例-2"><a href="#应用案例-2" class="headerlink" title="应用案例"></a>应用案例</h5><pre><code class="scala">object ReduceDemo01 &#123;  def main(args: Array[String]): Unit = &#123;    val list = List(1,20,30,4,5)    //接受一个函数时，可以直接传入一个匿名函数    val i = list.reduceLeft(sum)    //执行流程分析    //1、sum(1,20)  21    //2、sum(21+30) 51    //3、sum(51+4)  55    //4、sum(55+5)  60    println(i)  &#125;  def sum(n1:Int,n2:Int): Int =&#123;    println(n1+n2)    n1+n2  &#125;&#125;</code></pre><h6 id="执行逻辑图"><a href="#执行逻辑图" class="headerlink" title="执行逻辑图"></a>执行逻辑图</h6><p><img src="/scala/scala/image-20200229135700804.png" alt="image-20200229135700804"></p><p><strong>说明: .reduceRight(_ - _)反之同理</strong></p><h5 id="化简练习题"><a href="#化简练习题" class="headerlink" title="化简练习题"></a>化简练习题</h5><p>1)<strong>分析下面的代码输出什么结果</strong></p><pre><code class="scala">val list = List(1, 2, 3, 4 ,5)    def minus( num1 : Int, num2 : Int ): Int = &#123;      num1 - num2    &#125;println(list.reduceLeft(minus)) // 输出-13println(list.reduceRight(minus)) //输出3//1-(2-(3-(4-5)))println(list.reduce(minus)) //调用的是reduceLeft</code></pre><p>2)<strong>使用化简的方法求出</strong> List(3,4,2,7,5) 最小的值</p><pre><code class="scala">object aaa &#123;  def main(args: Array[String]): Unit = &#123;    val list = List(1,2,3,4,5)    val i = list.reduce(minfunction)    println(i)  &#125;  def minfunction(n1: Int, n2: Int): Int = &#123;    if (n1 &gt;= n2) n2 else n1  &#125;&#125;</code></pre><h4 id="折叠"><a href="#折叠" class="headerlink" title="折叠"></a>折叠</h4><h5 id="基本介绍-4"><a href="#基本介绍-4" class="headerlink" title="基本介绍"></a>基本介绍</h5><p><strong>fold函数将上一步返回的值作为函数的第一个参数继续传递参与运算，直到list中的所有元素被遍历。</strong></p><p><strong>可以把reduceLeft看做简化版的foldLeft。</strong></p><p>如何理解:def reduceLeft<a href="@deprecatedName('f">B &gt;: A</a> op: (B, A) &#x3D;&gt; B): B &#x3D; if (isEmpty) throw new UnsupportedOperationException(“empty.reduceLeft”)  else tail.foldLeft<a href="head">B</a>(op)</p><p>大家可以看到. reduceLeft就是调用的foldLeft<a href="head">B</a>，并且是默认从集合的head元素开始操作的。<br>相关函数：fold，foldLeft，foldRight，可以参考reduce的相关方法理解</p><h5 id="foldLeft和foldRight-缩写方法"><a href="#foldLeft和foldRight-缩写方法" class="headerlink" title="foldLeft和foldRight 缩写方法"></a>foldLeft和foldRight 缩写方法</h5><p>分别是：&#x2F;:和:\</p><pre><code class="scala">val list4 = List(1, 9, 2, 8)def minus(num1: Int, num2: Int): Int = &#123;num1 - num2&#125;    var i6 = (1 /: list4) (minus) // =等价=&gt; list4.foldLeft(1)(minus)    println(i6) // 输出?-19    i6 = (100 /: list4) (minus)    println(i6) // 输出?80    i6 = (list4 :\ 10) (minus) // list4.foldRight(10)(minus)    println(i6) // 输出?-4</code></pre><h5 id="案例-1"><a href="#案例-1" class="headerlink" title="案例"></a>案例</h5><pre><code class="scala">// 折叠val list = List(1, 2, 3, 4)def minus( num1 : Int, num2 : Int ): Int = &#123;num1 - num2&#125;//理解为(5,1,2,3,4)  //5-1=4//(5-1)-2=2//((5-1)-2)-3=-1//(((5-1)-2)-3)-4=5println(list.foldLeft(5)(minus)) // 函数的柯里化//理解为(1,2,3,4,5)//4-5=-1//3-(4-5)=4//2-(3-(4-5))=-2//1-(2-(3-(4-5)))-4=3println(list.foldRight(5)(minus)) //</code></pre><pre><code class="scala">object Exercise02 &#123;  def main(args: Array[String]): Unit = &#123;    val sentence = &quot;AAAAAAAAAABBBBBBBBCCCCCDDDDDDD&quot;    val arr = ArrayBuffer[Char]()    sentence.foldLeft(arr)(putArray)    println(arr)  &#125;  def putArray(arr:ArrayBuffer[Char],c:Char): ArrayBuffer[Char] =&#123;    arr += c    arr  &#125;&#125;//ArrayBuffer(A, A, A, A, A, A, A, A, A, A, B, B, B, B, B, B, B, B, C, C, C, C, C, D, D, D, D, D, D, D)</code></pre><h4 id="扫描"><a href="#扫描" class="headerlink" title="扫描"></a>扫描</h4><h5 id="基本介绍-5"><a href="#基本介绍-5" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>扫描，即对某个集合的所有元素做fold操作，但是<strong>会把产生的所有中间结果放置于一个集合中保存</strong>，这是和折叠的不同点</p><h5 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h5><pre><code class="scala">object ScanDemo01 &#123;  def main(args: Array[String]): Unit = &#123;    def minus(num1: Int, num2: Int): Int = &#123;      num1 - num2    &#125;    //5 (1,2,3,4,5) =&gt;(5,4,2,-1,-5,-10)    val i8 = (1 to 5).scanLeft(5)(minus) //IndexedSeq[Int]    println(i8)    def add(num1: Int, num2: Int): Int = &#123;      num1 + num2    &#125;    //5 (1,2,3,4,5) 5=&gt;(20,19,17,14,10,5)    val i9 = (1 to 5).scanRight(5)(add) //IndexedSeq[Int]    println(i9)  &#125;&#125;</code></pre><h5 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h5><p>1、将sentence 中各个字符，通过foldLeft存放到 一个ArrayBuffer中</p><pre><code class="scala">object Exercise02 &#123;  def main(args: Array[String]): Unit = &#123;    val sentence = &quot;AAAAAAAAAABBBBBBBBCCCCCDDDDDDD&quot;    val arr = ArrayBuffer[Char]()    sentence.foldLeft(arr)(putArray)    println(arr)  &#125;  //arr传入第一个参数，  def putArray(arr:ArrayBuffer[Char],c:Char): ArrayBuffer[Char] =&#123;    arr += c    arr  &#125;&#125;</code></pre><p>2、使用映射集合，统计一句话中，各个字母出现的次数<br>提示：Map<a href>Char, Int</a></p><pre><code class="scala">object Exercise03 &#123;  def main(args: Array[String]): Unit = &#123;  &#125;  val sentence = &quot;AAAAAAAAAABBBBBBBBCCCCCDDDDDDD&quot;  //可变map  def charCount(m: mutable.Map[Char, Int], c: Char): mutable.Map[Char, Int] = &#123;    //可变的map每次返回都是它本身    //k -&gt; v    m += (c -&gt; (m.getOrElse(c, 0) + 1))  &#125;  //不可变map  def charCount2(m: Map[Char, Int], c: Char): Map[Char, Int] = &#123;    //每次返回都是新的map    m + (c -&gt; (m.getOrElse(c, 0) + 1))  &#125;  //可变map无序的  println((mutable.Map[Char, Int]() /: sentence)(charCount))  //不可变map是有序的  println((Map[Char, Int]() /: sentence)(charCount2))&#125;</code></pre><h4 id="拉链-合并"><a href="#拉链-合并" class="headerlink" title="拉链(合并)"></a>拉链(合并)</h4><h5 id="基本介绍-6"><a href="#基本介绍-6" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>在开发中，当我们需要将两个集合进行 <strong>对偶元组合并</strong>，可以使用拉链。</p><h5 id="案例-2"><a href="#案例-2" class="headerlink" title="案例"></a>案例</h5><pre><code class="scala">// 拉链val list1 = List(1, 2 ,3)val list2 = List(4, 5, 6)val list3 = list1.zip(list2) // (1,4),(2,5),(3,6) println(&quot;list3=&quot; + list3)</code></pre><h5 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h5><p>拉链的本质就是两个集合的合并操作，合并后每个元素是一个 <strong>对偶元组</strong>。<br>操作的规则下图:</p><p><img src="/scala/scala/image-20200301133513917.png" alt="image-20200301133513917"></p><p>如果<strong>两个集合个数不对应，会造成数据丢失</strong>。<br>集合不限于List, 也可以是其它集合比如 Array<br>如果要取出合并后的各个对偶元组的数据，可以遍历</p><pre><code class="scala">for(item&lt;-list3)&#123; print(item._1 + &quot; &quot; + item._2) //取出时，按照元组的方式取出即可   &#125;</code></pre><h4 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h4><h5 id="基本说明"><a href="#基本说明" class="headerlink" title="基本说明"></a>基本说明</h5><p>通过iterator方法从集合获得一个迭代器，通过while循环和for表达式对集合进行遍历.(学习使用迭代器来遍历)</p><h5 id="案例-3"><a href="#案例-3" class="headerlink" title="案例"></a>案例</h5><pre><code class="scala">val iterator = List(1, 2, 3, 4, 5).iterator // 得到迭代器    println(&quot;--------遍历方式1 -----------------&quot;)    while (iterator.hasNext) &#123;        println(iterator.next())    &#125;    println(&quot;--------遍历方式2 for -----------------&quot;)//for循环  iterator必须是没有遍历过的，iterator跟一个指针一样    for(enum &lt;- iterator) &#123;      println(enum) //    &#125;</code></pre><h5 id="应用案例小结"><a href="#应用案例小结" class="headerlink" title="应用案例小结"></a>应用案例小结</h5><ol><li>iterator 的构建实际是 AbstractIterator 的一个匿名子类，该子类提供了</li></ol><pre><code class="scala">    /*     def iterator: Iterator[A] = new AbstractIterator[A] &#123;    var these = self    def hasNext: Boolean = !these.isEmpty    def next(): A =    */</code></pre><p>2）该AbstractIterator 子类提供了  hasNext next 等方法.<br>3）因此，我们可以使用 while的方式，使用hasNext next 方法变量</p><h4 id="流-Stream"><a href="#流-Stream" class="headerlink" title="流 Stream"></a>流 Stream</h4><h5 id="基本说明-1"><a href="#基本说明-1" class="headerlink" title="基本说明"></a>基本说明</h5><p><strong>stream是一个集合</strong>。这个集合，可以<strong>用于存放无穷多个元素</strong>，但是这无穷个元素并不会一次性生产出来，而是需要用到多大的区间，就会动态的生产，<strong>末尾元素遵循lazy规则(即：要使用结果才进行计算的) 。</strong></p><h5 id="创建Stream对象"><a href="#创建Stream对象" class="headerlink" title="创建Stream对象"></a>创建Stream对象</h5><pre><code class="scala">def numsForm(n: BigInt) : Stream[BigInt] = n #:: numsForm(n + 1)val stream1 = numsForm(1)</code></pre><h6 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h6><p>1、Stream 集合存放的数据类型是BigInt<br>2、numsForm 是自定义的一个函数，函数名是程序员指定的。<br>3、创建的集合的第一个元素是 n , 后续元素生成的规则是 n + 1<br>4、后续元素生成的规则是可以程序员指定的 ，比如 numsForm( n * 4)…</p><pre><code class="scala">//创建Streamdef numsForm(n: BigInt) : Stream[BigInt] = n #:: numsForm(n + 1)val stream1 = numsForm(1)println(stream1) //Stream(1, ?)//取出第一个元素println(&quot;head=&quot; + stream1.head) //head=1println(stream1.tail) //Stream(2, ?)println(stream1) //Stream(1, 2, ?)</code></pre><h5 id="注意："><a href="#注意：" class="headerlink" title="注意："></a><strong>注意：</strong></h5><p>如果使用流集合，就不能使用last属性，如果使用last集合就会进行无限循环</p><h4 id="视图-View"><a href="#视图-View" class="headerlink" title="视图 View"></a>视图 View</h4><h5 id="基本介绍-7"><a href="#基本介绍-7" class="headerlink" title="基本介绍"></a>基本介绍</h5><p><strong>Stream的懒加载特性，也可以对其他集合应用view方法</strong>来得到类似的效果，具有如下特点：<br>1）view方法产出一个总是被懒执行的集合。<br>2）view不会缓存数据，每次都要重新计算，比如遍历View时。</p><h5 id="应用案例-3"><a href="#应用案例-3" class="headerlink" title="应用案例"></a>应用案例</h5><p>请找到1-100 中，数字倒序排列 和它本身相同的所有数。(1 2, 11, 22, 33 …)</p><pre><code class="scala">def eq(i: Int): Boolean = &#123;i.toString.equals(i.toString.reverse)&#125;//说明: 没有使用viewval viewSquares1 = (1 to 100).filter(eq)println(viewSquares1)//Vector(1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 22, 33, 44, 55, 66, 77, 88, 99)//for (x &lt;- viewSquares1) &#123;&#125;//使用viewval viewSquares2 = (1 to 100).view.filter(eq)println(viewSquares2)//SeqViewF(...)</code></pre><h4 id="线程安全的集合"><a href="#线程安全的集合" class="headerlink" title="线程安全的集合"></a>线程安全的集合</h4><h5 id="基本介绍-8"><a href="#基本介绍-8" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>所有线程安全的集合都是以Synchronized开头的集合，不可变集合也是线程安全的，但是不方便</p><p>SynchronizedBuffer<br>SynchronizedMap<br>SynchronizedPriorityQueue<br>SynchronizedQueue<br>SynchronizedSet<br>SynchronizedStack</p><h4 id="并行集合"><a href="#并行集合" class="headerlink" title="并行集合"></a>并行集合</h4><h5 id="基本介绍-9"><a href="#基本介绍-9" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>1）Scala为了充分使用多核CPU，提供了并行集合（有别于前面的串行集合），用于多核环境的并行计算。<br>2）主要用到的算法有： <strong>Divide and conquer : 分治算法</strong>，Scala通过splitters(分解器)，combiners（组合器）等抽象层来实现，主要原理是将计算工作分解很多任务，分发给一些处理器去完成，并将它们处理结果合并返回</p><p>Work stealin算法【学数学】，主要用于任务调度负载均衡（load-balancing），通俗点完成自己的所有任务之后，发现其他人还有活没干完，主动（或被安排）帮他人一起干，这样达到尽早干完的目的</p><p><strong>.par</strong></p><h5 id="应用案例-4"><a href="#应用案例-4" class="headerlink" title="应用案例"></a>应用案例</h5><p>1）打印1~5</p><pre><code class="scala">(1 to 5).foreach(println(_))//12345println()(1 to 5).par.foreach(println(_))//34152</code></pre><p>2）查看并行集合中元素访问的线程</p><pre><code class="scala">object ParDemo02 &#123;  def main(args: Array[String]): Unit = &#123;    //获取线程名称，并distinct去重    val result1 = (0 to 100).map&#123;case _ =&gt; Thread.currentThread().getName&#125;.distinct    val result2 = (0 to 100).par.map&#123;case _ =&gt; Thread.currentThread().getName&#125;.distinct    println(result1)//Vector(main)    println(&quot;___________________________________________&quot;)    println(result2)//ParVector(ForkJoinPool-1-worker-13, ForkJoinPool-1-worker-15,.....)  &#125;&#125;</code></pre><h4 id="操作符"><a href="#操作符" class="headerlink" title="操作符"></a>操作符</h4><h5 id="基本介绍-10"><a href="#基本介绍-10" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>这部分内容没有必要刻意去理解和记忆，语法使用的多了，自然就会熟练的使用，该部分内容了解一下即可。</p><h5 id="操作符扩展"><a href="#操作符扩展" class="headerlink" title="操作符扩展"></a>操作符扩展</h5><p>1）如果想在变量名、类名等定义中使用语法关键字（保留字），可以配合反引号反引号 val <code>val</code> &#x3D; 42<br>2）中置操作符：A 操作符 B 等同于 A.操作符(B)  </p><p>3）后置操作符：A操作符 等同于 A.操作符，如果操作符定义的时候不带()则调用时不能加括号 </p><pre><code class="scala">class Operate &#123;  //定义函数/方法的时候，省略的()  def ++ = &quot;123&quot; &#125;// 操作符val oper = new Operateprintln(oper++)println(oper.++)</code></pre><p>4）前置操作符，+、-、！、~等操作符A等同于**A.unary_**操作符 </p><pre><code class="scala">class Operate &#123;  // 声明前置运算符  //unary ：一元运算符  def unary_! = println(&quot;!!!!!!!&quot;)&#125;// 操作符val oper = new Operate!oper //前置运算符</code></pre><p>5）赋值操作符，A 操作符&#x3D; B 等同于 A &#x3D; A 操作符 B  ，比如 A +&#x3D; B 等价 A &#x3D; A + B</p><h3 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h3><p>match</p><h5 id="基本介绍-11"><a href="#基本介绍-11" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>Scala中的模式匹配类似于Java中的switch语法，但是更加强大。</p><p>模式匹配语法中，采用<strong>match</strong>关键字声明，每个分支采用<strong>case</strong>关键字进行声明，当需要匹配时，会从第一个case分支开始，如果 匹配成功，那么执行对应的逻辑代码，如果匹配不成功，继续执行下一个分支进行判断。如果所有case都不匹配，那么会执行case _ 分支，类似于Java中default语句。</p><h5 id="应用案例-5"><a href="#应用案例-5" class="headerlink" title="应用案例"></a>应用案例</h5><pre><code class="scala">val oper = &#39;#&#39;val n1 = 20val n2 = 10var res = 0oper match &#123;case &#39;+&#39; =&gt; res = n1 + n2case &#39;-&#39; =&gt; res = n1 - n2case &#39;*&#39; =&gt; res = n1 * n2case &#39;/&#39; =&gt; res = n1 / n2case _ =&gt; println(&quot;oper error&quot;)&#125;println(&quot;res=&quot; + res)//0</code></pre><h5 id="match的细节和注意事项"><a href="#match的细节和注意事项" class="headerlink" title="match的细节和注意事项"></a>match的细节和注意事项</h5><p>1）如果所有case都不匹配，那么会执行case _ 分支，类似于Java中default语句<br>2）如果所有case都不匹配，又没有写case _ 分支，那么会抛出<strong>MatchError</strong><br>3）<strong>每个case中，不用break语句，自动中断case</strong><br>4）可以在match中使用其它类型，而不仅仅是字符</p><p>5）&#x3D;&gt; 等价于 java swtich 的 :<br>6）&#x3D;&gt; 后面的代码块到下一个 case， 是作为一个整体执行，可以使用{} 扩起来，也可以不扩。 </p><h5 id="模式匹配守卫"><a href="#模式匹配守卫" class="headerlink" title="模式匹配守卫"></a>模式匹配守卫</h5><h6 id="基本介绍-12"><a href="#基本介绍-12" class="headerlink" title="基本介绍"></a>基本介绍</h6><p>如果想要<strong>表达匹配某个范围的数据</strong>，就需要在模式匹配中增加条件守卫</p><h6 id="应用案例-6"><a href="#应用案例-6" class="headerlink" title="应用案例"></a>应用案例</h6><pre><code class="scala">for (ch &lt;- &quot;+-3!&quot;) &#123;var sign = 0var digit = 0ch match &#123;case &#39;+&#39; =&gt; sign = 1case &#39;-&#39; =&gt; sign = -1// 说明..case _ if ch.toString.equals(&quot;3&quot;) =&gt; digit = 3case _ =&gt; sign = 2&#125;println(ch + &quot; &quot; + sign + &quot; &quot; + digit)&#125;//+ 1 0//- -1 0//3 0 3//! 2 0</code></pre><pre><code class="scala">  def main(args: Array[String]): Unit = &#123;    for (ch &lt;- &quot;+-3!&quot;) &#123;      var sign = 0      var digit = 0      ch match &#123;        case &#39;+&#39; =&gt; sign = 1        case &#39;-&#39; =&gt; sign = -1        // 说明..        case _ if ch.toString.equals(&quot;3&quot;) =&gt; digit = 3        //可以有多个默认匹配，但是后面的匹配值无效          //！！如果把这个写在前面，则一直匹配这个，其他的匹配不到        case _ =&gt; digit = 3        case _ =&gt; sign = 2      &#125;      println(ch + &quot; &quot; + sign + &quot; &quot; + digit)    &#125;  &#125;</code></pre><h5 id="模式中的变量"><a href="#模式中的变量" class="headerlink" title="模式中的变量"></a>模式中的变量</h5><h6 id="基本介绍-13"><a href="#基本介绍-13" class="headerlink" title="基本介绍"></a>基本介绍</h6><p>如果在case关键字后跟变量名，那么match前表达式的值会赋给那个变量</p><h6 id="应用案例-7"><a href="#应用案例-7" class="headerlink" title="应用案例"></a>应用案例</h6><pre><code class="scala">val ch = &#39;V&#39;ch match &#123;case &#39;+&#39; =&gt; println(&quot;ok~&quot;)//模式变量，无条件匹配case mychar =&gt; println(&quot;ok~&quot; + mychar)case _ =&gt; println (&quot;ok~~&quot;)&#125;</code></pre><pre><code class="scala">//还可以当作返回值，匹配到的case的最后一行就是返回值 def main(args: Array[String]): Unit = &#123;    val ch = &#39;V&#39;    var ch1 = ch match &#123;      case &#39;+&#39; =&gt; println(&quot;ok~&quot;)      case aaa =&gt; ch+&quot;aaa&quot;      case _ if ch &gt;50 =&gt;println(&quot;ch&gt;5&quot;)      case _ =&gt; println(&quot;ok&quot;)    &#125;    println(ch1)  &#125;</code></pre><h5 id="类型匹配"><a href="#类型匹配" class="headerlink" title="类型匹配"></a>类型匹配</h5><h6 id="基本介绍-14"><a href="#基本介绍-14" class="headerlink" title="基本介绍"></a>基本介绍</h6><p>可以匹配对象的任意类型，这样做避免了使用isInstanceOf和asInstanceOf方法</p><h6 id="应用案例-8"><a href="#应用案例-8" class="headerlink" title="应用案例"></a>应用案例</h6><pre><code class="scala">// 类型匹配, obj 可能有如下的类型val a = 7val obj = if(a == 1) 1else if(a == 2) &quot;2&quot;else if(a == 3) BigInt(3)else if(a == 4) Map(&quot;aa&quot; -&gt; 1)else if(a == 5) Map(1 -&gt; &quot;aa&quot;)else if(a == 6) Array(1, 2, 3)else if(a == 7) Array(&quot;aa&quot;, 1)else if(a == 8) Array(&quot;aa&quot;)//在:后面匹配数据类型，不匹配就下一个val result = obj match &#123;case a : Int =&gt; acase b : Map[String, Int] =&gt; &quot;对象是一个字符串-数字的Map集合&quot;case c : Map[Int, String] =&gt; &quot;对象是一个数字-字符串的Map集合&quot;case d : Array[String] =&gt; &quot;对象是一个字符串数组&quot;case e : Array[Int] =&gt; &quot;对象是一个数字数组&quot;case f : BigInt =&gt; Int.MaxValuecase _ =&gt; &quot;啥也不是&quot;&#125;println(result)</code></pre><h6 id="类型匹配注意事项"><a href="#类型匹配注意事项" class="headerlink" title="类型匹配注意事项"></a>类型匹配注意事项</h6><p>1）<strong>Map[String, Int] 和Map[Int, String]是两种不同的类型，其它类推。</strong><br>2）在进行类型匹配时，编译器会预先检测是否有可能的匹配，如果没有则报错.</p><pre><code class="scala">val obj = 10val result = obj match &#123;case a : Int =&gt; acase b : Map[String, Int] =&gt; &quot;Map集合&quot;case _ =&gt; &quot;啥也不是&quot;&#125;</code></pre><p>3）一个说明:val result &#x3D; obj match { case i : Int &#x3D;&gt; i} <strong>case i : Int &#x3D;&gt; i 表示 将 i &#x3D; obj</strong> (其它类推)，然后再判断类型</p><p>4）如果 <strong>case _</strong> 出现在match 中间，则<strong>表示隐藏变量名</strong>，即不使用,而不是表示默认匹配。</p><pre><code class="scala">val obj = 10val result = obj match &#123;case a : Int =&gt; acase _ : BigInt =&gt; Int.MaxValuecase b : Map[String, Int] =&gt; &quot;Map集合&quot;case _ =&gt; &quot;啥也不是&quot;&#125;</code></pre><h5 id="匹配数组"><a href="#匹配数组" class="headerlink" title="匹配数组"></a>匹配数组</h5><h6 id="基本介绍-15"><a href="#基本介绍-15" class="headerlink" title="基本介绍"></a>基本介绍</h6><p><strong>Array(0)</strong> 匹配只有一个元素且为0的数组。<br><strong>Array(x,y)</strong> 匹配数组有两个元素，并将两个元素赋值为x和y。<strong>当然可以依次类推</strong>Array(x,y,z) 匹配数组有3个元素的等等….<br><strong>Array(0,_*)</strong> 匹配数组以0开始</p><h6 id="应用案例-9"><a href="#应用案例-9" class="headerlink" title="应用案例"></a>应用案例</h6><pre><code class="scala">for (arr &lt;- Array(Array(0), Array(1, 0), Array(0, 1, 0),Array(1, 1, 0), Array(1, 1, 0, 1))) &#123;val result = arr match &#123;case Array(0) =&gt; &quot;0&quot;case Array(x, y) =&gt; x + &quot;=&quot; + ycase Array(0, _*) =&gt; &quot;以0开头和数组&quot;case _ =&gt; &quot;什么集合都不是&quot;&#125;println(&quot;result = &quot; + result)&#125; </code></pre><h5 id="匹配列表"><a href="#匹配列表" class="headerlink" title="匹配列表"></a>匹配列表</h5><h6 id="应用案例-10"><a href="#应用案例-10" class="headerlink" title="应用案例"></a>应用案例</h6><pre><code class="scala">for (list &lt;- Array(List(0), List(1, 0), List(0, 0, 0), List(1, 0, 0))) &#123;val result = list match &#123;case 0 :: Nil =&gt; &quot;0&quot; //case x :: y :: Nil =&gt; x + &quot; &quot; + y //case 0 :: tail =&gt; &quot;0 ...&quot; //case _ =&gt; &quot;something else&quot;&#125;println(result)&#125;</code></pre><p>&#x2F;&#x2F;请思考，如果要匹配 List(88) 这样的<strong>只含有一个元素的列表,并原值返回</strong>.应该怎么写?</p><p>Nil是空集合</p><pre><code class="scala">object MatchLIst &#123;  def main(args: Array[String]): Unit = &#123;    for (list &lt;- Array(List(0), List(1, 0), List(23),List(0, 0, 1), List(1, 0, 0))) &#123;      val result = list match &#123;        case 0 :: Nil =&gt; &quot;0&quot; //        case x :: y :: Nil =&gt; x + &quot; &quot; + y         case 0 :: tail =&gt; &quot;0 ...&quot; //        //返回原值        case x :: Nil =&gt; x        case _ =&gt; &quot;something else&quot;      &#125;  &#125;</code></pre><h5 id="匹配元组"><a href="#匹配元组" class="headerlink" title="匹配元组"></a>匹配元组</h5><h6 id="应用案例-11"><a href="#应用案例-11" class="headerlink" title="应用案例"></a>应用案例</h6><pre><code class="scala">// 元组匹配// 元组匹配for (pair &lt;- Array((0, 1), (1, 0), (1, 1),(1,0,2))) &#123;val result = pair match &#123; // case (0, _) =&gt; &quot;0 ...&quot; //case (y, 0) =&gt; y // case _ =&gt; &quot;other&quot; //&#125;println(result)&#125;</code></pre><p>&#x2F;&#x2F;思考，如果要匹配 (10, 30) 这样任意两个元素的对偶元组，应该如何写?</p><pre><code class="scala">    for (pair &lt;- Array((0, 1), (1, 0),(10,30), (1, 1),(1,0,2))) &#123;      val result = pair match &#123; //        case (0, _) =&gt; &quot;0 ...&quot; //        case (y, 0) =&gt; y //        //任意的对偶元组        case (x,y) =&gt; (y,x)        case _ =&gt; &quot;other&quot; //.      &#125;</code></pre><h5 id="对象匹配"><a href="#对象匹配" class="headerlink" title="对象匹配"></a>对象匹配</h5><h6 id="基本介绍-16"><a href="#基本介绍-16" class="headerlink" title="基本介绍"></a>基本介绍</h6><p>对象匹配，什么才算是匹配呢？，规则如下:<br>case中对象的<strong>unapply</strong>方法(对象提取器**)返回Some集合则为匹配成功**<br><strong>返回none集合则为匹配失败</strong></p><h6 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h6><pre><code class="scala">//成功案例object Square &#123;  //unapply方法对象提取器  //接受double类型  //返回Option[Double]类型  //返回的值是 Some(math.sqrt(z))def unapply(z: Double): Option[Double] =&#123;    println(&quot;被调用&quot;+z)    Some(math.sqrt(z))  &#125;def apply(z: Double): Double = z * z&#125;// 模式匹配使用：val number: Double = 36.0number match &#123;//调用unapply方法(z:Double)，z的值是number//如果返回some(6)，则表示成功case Square(n) =&gt; println(n)case _ =&gt; println(&quot;nothing matched&quot;)&#125;//被调用36.0//6.0</code></pre><pre><code class="scala">//失败案例object Square &#123;  //unapply方法对象提取器  //接受double类型  //返回Option[Double]类型  //返回的值是 Some(math.sqrt(z))def unapply(z: Double): Option[Double] =&#123;    println(&quot;被调用&quot;+z)    None  &#125;def apply(z: Double): Double = z * z&#125;// 模式匹配使用：val number: Double = 36.0number match &#123;//调用unapply方法(z:Double)，z的值是number//如果返回some(6)，则表示成功case Square(n) =&gt; println(n)case _ =&gt; println(&quot;nothing matched&quot;)&#125;//被调用36.0//nothing matched</code></pre><h6 id="应用案例1的小结"><a href="#应用案例1的小结" class="headerlink" title="应用案例1的小结"></a>应用案例1的小结</h6><p>构建对象时apply会被调用 ，比如 val n1 &#x3D; Square(5)<br>当将 Square(n) 写在 case 后时[case Square(n) &#x3D;&gt; xxx]，会默认调用<strong>unapply 方法(对象提取器)</strong><br> number 会被 传递给def unapply(z: Double) 的 z 形参<br> 如果返回的是Some集合，则unapply提取器返回的结果会返回给 n 这个形参<br><strong>case中对象的unapply方法(提取器)返回some集合则为匹配成功</strong><br><strong>返回none集合则为匹配失败</strong></p><h6 id="案例2"><a href="#案例2" class="headerlink" title="案例2"></a>案例2</h6><pre><code class="scala">object MatchTupleDemo02 &#123;  def main(args: Array[String]): Unit = &#123;    val namesString = &quot;Alice,Bob,Thomas&quot;    //说明    namesString match &#123;      //调用unapplySeq(str)把namesString传给str      //返回多个值，如果返回的个数匹配(first, second, third)      //则成功，如果返回None，表示匹配失败了      case Names(first, second, third) =&gt; &#123;        println(&quot;the string contains three people&#39;s names&quot;)        // 打印字符串        println(s&quot;$first $second $third&quot;)      &#125;      case _ =&gt; println(&quot;nothing matched&quot;)    &#125;  &#125;&#125;object Names &#123;  def unapplySeq(str: String): Option[Seq[String]] = &#123;    if (str.contains(&quot;,&quot;)) Some(str.split(&quot;,&quot;))    else None  &#125;&#125;</code></pre><h6 id="应用案例2的小结"><a href="#应用案例2的小结" class="headerlink" title="应用案例2的小结"></a>应用案例2的小结</h6><p>当case 后面的对象提取器方法的参数为多个，则会默认<strong>调用def unapplySeq() 方法</strong><br><strong>如果unapplySeq返回是Some，获取其中的值,判断得到的sequence中的元素的个数是否是三个,如果是三个，则把三个元素分别取出，赋值给first，second和third</strong><br>其它的规则不变.</p><h4 id="for表达式中的模式"><a href="#for表达式中的模式" class="headerlink" title="for表达式中的模式"></a>for表达式中的模式</h4><h5 id="基本介绍-17"><a href="#基本介绍-17" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>for循环也可以进行模式匹配.</p><h5 id="应用案例-12"><a href="#应用案例-12" class="headerlink" title="应用案例"></a>应用案例</h5><pre><code class="scala">val map = Map(&quot;A&quot;-&gt;1, &quot;B&quot;-&gt;0, &quot;C&quot;-&gt;3)for ( (k, v) &lt;- map ) &#123;println(k + &quot; -&gt; &quot; + v)&#125;//只要v是0的key-value，其他的过滤for ((k, 0) &lt;- map) &#123;println(k + &quot; --&gt; &quot; + 0)&#125;//可以进行范围匹配，更加灵活for ((k, v) &lt;- map if v == 0) &#123;println(k + &quot; ---&gt; &quot; + v)&#125;</code></pre><h4 id="样例类"><a href="#样例类" class="headerlink" title="样例类"></a>样例类</h4><h5 id="基本介绍-18"><a href="#基本介绍-18" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>1）<strong>样例类仍然是类</strong><br>2）样例类用<strong>case关键字</strong>进行声明。<br>3）样例类是为<strong>模式匹配而优化的类</strong><br>4）构造器中的<strong>每一个参数都成为val</strong>——除非它被显式地声明为var（不建议这样做）</p><p>5）在样例类对应的伴生对象中提供apply方法让你不用new关键字就能构造出相应的对象<br>6）提供unapply方法让模式匹配可以工作<br>7）将自动生成toString、equals、hashCode和copy方法(有点类似模板类，直接给生成，供程序员使用)<br>8）除上述外，样例类和其他类完全一样。你可以添加方法和字段，扩展它们</p><h5 id="案例-4"><a href="#案例-4" class="headerlink" title="案例"></a>案例</h5><pre><code class="scala">abstract class Amountcase class Dollar(value: Double) extends Amount case class Currency(value: Double, unit: String) extends Amountcase object NoAmount extends Amount 说明: 这里的 Dollar，Currencry, NoAmount  是样例类。</code></pre><h6 id="实践案例1"><a href="#实践案例1" class="headerlink" title="实践案例1"></a>实践案例1</h6><p>当我们有一个类型为Amount的对象时，可以<strong>用模式匹配来匹配他的类型，并将属性值绑定到变量</strong>(即：把样例类对象的属性值提取到某个变量,该功能有用)</p><pre><code class="scala">object CaseClassDemo02 &#123;  def main(args: Array[String]): Unit = &#123;    for(amt&lt;-Array(Dollar2(1000.0),Currency2(1000.0,&quot;RMB&quot;),NoAmount2))&#123;      val result = amt match &#123;        case Dollar2(v)=&gt;&quot;$&quot;+v        case Currency2(v,u)=&gt;v+&quot;  &quot;+u        case NoAmount2 =&gt; &quot;&quot;      &#125;      println(amt+&quot;:&quot;+result)    &#125;  &#125;&#125;abstract class Amount2//都是样例类case class Dollar2(value: Double) extends Amount2case class Currency2(value: Double, unit: String) extends Amount2case object NoAmount2 extends Amount2</code></pre><h6 id="实践案例2"><a href="#实践案例2" class="headerlink" title="实践案例2"></a>实践案例2</h6><p>样例类的<strong>copy方法和带名参数</strong><br>copy创建一个与现有对象值相同的新对象，并可以通过带名参数来修改某些属性。</p><pre><code class="scala">object CaseClassDemo03 &#123;  def main(args: Array[String]): Unit = &#123;    val amt = new Currency3(3000.0,&quot;RMB&quot;)    val amt2: Currency3 = amt.copy()//创建的对象和amt属性一样    println(amt2)    val amt3 = amt.copy(value = 8000)    println(amt3)    val amt4: Currency3 = amt.copy(unit = &quot;美元&quot;)    println(amt4)  &#125;&#125;abstract class Amount3//都是样例类case class Dollar3(value: Double) extends Amount3case class Currency3(value: Double, unit: String) extends Amount3case object NoAmount3 extends Amount3</code></pre><h5 id="case语句的中置-缀-表达式"><a href="#case语句的中置-缀-表达式" class="headerlink" title="case语句的中置(缀)表达式"></a>case语句的中置(缀)表达式</h5><h6 id="基本介绍-19"><a href="#基本介绍-19" class="headerlink" title="基本介绍"></a>基本介绍</h6><p>什么是中置表达式？1 + 2，这就是一个中置表达式。如果unapply方法产出一个元组，你可以在case语句中使用中置表示法。比如可以匹配一个List序列</p><h6 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h6><pre><code class="scala">List(1, 3, 5, 9) match &#123; //修改并测试//1.两个元素间::叫中置表达式,至少first，second两个匹配才行.//2.first 匹配第一个 second 匹配第二个, rest 匹配剩余部分(5,9)case first :: second :: rest =&gt; println(first + second + rest.length) //case _ =&gt; println(&quot;匹配不到...&quot;)&#125;</code></pre><h4 id="匹配嵌套结构"><a href="#匹配嵌套结构" class="headerlink" title="匹配嵌套结构"></a>匹配嵌套结构</h4><h5 id="基本介绍-20"><a href="#基本介绍-20" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>操作原理类似于正则表达式</p><h5 id="最佳实践案例-商品捆绑打折出售"><a href="#最佳实践案例-商品捆绑打折出售" class="headerlink" title="最佳实践案例-商品捆绑打折出售"></a>最佳实践案例-商品捆绑打折出售</h5><p>现在有一些商品，请使用Scala设计相关的样例类，完成商品捆绑打折出售。要求<br>商品捆绑可以是单个商品，也可以是多个商品。<br>打折时按照折扣x元进行设计.<br>能够统计出所有捆绑商品打折后的最终价格</p><pre><code class="scala">object SalesDemo &#123;  def main(args: Array[String]): Unit = &#123;    //匹配嵌套结构(就是Bundle的对象)    val sale = Bundle(&quot;书籍&quot;, 10, Book(&quot;漫画&quot;, 40), Bundle(&quot;文学作品&quot;, 20, Book(&quot;《阳关》&quot;, 80), Book(&quot;《围城》&quot;, 30)))    val res = sale match &#123;      case Bundle(_, _, Book(desc, sss), _*) =&gt; (desc, sss)    &#125;    val result2 = sale match &#123;      case Bundle(_, _, art@Book(_, _), rest@_*) =&gt; (art, rest)    &#125;    val result3 = sale match &#123;      case Bundle(_, _, art@Book(_, _), rest ) =&gt; (art, rest)    &#125;    println(result3)  &#125;&#125;//创建样例类abstract class Item//书名，价格case class Book(description: String, price: Double) extends Item//什么类型，折扣价格，书case class Bundle(description: String, discount: Double, item: Item*) extends Item</code></pre><h6 id="知识点1-将descr绑定到第一个Book的描述"><a href="#知识点1-将descr绑定到第一个Book的描述" class="headerlink" title="知识点1-将descr绑定到第一个Book的描述"></a>知识点1-将descr绑定到第一个Book的描述</h6><p>取出这个嵌套结构中的 “漫画”</p><pre><code class="scala">val sale = Bundle(&quot;书籍&quot;, 10,  Book(&quot;漫画&quot;, 40), Bundle(&quot;文学作品&quot;, 20, Book(&quot;《阳关》&quot;, 80), Book(&quot;《围城》&quot;, 30)))val res = sale match  &#123;//如果我们进行对象匹配时，不想接受某些值，则使用_ 忽略即可，_* 表示所有case Bundle(_, _, Book(desc, _), _*) =&gt; desc&#125;</code></pre><h6 id="知识点2-通过-表示法将嵌套的值绑定到变量。-绑定剩余Item到rest"><a href="#知识点2-通过-表示法将嵌套的值绑定到变量。-绑定剩余Item到rest" class="headerlink" title="知识点2-通过@表示法将嵌套的值绑定到变量。_*绑定剩余Item到rest"></a>知识点2-通过@表示法将嵌套的值绑定到变量。_*绑定剩余Item到rest</h6><p>如何将 这个嵌套结构中的 “漫画” 和 第二个Bundle部分 绑定到变量，即赋值到变量中.</p><pre><code class="scala">val sale = Bundle(&quot;书籍&quot;, 10,  Book(&quot;漫画&quot;, 40), Bundle(&quot;文学作品&quot;, 20, Book(&quot;《阳关》&quot;, 80), Book(&quot;《围城》&quot;, 30)))//art 代表第一个book，rest代表了后面的可变参数,是一个WrappedArray类型的val result2 = sale match &#123;case Bundle(_, _, art @ Book(_, _), rest @ _*) =&gt; (art, rest)&#125;println(result2)println(&quot;art =&quot; + result2._1)println(&quot;rest=&quot; + result2._2)</code></pre><h6 id="知识点3-不使用-绑定剩余Item到rest"><a href="#知识点3-不使用-绑定剩余Item到rest" class="headerlink" title="知识点3-不使用_*绑定剩余Item到rest"></a>知识点3-不使用_*绑定剩余Item到rest</h6><pre><code class="scala">val result2 = sale match &#123;//说明因为没有使用 _* 即明确说明没有多个Bundle,所以返回的rest，就不是WrappedArray了。case Bundle(_, _, art @ Book(_, _), rest) =&gt; (art, rest)&#125;println(result2)println(&quot;art =&quot; + result2._1)println(&quot;rest=&quot; + result2._2)</code></pre><h6 id="最佳实践案例-商品捆绑打折出售-1"><a href="#最佳实践案例-商品捆绑打折出售-1" class="headerlink" title="最佳实践案例-商品捆绑打折出售"></a>最佳实践案例-商品捆绑打折出售</h6><p>现在有一些商品，请使用Scala设计相关的样例类，完成商品可以捆绑打折出售。要求<br>商品捆绑可以是单个商品，也可以是多个商品。<br>打折时按照折扣xx元进行设计.<br>能够统计出所有捆绑商品打折后的最终价格</p><pre><code class="scala">def price (it:Item):Double=&#123;    it match &#123;      case Book(description, price) =&gt;price      //获取书的价格减去折扣的价格      case Bundle(description, discount, item@_*) =&gt; item.map(price(_)).sum-discount    &#125;  &#125;&#125;//创建样例类abstract class Item//书名，价格case class Book(description: String, price: Double) extends Item//什么类型，折扣价格，书case class Bundle(description: String, discount: Double, item: Item*) extends Item</code></pre><h4 id="密封类"><a href="#密封类" class="headerlink" title="密封类"></a>密封类</h4><h5 id="基本介绍-21"><a href="#基本介绍-21" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>如果想让case类的所有子类都必须<strong>在申明该类的相同的源文件中定义</strong>，可以将样例类的通用超类声明为<strong>sealed</strong>，这个超类称之为密封类。<br><strong>密封就是不能在其他文件中定义子类。</strong></p><pre><code class="scala">abstract sealed class Item//就算是在同一个包也不行</code></pre><h4 id="偏函数-partial-function"><a href="#偏函数-partial-function" class="headerlink" title="偏函数(partial function)"></a>偏函数(partial function)</h4><h5 id="基本介绍-22"><a href="#基本介绍-22" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>在<strong>对符合某个条件</strong>，而不是所有情况进行逻辑操作时，使用偏函数是一个不错的选择<br>将包在大括号内的一组case语句封装为函数，我们称之为<strong>偏函数</strong>，它只对会作用于指定类型的参数或指定范围值的参数实施计算，超出范围的值会忽略（未必会忽略，这取决于你打算怎样处理）</p><p>偏函数在Scala中是一个特质<strong>PartialFunction</strong></p><h5 id="案例-5"><a href="#案例-5" class="headerlink" title="案例"></a>案例</h5><p>1)将集合list中的所有数字+1，并返回一个新的集合</p><p>2)要求忽略掉 非数字 的元素，即返回的 新的集合 形式为 <strong>(2, 3, 4, 5)</strong></p><pre><code class="scala">object PartialFunctionDemo02 &#123;  def main(args: Array[String]): Unit = &#123;    //使用偏函数解决    val list = List(1, 2, 3, 4, &quot;hello&quot;)    //定义一个偏函数    //接受的类型是Any，返回的类型是Int类型    //不接收也会报错    val partialFun = new PartialFunction[Any,Int] &#123;      //如果返回true，就调用apply方法，构建一个对象实例      //false，apply就不执行      override def isDefinedAt(x: Any)=&#123;        println(&quot;调用·1·&quot;)        if (x.isInstanceOf[Int]) true else false      &#125;      override def apply(v1: Any): Int = &#123;        println(v1.getClass)        v1.asInstanceOf[Int]+1      &#125;    &#125;    println(list.collect(partialFun))  &#125;&#125;</code></pre><h5 id="偏函数的小结"><a href="#偏函数的小结" class="headerlink" title="偏函数的小结"></a>偏函数的小结</h5><p>1、使用构建特质的实现类(使用的方式是<strong>PartialFunction</strong>的匿名子类)<br>2、<strong>PartialFunction</strong> 是个特质(看源码)<br>3、构建偏函数时，参数形式   [Any, Int]是泛型，第一个表示参数类型，第二个表示返回参数<br>4、当使用偏函数时，会遍历集合的所有元素，编译器执行流程时先执行isDefinedAt()如果为true ,就会执行 apply，构建一个新的Int 对象返回<br>5、执行<strong>isDefinedAt</strong>() 为false 就过滤掉这个元素，即不构建新的Int对象.<br>6、<strong>map函数不支持偏函数</strong>，因为map底层的机制就是所有循环遍历，无法过滤处理原来集合的元素<br>7、<strong>collect</strong>函数支持偏函数</p><h6 id="偏函数执行流程"><a href="#偏函数执行流程" class="headerlink" title="偏函数执行流程"></a>偏函数执行流程</h6><p>new PartialFunction类后，定好返回值，重写方法 isDefinedAt()和apply()方法，一个个元素放到isDefinedAt()方法中判断，返回true或false，true则执行apply方法，执行里面的语句并返回</p><h5 id="偏函数简化形式"><a href="#偏函数简化形式" class="headerlink" title="偏函数简化形式"></a>偏函数简化形式</h5><h6 id="1-简化形式1"><a href="#1-简化形式1" class="headerlink" title="1)简化形式1"></a>1)简化形式1</h6><pre><code class="scala">def f2: PartialFunction[Any, Int] = &#123;  case i: Int =&gt; i + 1 // case语句可以自动转换为偏函数&#125;val list2 = List(1, 2, 3, 4,&quot;ABC&quot;).collect(f2)</code></pre><h6 id="2-简化形式2"><a href="#2-简化形式2" class="headerlink" title="2)简化形式2"></a>2)简化形式2</h6><pre><code class="scala">val list3 = List(1, 2, 3, 4,&quot;ABC&quot;).collect&#123; case i: Int =&gt; i + 1 &#125;println(list3)</code></pre><h4 id="作为参数的函数"><a href="#作为参数的函数" class="headerlink" title="作为参数的函数"></a>作为参数的函数</h4><h5 id="基本介绍-23"><a href="#基本介绍-23" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>函数作为一个变量传入到了另一个函数中，那么该作为参数的函数的类型是：function1，即：(参数类型) &#x3D;&gt; 返回类型</p><h5 id="应用实例-1"><a href="#应用实例-1" class="headerlink" title="应用实例"></a>应用实例</h5><pre><code class="scala">def main(args: Array[String]): Unit = &#123;def plus(x:Int,y: Int) = 3 +x    val result1 = Array(1,2,3,4).map(plus(_,0))    println(result1.mkString(&quot;,&quot;))    //输出它的类型    println(plus _)  &#125;</code></pre><h5 id="实例小结"><a href="#实例小结" class="headerlink" title="实例小结"></a>实例小结</h5><p>map(plus(<em>)) 中的plus(</em>) 就是将plus这个函数当做一个参数传给了map，_这里代表从集合中遍历出来的一个元素。</p><p>plus(_) 这里也可以写成 plus 表示对 Array(1,2,3,4) 遍历，将每次遍历的元素传给plus的 x<br>进行 3 + x 运算后，返回新的Int ，并加入到新的集合 result1中<br>def map[B, That](f: A &#x3D;&gt; B) 的声明中的 f: A &#x3D;&gt; B 一个函数</p><h4 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h4><h5 id="基本介绍-24"><a href="#基本介绍-24" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>没有名字的函数就是匿名函数，可以通过函数表达式来设置匿名函数</p><h5 id="应用实例-2"><a href="#应用实例-2" class="headerlink" title="应用实例"></a>应用实例</h5><pre><code class="scala">  def main(args: Array[String]): Unit = &#123;    //不要写返回类型，写上报错,自动类型推导    //多行使用大括号包起来    //不需要def函数命    //多行，使用&#123;&#125;包括    val triple = (x: Double) =&gt; &#123;      println(&quot;x=&quot;+x)      3 * x    &#125;    //普通函数不写参数会报错    //这里会输出他的类型    println(triple)    println(triple(3))  &#125;</code></pre><h4 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h4><h5 id="基本介绍-25"><a href="#基本介绍-25" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>能够接受函数作为参数的函数，叫做高阶函数 (higher-order function)。可使应用程序更加健壮。</p><h5 id="高阶函数基本使用"><a href="#高阶函数基本使用" class="headerlink" title="高阶函数基本使用"></a>高阶函数基本使用</h5><pre><code class="scala">  def main(args: Array[String]): Unit = &#123;    def test(f: Double =&gt; Double,d: Double=&gt;Double, n1: Double) = &#123;        //可以函数调用函数      f(d(n1))    &#125;    def mod(d: Double): Int = &#123;      d.toInt % 2    &#125;    def sum(d: Double): Double = &#123;      d + d    &#125;    val unit = test(sum,mod, 1)    println(unit)  &#125;</code></pre><pre><code class="scala"> def main(args: Array[String]): Unit = &#123;    //minusxy是一个高阶函数    def minusxy(x:Int) =&#123;      (y:Int)=&gt;x-y//返回一个匿名函数    &#125;    //只要 unit对象不变，就一直是3    val unit = minusxy(3)    println(minusxy _)    println(unit(2))    println(unit(4))    //也可以这么写    println(minusxy(9)(4))  &#125;</code></pre><h6 id="高级函数案例的小结"><a href="#高级函数案例的小结" class="headerlink" title="高级函数案例的小结"></a>高级函数案例的小结</h6><p>函数名为 <strong>minusxy</strong><br>该函数返回一个<strong>匿名函数  (y: Int) &#x3D; &gt; x -y</strong><br>说明val result3 &#x3D; minusxy(3)(5)</p><p>minusxy(3)执行minusxy(x: Int)得到 (y: Int) &#x3D;&gt; 3 - y 这个匿名函数<br>minusxy(3)(5)执行 (y: Int) &#x3D;&gt; x - y 这个匿名函数<br>也可以分步执行: val f1 &#x3D; minusxy(3);   val res &#x3D; f1(90)</p><h4 id="参数-类型-推断"><a href="#参数-类型-推断" class="headerlink" title="参数(类型)推断"></a>参数(类型)推断</h4><h5 id="基本介绍-26"><a href="#基本介绍-26" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>参数推断省去类型信息（在某些情况下[需要有应用场景]，参数类型是可以推断出来的，如list&#x3D;(1,2,3) list.map()   map中函数参数类型是可以推断的)，同时也可以进行相应的简写。</p><h5 id="参数类型推断写法说明"><a href="#参数类型推断写法说明" class="headerlink" title="参数类型推断写法说明"></a>参数类型推断写法说明</h5><p>参数类型是可以推断时，可以省略参数类型<br>当传入的函数，只有单个参数时，可以省去括号<br>如果变量只在&#x3D;&gt;右边只出现一次，可以用_来代替</p><h5 id="应用案例-13"><a href="#应用案例-13" class="headerlink" title="应用案例"></a>应用案例</h5><pre><code class="scala">    val list = List(1,2,3,4)    println(list.map(x=&gt;x+1))//List(2, 3, 4, 5)    println(list.map((x)=&gt;x+1))//List(2, 3, 4, 5)    println(list.map(x=&gt;x+1))//List(2, 3, 4, 5)    println(list.map(_+1))//List(2, 3, 4, 5)    val res = list.reduce(_+_)//10</code></pre><h4 id="闭包-closure"><a href="#闭包-closure" class="headerlink" title="闭包(closure)"></a>闭包(closure)</h4><h5 id="基本介绍-27"><a href="#基本介绍-27" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>基本介绍：<strong>闭包就是一个函数和与其相关的引用环境组合的一个整体(实体)。</strong></p><h5 id="案例演示"><a href="#案例演示" class="headerlink" title="案例演示"></a>案例演示</h5><pre><code class="scala"> def main(args: Array[String]): Unit = &#123;    //minusxy是一个高阶函数    def minusxy(x:Int) =&#123;      (y:Int)=&gt;x-y//返回一个匿名函数    &#125;    //只要 unit对象不变，就一直是3     //这里的unit就是闭包！！！    val unit = minusxy(3)    println(minusxy _)    println(unit(2))    println(unit(4))    //也可以这么写    println(minusxy(9)(4))  &#125;</code></pre><h5 id="代码小结"><a href="#代码小结" class="headerlink" title="代码小结"></a>代码小结</h5><p>1、第1点  (y: Int) &#x3D;&gt; x – y</p><p>2、返回的是一个匿名函数 ，因为该函数引用到到函数外的 x,那么  该函数和x整体形成一个闭包<br>如：这里 val f &#x3D; minusxy(20) 的f函数就是闭包 </p><p>你可以这样理解，返回函数是一个对象，而x就是该对象的一个字段，他们共同形成一个闭包<br>3、当多次调用f时（可以理解多次调用闭包），<strong>发现使用的是同一个x, 所以x不变。</strong><br>4、在使用闭包时，主要搞清楚返回函数引用了函数外的哪些变量，因为他们会组合成一个整体(实体),形成一个闭包</p><h6 id="闭包的最佳实践"><a href="#闭包的最佳实践" class="headerlink" title="闭包的最佳实践"></a>闭包的最佳实践</h6><p>请编写一个程序，具体要求如下<br>编写一个函数 makeSuffix(suffix: String)  可以接收一个文件后缀名(比如.jpg)，并返回一个闭包<br>调用闭包，可以传入一个文件名，如果该文件名没有指定的后缀(比如.jpg) ,则返回 文件名.jpg , 如果已经有.jpg后缀，则返回原文件名。<br>要求使用闭包的方式完成<br>String.endsWith(xx)</p><pre><code class="scala">  def main(args: Array[String]): Unit = &#123;    val f = makeSuffix(&quot;.jpg&quot;)    println(f(&quot;asdf&quot;))    println(f(&quot;asdf.jpg&quot;))  &#125;    def makeSuffix(suffix: String) =&#123;    //使用到suffix    (filename: String) =&gt; &#123;        //endwith，判断filename是不是以suffix为结尾      if (filename.endsWith(suffix)) &#123;        filename      &#125; else &#123;        filename + suffix      &#125;    &#125;  &#125;</code></pre><h4 id="函数柯里化"><a href="#函数柯里化" class="headerlink" title="函数柯里化"></a>函数柯里化</h4><h5 id="基本介绍-28"><a href="#基本介绍-28" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>函数编程中，<strong>接受多个参数的函数都可以转化为接受单个参数的函数，这个转化过程就叫柯里化</strong></p><p><strong>柯里化就是证明了函数只需要一个参数而已</strong>。其实我们刚才的学习过程中，已经涉及到了柯里化操作。</p><p>不用设立柯里化存在的意义这样的命题。柯里化就是以函数为主体这种思想发展的必然产生的结果。(即：柯里化是面向函数思想的必然产生结果)</p><h5 id="函数柯里化快速入门"><a href="#函数柯里化快速入门" class="headerlink" title="函数柯里化快速入门"></a>函数柯里化快速入门</h5><p>编写一个函数，接收两个整数，可以返回两个数的乘积，要求:</p><p>使用常规的方式完成<br>使用闭包的方式完成<br>使用函数柯里化完成</p><pre><code class="scala">//说明def mul(x: Int, y: Int) = x * yprintln(mul(10, 10))//闭包def mulCurry(x: Int) = (y: Int) =&gt; x * yprintln(mulCurry(10)(9))//柯里化def mulCurry2(x: Int)(y:Int) = x * yprintln(mulCurry2(10)(8))</code></pre><h4 id="控制抽象"><a href="#控制抽象" class="headerlink" title="控制抽象"></a>控制抽象</h4><h5 id="控制抽象基本介绍"><a href="#控制抽象基本介绍" class="headerlink" title="控制抽象基本介绍"></a>控制抽象基本介绍</h5><p>控制抽象是这样的函数，满足如下条件<br><strong>1、参数是函数</strong><br><strong>2、函数参数没有输入值也没有返回值</strong></p><h5 id="应用案例-14"><a href="#应用案例-14" class="headerlink" title="应用案例"></a>应用案例</h5><pre><code class="scala">  def main(args: Array[String]): Unit = &#123;    //接受的函数，没有输入也没有输出的函数    def myRunInThread(f1: () =&gt; Unit) = &#123;      new Thread &#123;        override def run(): Unit = &#123;          f1()        &#125;      &#125;.start()    &#125;    myRunInThread &#123;      () =&gt; println(&quot;干活咯！5秒完成...&quot;)        Thread.sleep(5000)        println(&quot;干完咯！&quot;)    &#125;  &#125;</code></pre><h5 id="控制抽象while循环案例"><a href="#控制抽象while循环案例" class="headerlink" title="控制抽象while循环案例"></a>控制抽象while循环案例</h5><pre><code class="scala">  def main(args: Array[String]): Unit = &#123;    var x = 10    while (x &gt; 0) &#123;      x -= 1      println(x)    &#125;    //上下等价    x=10    until(x&gt;0)&#123;      x-=1      println(x)    &#125;    //使用控制抽象写出until，实现类似效果    def until(f: =&gt; Boolean)(f2 : =&gt; Unit): Unit = &#123;      if(f)&#123;        f2        until(f)(f2)      &#125;    &#125;        &#125;</code></pre><h4 id="使用递归的方式去思考-去编程"><a href="#使用递归的方式去思考-去编程" class="headerlink" title="使用递归的方式去思考,去编程"></a>使用递归的方式去思考,去编程</h4><h5 id="基本介绍-29"><a href="#基本介绍-29" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>1、<strong>Scala 是运行在 Java 虚拟机（Java Virtual Machine）之上</strong>，因此具有如下特点:<br>2、<strong>轻松实现和丰富的 Java 类库互联互通。</strong><br>3、<strong>它既支持面向对象的编程方式，又支持函数式编程。</strong></p><p>4、它写出的程序像动态语言一样简洁，但事实上它确是严格意义上的静态语言。<br>Scala 就像一位武林中的集大成者，将过去几十年计算机语言发展历史中的精萃集于一身，化繁为简，为程序员们提供了一种新的选择。设计者马丁·奥得斯基 希望程序员们将编程作为简洁，高效，令人愉快的工作。同时也让程序员们进行关于编程思想的新的思考。</p><h5 id="Scala提倡函数式编程-递归思想"><a href="#Scala提倡函数式编程-递归思想" class="headerlink" title="Scala提倡函数式编程(递归思想)"></a>Scala提倡函数式编程(递归思想)</h5><p>编程范式:<br>1、在所有的编程范式中，面向对象编程（Object-Oriented Programming）无疑是最大的赢家。<br>2、但其实面向对象编程并不是一种严格意义上的编程范式，严格意义上的<strong>编程范式分为：命令式编程（Imperative Programming）、函数式编程（Functional Programming）和逻辑式编程（Logic Programming）</strong>。<strong>面向对象编程只是上述几种范式的一个交叉产物</strong>，更多的还是继承了命令式编程的基因。<br>3、在传统的语言设计中，只有命令式编程得到了强调，那就是程序员要告诉计算机应该怎么做。而递归则通过灵巧的函数定义，告诉计算机做什么。因此在使用命令式编程思维的程序中，是现在多数程序采用的编程方式，递归出镜的几率很少，而在函数式编程中，大家可以随处见到递归的方式。</p><h5 id="应用实例-3"><a href="#应用实例-3" class="headerlink" title="应用实例"></a>应用实例</h5><pre><code class="scala">//常规方式 def main(args: Array[String]): Unit = &#123;    val now:Date = new Date()    val dateFormat : SimpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;)    val date = dateFormat.format(now)    println(date)    var i = BigInt(1)    var sum =BigInt(0)    val maxVal = BigInt(99999991)    while(i&lt;maxVal)&#123;      sum += i      i+=1    &#125;    println(sum)    println(dateFormat.format(new Date()))  &#125;</code></pre><pre><code class="scala">//递归方，计算时间不会比常规慢，当时递归使用时有陷阱def main(args: Array[String]): Unit = &#123;    val now: Date = new Date()    val dateFormat: SimpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;)    val date = dateFormat.format(now)    println(date)    var i = BigInt(1)    var sum = BigInt(0)    val int: BigInt = mx(i, sum)    println(int)    println(dateFormat.format(new Date()))  &#125;  def mx(num: BigInt, sum: BigInt):BigInt = &#123;    if (num &lt;= 99999991) &#123;      mx(num + 1, sum + num)    &#125; else &#123;      sum    &#125;  &#125;</code></pre><h5 id="求最大值"><a href="#求最大值" class="headerlink" title="求最大值"></a>求最大值</h5><pre><code class="scala">def max(xs: List[Int]): Int = &#123;    if (xs.size == 1) &#123;      xs.head    &#125; else if (xs.head &gt; max(xs.tail)) &#123;      xs.head    &#125; else &#123;      max(xs.tail)    &#125;  &#125;</code></pre><h5 id="翻转字符串"><a href="#翻转字符串" class="headerlink" title="翻转字符串"></a>翻转字符串</h5><pre><code class="scala">def reverse(xs: String): String =    if (xs.length == 1) xs else reverse(xs.tail) + xs.head</code></pre><h5 id="求阶乘"><a href="#求阶乘" class="headerlink" title="求阶乘"></a>求阶乘</h5><pre><code class="scala">def factorial(n: Int): Int =    if (n == 0) 1 else n * factorial(n - 1)</code></pre><h4 id="项目开发流程"><a href="#项目开发流程" class="headerlink" title="项目开发流程"></a>项目开发流程</h4><p>0、可行性分析(干什么，谁来干？)</p><p>1、需求分析（需求分析师，分析报告）</p><p>2、设计阶段（架构师，什么架构，什么计数，什么系统，设计文档，界面）</p><p>3、实现阶段（软件工程师，看懂文档，实现模块）</p><p>4、测试阶段(测试工程师，一边开发，一边测试，白盒，黑盒，灰盒)</p><p>5、实施阶段（实施工程师，部署到系统上，正确运行）</p><p>6、维护阶段（不一定有专人，没人找经理）</p><h4 id="Akka-介绍"><a href="#Akka-介绍" class="headerlink" title="Akka 介绍"></a>Akka 介绍</h4><h5 id="基本介绍-30"><a href="#基本介绍-30" class="headerlink" title="基本介绍"></a>基本介绍</h5><p>1、Akka是JAVA虚拟机JVM平台上构建高并发、分布式和容错应用的工具包和运行时，你可以理解成<strong>Akka是编写并发程序的框架。</strong><br>2、Akka用Scala语言写成，<strong>同时提供了Scala和JAVA的开发接口。</strong><br>3、<strong>Akka主要解决的问题是：可以轻松的写出高效稳定的并发程序，程序员不再过多的考虑线程、锁和资源竞争等细节。</strong></p><h5 id="Actor-模型用于解决什么问题"><a href="#Actor-模型用于解决什么问题" class="headerlink" title="Actor 模型用于解决什么问题"></a>Actor 模型用于解决什么问题</h5><p>1、处理并发问题关键是要保证共享数据的一致性和正确性，因为程序是多线程时，多个线程对同一个数据进行修改，若不加同步条件，势必会造成数据污染。但是当我们对关键代码加入同步条件synchronized 后，实际上大并发就会阻塞在这段代码，对程序效率有很大影响。</p><p>2、若是用单线程处理，不会有数据一致性的问题，但是系统的性能又不能保证。<br>3、Actor 模型的出现解决了这个问题，<strong>简化并发编程，提升程序性能</strong>。 你可以这里理解：Actor 模型是一种处理并发问题的解决方案，很牛!</p><h5 id="Actor模型及其说明"><a href="#Actor模型及其说明" class="headerlink" title="Actor模型及其说明"></a>Actor模型及其说明</h5><p><img src="/scala/scala/image-20200310190600614.png" alt="image-20200310190600614"></p><p>1、<strong>Akka 处理并发的方法基于 Actor 模型。</strong>(示意图)<br>2、在基于 Actor 的系统里，所有的事物都是 Actor，就好像在面向对象设计里面所有的事物都是对象一样。<br>3、<strong>Actor 模型是作为一个并发模型设计和架构的。Actor 与 Actor 之间只能通过消息通信，如图的信封.</strong></p><p>4、Actor 与 Actor 之间只能用消息进行通信，当一个 Actor 给另外一个 Actor发消息，消息是有顺序的(消息队列)，只需要将消息投寄的相应的邮箱即可。</p><p>5、怎么处理消息是由接收消息的Actor决定的，发送消息Actor可以等待回复，也可以异步处理【ajax】</p><p>6、<strong>ActorSystem 的职责是负责创建并管理其创建的 Actor</strong>， <strong>ActorSystem 是单例的</strong>(可以理解ActorSystem是一个工厂，专门创建Actor)，一个 JVM 进程中有一个即可，而 Acotr 是可以有多个的。</p><p>7、Actor模型是对并发模型进行了更高的抽象。</p><p>8、Actor模型是异步、非阻塞、高性能的事件驱动编程模型。[什么是异步、非阻塞, 最经典的案例就是ajax异步请求处理 ]</p><p>9、<strong>Actor模型是轻量级事件处理（1GB 内存可容纳百万级别个 Actor），因此处理大并发性能高.</strong></p><h5 id="Actor模型工作机制说明"><a href="#Actor模型工作机制说明" class="headerlink" title="Actor模型工作机制说明"></a>Actor模型工作机制说明</h5><p><img src="/scala/scala/image-20200310191255505.png" alt="image-20200310191255505"></p><pre><code class="scala">B Actor 中receive 方法 &#123;//1.消息接收和处理//2. 通过sender() 方法可以得到发送消息的Actor的ActorRef, 通过这个ActorRef，B Actor 也可以回复消息&#125;</code></pre><p>1、<strong>ActorySystem创建Actor</strong><br>2、ActorRef:可以理解成是Actor的代理或者引用。消息是通过ActorRef来发送,而不能通过Actor 发送消息，通过哪个ActorRef 发消息，就表示把该消息发给哪个Actor<br>3、消息发送到<strong>Dispatcher Message (消息分发器)<strong>，它得到消息后，会将消息进行分发到对应的</strong>MailBox</strong>。(注: Dispatcher Message 可以理解成是一个线程池, <strong>MailBox</strong> 可以理解成是消息队列，可以缓冲多个消息，遵守FIFO)<br>4、Actor 可以通过 <strong>receive</strong>方法来获取消息，然后进行处理。</p><h5 id="Actor间传递消息机制-对照工作机制示意图理解"><a href="#Actor间传递消息机制-对照工作机制示意图理解" class="headerlink" title="Actor间传递消息机制(对照工作机制示意图理解)"></a>Actor间传递消息机制(对照工作机制示意图理解)</h5><p>1、每一个消息就是一个<strong>Message</strong>对象。Message 继承了Runable， 因为Message就是线程类。<br>2、从Actor模型工作机制看上去很麻烦，但是程序员编程时只需要编写Actor就可以了，其它的交给Actor模型完成即可。<br>3、A Actor要给B Actor 发送消息，那么A Actor 要先拿到(也称为持有) B Actor 的 代理对象<strong>ActorRef</strong> 才能发送消息 </p><h5 id="Actor模型快速入门"><a href="#Actor模型快速入门" class="headerlink" title="Actor模型快速入门"></a>Actor模型快速入门</h5><p>1、编写一个Actor, 比如SayHelloActor<br>2、SayHelloActor 可以给自己发送消息 ,如图</p><p><img src="/scala/scala/image-20200310200039436.png" alt="image-20200310200039436"></p><p>3、要求使用Maven的方式来构建项目,这样可以很好的解决项目开发包的依赖关系。[scala 和 akka]</p><pre><code class="scala">import akka.actor.&#123;Actor, ActorRef, ActorSystem, Props&#125;//继承Actor，重写核心方法  receiveclass SayHelloActor extends Actor &#123;  //receive方法会被该MailBox(实现类Runnable方法)调用  //当MailBox接收到消息，调用receive获取消息  //  type Receive = PartialFunction[Any, Unit]  override def receive: Receive = &#123;    case &quot;hello&quot; =&gt; println(&quot;hello too&quot;)    case &quot;ok&quot; =&gt; println(&quot;ok too&quot;)    case &quot;exit&quot;=&gt;&#123;      println(&quot;退出系统&quot;)      context.stop(self)//停止Actorref      context.system.terminate()//退出ActorSystem系统    &#125;    case _ =&gt; println(&quot;匹配不到&quot;)  &#125;&#125;object SayHelloActorDemo &#123;  //先创建一个ActorSystem，专门用于创建Actor  private val actorFactory: ActorSystem = ActorSystem(&quot;ActorFactory&quot;)  //创建一个Actor的同时，返回ActorRef  //说明  //1、Props[SayHelloActor]创建了一个sayHelloActor的实例，使用了反射机制  //2、Actor01  名字，名字不要重复  //3、ActorRef  类型，当前Actor01的引用  private val actorRef01: ActorRef = actorFactory.actorOf(Props[SayHelloActor], &quot;Actor01&quot;)  def main(args: Array[String]): Unit = &#123;    //给自己发消息，测试    actorRef01!&quot;hello&quot;    actorRef01!&quot;ok&quot;    actorRef01!&quot;ok~&quot;    actorRef01!&quot;exit&quot;  &#125;&#125;</code></pre><h5 id="Actor自我通讯机制原理图"><a href="#Actor自我通讯机制原理图" class="headerlink" title="Actor自我通讯机制原理图"></a>Actor自我通讯机制原理图</h5><p><img src="/scala/scala/image-20200310200445268.png" alt="image-20200310200445268"></p><h5 id="小结和说明"><a href="#小结和说明" class="headerlink" title="小结和说明"></a>小结和说明</h5><p>当程序执行 aActorRef &#x3D; actorFactory.actorOf(Props[AActor], “aActor”) ，会完成如下任务 [这是非常重要的方法]<br>1、actorFactory 是 <strong>ActorSystem(“ActorFactory”)</strong> 这样创建的。<br>2、这里的 Props[AActor] 会使用反射机制，创建一个AActor 对象，如果是actorFactory.actorOf(Props(new AActor(bActorRef)), “aActorRef”) 形式，就是使用new 的方式创建一个AActor对象, 注意Props() 是小括号。<br>3、会创建一个AActor 对象的代理对象 aActorRef , 使用aActorRef 才能发送消息<br>4、会在底层创建 <strong>Dispather Message</strong> ，是一个<strong>线程池，用于分发消息</strong>， 消息是发送到对应的Actor的 <strong>MailBox</strong></p><p>5、会在底层创建AActor 的<strong>MailBox</strong> 对象，该对象是一个<strong>队列</strong>，可<strong>接收Dispatcher Message 发送的消息</strong><br>6、<strong>MailBox 实现了Runnable 接口，是一个线程</strong>，一直运行并调用Actor的receive 方法，因此当Dispather 发送消息到MailBox时，<strong>Actor 在receive 方法就可以得到信息.</strong><br>7、aActorRef <strong>!</strong>  “hello”, 表示把hello消息发送到A Actor 的mailbox （通过Dispatcher Message 转发）</p><h5 id="两个Actor的通讯机制原理图"><a href="#两个Actor的通讯机制原理图" class="headerlink" title="两个Actor的通讯机制原理图"></a>两个Actor的通讯机制原理图</h5><p><img src="/scala/scala/image-20200311105610114.png" alt="image-20200311105610114"></p><h5 id="两个Actor的通讯小结和说明"><a href="#两个Actor的通讯小结和说明" class="headerlink" title="两个Actor的通讯小结和说明"></a>两个Actor的通讯小结和说明</h5><p>1、两个Actor通讯机制和Actor 自身发消息机制基本一样，只是要注意如下<br>2、如果A Actor 在需要给B Actor 发消息，则需要持有B Actor 的 <strong>ActorRef</strong>，可以通过创建时，传入 B Actor的 代理对象(ActorRef)<br>3、当B Actor 在receive 方法中接收到消息，需要回复时，可以通过sender() 获取到发送Actor的 代理对象。</p><h5 id="单机版应用实例"><a href="#单机版应用实例" class="headerlink" title="单机版应用实例"></a>单机版应用实例</h5><p>1、编写2个 Actor , 分别是  AActor 和 BActor<br>2、AActor和BActor之间可以相互发送消息.</p><p>3、加强对Actor传递消息机制的理解。</p><pre><code class="scala">import akka.actor.&#123;Actor, ActorRef&#125;//给bActor发消息需要bActorRef，传入一个class AActor(actorRef:ActorRef) extends Actor &#123;  val bActorRef: ActorRef = actorRef  var i = 0  override def receive: Receive =    &#123;      case &quot;star&quot; =&gt; &#123;        println(&quot;AActor出招了...&quot;)        self ! &quot;我打&quot;      &#125;      case &quot;我打&quot; =&gt; &#123;        //给bActor发送消息        println(&quot;黄飞鸿  我打  飞上无影脚&quot;)        Thread.sleep(1000)        bActorRef ! &quot;我打&quot; //发给bactor邮箱      &#125;  &#125;&#125;</code></pre><pre><code class="scala">import akka.actor.&#123;Actor, ActorRef&#125;class BActor extends Actor &#123;  override def receive: Receive = &#123;    case &quot;我打&quot; =&gt; &#123;      println(&quot;乔峰 挺猛  降龙十八掌1&quot;)      Thread.sleep(1000)      sender() ! &quot;我打&quot;    &#125;  &#125;&#125;</code></pre><pre><code class="scala">import akka.actor.&#123;ActorRef, ActorSystem, Props&#125;//运行类object ActorGame &#123;  def main(args: Array[String]): Unit = &#123;    val actorfactory: ActorSystem = ActorSystem(&quot;actorfactory&quot;)      //bActor的引用      //这种创建方式是单机版的    val bActor: ActorRef = actorfactory.actorOf(Props[BActor], &quot;bActor&quot;)    //因为有参数，所以new    val aActor: ActorRef = actorfactory.actorOf(Props(new AActor(bActor)), &quot;aActor&quot;)    aActor ! &quot;star&quot;  &#125;&#125;</code></pre><h5 id="如何理解Actor-的receive-方法被调用"><a href="#如何理解Actor-的receive-方法被调用" class="headerlink" title="如何理解Actor 的receive 方法被调用?"></a>如何理解Actor 的receive 方法被调用?</h5><p>1、每个<strong>Actor</strong>对应<strong>MailBox</strong><br>2、<strong>MailBox</strong> 实现了<strong>Runnable</strong> 接口，处于运行的状态<br>3、当有消息到达<strong>MailBox</strong>,就会去调用Actor的<strong>receive</strong>方法，将消息<strong>推送给receive</strong> </p><h5 id="Akka执行流程"><a href="#Akka执行流程" class="headerlink" title="Akka执行流程"></a>Akka执行流程</h5><p>1、先创建ActorSystem</p><p>2、通过ActorySystem创建对应的Actor&#x2F;ActorRef(关联关系)</p><p>3、通过ActorRef！消息[发送ActorRef对应的Actor的milaBox]</p><p>4、先将消息发送给Dispatcher  Message（中间件）（Dispatcher线程池）</p><p>5、Dispatcher Message将消息抓发对应的Actor的MailBox</p><p>6、当Actor的MailBox收到消息后，就会调用Actor的receive方法，把消息推送给Actor</p><p>7、如果希望回复消息通过sender()!”消息“（send可以获取到发送消息的ActorRef）</p><h5 id="Akka网络编程"><a href="#Akka网络编程" class="headerlink" title="Akka网络编程"></a>Akka网络编程</h5><p>Akka支持<strong>面向大并发后端服务程序</strong>，网络通信这块是服务端程序重要的一部分。</p><p>网络编程有两种:<br>1、<strong>TCP socket编程</strong>，是网络编程的主流。之所以叫Tcp socket编程，是因为底层是基于Tcp&#x2F;ip协议的. 比如: QQ聊天 [示意图]</p><p>2、<strong>b&#x2F;s结构的http编程</strong>，我们使用浏览器去访问服务器时，使用的就是http协议，而http底层依旧是用tcp socket实现的协议(tcp&#x2F;ip)</p><h5 id="协议-tcp-x2F-ip"><a href="#协议-tcp-x2F-ip" class="headerlink" title="协议(tcp&#x2F;ip)"></a>协议(tcp&#x2F;ip)</h5><p>TCP&#x2F;IP（Transmission Control Protocol&#x2F;Internet Protocol)的简写,中文译名为传输控制协议&#x2F;因特网互联协议，又叫网络通讯协议，这个协议是Internet最基本的协议、Internet国际互联网络的基础，简单地说，就是由网络层的IP协议和传输层的TCP协议组成的。 比如: 京东商城 【属于 web 开发范畴 】</p><h5 id="OSI与Tcp-x2F-ip参考模型-推荐tcp-x2F-ip协议3卷"><a href="#OSI与Tcp-x2F-ip参考模型-推荐tcp-x2F-ip协议3卷" class="headerlink" title="OSI与Tcp&#x2F;ip参考模型 (推荐tcp&#x2F;ip协议3卷)"></a>OSI与Tcp&#x2F;ip参考模型 (推荐tcp&#x2F;ip协议3卷)</h5><p><img src="/scala/scala/image-20200311112906601.png" alt="image-20200311112906601"></p><h5 id="网络编程基础知识"><a href="#网络编程基础知识" class="headerlink" title="网络编程基础知识"></a>网络编程基础知识</h5><h6 id="端口-port-分类"><a href="#端口-port-分类" class="headerlink" title="端口(port)-分类"></a>端口(port)-分类</h6><p>1）<strong>0号是保留端口.</strong><br>2）<strong>1-1024是固定端口</strong><br>又叫有名端口,即被某些程序固定使用,一般程序员不使用.<br>22: SSH远程登录协议  23: telnet使用  21: ftp使用<br>25: smtp服务使用     80: iis使用7: echo服务<br>3）<strong>1025-65535是动态端口</strong> 这些端口，程序员可以使用.</p><h6 id="端口-port-使用注意"><a href="#端口-port-使用注意" class="headerlink" title="端口(port)-使用注意"></a>端口(port)-使用注意</h6><p>在计算机(尤其是做服务器)要尽可能的少开端口[]<br>一个端口只能被一个程序监听( )<br>如果使用 <strong>netstat –an</strong> 可以查看本机有哪些端口在监听<br>可以使用 <strong>netstat –anb</strong> 来查看监听端口的pid,在结合任务管理器关闭不安全的端口.</p><h5 id="Akka网络编程实例-小黄鸡客服"><a href="#Akka网络编程实例-小黄鸡客服" class="headerlink" title="Akka网络编程实例_小黄鸡客服"></a>Akka网络编程实例_小黄鸡客服</h5><p><img src="/scala/scala/image-20200321132011289.png" alt="image-20200321132011289"></p><p><img src="/scala/scala/image-20200321132422962.png" alt="image-20200321132422962"></p><p>Client端</p><pre><code class="scala">//clientimport akka.actor.&#123;Actor, ActorRef, ActorSelection, ActorSystem, Props&#125;import com.star.akka.yellowchicken.common.&#123;ClientMessage, ServerMessage&#125;import com.typesafe.config.ConfigFactoryimport scala.io.StdInclass CustomerActor(serverHost: String, serverPort: Int) extends Actor &#123;  //定义一个YellowChickenServerRef  var serverActorRef: ActorSelection = _  //在Actor中有一个方法PreStart方法，他会在actor运行前执行  //在akka的开发中，通常将初始化的工作，放在preStart方法  override def preStart(): Unit = &#123;    println(&quot;preStart() 执行&quot;)    serverActorRef = context.actorSelection(s&quot;akka.tcp://Server@$&#123;serverHost&#125;:$&#123;serverPort&#125;/user/YellowChickenServer&quot;)    println(&quot;serverActorRef=&quot; + serverActorRef)  &#125;  override def receive: Receive = &#123;    case &quot;start&quot; =&gt; println(&quot;start,客户端运行，可以咨询问题&quot;)    case mes: String =&gt; &#123;      //发给小黄鸡客服      serverActorRef ! ClientMessage(mes) //使用ClientMessage case class apply    &#125;    //如果接收到服务器的回复    case ServerMessage(mes) =&gt; &#123;      println(s&quot;收到小黄鸡客服(Server): $mes&quot;)    &#125;  &#125;&#125;//主程序-入口object CustomerActor extends App &#123;  val (clientHost, clientPort, serverHost, serverPort) = (&quot;127.0.0.1&quot;, 9990, &quot;127.0.0.1&quot;, 9999)  val config = ConfigFactory.parseString(    s&quot;&quot;&quot;       |akka.actor.provider=&quot;akka.remote.RemoteActorRefProvider&quot;       |akka.remote.netty.tcp.hostname=$clientHost       |akka.remote.netty.tcp.port=$clientPort        &quot;&quot;&quot;.stripMargin)  //创建ActorSystem  val clientActorSystem = ActorSystem(&quot;client&quot;, config)  //创建CustomerActor的实例和引用  val customerActorRef: ActorRef = clientActorSystem.actorOf(Props(new CustomerActor(serverHost, serverPort)), &quot;CustomerActor&quot;)  //启动customerRef/也可以理解启动Actor  customerActorRef ! &quot;start&quot;  //客户端可以发送消息给服务器  while (true) &#123;    println(&quot;请输入要咨询的问题&quot;)    val mes = StdIn.readLine()    customerActorRef ! mes  &#125;&#125;</code></pre><p>Common</p><pre><code class="scala">//common，他们公用的样例类//样例类传入参数，默认成为只读属性//使用样例类来构建协议//客户端发给服务器协议(序列化的对象)case class ClientMessage(mes: String)//服务端发给客户端的协议(样例类对象)case class ServerMessage(mes: String)</code></pre><p>Server端</p><pre><code class="scala">//server端import akka.actor.&#123;Actor, ActorRef, ActorSystem, Props&#125;import com.star.akka.yellowchicken.common.&#123;ClientMessage, ServerMessage&#125;import com.typesafe.config.ConfigFactoryclass YellowChickenServer extends Actor&#123;  override def receive:Receive = &#123;    case &quot;start&quot; =&gt; println(&quot;start 小黄鸡客服开始工作了....&quot;)    //如果接收到ClientMessage    case ClientMessage(mes) =&gt; &#123;      //使用match --case 匹配(模糊)      mes match &#123;        case &quot;大数据学费&quot; =&gt; sender() ! ServerMessage(&quot;35000RMB&quot;)        case &quot;学校地址&quot; =&gt; sender() ! ServerMessage(&quot;北京昌平xx路xx大楼&quot;)        case &quot;学习什么技术&quot; =&gt; sender() ! ServerMessage(&quot;大数据 前端 python&quot;)        case _ =&gt; sender() ! ServerMessage(&quot;你说的啥子~&quot;)      &#125;    &#125;  &#125;&#125;//主程序-入口object YellowChickenServer extends App &#123;  val host = &quot;127.0.0.1&quot; //服务端ip地址  val port = 9999  //创建config对象,指定协议类型，监听的ip和端口  val config = ConfigFactory.parseString(    s&quot;&quot;&quot;       |akka.actor.provider=&quot;akka.remote.RemoteActorRefProvider&quot;       |akka.remote.netty.tcp.hostname=$host       |akka.remote.netty.tcp.port=$port        &quot;&quot;&quot;.stripMargin)  //创建ActorSystem  //url (统一资源定位)  val serverActorSystem = ActorSystem(&quot;Server&quot;,config)  //创建YellowChickenServer 的actor和返回actorRef  val yellowChickenServerRef: ActorRef = serverActorSystem.actorOf(Props[YellowChickenServer],&quot;YellowChickenServer&quot;)  //启动  yellowChickenServerRef ! &quot;start&quot;&#125;</code></pre><h5 id="Spark-Master-Worker-进程通讯项目"><a href="#Spark-Master-Worker-进程通讯项目" class="headerlink" title="Spark Master Worker 进程通讯项目"></a>Spark Master Worker 进程通讯项目</h5><p>1)worker注册到Master, Master完成注册，并回复worker注册成功</p><p>2)worker定时发送心跳，并在Master接收到</p><p>3)Master接收到worker心跳后，要更新该worker的最近一次发送心跳的时间</p><p>4)给Master启动定时任务，定时检测注册的worker有哪些没有更新心跳,并将其从hashmap中删除</p><p>5)master worker 进行分布式部署(Linux系统)-》如何给maven项目打包-&gt;上传linux</p><h6 id="实现功能1"><a href="#实现功能1" class="headerlink" title="实现功能1"></a>实现功能1</h6><p>1)worker注册到Master, Master完成注册，并回复worker注册成功</p><p>2)worker定时发送心跳，并在Master接收到</p><p>master端</p><pre><code class="scala">//master端class SparkMaster extends Actor &#123;  //定义一个hashmap，存放注册信息  val workers = new mutable.HashMap[String, WorkerInfo]()  override def receive: Receive = &#123;    case &quot;start&quot; =&gt; println(&quot;master服务器启动&quot;)    case RegisterWorkerInfo(id, cpu, ram) =&gt; &#123;      //接收到客户端注册信息      if (!workers.contains(id)) &#123;        val workerInfo = new WorkerInfo(id, cpu, ram)        workers += (id -&gt; workerInfo)        println(s&quot;服务器数据：$&#123;workers&#125;&quot;)        sender()!RegisteredWorkerInfo      &#125;    &#125;  &#125;&#125;object SparkMaster &#123;  def main(args: Array[String]): Unit = &#123;    val host = &quot;127.0.0.1&quot; //服务端ip地址    val port = 9999    val conf = ConfigFactory.parseString(      s&quot;&quot;&quot;         |akka.actor.provider=&quot;akka.remote.RemoteActorRefProvider&quot;         |akka.remote.netty.tcp.hostname=$host         |akka.remote.netty.tcp.port=$port        &quot;&quot;&quot;.stripMargin)    val sparkmaster: ActorSystem = ActorSystem(&quot;sparkmaster&quot;, conf)    val sparkmasterRef: ActorRef = sparkmaster.actorOf(Props[SparkMaster], &quot;sparkmaster01&quot;)    sparkmasterRef ! (&quot;start&quot;)  &#125;&#125;</code></pre><p>worker端</p><pre><code class="scala">class SparkWorker(masterHost: String, masterPort: Int) extends Actor &#123;  //master的代理对象即引用  var masterPorxy: ActorSelection = _  //随机数的id值  val id = UUID.randomUUID().toString  override def preStart(): Unit = &#123;    masterPorxy = context.actorSelection(s&quot;akka.tcp://sparkmaster@$&#123;masterHost&#125;:$&#123;masterPort&#125;/user/sparkmaster01&quot;)  &#125;  override def receive: Receive = &#123;    case &quot;start&quot; =&gt; &#123;      println(&quot;worker启动了&quot;)      //向master发送注册信息      masterPorxy ! RegisterWorkerInfo(id, 16, 16 * 1024)    &#125;    case RegisteredWorkerInfo=&gt;&#123;      println(s&quot;workid$&#123;id&#125;注册成功&quot;)    &#125;  &#125;&#125;object SparkWorker &#123;  def main(args: Array[String]): Unit = &#123;    val (workerhost, workerport, masterHost, masterPort) = (&quot;127.0.0.1&quot;, 9990, &quot;127.0.0.1&quot;, 9999)    val config = ConfigFactory.parseString(      s&quot;&quot;&quot;         |akka.actor.provider=&quot;akka.remote.RemoteActorRefProvider&quot;         |akka.remote.netty.tcp.hostname=$workerhost         |akka.remote.netty.tcp.port=$workerport        &quot;&quot;&quot;.stripMargin)    val sparkwork: ActorSystem = ActorSystem(&quot;sparkwork&quot;, config)    val sparkworkRef: ActorRef = sparkwork.actorOf(Props(new SparkWorker(masterHost, masterPort)), &quot;sparkwork01&quot;)    sparkworkRef ! &quot;start&quot;  &#125;&#125;</code></pre><p>common，workermaster两个共同使用的</p><pre><code class="scala">//id   cpu 内存case class RegisterWorkerInfo(id:String,cpu:Int,ram:Int)//保存信息存放到maseter的hashmap中，可以扩展class WorkerInfo(val id:String,val cpu:Int,val ram:Int)//worker注册成功，master返回的信息case object RegisteredWorkerInfo</code></pre><h6 id="实现功能2"><a href="#实现功能2" class="headerlink" title="实现功能2"></a>实现功能2</h6><p>master端</p><pre><code class="scala">//接收到心跳 ，刷新对应的信息case HearBeat(id)=&gt;&#123;      val info: WorkerInfo = workers(id)      info.lastHeartBeat = System.currentTimeMillis()      println(s&quot;更新$&#123;id&#125;时间成功&quot;)    &#125;</code></pre><p>common端</p><pre><code class="scala">//worker每隔一定时间定时器给自己发送消息case object SendHeartBeat//worker每隔一定时间由定时器触发，向master发送消息case class HearBeat(id:String)//扩展了workerInfoclass WorkerInfo(val id:String,val cpu:Int,val ram:Int)&#123;  var lastHeartBeat :Long = System.currentTimeMillis()&#125;</code></pre><p>worker端</p><pre><code class="scala">    case RegisteredWorkerInfo=&gt;&#123;      println(s&quot;workid$&#123;id&#125;注册成功&quot;)      //注册成功向master发送心跳      //0 延时时间      //2、3秒执行一次      //3、发送给谁  self  发送给自己      //4、发送的内容      context.system.scheduler.schedule(0 millis,3000 millis,self,SendHeartBeat)    &#125;    case SendHeartBeat=&gt;&#123;      masterPorxy! HearBeat(id)    &#125;</code></pre><h6 id="实现功能3"><a href="#实现功能3" class="headerlink" title="实现功能3"></a>实现功能3</h6><p>master启动定时任务，定时检测worker有没有超时（没有worker端的事情）</p><p>master端</p><pre><code class="scala">    case &quot;start&quot; =&gt; &#123;      println(&quot;master服务器启动&quot;)      self ! StartTimeOutWorker    &#125;    case StartTimeOutWorker =&gt; &#123;      println(&quot;开始定时检测&quot;)      //0 延时时间      //2、9秒执行一次      //3、发送给谁  self  发送给自己      //4、发送的内容RemoveTimeOutWorker      import context.dispatcher      context.system.scheduler.schedule(0 millis, 9000 millis, self, RemoveTimeOutWorker)    &#125;    case RemoveTimeOutWorker =&gt; &#123;      val time: Long = System.currentTimeMillis()      val values: Iterable[WorkerInfo] = workers.values      values.filter(t =&gt; (time - t.lastHeartBeat) &gt; 6000).foreach(v =&gt; workers.remove(v.id))      println(s&quot;当前有$&#123;workers.size&#125;个worker存活&quot;)    &#125;</code></pre><p>common端</p><pre><code class="scala">//master给自己发送一个触发检测超时的worker信息case object StartTimeOutWorker//master给自己发送信息，检测worker心跳是否超时case object RemoveTimeOutWorker</code></pre>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
