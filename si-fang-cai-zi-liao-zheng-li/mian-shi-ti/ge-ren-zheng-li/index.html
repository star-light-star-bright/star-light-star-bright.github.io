<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="别人的知识点整理, 张文辉的笔记">
    <meta name="description" content="这是我的博客网站">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>别人的知识点整理 | 张文辉的笔记</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 6.2.0"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">张文辉的笔记</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">张文辉的笔记</div>
        <div class="logo-desc">
            
            这是我的博客网站
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/13.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">别人的知识点整理</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/">
                                <span class="chip bg-color">面试题</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/" class="post-category">
                                面试题
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2020-11-03
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p>Hive on Spark参数调优姿势小结</p>
<p>本文简单列举一些调优项。为了符合实际情况，Spark也采用on YARN部署方式来说明。</p>
<pre><code class="python">Driver参数

spark.driver.cores
该参数表示每个Executor可利用的CPU核心数。其值不宜设定过大，因为Hive的底层以HDFS存储，而HDFS有时对高并发写入处理不太好，容易造成race condition。根据我们的实践，设定在3~6之间比较合理。
假设我们使用的服务器单节点有32个CPU核心可供使用。考虑到系统基础服务和HDFS等组件的余量，一般会将YARN NodeManager的yarn.nodemanager.resource.cpu-vcores参数设为28，也就是YARN能够利用其中的28核，此时将spark.executor.cores设为4最合适，最多可以正好分配给7个Executor而不造成浪费。又假设yarn.nodemanager.resource.cpu-vcores为26，那么将spark.executor.cores设为5最合适，只会剩余1个核。
由于一个Executor需要一个YARN Container来运行，所以还需保证spark.executor.cores的值不能大于单个Container能申请到的最大核心数，即yarn.scheduler.maximum-allocation-vcores的值。
</code></pre>
<pre><code class="python">spark.executor.memory/spark.yarn.executor.memoryOverhead
这两个参数分别表示每个Executor可利用的堆内内存量和堆外内存量。堆内内存越大，Executor就能缓存更多的数据，在做诸如map join之类的操作时就会更快，但同时也会使得GC变得更麻烦。Hive官方提供了一个计算Executor总内存量的经验公式，如下：
yarn.nodemanager.resource.memory-mb * (spark.executor.cores / yarn.nodemanager.resource.cpu-vcores)
其实就是按核心数的比例分配。在计算出来的总内存量中，80%~85%划分给堆内内存，剩余的划分给堆外内存。

假设集群中单节点有128G物理内存，yarn.nodemanager.resource.memory-mb（即单个NodeManager能够利用的主机内存量）设为120G，那么总内存量就是：120 * 1024 * (4 / 28) ≈ 17554MB。再按8:2比例划分的话，最终spark.executor.memory设为约13166MB，spark.yarn.executor.memoryOverhead设为约4389MB。

与上一节同理，这两个内存参数相加的总量也不能超过单个Container最多能申请到的内存量，即yarn.scheduler.maximum-allocation-mb。
</code></pre>
<pre><code class="python">spark.executor.instances
该参数表示执行查询时一共启动多少个Executor实例，这取决于每个节点的资源分配情况以及集群的节点数。若我们一共有10台32C/128G的节点，并按照上述配置（即每个节点承载7个Executor），那么理论上讲我们可以将spark.executor.instances设为70，以使集群资源最大化利用。但是实际上一般都会适当设小一些（推荐是理论值的一半左右），因为Driver也要占用资源，并且一个YARN集群往往还要承载除了Hive on Spark之外的其他业务。

spark.dynamicAllocation.enabled
上面所说的固定分配Executor数量的方式可能不太灵活，尤其是在Hive集群面向很多用户提供分析服务的情况下。所以更推荐将spark.dynamicAllocation.enabled参数设为true，以启用Executor动态分配。
</code></pre>
<pre><code class="python">Driver参数
spark.driver.cores
该参数表示每个Driver可利用的CPU核心数。绝大多数情况下设为1都够用。
spark.driver.memory/spark.driver.memoryOverhead
这两个参数分别表示每个Driver可利用的堆内内存量和堆外内存量。根据资源富余程度和作业的大小，一般是将总量控制在512MB~4GB之间，并且沿用Executor内存的“二八分配方式”。例如，spark.driver.memory可以设为约819MB，spark.driver.memoryOverhead设为约205MB，加起来正好1G。
</code></pre>
<pre><code class="python">Hive参数
绝大部分Hive参数的含义和调优方法都与on MR时相同，但仍有两个需要注意。

hive.auto.convert.join.noconditionaltask.size
我们知道，当Hive中做join操作的表有一方是小表时，如果hive.auto.convert.join和hive.auto.convert.join.noconditionaltask开关都为true（默认即如此），就会自动转换成比较高效的map-side join。而hive.auto.convert.join.noconditionaltask.size这个参数就是map join转化的阈值，在Hive on MR下默认为10MB。

但是Hive on MR下统计表的大小时，使用的是数据在磁盘上存储的近似大小，而Hive on Spark下则改用在内存中存储的近似大小。由于HDFS上的数据很有可能被压缩或序列化，使得大小减小，所以由MR迁移到Spark时要适当调高这个参数，以保证map join正常转换。一般会设为100~200MB左右，如果内存充裕，可以更大点。

hive.merge.sparkfiles
小文件是HDFS的天敌，所以Hive原生提供了合并小文件的选项，在on  MR时是hive.merge.mapredfiles，但是on Spark时会改成hive.merge.sparkfiles，注意要把这个参数设为true。至于小文件合并的阈值参数，即hive.merge.smallfiles.avgsize与hive.merge.size.per.task都没有变化。
</code></pre>
<h2 id="Hive性能优化"><a href="#Hive性能优化" class="headerlink" title="Hive性能优化"></a><strong>Hive性能优化</strong></h2><h4 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a><strong>1.介绍</strong></h4><p>首先，我们来看看Hadoop的计算框架特性，在此特性下会衍生哪些问题？</p>
<ul>
<li>数据量大不是问题，数据倾斜是个问题。</li>
<li>jobs数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次汇总，产生十几个jobs，耗时很长。原因是map reduce作业初始化的时间是比较长的。</li>
<li>sum,count,max,min等UDAF，不怕数据倾斜问题,hadoop在map端的汇总合并优化，使数据倾斜不成问题。</li>
<li>count(distinct ),在数据量大的情况下，效率较低，如果是多count(distinct )效率更低，因为count(distinct)是按group by 字段分组，按distinct字段排序，一般这种分布方式是很倾斜的。举个例子：比如男uv,女uv，像淘宝一天30亿的pv，如果按性别分组，分配2个reduce,每个reduce处理15亿数据。</li>
</ul>
<p>面对这些问题，我们能有哪些有效的优化手段呢？下面列出一些在工作有效可行的优化手段：</p>
<ul>
<li>好的模型设计事半功倍。</li>
<li>解决数据倾斜问题。</li>
<li>减少job数。</li>
<li>设置合理的map reduce的task数，能有效提升性能。(比如，10w+级别的计算，用160个reduce，那是相当的浪费，1个足够)。</li>
<li>了解数据分布，自己动手解决数据倾斜问题是个不错的选择。set hive.groupby.skewindata&#x3D;true;这是通用的算法优化，但算法优化有时不能适应特定业务背景，开发人员了解业务，了解数据，可以通过业务逻辑精确有效的解决数据倾斜问题。</li>
<li>数据量较大的情况下，慎用count(distinct)，count(distinct)容易产生倾斜问题。</li>
<li>对小文件进行合并，是行至有效的提高调度效率的方法，假如所有的作业设置合理的文件数，对云梯的整体调度效率也会产生积极的正向影响。</li>
<li>优化时把握整体，单个作业最优不如整体最优。</li>
</ul>
<p>而接下来，我们心中应该会有一些疑问，影响性能的根源是什么？</p>
<h4 id="2-性能低下的根源"><a href="#2-性能低下的根源" class="headerlink" title="2.性能低下的根源"></a><strong>2.性能低下的根源</strong></h4><p>hive性能优化时，把HiveQL当做M&#x2F;R程序来读，即从M&#x2F;R的运行角度来考虑优化性能，从更底层思考如何优化运算性能，而不仅仅局限于逻辑代码的替换层面。</p>
<p>RAC（Real Application Cluster）真正应用集群就像一辆机动灵活的小货车，响应快；Hadoop就像吞吐量巨大的轮船，启动开销大，如果每次只做小数量的输入输出，利用率将会很低。所以用好Hadoop的首要任务是增大每次任务所搭载的数据量。</p>
<p>Hadoop的核心能力是parition和sort，因而这也是优化的根本。</p>
<p>观察Hadoop处理数据的过程，有几个显著的特征：</p>
<ul>
<li>数据的大规模并不是负载重点，造成运行压力过大是因为运行数据的倾斜。</li>
<li>jobs数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联对此汇总，产生几十个jobs，将会需要30分钟以上的时间且大部分时间被用于作业分配，初始化和数据输出。M&#x2F;R作业初始化的时间是比较耗时间资源的一个部分。</li>
<li>在使用SUM，COUNT，MAX，MIN等UDAF函数时，不怕数据倾斜问题，Hadoop在Map端的汇总合并优化过，使数据倾斜不成问题。</li>
<li>COUNT(DISTINCT)在数据量大的情况下，效率较低，如果多COUNT(DISTINCT)效率更低，因为COUNT(DISTINCT)是按GROUP BY字段分组，按DISTINCT字段排序，一般这种分布式方式是很倾斜的；比如：男UV，女UV，淘宝一天30亿的PV，如果按性别分组，分配2个reduce,每个reduce处理15亿数据。</li>
<li>数据倾斜是导致效率大幅降低的主要原因，可以采用多一次 Map&#x2F;Reduce 的方法， 避免倾斜。</li>
</ul>
<p>最后得出的结论是：避实就虚，用 job 数的增加，输入量的增加，占用更多存储空间，充分利用空闲 CPU 等各种方法，分解数据倾斜造成的负担。</p>
<h4 id="3-配置角度优化"><a href="#3-配置角度优化" class="headerlink" title="3.配置角度优化"></a><strong>3.配置角度优化</strong></h4><p>我们知道了性能低下的根源，同样，我们也可以从Hive的配置解读去优化。Hive系统内部已针对不同的查询预设定了优化方法，用户可以通过调整配置进行控制， 以下举例介绍部分优化的策略以及优化控制选项。</p>
<pre><code class="python">#### 3.1列裁剪**

Hive 在读数据的时候，可以只读取查询中所需要用到的列，而忽略其它列。例如，若有以下查询：

SELECT a,b FROM q WHERE e&lt;10;

在实施此项查询中，Q 表有 5 列（a，b，c，d，e），Hive 只读取查询逻辑中真实需要 的 3 列 a、b、e，而忽略列 c，d；这样做节省了读取开销，中间表存储开销和数据整合开销。

裁剪所对应的参数项为：hive.optimize.cp=true（默认值为真）

#### **3.2分区裁剪**

可以在查询的过程中减少不必要的分区。例如，若有以下查询：

SELECT * FROM (SELECTT a1,COUNT(1) FROM T GROUP BY a1) subq WHERE subq.prtn=100; #（多余分区）SELECT * FROM T1 JOIN (SELECT * FROM T2) subq ON (T1.a1=subq.a2) WHERE subq.prtn=100;

查询语句若将“subq.prtn=100”条件放入子查询中更为高效，可以减少读入的分区 数目。Hive 自动执行这种裁剪优化。

分区参数为：hive.optimize.pruner=true（默认值为真）

#### **3.3JOIN操作**

在编写带有 join 操作的代码语句时，应该将条目少的表/子查询放在 Join 操作符的左边。因为在 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，载入条目较少的表 可以有效减少 OOM（out of memory）即内存溢出。所以对于同一个 key 来说，对应的 value 值小的放前，大的放后，这便是“小表放前”原则。若一条语句中有多个 Join，依据 Join 的条件相同与否，有不同的处理方法。

#### **3.3.1JOIN原则**

在使用写有 Join 操作的查询语句时有一条原则：应该将条目少的表/子查询放在 Join 操作符的左边。原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率。对于一条语句中有多个 Join 的情况，如果 Join 的条件相同，比如查询：

INSERT OVERWRITE TABLE pv_users

SELECT pv.pageid, u.age FROM page_view p

JOIN user u ON (pv.userid = u.userid)

JOIN newuser x ON (u.userid = x.userid);

- 如果 Join 的 key 相同，不管有多少个表，都会则会合并为一个 Map-Reduce
- 一个 Map-Reduce 任务，而不是 ‘n’ 个
- 在做 OUTER JOIN 的时候也是一样

如果 Join 的条件不相同，比如：

INSERT OVERWRITE TABLE pv_users

SELECT pv.pageid, u.age FROM page_view p

JOIN user u ON (pv.userid = u.userid)

JOIN newuser x on (u.age = x.age);

Map-Reduce 的任务数目和 Join 操作的数目是对应的，上述查询和以下查询是等价的：

INSERT OVERWRITE TABLE tmptable

SELECT * FROM page_view p JOIN user u

ON (pv.userid = u.userid);

INSERT OVERWRITE TABLE pv_users

SELECT x.pageid, x.age FROM tmptable x

JOIN newuser y ON (x.age = y.age);

#### **3.4MAP JOIN操作**

Join 操作在 Map 阶段完成，不再需要Reduce，前提条件是需要的数据在 Map 的过程中可以访问到。比如查询：

INSERT OVERWRITE TABLE pv_users

SELECT /*+ MAPJOIN(pv) */ pv.pageid, u.age

FROM page_view pv

JOIN user u ON (pv.userid = u.userid);

可以在 Map 阶段完成 Join.

相关的参数为：

- **hive.join.emit.interval = 1000**
- **hive.mapjoin.size.key = 10000**
- **hive.mapjoin.cache.numrows = 10000**

#### **3.5GROUP BY操作**

进行GROUP BY操作时需要注意一下几点：

- **Map端部分聚合**

事实上并不是所有的聚合操作都需要在reduce部分进行，很多聚合操作都可以先在Map端进行部分聚合，然后reduce端得出最终结果。

这里需要修改的参数为：

hive.map.aggr=true（用于设定是否在 map 端进行聚合，默认值为真） hive.groupby.mapaggr.checkinterval=100000（用于设定 map 端进行聚合操作的条目数）

- **有数据倾斜时进行负载均衡**

此处需要设定 hive.groupby.skewindata，当选项设定为 true 是，生成的查询计划有两 个 MapReduce 任务。在第一个 MapReduce 中，map 的输出结果集合会随机分布到 reduce 中， 每个 reduce 做部分聚合操作，并输出结果。这样处理的结果是，相同的 Group By Key 有可 能分发到不同的 reduce 中，从而达到负载均衡的目的；第二个 MapReduce 任务再根据预处 理的数据结果按照 Group By Key 分布到 reduce 中（这个过程可以保证相同的 Group By Key 分布到同一个 reduce 中），最后完成最终的聚合操作。

#### **3.6合并小文件**

我们知道文件数目小，容易在文件存储端造成瓶颈，给 HDFS 带来压力，影响处理效率。对此，可以通过合并Map和Reduce的结果文件来消除这样的影响。

用于设置合并属性的参数有：

- 是否合并Map输出文件：hive.merge.mapfiles=true（默认值为真）
- 是否合并Reduce 端输出文件：hive.merge.mapredfiles=false（默认值为假）
- 合并文件的大小：hive.merge.size.per.task=256*1000*1000（默认值为 256000000）
</code></pre>
<h4 id="4-程序角度优化"><a href="#4-程序角度优化" class="headerlink" title="4.程序角度优化"></a><strong>4.程序角度优化</strong></h4><pre><code class="python">#### 4.1熟练使用SQL提高查询**

熟练地使用 SQL，能写出高效率的查询语句。

场景：有一张 user 表，为卖家每天收到表，user_id，ds（日期）为 key，属性有主营类目，指标有交易金额，交易笔数。每天要取前10天的总收入，总笔数，和最近一天的主营类目。

 **解决方法 1**

如下所示：常用方法

INSERT OVERWRITE TABLE t1

SELECT user_id,substr(MAX(CONCAT(ds,cat),9) AS main_cat) FROM users

WHERE ds=20120329 // 20120329 为日期列的值，实际代码中可以用函数表示出当天日期 GROUP BY user_id;

INSERT OVERWRITE TABLE t2

SELECT user_id,sum(qty) AS qty,SUM(amt) AS amt FROM users

WHERE ds BETWEEN 20120301 AND 20120329

GROUP BY user_id

SELECT t1.user_id,t1.main_cat,t2.qty,t2.amt FROM t1

JOIN t2 ON t1.user_id=t2.user_id

下面给出方法1的思路，实现步骤如下：

第一步：利用分析函数，取每个 user_id 最近一天的主营类目，存入临时表 t1。

第二步：汇总 10 天的总交易金额，交易笔数，存入临时表 t2。

第三步：关联 t1，t2，得到最终的结果。

**解决方法 2**

如下所示：优化方法

SELECT user_id,substr(MAX(CONCAT(ds,cat)),9) AS main_cat,SUM(qty),SUM(amt) FROM users

WHERE ds BETWEEN 20120301 AND 20120329

GROUP BY user_id

在工作中我们总结出：方案 2 的开销等于方案 1 的第二步的开销，性能提升，由原有的 25 分钟完成，缩短为 10 分钟以内完成。节省了两个临时表的读写是一个关键原因，这种方式也适用于 Oracle 中的数据查找工作。

SQL 具有普适性，很多 SQL 通用的优化方案在 Hadoop 分布式计算方式中也可以达到效果。

#### **4.2无效ID在关联时的数据倾斜问题**

问题：日志中常会出现信息丢失，比如每日约为 20 亿的全网日志，其中的 user_id 为主 键，在日志收集过程中会丢失，出现主键为 null 的情况，如果取其中的 user_id 和 bmw_users 关联，就会碰到数据倾斜的问题。原因是 Hive 中，主键为 null 值的项会被当做相同的 Key 而分配进同一个计算 Map。

**解决方法 1：user_id 为空的不参与关联，子查询过滤 null**

SELECT * FROM log a

JOIN bmw_users b ON a.user_id IS NOT NULL AND a.user_id=b.user_id

UNION All SELECT * FROM log a WHERE a.user_id IS NULL

**解决方法 2 如下所示：函数过滤 null**

SELECT * FROM log a LEFT OUTER

JOIN bmw_users b ON

CASE WHEN a.user_id IS NULL THEN CONCAT(‘dp_hive’,RAND()) ELSE a.user_id END =b.user_id;

调优结果：原先由于数据倾斜导致运行时长超过 1 小时，解决方法 1 运行每日平均时长 25 分钟，解决方法 2 运行的每日平均时长在 20 分钟左右。优化效果很明显。

我们在工作中总结出：解决方法2比解决方法1效果更好，不但IO少了，而且作业数也少了。解决方法1中log读取两次，job 数为2。解决方法2中 job 数是1。这个优化适合无效 id（比如-99、 ‘’，null 等）产生的倾斜问题。把空值的 key 变成一个字符串加上随机数，就能把倾斜的 数据分到不同的Reduce上，从而解决数据倾斜问题。因为空值不参与关联，即使分到不同 的 Reduce 上，也不会影响最终的结果。附上 Hadoop 通用关联的实现方法是：关联通过二次排序实现的，关联的列为 partion key，关联的列和表的 tag 组成排序的 group key，根据 pariton key分配Reduce。同一Reduce内根据group key排序。

#### **4.3不同数据类型关联产生的倾斜问题**

问题：不同数据类型 id 的关联会产生数据倾斜问题。

一张表 s8 的日志，每个商品一条记录，要和商品表关联。但关联却碰到倾斜的问题。s8 的日志中有 32 为字符串商品 id，也有数值商品 id，日志中类型是 string 的，但商品中的 数值 id 是 bigint 的。猜想问题的原因是把 s8 的商品 id 转成数值 id 做 hash 来分配 Reduce， 所以字符串 id 的 s8 日志，都到一个 Reduce 上了，解决的方法验证了这个猜测。

**解决方法：把数据类型转换成字符串类型**

SELECT * FROM s8_log a LEFT OUTERJOIN r_auction_auctions b ON a.auction_id=CASE(b.auction_id AS STRING)

调优结果显示：数据表处理由 1 小时 30 分钟经代码调整后可以在 20 分钟内完成。

#### **4.4利用Hive对UNION ALL优化的特性**

多表 union all 会优化成一个 job。

问题：比如推广效果表要和商品表关联，效果表中的 auction_id 列既有 32 为字符串商 品 id，也有数字 id，和商品表关联得到商品的信息。

解决方法：Hive SQL 性能会比较好

SELECT * FROM effect a

JOIN

(SELECT auction_id AS auction_id FROM auctions

UNION All

SELECT auction_string_id AS auction_id FROM auctions) b

ON a.auction_id=b.auction_id

比分别过滤数字 id，字符串 id 然后分别和商品表关联性能要好。

这样写的好处：1 个 MapReduce 作业，商品表只读一次，推广效果表只读取一次。把 这个 SQL 换成 Map/Reduce 代码的话，Map 的时候，把 a 表的记录打上标签 a，商品表记录 每读取一条，打上标签 b，变成两个&lt;key,value&gt;对，&lt;(b,数字 id),value&gt;，&lt;(b,字符串 id),value&gt;。

所以商品表的 HDFS 读取只会是一次。

#### **4.5解决Hive对UNION ALL优化的短板**

Hive 对 union all 的优化的特性：对 union all 优化只局限于非嵌套查询。

-**消灭子查询内的 group by**

示例 1：子查询内有 group by

SELECT * FROM

(SELECT * FROM t1 GROUP BY c1,c2,c3 UNION ALL SELECT * FROM t2 GROUP BY c1,c2,c3)t3

GROUP BY c1,c2,c3

从业务逻辑上说，子查询内的 GROUP BY 怎么都看显得多余（功能上的多余，除非有 COUNT(DISTINCT)），如果不是因为 Hive Bug 或者性能上的考量（曾经出现如果不执行子查询 GROUP BY，数据得不到正确的结果的 Hive Bug）。所以这个 Hive 按经验转换成如下所示：

SELECT * FROM (SELECT * FROM t1 UNION ALL SELECT * FROM t2)t3 GROUP BY c1,c2,c3

调优结果：经过测试，并未出现 union all 的 Hive Bug，数据是一致的。MapReduce 的 作业数由 3 减少到 1。

t1 相当于一个目录，t2 相当于一个目录，对 Map/Reduce 程序来说，t1，t2 可以作为 Map/Reduce 作业的 mutli inputs。这可以通过一个 Map/Reduce 来解决这个问题。Hadoop 的 计算框架，不怕数据多，就怕作业数多。

但如果换成是其他计算平台如 Oracle，那就不一定了，因为把大的输入拆成两个输入， 分别排序汇总后 merge（假如两个子排序是并行的话），是有可能性能更优的（比如希尔排 序比冒泡排序的性能更优）。

- **消灭子查询内的 COUNT(DISTINCT)，MAX，MIN。**

SELECT * FROM

(SELECT * FROM t1

UNION ALL SELECT c1,c2,c3 COUNT(DISTINCT c4) FROM t2 GROUP BY c1,c2,c3) t3

GROUP BY c1,c2,c3;

由于子查询里头有 COUNT(DISTINCT)操作，直接去 GROUP BY 将达不到业务目标。这时采用 临时表消灭 COUNT(DISTINCT)作业不但能解决倾斜问题，还能有效减少 jobs。

INSERT t4 SELECT c1,c2,c3,c4 FROM t2 GROUP BY c1,c2,c3;

SELECT c1,c2,c3,SUM(income),SUM(uv) FROM

(SELECT c1,c2,c3,income,0 AS uv FROM t1

UNION ALL

SELECT c1,c2,c3,0 AS income,1 AS uv FROM t2) t3

GROUP BY c1,c2,c3;

job 数是 2，减少一半，而且两次 Map/Reduce 比 COUNT(DISTINCT)效率更高。

调优结果：千万级别的类目表，member 表，与 10 亿级得商品表关联。原先 1963s 的任务经过调整，1152s 即完成。

- **消灭子查询内的 JOIN**

SELECT * FROM

(SELECT * FROM t1 UNION ALL SELECT * FROM t4 UNION ALL SELECT * FROM t2 JOIN t3 ON t2.id=t3.id) x

GROUP BY c1,c2;

上面代码运行会有 5 个 jobs。加入先 JOIN 生存临时表的话 t5，然后 UNION ALL，会变成 2 个 jobs。

INSERT OVERWRITE TABLE t5

SELECT * FROM t2 JOIN t3 ON t2.id=t3.id;

SELECT * FROM (t1 UNION ALL t4 UNION ALL t5);

调优结果显示：针对千万级别的广告位表，由原先 5 个 Job 共 15 分钟，分解为 2 个 job 一个 8-10 分钟，一个3分钟。

**4.6GROUP BY替代COUNT(DISTINCT)达到优化效果**

计算 uv 的时候，经常会用到 COUNT(DISTINCT)，但在数据比较倾斜的时候 COUNT(DISTINCT) 会比较慢。这时可以尝试用 GROUP BY 改写代码计算 uv。

- **原有代码**

INSERT OVERWRITE TABLE s_dw_tanx_adzone_uv PARTITION (ds=20120329)SELECT 20120329 AS thedate,adzoneid,COUNT(DISTINCT acookie) AS uv FROM s_ods_log_tanx_pv t WHERE t.ds=20120329 GROUP BY adzoneid

关于COUNT(DISTINCT)的数据倾斜问题不能一概而论，要依情况而定，下面是我测试的一组数据：

测试数据：169857条

**\#统计每日IP**

CREATE TABLE ip_2014_12_29 AS SELECT COUNT(DISTINCT ip) AS IP FROM logdfs WHERE logdate=’2014_12_29′;

耗时：24.805 seconds

**\#统计每日IP（改造）**

CREATE TABLE ip_2014_12_29 AS SELECT COUNT(1) AS IP FROM (SELECT DISTINCT ip from logdfs WHERE logdate=’2014_12_29′) tmp;

耗时：46.833 seconds

测试结果表名：明显改造后的语句比之前耗时，这是因为改造后的语句有2个SELECT，多了一个job，这样在数据量小的时候，数据不会存在倾斜问题。**5.优化总结**
</code></pre>
<h4 id="5-优化总结"><a href="#5-优化总结" class="headerlink" title="5.优化总结"></a><strong>5.优化总结</strong></h4><pre><code class="python">优化时，把hive sql当做mapreduce程序来读，会有意想不到的惊喜。理解hadoop的核心能力，是hive优化的根本。这是这一年来，项目组所有成员宝贵的经验总结。

长期观察hadoop处理数据的过程，有几个显著的特征:

不怕数据多，就怕数据倾斜。

对jobs数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次汇总，产生十几个jobs，没半小时是跑不完的。map reduce作业初始化的时间是比较长的。

对sum，count来说，不存在数据倾斜问题。

对count(distinct ),效率较低，数据量一多，准出问题，如果是多count(distinct )效率更低。

优化可以从几个方面着手：

好的模型设计事半功倍。

解决数据倾斜问题。

减少job数。

设置合理的map reduce的task数，能有效提升性能。(比如，10w+级别的计算，用160个reduce，那是相当的浪费，1个足够)。

自己动手写sql解决数据倾斜问题是个不错的选择。set hive.groupby.skewindata=true;这是通用的算法优化，但算法优化总是漠视业务，习惯性提供通用的解决方法。Etl开发人员更了解业务，更了解数据，所以通过业务逻辑解决倾斜的方法往往更精确，更有效。

对count(distinct)采取漠视的方法，尤其数据大的时候很容易产生倾斜问题，不抱侥幸心理。自己动手，丰衣足食。

对小文件进行合并，是行至有效的提高调度效率的方法，假如我们的作业设置合理的文件数，对云梯的整体调度效率也会产生积极的影响。

优化时把握整体，单个作业最优不如整体最优。
</code></pre>
<h4 id="6-优化的常用手段"><a href="#6-优化的常用手段" class="headerlink" title="6.优化的常用手段"></a><strong>6.优化的常用手段</strong></h4><pre><code class="python">主要由三个属性来决定：

hive.exec.reducers.bytes.per.reducer   ＃这个参数控制一个job会有多少个reducer来处理，依据的是输入文件的总大小。默认1GB。

hive.exec.reducers.max    ＃这个参数控制最大的reducer的数量， 如果 input / bytes per reduce &gt; max  则会启动这个参数所指定的reduce个数。  这个并不会影响mapre.reduce.tasks参数的设置。默认的max是999。

mapred.reduce.tasks ＃这个参数如果指定了，hive就不会用它的estimation函数来自动计算reduce的个数，而是用这个参数来启动reducer。默认是-1。

6.1参数设置的影响

如果reduce太少：如果数据量很大，会导致这个reduce异常的慢，从而导致这个任务不能结束，也有可能会OOM 2、如果reduce太多：  产生的小文件太多，合并起来代价太高，namenode的内存占用也会增大。如果我们不指定mapred.reduce.tasks， hive会自动计算需要多少个reducer。
</code></pre>
<h4 id="7-常用函数"><a href="#7-常用函数" class="headerlink" title="7.常用函数"></a><strong>7.常用函数</strong></h4><pre><code class="python">## 1、LIKE比较: LIKE
语法: A LIKE B
操作类型: strings
描述: 如果字符串A或者字符串B为NULL，则返回NULL；如果字符串A符合表达式B 的正则语法，则为TRUE；否则为FALSE。B中字符”_”表示任意单个字符，而字符”%”表示任意数量的字符。
hive&gt; select 1 from iteblog where &#39;football&#39; like &#39;foot%&#39;;
1
hive&gt; select 1 from iteblog where &#39;football&#39; like &#39;foot____&#39;;
1
&lt;strong&gt;注意：否定比较时候用NOT A LIKE B&lt;/strong&gt;
hive&gt; select 1 from iteblog where NOT &#39;football&#39; like &#39;fff%&#39;;
1

## 2、REGEXP操作: REGEXP
语法: A REGEXP B
操作类型: strings
描述: 功能与RLIKE相同

hive&gt; select 1 from iteblog where &#39;footbar&#39; REGEXP &#39;^f.*r$&#39;;
1

## 3.INSTR 操作
语法：INSTR(str1,str2)
操作类型strings
描述： 返回匹配的字母的索引值。
hive&gt; SELECT INSTR(&#39;MySQL INSTR&#39;, &#39;SQL&#39;); 包含数据默认从1 开始
3
hive&gt; SELECT INSTR(&#39;MySQL INSTR&#39;, &#39;sql&#39;); 等于零证明里面没有数据
0
hive&gt; hive&gt; SELECT INSTR(&#39;MySQL INSTR&#39;, &#39;sql&#39;); 匹配到首位从1 开始
1

## 数据运算：
# 1、加法操作: +
语法: A + B
操作类型：所有数值类型
说明：返回A与B相加的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。比如，int + int 一般结果为int类型，而 int + double 一般结果为double类型

hive&gt; select 1 + 9 from iteblog;
10
hive&gt; create table iteblog as select 1 + 1.2 from iteblog;
hive&gt; describe iteblog;
_c0     double
# 2、减法操作: -
语法: A – B
操作类型：所有数值类型
说明：返回A与B相减的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。比如，int – int 一般结果为int类型，而 int – double 一般结果为double类型

hive&gt; select 10 – 5 from iteblog;
5
hive&gt; create table iteblog as select 5.6 – 4 from iteblog;
hive&gt; describe iteblog;
_c0     double
# 3、乘法操作: *
语法: A * B
操作类型：所有数值类型
说明：返回A与B相乘的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。注意，如果A乘以B的结果超过默认结果类型的数值范围，则需要通过cast将结果转换成范围更大的数值类型

hive&gt; select 40 * 5 from iteblog;
200
# 4、除法操作: /
语法: A / B
操作类型：所有数值类型
说明：返回A除以B的结果。结果的数值类型为double

hive&gt; select 40 / 5 from iteblog;
8.0
注意：hive中最高精度的数据类型是double,只精确到小数点后16位，在做除法运算的时候要特别注意

hive&gt;select ceil(28.0/6.999999999999999999999) from iteblog limit 1;   
结果为4
hive&gt;select ceil(28.0/6.99999999999999) from iteblog limit 1;          
结果为5
# 5、取余操作: %
语法: A % B
操作类型：所有数值类型
说明：返回A除以B的余数。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。

hive&gt; select 41 % 5 from iteblog;
1
hive&gt; select 8.4 % 4 from iteblog;
0.40000000000000036
&lt;strong&gt;注意&lt;/strong&gt;：精度在hive中是个很大的问题，类似这样的操作最好通过round指定精度
hive&gt; select round(8.4 % 4 , 2) from iteblog;
0.4
# 6、位与操作: &amp;
语法: A &amp; B
操作类型：所有数值类型
说明：返回A和B按位进行与操作的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。

hive&gt; select 4 &amp; 8 from iteblog;
0
hive&gt; select 6 &amp; 4 from iteblog;
4
# 7、位或操作: |
语法: A | B
操作类型：所有数值类型
说明：返回A和B按位进行或操作的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。

hive&gt; select 4 | 8 from iteblog;
12
hive&gt; select 6 | 8 from iteblog;
14
# 8、位异或操作: ^
语法: A ^ B
操作类型：所有数值类型
说明：返回A和B按位进行异或操作的结果。结果的数值类型等于A的类型和B的类型的最小父类型（详见数据类型的继承关系）。

hive&gt; select 4 ^ 8 from iteblog;
12
hive&gt; select 6 ^ 4 from iteblog;
2
# 9．位取反操作: ~
语法: ~A
操作类型：所有数值类型
说明：返回A按位取反操作的结果。结果的数值类型等于A的类型。

hive&gt; select ~6 from iteblog;
-7
hive&gt; select ~4 from iteblog;
-5
# 逻辑运算：
## 1、逻辑与操作: AND
语法: A AND B
操作类型：boolean
说明：如果A和B均为TRUE，则为TRUE；否则为FALSE。如果A为NULL或B为NULL，则为NULL

hive&gt; select 1 from iteblog where 1=1 and 2=2;
1
## 2、逻辑或操作: OR
语法: A OR B
操作类型：boolean
说明：如果A为TRUE，或者B为TRUE，或者A和B均为TRUE，则为TRUE；否则为FALSE

hive&gt; select 1 from iteblog where 1=2 or 2=2;
1
## 3、逻辑非操作: NOT
语法: NOT A
操作类型：boolean
说明：如果A为FALSE，或者A为NULL，则为TRUE；否则为FALSE

hive&gt; select 1 from iteblog where not 1=2;
1
## 数值计算：
## 1、取整函数: round
语法: round(double a)
返回值: BIGINT
说明: 返回double类型的整数值部分 （遵循四舍五入）

hive&gt; select round(3.1415926) from iteblog;
3
hive&gt; select round(3.5) from iteblog;
4
hive&gt; create table iteblog as select round(9542.158) from iteblog;
hive&gt; describe iteblog;
_c0     bigint
## 2、指定精度取整函数: round
语法: round(double a, int d)
返回值: DOUBLE
说明: 返回指定精度d的double类型

hive&gt; select round(3.1415926,4) from iteblog;
3.1416
## 3、向下取整函数: floor
语法: floor(double a)
返回值: BIGINT
说明: 返回等于或者小于该double变量的最大的整数

hive&gt; select floor(3.1415926) from iteblog;
3
hive&gt; select floor(25) from iteblog;
25
## 4、向上取整函数: ceil
语法: ceil(double a)
返回值: BIGINT
说明: 返回等于或者大于该double变量的最小的整数

hive&gt; select ceil(3.1415926) from iteblog;
4
hive&gt; select ceil(46) from iteblog;
46
## 5、向上取整函数: ceiling
语法: ceiling(double a)
返回值: BIGINT
说明: 与ceil功能相同

hive&gt; select ceiling(3.1415926) from iteblog;
4
hive&gt; select ceiling(46) from iteblog;
46
## 6、取随机数函数: rand
语法: rand(),rand(int seed)
返回值: double
说明: 返回一个0到1范围内的随机数。如果指定种子seed，则会等到一个稳定的随机数序列

hive&gt; select rand() from iteblog;
0.5577432776034763
hive&gt; select rand() from iteblog;
0.6638336467363424
hive&gt; select rand(100) from iteblog;
0.7220096548596434
hive&gt; select rand(100) from iteblog;
0.7220096548596434
## 7、自然指数函数: exp
语法: exp(double a)
返回值: double
说明: 返回自然对数e的a次方

hive&gt; select exp(2) from iteblog;
7.38905609893065
&lt;strong&gt;自然对数函数&lt;/strong&gt;: ln
&lt;strong&gt;语法&lt;/strong&gt;: ln(double a)
&lt;strong&gt;返回值&lt;/strong&gt;: double
&lt;strong&gt;说明&lt;/strong&gt;: 返回a的自然对数
1
hive&gt; select ln(7.38905609893065) from iteblog;
2.0
## 8、以10为底对数函数: log10
语法: log10(double a)
返回值: double
说明: 返回以10为底的a的对数

hive&gt; select log10(100) from iteblog;
2.0
## 9、以2为底对数函数: log2
语法: log2(double a)
返回值: double
说明: 返回以2为底的a的对数

hive&gt; select log2(8) from iteblog;
3.0
## 10、对数函数: log
语法: log(double base, double a)
返回值: double
说明: 返回以base为底的a的对数

hive&gt; select log(4,256) from iteblog;
4.0
## 11、幂运算函数: pow
语法: pow(double a, double p)
返回值: double
说明: 返回a的p次幂

hive&gt; select pow(2,4) from iteblog;
16.0
## 12、幂运算函数: power
语法: power(double a, double p)
返回值: double
说明: 返回a的p次幂,与pow功能相同

hive&gt; select power(2,4) from iteblog;
16.0
## 13、开平方函数: sqrt
语法: sqrt(double a)
返回值: double
说明: 返回a的平方根

hive&gt; select sqrt(16) from iteblog;
4.0
## 14、二进制函数: bin
语法: bin(BIGINT a)
返回值: string
说明: 返回a的二进制代码表示

hive&gt; select bin(7) from iteblog;
111
## 15、十六进制函数: hex
语法: hex(BIGINT a)
返回值: string
说明: 如果变量是int类型，那么返回a的十六进制表示；如果变量是string类型，则返回该字符串的十六进制表示

hive&gt; select hex(17) from iteblog;
11
hive&gt; select hex(‘abc’) from iteblog;
616263
## 16、反转十六进制函数: unhex
语法: unhex(string a)
返回值: string
说明: 返回该十六进制字符串所代码的字符串

hive&gt; select unhex(‘616263’) from iteblog;
abc
hive&gt; select unhex(‘11’) from iteblog;
-
hive&gt; select unhex(616263) from iteblog;
abc
## 17、进制转换函数: conv
语法: conv(BIGINT num, int from_base, int to_base)
返回值: string
说明: 将数值num从from_base进制转化到to_base进制

hive&gt; select conv(17,10,16) from iteblog;
11
hive&gt; select conv(17,10,2) from iteblog;
10001
## 18、绝对值函数: abs
语法: abs(double a) abs(int a)
返回值: double int
说明: 返回数值a的绝对值

hive&gt; select abs(-3.9) from iteblog;
3.9
hive&gt; select abs(10.9) from iteblog;
10.9
## 19、正取余函数: pmod
语法: pmod(int a, int b),pmod(double a, double b)
返回值: int double
说明: 返回正的a除以b的余数

hive&gt; select pmod(9,4) from iteblog;
1
hive&gt; select pmod(-9,4) from iteblog;
3
## 20、正弦函数: sin
语法: sin(double a)
返回值: double
说明: 返回a的正弦值

hive&gt; select sin(0.8) from iteblog;
0.7173560908995228
## 21、反正弦函数: asin
语法: asin(double a)
返回值: double
说明: 返回a的反正弦值

hive&gt; select asin(0.7173560908995228) from iteblog;
0.8
## 22、余弦函数: cos
语法: cos(double a)
返回值: double
说明: 返回a的余弦值

hive&gt; select cos(0.9) from iteblog;
0.6216099682706644
## 23、反余弦函数: acos
语法: acos(double a)
返回值: double
说明: 返回a的反余弦值

hive&gt; select acos(0.6216099682706644) from iteblog;
0.9
## 24、positive函数: positive
语法: positive(int a), positive(double a)
返回值: int double
说明: 返回a

hive&gt; select positive(-10) from iteblog;
-10
hive&gt; select positive(12) from iteblog;
12
## 25、negative函数: negative
语法: negative(int a), negative(double a)
返回值: int double
说明: 返回-a

hive&gt; select negative(-5) from iteblog;
5
hive&gt; select negative(8) from iteblog;
-8
## 日期函数：
## 1、UNIX时间戳转日期函数: from_unixtime
语法: from_unixtime(bigint unixtime[, string format])
返回值: string
说明: 转化UNIX时间戳（从1970-01-01 00:00:00 UTC到指定时间的秒数）到当前时区的时间格式

hive&gt; select from_unixtime(1323308943,&#39;yyyyMMdd&#39;) from iteblog;
20111208
## 2、获取当前UNIX时间戳函数: unix_timestamp
语法: unix_timestamp()
返回值: bigint
说明: 获得当前时区的UNIX时间戳

hive&gt; select unix_timestamp() from iteblog;
1323309615
## 3、日期转UNIX时间戳函数: unix_timestamp
语法: unix_timestamp(string date)
返回值: bigint
说明: 转换格式为&quot;yyyy-MM-dd HH:mm:ss&quot;的日期到UNIX时间戳。如果转化失败，则返回0。

hive&gt; select unix_timestamp(&#39;2011-12-07 13:01:03&#39;) from iteblog;
1323234063
## 4、指定格式日期转UNIX时间戳函数: unix_timestamp
语法: unix_timestamp(string date, string pattern)
返回值: bigint
说明: 转换pattern格式的日期到UNIX时间戳。如果转化失败，则返回0。

hive&gt; select unix_timestamp(&#39;20111207 13:01:03&#39;,&#39;yyyyMMdd HH:mm:ss&#39;) from iteblog;
1323234063
## 5、日期时间转日期函数: to_date
语法: to_date(string timestamp)
返回值: string
说明: 返回日期时间字段中的日期部分。

hive&gt; select to_date(&#39;2011-12-08 10:03:01&#39;) from iteblog;
2011-12-08
## 6、日期转年函数: year
语法: year(string date)
返回值: int
说明: 返回日期中的年。

hive&gt; select year(&#39;2011-12-08 10:03:01&#39;) from iteblog;
2011
hive&gt; select year(&#39;2012-12-08&#39;) from iteblog;
2012
## 7、日期转月函数: month
语法: month (string date)
返回值: int
说明: 返回日期中的月份。

hive&gt; select month(&#39;2011-12-08 10:03:01&#39;) from iteblog;
12
hive&gt; select month(&#39;2011-08-08&#39;) from iteblog;
8
## 8、日期转天函数: day
语法: day (string date)
返回值: int
说明: 返回日期中的天。

hive&gt; select day(&#39;2011-12-08 10:03:01&#39;) from iteblog;
8
hive&gt; select day(&#39;2011-12-24&#39;) from iteblog;
24
## 9、日期转小时函数: hour
语法: hour (string date)
返回值: int
说明: 返回日期中的小时。

hive&gt; select hour(&#39;2011-12-08 10:03:01&#39;) from iteblog;
10
## 10、日期转分钟函数: minute
语法: minute (string date)
返回值: int
说明: 返回日期中的分钟。

hive&gt; select minute(&#39;2011-12-08 10:03:01&#39;) from iteblog;
3
## 11、日期转秒函数: second
语法: second (string date)
返回值: int
说明: 返回日期中的秒。

hive&gt; select second(&#39;2011-12-08 10:03:01&#39;) from iteblog;
1
## 12、日期转周函数: weekofyear
语法: weekofyear (string date)
返回值: int
说明: 返回日期在当前的周数。

hive&gt; select weekofyear(&#39;2011-12-08 10:03:01&#39;) from iteblog;
49
## 13、日期比较函数: datediff
语法: datediff(string enddate, string startdate)
返回值: int
说明: 返回结束日期减去开始日期的天数。

hive&gt; select datediff(&#39;2012-12-08&#39;,&#39;2012-05-09&#39;) from iteblog;
213
## 14、日期增加函数: date_add
语法: date_add(string startdate, int days)
返回值: string
说明: 返回开始日期startdate增加days天后的日期。

hive&gt; select date_add(&#39;2012-12-08&#39;,10) from iteblog;
2012-12-18
## 15、日期减少函数: date_sub
语法: date_sub (string startdate, int days)
返回值: string
说明: 返回开始日期startdate减少days天后的日期。

hive&gt; select date_sub(&#39;2012-12-08&#39;,10) from iteblog;
2012-11-28
## 条件函数
# 1、If函数: if
语法: if(boolean testCondition, T valueTrue, T valueFalseOrNull)
返回值: T
说明: 当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull

hive&gt; select if(1=2,100,200) from iteblog;
200
hive&gt; select if(1=1,100,200) from iteblog;
100
# 2、非空查找函数: COALESCE
语法: COALESCE(T v1, T v2, …)
返回值: T
说明: 返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL

hive&gt; select COALESCE(null,&#39;100&#39;,&#39;50′) from iteblog;
100
## 3. 非空查找函数： NVL
NVL
nvl(COMMISSION_PCT,0)
如果第一个参数为null，则返回第二个参数
如果第一个参数为非null，则返回第一个参数
 -- nvl 和 coalesce的区别
    COALESCE(EXPR1,EXPR2,EXPR3...EXPRn)
从左往右数，遇到第一个非null值，则返回该非null值。
多层判断

第一点区别：从上面可以知道，nvl只适合于两个参数的，COALESCE适合于多个参数。

第二点区别：COALESCE里的所有参数类型必须保持一致，nvl可以不一致。
## 4.1、条件判断函数：CASE
语法: CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END
返回值: T
说明：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回f
hive&gt; Select case 100 when 50 then &#39;tom&#39; when 100 then &#39;mary&#39; else &#39;tim&#39; end from iteblog;
mary
hive&gt; Select case 200 when 50 then &#39;tom&#39; when 100 then &#39;mary&#39; else &#39;tim&#39; end from iteblog;
tim
## 4.2、条件判断函数：CASE
语法: CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END
返回值: T
说明：如果a为TRUE,则返回b；如果c为TRUE，则返回d；否则返回e

hive&gt; select case when 1=2 then &#39;tom&#39; when 2=2 then &#39;mary&#39; else &#39;tim&#39; end from iteblog;
mary
hive&gt; select case when 1=1 then &#39;tom&#39; when 2=2 then &#39;mary&#39; else &#39;tim&#39; end from iteblog;
tom
## 字符串函数:
#1、字符串长度函数：length
语法: length(string A)
返回值: int
说明：返回字符串A的长度

hive&gt; select length(&#39;abcedfg&#39;) from iteblog;
7
#2、字符串反转函数：reverse
语法: reverse(string A)
返回值: string
说明：返回字符串A的反转结果

hive&gt; select reverse(abcedfg’) from iteblog;
gfdecba
#3、字符串连接函数：concat
语法: concat(string A, string B…)
返回值: string
说明：返回输入字符串连接后的结果，支持任意个输入字符串
#注意： select concat(&#39;abc&#39;,NULL,&#39;asd&#39;); 最后返回的是null,如果三个字符串中有一个是null 的 则最后返回null值。
hive&gt; select concat(‘abc’,&#39;def’,&#39;gh’) from iteblog;
abcdefgh
#4、带分隔符字符串连接函数：concat_ws
#注意里面传递的参数如果有一个是nu l l值 则最后返回的是nu l l
语法: concat_ws(string SEP, string A, string B…),concat_ws(string SEP,Array(str1,str2))
返回值: string
说明：返回输入字符串连接后的结果，SEP表示各个字符串间的分隔符

hive&gt; select concat_ws(&#39;,&#39;,&#39;abc&#39;,&#39;def&#39;,&#39;gh&#39;) from iteblog;
abc,def,gh
#5、字符串截取函数：substr,substring
语法: substr(string A, int start),substring(string A, int start)
返回值: string
说明：返回字符串A从start位置到结尾的字符串

hive&gt; select substr(&#39;abcde&#39;,3) from iteblog;
cde
hive&gt; select substring(&#39;abcde&#39;,3) from iteblog;
cde
hive&gt;  select substr(&#39;abcde&#39;,-1) from iteblog;  （和ORACLE相同）
e
#6、字符串截取函数：substr,substring
#包头不包未，索引从1开始
语法: substr(string A, int start, int len),substring(string A, int start, int len)
返回值: string
说明：返回字符串A从start位置开始，长度为len的字符串

hive&gt; select substr(&#39;abcde&#39;,3,2) from iteblog;
cd
hive&gt; select substring(&#39;abcde&#39;,3,2) from iteblog;
cd
hive&gt;select substring(&#39;abcde&#39;,-2,2) from iteblog;
de
#7、字符串转大写函数：upper,ucase
语法: upper(string A) ucase(string A)
返回值: string
说明：返回字符串A的大写格式

hive&gt; select upper(&#39;abSEd&#39;) from iteblog;
ABSED
hive&gt; select ucase(&#39;abSEd&#39;) from iteblog;
ABSED
#8、字符串转小写函数：lower,lcase
语法: lower(string A) lcase(string A)
返回值: string
说明：返回字符串A的小写格式

hive&gt; select lower(&#39;abSEd&#39;) from iteblog;
absed
hive&gt; select lcase(&#39;abSEd&#39;) from iteblog;
absed
#9、去空格函数：trim
语法: trim(string A)
返回值: string
说明：去除字符串两边的空格

hive&gt; select trim(&#39; abc &#39;) from iteblog;
abc
#10、左边去空格函数：ltrim
语法: ltrim(string A)
返回值: string
说明：去除字符串左边的空格

hive&gt; select ltrim(&#39; abc &#39;) from iteblog;
abc
#11、右边去空格函数：rtrim
语法: rtrim(string A)
返回值: string
说明：去除字符串右边的空格

hive&gt; select rtrim(&#39; abc &#39;) from iteblog;
abc
#12、正则表达式替换函数：regexp_replace
语法: regexp_replace(string A, string B, string C)
返回值: string
说明：将字符串A中的符合java正则表达式B的部分替换为C。注意，在有些情况下要使用转义字符,类似oracle中的regexp_replace函数。

hive&gt; select regexp_replace(&#39;foobar&#39;, &#39;oo|ar&#39;, &#39;&#39;) from iteblog;
fb
#13、正则表达式解析函数：regexp_extract
语法: regexp_extract(string subject, string pattern, int index)
返回值: string
说明：将字符串subject按照pattern正则表达式的规则拆分，返回index指定的字符。

hive&gt; select regexp_extract(&#39;foothebar&#39;, &#39;foo(.*?)(bar)&#39;, 1) from iteblog;
the
hive&gt; select regexp_extract(&#39;foothebar&#39;, &#39;foo(.*?)(bar)&#39;, 2) from iteblog;
bar
hive&gt; select regexp_extract(&#39;foothebar&#39;, &#39;foo(.*?)(bar)&#39;, 0) from iteblog;
foothebar
strong&gt;注意，在有些情况下要使用转义字符，下面的等号要用双竖线转义，这是java正则表达式的规则。
select data_field,
  regexp_extract(data_field,&#39;.*?bgStart\\=([^&amp;]+)&#39;,1) as aaa,
  regexp_extract(data_field,&#39;.*?contentLoaded_headStart\\=([^&amp;]+)&#39;,1) as bbb,
  regexp_extract(data_field,&#39;.*?AppLoad2Req\\=([^&amp;]+)&#39;,1) as ccc
  from pt_nginx_loginlog_st
  where pt = &#39;2012-03-26&#39; limit 2;
#14.1、URL解析函数：parse_url
语法: parse_url(string urlString, string partToExtract [, string keyToExtract])
返回值: string
说明：返回URL中指定的部分。partToExtract的有效值为：HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO.

hive&gt; select parse_url(&#39;https://www.iteblog.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;, &#39;HOST&#39;) from iteblog;
facebook.com
hive&gt; select parse_url(&#39;https://www.iteblog.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;, &#39;QUERY&#39;, &#39;k1&#39;) from iteblog;
v1
#14.2、URL解析函数: parse_url_tuple
语法: parse_url(string urlString, string partToExtract [, string keyToExtract])，parse_url_tuple功能类似parse_url()，但它可以同时提取多个部分并返回
返回值: string
　　　　说明：返回URL中指定的部分。partToExtract的有效值为：HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO.  
                      
hive&gt; select parse_url_tuple(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1’, ‘QUERY:k1’, ‘QUERY:k2’);
v1 v2     
                             
#15.1、json解析函数：get_json_object
语法: get_json_object(string json_string, string path)
返回值: string
说明：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL。

hive&gt; select  get_json_object(&#39;&#123;&quot;store&quot;:
&gt;   &#123;&quot;fruit&quot;:\[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;,&#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],
&gt;    &quot;bicycle&quot;:&#123;&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;&#125;
&gt;   &#125;,
&gt;  &quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;,
&gt;  &quot;owner&quot;:&quot;amy&quot;
&gt; &#125;
&gt; &#39;,&#39;$.owner&#39;) from iteblog;
amy
#15.2、json解析函数：json_tuple
json_tuple 相对于 get_json_object 的优势就是一次可以解析多个 Json 字段。但是如果我们有个 Json 数组，这两个函数都无法处理.
json_tuple(jsonStr, k1, k2, ...)
参数为一组键k1，k2……和JSON字符串，返回值的元组.
set hivevar:msg=&#123;
&quot;message&quot;:&quot;2015/12/08 09:14:4&quot;, 
&quot;client&quot;: &quot;10.108.24.253&quot;, 
&quot;server&quot;: &quot;passport.suning.com&quot;, 
&quot;request&quot;: &quot;POST /ids/needVerifyCode HTTP/1.1&quot;,
&quot;server&quot;: &quot;passport.sing.co&quot;,
&quot;version&quot;:&quot;1&quot;,
&quot;timestamp&quot;:&quot;2015-12-08T01:14:43.273Z&quot;,
&quot;type&quot;:&quot;B2C&quot;,&quot;center&quot;:&quot;JSZC&quot;,
&quot;system&quot;:&quot;WAF&quot;,&quot;clientip&quot;:&quot;192.168.61.4&quot;,
&quot;host&quot;:&quot;wafprdweb03&quot;,
&quot;path&quot;:&quot;/usr/local/logs/waf.error.log&quot;,
&quot;redis&quot;:&quot;192.168.24.46&quot;&#125;                              
hive&gt; select a.* from test lateral view json_tuple(‘$&#123;hivevar:msg&#125;’,’server’,’host’) a as f1,f2;     passport.sing.com wafprdweb03                         
                                                          
#16、空格字符串函数：space
语法: space(int n)
返回值: string
说明：返回长度为n的字符串

hive&gt; select space(10) from iteblog;
hive&gt; select length(space(10)) from iteblog;
10
#17、重复字符串函数：repeat
语法: repeat(string str, int n)
返回值: string
说明：返回重复n次后的str字符串

hive&gt; select repeat(&#39;abc&#39;,5) from iteblog;
abcabcabcabcabc
#18、首字符ascii函数：ascii
语法: ascii(string str)
返回值: int
说明：返回字符串str第一个字符的ascii码

hive&gt; select ascii(&#39;abcde&#39;) from iteblog;
97
#19、左补足函数：lpad
语法: lpad(string str, int len, string pad)
返回值: string
说明：将str进行用pad进行左补足到len位

hive&gt; select lpad(&#39;abc&#39;,10,&#39;td&#39;) from iteblog;
tdtdtdtabc
注意：与GP，ORACLE不同，pad 不能默认
#20、右补足函数：rpad
语法: rpad(string str, int len, string pad)
返回值: string
说明：将str进行用pad进行右补足到len位

hive&gt; select rpad(&#39;abc&#39;,10,&#39;td&#39;) from iteblog;
abctdtdtdt
#21、分割字符串函数: split
语法: split(string str, string pat)
返回值: array
说明: 按照pat字符串分割str，会返回分割后的字符串数组

hive&gt; select split(&#39;abtcdtef&#39;,&#39;t&#39;) from iteblog;
[&quot;ab&quot;,&quot;cd&quot;,&quot;ef&quot;]
#22、集合查找函数: find_in_set
语法: find_in_set(string str, string strList)
返回值: int
说明: 返回str在strlist第一次出现的位置，strlist是用逗号分割的字符串。如果没有找该str字符，则返回0

hive&gt; select find_in_set(&#39;ab&#39;,&#39;ef,ab,de&#39;) from iteblog;
2
hive&gt; select find_in_set(&#39;at&#39;,&#39;ef,ab,de&#39;) from iteblog;
0
                              
#23、greatest函数
greatest(col_a, col_b, ..., col_n)比较n个column的大小，过滤掉null，但是当某个column中是string，而其他是int/double/float等时，返回null

select greatest(-1, 0, 5, 8) 
返回：8

select greatest(-1, 0, 5, 8, null) 
返回：NULL

select greatest(-1, 0, 5, 8, &quot;dfsf&quot;) 
返回：NULL

select greatest(&quot;2020-02-26&quot;,&quot;2020-02-23&quot;,&quot;2020-02-22&quot;) 
返回：
2020-02-26  

#24、least函数
select least(-1, 0, 5, 8)
结果：
-1

select least(-1, 0, 5, 8, null)
结果：
NULL

select least(-1, 0, 5, 8, &quot;dfsf&quot;)
结果：NULL

select least(&quot;2020-02-26&quot;,&quot;2020-02-23&quot;,&quot;2020-02-22&quot;)
结果：
2020-02-22                             
                              
##复合类型构建操作
#1、Map类型构建: map
语法: map (key1, value1, key2, value2, …)
说明：根据输入的key和value对构建map类型

hive&gt; Create table iteblog as select map(&#39;100&#39;,&#39;tom&#39;,&#39;200&#39;,&#39;mary&#39;) as t from iteblog;
hive&gt; describe iteblog;
t       map&lt;string ,string&gt;
hive&gt; select t from iteblog;
&#123;&quot;100&quot;:&quot;tom&quot;,&quot;200&quot;:&quot;mary&quot;&#125;
#2、Struct类型构建: struct
语法: struct(val1, val2, val3, …)
说明：根据输入的参数构建结构体struct类型

hive&gt; create table iteblog as select struct(&#39;tom&#39;,&#39;mary&#39;,&#39;tim&#39;) as t from iteblog;
hive&gt; describe iteblog;
t       struct&lt;col1:string ,col2:string,col3:string&gt;
hive&gt; select t from iteblog;
&#123;&quot;col1&quot;:&quot;tom&quot;,&quot;col2&quot;:&quot;mary&quot;,&quot;col3&quot;:&quot;tim&quot;&#125;
#3、array类型构建: array
语法: array(val1, val2, …)
说明：根据输入的参数构建数组array类型

hive&gt; create table iteblog as select array(&quot;tom&quot;,&quot;mary&quot;,&quot;tim&quot;) as t from iteblog;
hive&gt; describe iteblog;
t       array&lt;string&gt;
hive&gt; select t from iteblog;
[&quot;tom&quot;,&quot;mary&quot;,&quot;tim&quot;]
##4. 窗口函数：
窗口函数
row_number()over(partition by order by )：从1开始按照partition by 字段进行排序
rank()over(partition by order by )：从1开始按照partition by 字段排序，遇到并列取相同数值，后面的跳跃计数
dense_rank()over(partition by order by )：从1开始按照partition by 字段排序，遇到并列取相同数值，后面的顺序计数
sum()over(partition by order by 
#rows between unbounded preceding and current row)：根据某列进行汇总，常用在算累计销售额或者累计订单量，#之后的rows between unbounded preceding and current row）代表累计增加之前的行，可加可不加
lead(col,n,default)over(partition by order by )：col字段向上n行，default为向上为NULL时的默认值
lag(col,n,default)over(partition by order by)：col字段向下n行，default为向下为NULL时的默认值
first_value(col)over(partition by order by)：分组排序后，组内的第一个col的值
last_value(col)over(partition by order by)：分组排序后，组内的最后一个col的值
排序：
1.order by：全局排序，缺点是会把分组的数据放到一个Reduce中
2.sort by：局部排序，把每个Reduce中的数据进行排序，Reduce数量在2个以上有用，缺点是输出排序结果会有重叠
3.distribute by：提前排序，在Map端先进行排序，再结合sort by，把排好序的数据放到同一个Reduce中再进行局部排序，如同group by，放在sort by之前，例如：select * from A group by col distribute by time sort by id
4.cluster by：如果distribute by和sort by 的字段相同，就可以用cluster by代替他们的组合用法，缺点是只能升序，不能asc和desc，而且只能保证一个分区内的排序，例如：select * from A cluster by id (同select * from A 5.distribute by id sort by id)
列转行：
1.concat_ws(&#39;,&#39;,collect_set(col))：collect_list 不去重，collect_set 去重。col的数据类型要求是string
union all/union
行转列：
2.lateral view explode(split(col,&#39;,&#39;)) 虚拟表别名 as 列别名：用法很特殊，放在from tablename和where中间，原理是lateral view(侧视图)与UDTF函数将一行拆分为多行，创建一个虚拟表，再与原表进行笛卡尔积关联。explode()是专门处理map/array结构数据的，解析json数据要配合get_json_object()
case when...end
hive去重方式：
group by
distinct
row_number()
时间区段的提取：Extract

-- field可以是day、hour、minute, month, quarter等等
-- source可以是date、timestamp类型
extract(field FROM source)
SELECT extract(year FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 2020
SELECT extract(quarter FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 3
SELECT extract(month FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 8
SELECT extract(week FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 31,一年中的第几周
SELECT extract(day FROM &#39;2020-08-05 09:30:08&#39;);  -- 结果为 5
SELECT extract(hour FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 9
SELECT extract(minute FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 30
SELECT extract(second FROM &#39;2020-08-05 09:30:08&#39;);   -- 结果为 8

next_day(STRING start_date, STRING day_of_week)
-- 返回当前日期对应的下一个周几对应的日期
-- 2020-08-05为周三
SELECT next_day(&#39;2020-08-05&#39;,&#39;MO&#39;) -- 下一个周一对应的日期：2020-08-10
SELECT next_day(&#39;2020-08-05&#39;,&#39;TU&#39;) -- 下一个周二对应的日期：2020-08-11
SELECT next_day(&#39;2020-08-05&#39;,&#39;WE&#39;) -- 下一个周三对应的日期：2020-08-12
SELECT next_day(&#39;2020-08-05&#39;,&#39;TH&#39;) -- 下一个周四对应的日期：2020-08-06，即为本周四
SELECT next_day(&#39;2020-08-05&#39;,&#39;FR&#39;) -- 下一个周五对应的日期：2020-08-07，即为本周五
SELECT next_day(&#39;2020-08-05&#39;,&#39;SA&#39;) -- 下一个周六对应的日期：2020-08-08，即为本周六
SELECT next_day(&#39;2020-08-05&#39;,&#39;SU&#39;) -- 下一个周日对应的日期：2020-08-09，即为本周日
-- 星期一到星期日的英文（Monday，Tuesday、Wednesday、Thursday、Friday、Saturday、Sunday）
使用
那么该如何获取当前日期所在周的周一对应的日期呢？只需要先获取当前日期的下周一对应的日期，然后减去7天，即可获得：
SELECT date_add(next_day(&#39;2020-08-05&#39;,&#39;MO&#39;),-7);
同理，获取当前日期所在周的周日对应的日期，只需要先获取当前日期的下周一对应的日期，然后减去1天，即可获得：
select date_add(next_day(&#39;2020-08-05&#39;,&#39;MO&#39;),-1) 
-- 2020-08-09
月的提取

语法
至于怎么将月份从单一日期提取出来呢，LAST_DAY这个函数可以将每个月中的日期变成该月的最后一天(28号，29号，30号或31号)，如下：
last_day(STRING date)

*使用
SELECT last_day(&#39;2020-08-05&#39;); -- 2020-08-31

除了上面的方式，也可以使用date_format函数，比如：
SELECT date_format(&#39;2020-08-05&#39;,&#39;yyyy-MM&#39;);
-- 2020-08
日期的范围
上面可供提取的字段，不同的数据库存在些许的差异。以Hive为例，支持day, dayofweek, hour, minute, month, quarter, second, week 和 year。其中周、月、年使用最为广泛，因为无论是公司内部产品，还是商用的产品所提供的数据后台统计，周报和月报(比如近7天、近30天)最注重表现的周期。
SELECT date_add(next_day(&#39;2020-08-05&#39;,&#39;MO&#39;),-7);

月的Window：使用add_months加上trunc()的应用
-- 2020-07-05
select add_months(&#39;2020-08-05&#39;, -1)

-- 返回当前日期的月初日期
-- 2020-08-01
select trunc(&quot;2020-08-05&quot;,&#39;MM&#39;)

由上面范例可见，单纯使用add_months，减N个月的用法，可以刚好取到整数月的数据，但如果加上trunc()函数，则会从前N个月的一号开始取值。
-- 选取2020-07-05到2020-08-05所有数据
BETWEEN add_months(&#39;2020-08-05&#39;, -1) AND &#39;2020-08-05&#39; 
-- 选取2020-07-01到2020-08-05之间所有数据
BETWEEN add_months(trunc(&quot;2020-08-05&quot;,&#39;MM&#39;),-1) AND &#39;2020-08-05&#39; 
第二：临时表与Common Table Expression (WITH)
这两种方法是日常工作中经常被使用到，对于一些比较复杂的计算任务，为了避免过多的JOIN，通常会先把一些需要提取的部分数据使用临时表或是CTE的形式在主要查询区块前进行提取。

第二：临时表与Common Table Expression (WITH)
临时表的作法：
CREATE TEMPORARY TABLE table_1 AS  
    SELECT 
        columns
    FROM table A;
CREATE TEMPORARY table_2 AS 
    SELECT
        columns
    FROM table B;

SELECT
    table_1.columns,
    table_2.columns, 
    c.columns 
FROM table C JOIN table_1
     JOIN table_2;
CTE的作法：
-- 注意Hive、Impala支持这种语法，低版本的MySQL不支持(高版本支持)
WITH employee_by_title_count AS (
    SELECT
        t.name as job_title
        , COUNT(e.id) as amount_of_employees
    FROM employees e
        JOIN job_titles t on e.job_title_id = t.id
    GROUP BY 1
),
salaries_by_title AS (
     SELECT
         name as job_title
         , salary
     FROM job_titles
)
SELECT *
FROM employee_by_title_count e
    JOIN salaries_by_title s ON s.job_title = e.job_title
  
</code></pre>
<h4 id="8-Aggregation-与CASE-WHEN的结合使用"><a href="#8-Aggregation-与CASE-WHEN的结合使用" class="headerlink" title="8.Aggregation 与CASE WHEN的结合使用**"></a><strong>8.Aggregation 与CASE WHEN的结合使用</strong>**</h4><pre><code class="sql">将Aggregation function (SUM/COUNT/COUNT DISTINCT/MIN/MAX) 结合CASE WHEN是最强大且最有趣的使用方式。这样的使用创造出一种类似EXCEL中SUMIF/COUNTIF的效果，可以用这个方式做出很多高效的分析。
数据准备
CREATE TABLE order(
    register_date string,
    order_date string,
    user_id string,
    country string,
    order_sales decimal(10,2),
    order_id string);

INSERT INTO TABLE order VALUES(&quot;2020-06-07&quot;,&quot;2020-06-09&quot;,&quot;001&quot;,&#39;c0&#39;,210,&quot;o1&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-08&quot;,&quot;2020-06-09&quot;,&quot;002&quot;,&#39;c1&#39;,220,&quot;o2&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-07&quot;,&quot;2020-06-10&quot;,&quot;003&quot;,&#39;c2&#39;,230,&quot;o3&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-09&quot;,&quot;2020-06-10&quot;,&quot;004&quot;,&#39;c3&#39;,200,&quot;o4&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-07&quot;,&quot;2020-06-20&quot;,&quot;005&quot;,&#39;c4&#39;,300,&quot;o5&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-10&quot;,&quot;2020-06-23&quot;,&quot;006&quot;,&#39;c5&#39;,400,&quot;o6&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-07&quot;,&quot;2020-06-19&quot;,&quot;007&quot;,&#39;c6&#39;,600,&quot;o7&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-12&quot;,&quot;2020-06-18&quot;,&quot;008&quot;,&#39;c7&#39;,700,&quot;o8&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-07&quot;,&quot;2020-06-09&quot;,&quot;009&quot;,&#39;c8&#39;,100,&quot;o9&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-15&quot;,&quot;2020-06-18&quot;,&quot;0010&quot;,&#39;c9&#39;,200,&quot;o10&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-15&quot;,&quot;2020-06-19&quot;,&quot;0011&quot;,&#39;c10&#39;,250,&quot;o11&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-12&quot;,&quot;2020-06-29&quot;,&quot;0012&quot;,&#39;c11&#39;,270,&quot;o12&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-16&quot;,&quot;2020-06-19&quot;,&quot;0013&quot;,&#39;c12&#39;,230,&quot;o13&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-17&quot;,&quot;2020-06-20&quot;,&quot;0014&quot;,&#39;c13&#39;,290,&quot;o14&quot;);
INSERT INTO TABLE order VALUES(&quot;2020-06-20&quot;,&quot;2020-06-29&quot;,&quot;0015&quot;,&#39;c14&#39;,203,&quot;o15&quot;);
</code></pre>
<p>CASE WHEN 时间，进行留存率&#x2F;使用率的分析</p>
<pre><code class="sql">-- 允许多列去重
set hive.groupby.skewindata = false
-- 允许使用位置编号分组或排序
set hive.groupby.orderby.position.alias = true

SELECT
    date_add(Next_day(register_date, &#39;MO&#39;),-1) AS week_end,
    COUNT(DISTINCT CASE WHEN order_date BETWEEN register_date AND date_add(register_date,6) THEN user_id END) AS first_week_order,
    COUNT(DISTINCT CASE WHEN order_date BETWEEN date_add(register_date ,7) AND date_add(register_date,13) THEN user_id END) AS sencod_week_order,
    COUNT(DISTINCT CASE WHEN order_date BETWEEN date_add(register_date ,14) AND date_add(register_date,20) THEN user_id END) as third_week_order
FROM order
GROUP BY 1
上面的示例可以得知到用户在注册之后，有没有创建订单的行为。比如注册后的第一周，第二周，第三周分别有多少下单用户，这样可以分析出用户的使用情况和留存情况。
CASE WHEN 时间，进行每个用户消费金额的分析
SELECT
    user_id,
    SUM (CASE WHEN order_date BETWEEN register_date AND date_add(register_date,6) THEN order_sales END) AS first_week_amount,
    SUM (CASE WHEN order_date BETWEEN date_add(register_date ,7) AND date_add(register_date,13) THEN order_sales END) AS second_week_amount
    FROM order
GROUP BY 1
通过筛选出注册与消费的日期，并且进行消费金额统计，每个用户在每段时间段(注册后第一周、第二周…以此类推)的消费金额，可以观察用户是否有持续维持消费习惯或是消费金额变低等分析。
CASE WHEN数量，消费金额超过某一定额的数量分析
SELECT
    user_id,
    COUNT(DISTINCT CASE WHEN order_sales &gt;= 100 THEN order_id END) AS count_of_order_greateer_than_100
FROM order
GROUP BY 1
上面的示例就是类似countif的用法，针对每个用户，统计其订单金额大于某个值的订单数量，分析去筛选出高价值的顾客。
CASE WHEN数量，加上时间的用法
SELECT
    user_id,
    MIN(CASE WHEN order_sales &gt; 100 THEN order_date END) AS first_order_date_over1000,
    MAX(CASE WHEN order_sales &gt; 100 THEN order_date END) AS recent_order_date_over100
FROM order
GROUP BY 1
CASE WHEN加上MIN/MAX时间，可以得出该用户在其整个使用过程中，首次购买超过一定金额的订单日期，以及最近一次购买超过一定金额的订单日期。

用户访问session分析
数据准备
CREATE TABLE user_visit_action( 
    user_id string,
    session_id string,
    page_url string,
    action_time string);
    
INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss001&quot;,&quot;http://a.com&quot;,&quot;2020-08-06 13:34:11.478&quot;);
INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss001&quot;,&quot;http://b.com&quot;,&quot;2020-08-06 13:35:11.478&quot;);
INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss001&quot;,&quot;http://c.com&quot;,&quot;2020-08-06 13:36:11.478&quot;);

INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss002&quot;,&quot;http://a.com&quot;,&quot;2020-08-06 14:30:11.478&quot;);
INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss002&quot;,&quot;http://b.com&quot;,&quot;2020-08-06 14:31:11.478&quot;);
INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss002&quot;,&quot;http://e.com&quot;,&quot;2020-08-06 14:33:11.478&quot;);
INSERT INTO TABLE user_visit_action VALUES(&quot;001&quot;,&quot;ss002&quot;,&quot;http://f.com&quot;,&quot;2020-08-06 14:35:11.478&quot;);

INSERT INTO TABLE user_visit_action VALUES(&quot;002&quot;,&quot;ss003&quot;,&quot;http://u.com&quot;,&quot;2020-08-06 18:34:11.478&quot;);
INSERT INTO TABLE user_visit_action VALUES(&quot;002&quot;,&quot;ss003&quot;,&quot;http://k.com&quot;,&quot;2020-08-06 18:38:11.478&quot;);
用户访问session分析
SELECT
    user_id,
    session_id,
    page_url,
    DENSE_RANK() OVER (PARTITION BY user_id, session_id ORDER BY action_time ASC) AS page_order,
    MIN(action_time) OVER (PARTITION BY user_id, session_id) AS session_start_time,
    MAX(action_time) OVER (PARTITION BY user_id, session_id) AS session_finisht_time
FROM user_visit_action

user_id	session_id	page_url	page_order	session_start_time	session_finisht_time
001	ss001	http://a.com	1	2020-08-06 13:34:11.478	2020-08-06 13:36:11.478
001	ss001	http://b.com	2	2020-08-06 13:34:11.478	2020-08-06 13:36:11.478
001	ss001	http://c.com	3	2020-08-06 13:34:11.478	2020-08-06 13:36:11.478
001	ss002	http://a.com	1	2020-08-06 14:30:11.478	2020-08-06 14:35:11.478
001	ss002	http://b.com	2	2020-08-06 14:30:11.478	2020-08-06 14:35:11.478
001	ss002	http://e.com	3	2020-08-06 14:30:11.478	2020-08-06 14:35:11.478
001	ss002	http://f.com	4	2020-08-06 14:30:11.478	2020-08-06 14:35:11.478
002	ss003	http://u.com	1	2020-08-06 18:34:11.478	2020-08-06 18:38:11.478
002	ss003	http://k.com	2	2020-08-06 18:34:11.478	2020-08-06 18:38:11.478
</code></pre>
<h4 id="9-自定义UDF解析JSON"><a href="#9-自定义UDF解析JSON" class="headerlink" title="9. 自定义UDF解析JSON"></a>9. <strong>自定义UDF解析JSON</strong></h4><pre><code class="python">## JSONObject解析JSON对象
import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hive.ql.exec.UDF;
import org.json.JSONException;
import org.json.JSONObject;
import org.json.JSONTokener;
/** * JSON对象解析UDF * date:2017-04-20 */
public class GetJsonObject extends UDF &#123;
    /** * 解析json并返回对应的值。例如 add jar jar/bdp_udf_demo-1.0.0.jar; create temporary function getJsonObject as &#39;com.jd.bdp.util.udf.GetJsonObject&#39;; select getJsonObject(json字符串,key值) * @param jsonStr * @param objName * @return */
    public String evaluate(String jsonStr,String objName) throws JSONException &#123;
        if(StringUtils.isBlank(jsonStr)|| StringUtils.isBlank(objName))&#123;
            return null;
        &#125;
        JSONObject jsonObject = new JSONObject(new JSONTokener(jsonStr));
        Object objValue = jsonObject.get(objName);
        if(objValue==null)&#123;
            return null;
        &#125;
        return objValue.toString();
    &#125;
&#125;

## JSONArray解析JSON数组对象 
import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.io.Text;
import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONTokener;
import java.util.ArrayList;
/** * JSON数组对象解析UDF * date:2017-04-20 */
public class GetJsonArray extends UDF &#123;
    /** * 解析json并返回对应子json字符串数组,例如 add jar jar/bdp_udf_demo-1.0.0.jar;
    create temporary function getJsonArray as &#39;com.jd.bdp.util.udf.GetJsonArray&#39;; 
    select getJsonArray(json字符串) * @param jsonArrayStr * @return * @throws HiveException */
    public ArrayList&lt;Text&gt; evaluate(String jsonArrayStr) throws JSONException &#123;
        if(StringUtils.isBlank(jsonArrayStr)||StringUtils.isBlank(jsonArrayStr))&#123;
            return null;
        &#125;
        ArrayList&lt;Text&gt; textList = new ArrayList&lt;Text&gt;();
        if(!jsonArrayStr.trim().startsWith(&quot;[&quot;))&#123;
            textList.add(new Text(jsonArrayStr));
        &#125;else&#123;
            JSONArray jsonArray = new JSONArray(new JSONTokener(jsonArrayStr));
            Text[] jsonTexts = new Text[jsonArray.length()];
            for(int i=0;i&lt;jsonArray.length();i++)&#123;
                String json = jsonArray.getJSONObject(i).toString();
                textList.add(new Text(json));
            &#125;
        &#125;
        return textList;
    &#125;
&#125;
</code></pre>
<h2 id="Git命令整理"><a href="#Git命令整理" class="headerlink" title="Git命令整理**"></a>Git命令整理**</h2><pre><code class="python">git push -f origin 分支名 #强制推送  当出现[reject] master -&gt; master(non-fast-forward)时
git config –list     #获取Git配置信息

git init # 初始化仓库
git add .(文件name) # 添加文件到暂存区
git commit -m &quot;first commit&quot; # 添加文件到本地仓库并提交描述信息
git remote add origin 远程仓库地址 # 链接远程仓库，创建主分支
git pull origin master --allow-unrelated-histories # 把本地仓库的变化连接到远程仓库主分支

git clone -b 分支名 url  #下载某个分支下的代码
git push -u origin master # 把本地仓库的文件推送到远程仓库
git branch      #显示所有本地分支
git checkout [branch-name]    #切换到指定分支，并更新工作区
git merge [branch]    #合并指定分支到当前分支
git branch -d [branch-name]    #删除分支
git checkout -b [branch] #新建一个分支，并切换到该分支

git rm [file1] [file2] ...# 删除工作区文件，并且将这次删除放入暂存区
git rm --cached [file]# 停止追踪指定文件，但该文件会保留在工作区
git commit -a # 提交工作区自上次commit之后的变化，直接到仓库区
git tag [tag] # 新建一个tag在当前commit
git tag [tag] [commit] #新建一个tag在指定commit
git tag -d [tag] #删除本地tag
git push [remote] [tag] #提交指定tag
git status  #查看仓库状态，显示有变更的文件
git pull [remote] [branch] #取回远程仓库的变化，并与本地分支合并

git stash #暂时将未提交的变化移除，稍后再移入
git rebase [branch]
git reset HEAD [file]   #用版本库里的版本替换工作去的版本，无论工作区是修改还是删除可以把暂存区的修改撤销掉
git reset --hard HEAD^
或	
git reset --hard 3628164     # 版本回退
git fetch origin master   # 取回origin主机的master分支
git diff HEAD # 显示工作区与当前分支最新commit之间的差异
git reflog # 可以很好地帮助你恢复你误操作的数据，
</code></pre>
<h2 id="java重复造轮子"><a href="#java重复造轮子" class="headerlink" title="java重复造轮子"></a><strong>java重复造轮子</strong></h2><pre><code class="java">package org.jeecgframework.core.util;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

import net.sf.json.JSONArray;
import net.sf.json.JSONObject;

import org.apache.commons.beanutils.BeanUtils;
import org.codehaus.jackson.JsonParseException;
import org.codehaus.jackson.map.JsonMappingException;
import org.codehaus.jackson.map.ObjectMapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import net.sf.json.JSONArray;
import net.sf.json.JSONObject;

/**
 * JSON和JAVA的POJO的相互转换
 * @author  张代浩
 * JSONHelper.java
 */
public final class JSONHelper &#123;
    private static final Logger logger = LoggerFactory.getLogger(JSONHelper.class);

    // 将数组转换成JSON
    public static String array2json(Object object) &#123;
        JSONArray jsonArray = JSONArray.fromObject(object);
        return jsonArray.toString();
    &#125;

    // 将JSON转换成数组,其中valueClz为数组中存放的对象的Class
    public static Object json2Array(String json, Class valueClz) &#123;
        JSONArray jsonArray = JSONArray.fromObject(json);
        return JSONArray.toArray(jsonArray, valueClz);
    &#125;

    // 将Collection转换成JSON
    public static String collection2json(Object object) &#123;
        JSONArray jsonArray = JSONArray.fromObject(object);
        return jsonArray.toString();
    &#125;

    // 将Map转换成JSON
    public static String map2json(Object object) &#123;
        JSONObject jsonObject = JSONObject.fromObject(object);
        return jsonObject.toString();
    &#125;

    // 将JSON转换成Map,其中valueClz为Map中value的Class,keyArray为Map的key
    public static Map json2Map(Object[] keyArray, String json, Class valueClz) &#123;
        JSONObject jsonObject = JSONObject.fromObject(json);
        Map classMap = new HashMap();

        for (int i = 0; i &lt; keyArray.length; i++) &#123;
            classMap.put(keyArray[i], valueClz);
        &#125;

        return (Map) JSONObject.toBean(jsonObject, Map.class, classMap);
    &#125;

    // 将POJO转换成JSON
    public static String bean2json(Object object) &#123;
        JSONObject jsonObject = JSONObject.fromObject(object);
        return jsonObject.toString();
    &#125;

    // 将JSON转换成POJO,其中beanClz为POJO的Class
    public static Object json2Object(String json, Class beanClz) &#123;
        return JSONObject.toBean(JSONObject.fromObject(json), beanClz);
    &#125;

    /**
     * json转换为java对象
     * 
     * &lt;pre&gt;
     * return JackJson.fromJsonToObject(this.answersJson, JackJson.class);
     * &lt;/pre&gt;
     * 
     * @param &lt;T&gt;
     *            要转换的对象
     * @param json
     *            字符串
     * @param valueType
     *            对象的class
     * @return 返回对象
     */
    public static &lt;T&gt; T fromJsonToObject(String json, Class&lt;T&gt; valueType) &#123;
        ObjectMapper mapper = new ObjectMapper();
        try &#123;
            return mapper.readValue(json, valueType);
        &#125; catch (JsonParseException e) &#123;
            logger.error(&quot;JsonParseException: &quot;, e);
        &#125; catch (JsonMappingException e) &#123;
            logger.error(&quot;JsonMappingException: &quot;, e);
        &#125; catch (IOException e) &#123;
            logger.error(&quot;IOException: &quot;, e);
        &#125;
        return null;
    &#125;

    // 将String转换成JSON
    public static String string2json(String key, String value) &#123;
        JSONObject object = new JSONObject();
        object.put(key, value);
        return object.toString();
    &#125;

    // 将JSON转换成String
    public static String json2String(String json, String key) &#123;
        JSONObject jsonObject = JSONObject.fromObject(json);
        return jsonObject.get(key).toString();
    &#125;

    /***
     * 将List对象序列化为JSON文本
     */
    public static &lt;T&gt; String toJSONString(List&lt;T&gt; list) &#123;
        JSONArray jsonArray = JSONArray.fromObject(list);

        return jsonArray.toString();
    &#125;

    /***
     * 将对象序列化为JSON文本
     * 
     * @param object
     * @return
     */
    public static String toJSONString(Object object) &#123;
        JSONArray jsonArray = JSONArray.fromObject(object);

        return jsonArray.toString();
    &#125;

    /***
     * 将JSON对象数组序列化为JSON文本
     * 
     * @param jsonArray
     * @return
     */
    public static String toJSONString(JSONArray jsonArray) &#123;
        return jsonArray.toString();
    &#125;

    /***
     * 将JSON对象序列化为JSON文本
     * 
     * @param jsonObject
     * @return
     */
    public static String toJSONString(JSONObject jsonObject) &#123;
        return jsonObject.toString();
    &#125;

    /***
     * 将对象转换为List对象
     * 
     * @param object
     * @return
     */
    public static List toArrayList(Object object) &#123;
        List arrayList = new ArrayList();

        JSONArray jsonArray = JSONArray.fromObject(object);

        Iterator it = jsonArray.iterator();
        while (it.hasNext()) &#123;
            JSONObject jsonObject = (JSONObject) it.next();

            Iterator keys = jsonObject.keys();
            while (keys.hasNext()) &#123;
                Object key = keys.next();
                Object value = jsonObject.get(key);
                arrayList.add(value);
            &#125;
        &#125;

        return arrayList;
    &#125;

    /* *//***
     * 将对象转换为Collection对象
     * 
     * @param object
     * @return
     */
    /*
     * public static Collection toCollection(Object object) &#123; JSONArray
     * jsonArray = JSONArray.fromObject(object);
     * 
     * return JSONArray.toCollection(jsonArray); &#125;
     */

    /***
     * 将对象转换为JSON对象数组
     * 
     * @param object
     * @return
     */
    public static JSONArray toJSONArray(Object object) &#123;
        return JSONArray.fromObject(object);
    &#125;

    /***
     * 将对象转换为JSON对象
     * 
     * @param object
     * @return
     */
    public static JSONObject toJSONObject(Object object) &#123;
        return JSONObject.fromObject(object);
    &#125;

    /***
     * 将对象转换为HashMap
     * 
     * @param object
     * @return
     */
    public static HashMap toHashMap(Object object) &#123;
        HashMap&lt;String, Object&gt; data = new HashMap&lt;String, Object&gt;();
        JSONObject jsonObject = JSONHelper.toJSONObject(object);
        Iterator it = jsonObject.keys();
        while (it.hasNext()) &#123;
            String key = String.valueOf(it.next());
            Object value = jsonObject.get(key);
            data.put(key, value);
        &#125;

        return data;
    &#125;

    /***
     * 将对象转换为List&lt;Map&lt;String,Object&gt;&gt;
     * 
     * @param object
     * @return
     */
    // 返回非实体类型(Map&lt;String,Object&gt;)的List
    public static List&lt;Map&lt;String, Object&gt;&gt; toList(Object object) &#123;
        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;Map&lt;String, Object&gt;&gt;();
        JSONArray jsonArray = JSONArray.fromObject(object);
        for (Object obj : jsonArray) &#123;
            JSONObject jsonObject = (JSONObject) obj;
            Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;();
            Iterator it = jsonObject.keys();
            while (it.hasNext()) &#123;
                String key = (String) it.next();
                Object value = jsonObject.get(key);
                map.put((String) key, value);
            &#125;
            list.add(map);
        &#125;
        return list;
    &#125;
    
    // 返回非实体类型(Map&lt;String,Object&gt;)的List
    public static List&lt;Map&lt;String, Object&gt;&gt; toList(JSONArray jsonArray) &#123;
        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;Map&lt;String, Object&gt;&gt;();
        for (Object obj : jsonArray) &#123;
            JSONObject jsonObject = (JSONObject) obj;
            Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;();
            Iterator it = jsonObject.keys();
            while (it.hasNext()) &#123;
                String key = (String) it.next();
                Object value = jsonObject.get(key);
                map.put((String) key, value);
            &#125;
            list.add(map);
        &#125;
        return list;
    &#125;

    /***
     * 将JSON对象数组转换为传入类型的List
     * 
     * @param &lt;T&gt;
     * @param jsonArray
     * @param objectClass
     * @return
     */
    public static &lt;T&gt; List&lt;T&gt; toList(JSONArray jsonArray, Class&lt;T&gt; objectClass) &#123;
        return JSONArray.toList(jsonArray, objectClass);
    &#125;

    /***
     * 将对象转换为传入类型的List
     * 
     * @param &lt;T&gt;
     * @param jsonArray
     * @param objectClass
     * @return
     */
    public static &lt;T&gt; List&lt;T&gt; toList(Object object, Class&lt;T&gt; objectClass) &#123;
        JSONArray jsonArray = JSONArray.fromObject(object);

        return JSONArray.toList(jsonArray, objectClass);
    &#125;

    /***
     * 将JSON对象转换为传入类型的对象
     * 
     * @param &lt;T&gt;
     * @param jsonObject
     * @param beanClass
     * @return
     */
    public static &lt;T&gt; T toBean(JSONObject jsonObject, Class&lt;T&gt; beanClass) &#123;
        return (T) JSONObject.toBean(jsonObject, beanClass);
    &#125;

    /***
     * 将将对象转换为传入类型的对象
     * 
     * @param &lt;T&gt;
     * @param object
     * @param beanClass
     * @return
     */
    public static &lt;T&gt; T toBean(Object object, Class&lt;T&gt; beanClass) &#123;
        JSONObject jsonObject = JSONObject.fromObject(object);

        return (T) JSONObject.toBean(jsonObject, beanClass);
    &#125;

    /***
     * 将JSON文本反序列化为主从关系的实体
     * 
     * @param &lt;T&gt;
     *            泛型T 代表主实体类型
     * @param &lt;D&gt;
     *            泛型D 代表从实体类型
     * @param jsonString
     *            JSON文本
     * @param mainClass
     *            主实体类型
     * @param detailName
     *            从实体类在主实体类中的属性名称
     * @param detailClass
     *            从实体类型
     * @return
     */
    public static &lt;T, D&gt; T toBean(String jsonString, Class&lt;T&gt; mainClass,
            String detailName, Class&lt;D&gt; detailClass) &#123;
        JSONObject jsonObject = JSONObject.fromObject(jsonString);
        JSONArray jsonArray = (JSONArray) jsonObject.get(detailName);

        T mainEntity = JSONHelper.toBean(jsonObject, mainClass);
        List&lt;D&gt; detailList = JSONHelper.toList(jsonArray, detailClass);

        try &#123;
            BeanUtils.setProperty(mainEntity, detailName, detailList);
        &#125; catch (Exception ex) &#123;
            throw new RuntimeException(&quot;主从关系JSON反序列化实体失败！&quot;);
        &#125;

        return mainEntity;
    &#125;

    /***
     * 将JSON文本反序列化为主从关系的实体
     * 
     * @param &lt;T&gt;泛型T 代表主实体类型
     * @param &lt;D1&gt;泛型D1 代表从实体类型
     * @param &lt;D2&gt;泛型D2 代表从实体类型
     * @param jsonString
     *            JSON文本
     * @param mainClass
     *            主实体类型
     * @param detailName1
     *            从实体类在主实体类中的属性
     * @param detailClass1
     *            从实体类型
     * @param detailName2
     *            从实体类在主实体类中的属性
     * @param detailClass2
     *            从实体类型
     * @return
     */
    public static &lt;T, D1, D2&gt; T toBean(String jsonString, Class&lt;T&gt; mainClass,
            String detailName1, Class&lt;D1&gt; detailClass1, String detailName2,
            Class&lt;D2&gt; detailClass2) &#123;
        JSONObject jsonObject = JSONObject.fromObject(jsonString);
        JSONArray jsonArray1 = (JSONArray) jsonObject.get(detailName1);
        JSONArray jsonArray2 = (JSONArray) jsonObject.get(detailName2);

        T mainEntity = JSONHelper.toBean(jsonObject, mainClass);
        List&lt;D1&gt; detailList1 = JSONHelper.toList(jsonArray1, detailClass1);
        List&lt;D2&gt; detailList2 = JSONHelper.toList(jsonArray2, detailClass2);

        try &#123;
            BeanUtils.setProperty(mainEntity, detailName1, detailList1);
            BeanUtils.setProperty(mainEntity, detailName2, detailList2);
        &#125; catch (Exception ex) &#123;
            throw new RuntimeException(&quot;主从关系JSON反序列化实体失败！&quot;);
        &#125;

        return mainEntity;
    &#125;

    /***
     * 将JSON文本反序列化为主从关系的实体
     * 
     * @param &lt;T&gt;泛型T 代表主实体类型
     * @param &lt;D1&gt;泛型D1 代表从实体类型
     * @param &lt;D2&gt;泛型D2 代表从实体类型
     * @param jsonString
     *            JSON文本
     * @param mainClass
     *            主实体类型
     * @param detailName1
     *            从实体类在主实体类中的属性
     * @param detailClass1
     *            从实体类型
     * @param detailName2
     *            从实体类在主实体类中的属性
     * @param detailClass2
     *            从实体类型
     * @param detailName3
     *            从实体类在主实体类中的属性
     * @param detailClass3
     *            从实体类型
     * @return
     */
    public static &lt;T, D1, D2, D3&gt; T toBean(String jsonString,
            Class&lt;T&gt; mainClass, String detailName1, Class&lt;D1&gt; detailClass1,
            String detailName2, Class&lt;D2&gt; detailClass2, String detailName3,
            Class&lt;D3&gt; detailClass3) &#123;
        JSONObject jsonObject = JSONObject.fromObject(jsonString);
        JSONArray jsonArray1 = (JSONArray) jsonObject.get(detailName1);
        JSONArray jsonArray2 = (JSONArray) jsonObject.get(detailName2);
        JSONArray jsonArray3 = (JSONArray) jsonObject.get(detailName3);

        T mainEntity = JSONHelper.toBean(jsonObject, mainClass);
        List&lt;D1&gt; detailList1 = JSONHelper.toList(jsonArray1, detailClass1);
        List&lt;D2&gt; detailList2 = JSONHelper.toList(jsonArray2, detailClass2);
        List&lt;D3&gt; detailList3 = JSONHelper.toList(jsonArray3, detailClass3);

        try &#123;
            BeanUtils.setProperty(mainEntity, detailName1, detailList1);
            BeanUtils.setProperty(mainEntity, detailName2, detailList2);
            BeanUtils.setProperty(mainEntity, detailName3, detailList3);
        &#125; catch (Exception ex) &#123;
            throw new RuntimeException(&quot;主从关系JSON反序列化实体失败！&quot;);
        &#125;

        return mainEntity;
    &#125;

    /***
     * 将JSON文本反序列化为主从关系的实体
     * 
     * @param &lt;T&gt;
     *            主实体类型
     * @param jsonString
     *            JSON文本
     * @param mainClass
     *            主实体类型
     * @param detailClass
     *            存放了多个从实体在主实体中属性名称和类型
     * @return
     */
    public static &lt;T&gt; T toBean(String jsonString, Class&lt;T&gt; mainClass,
            HashMap&lt;String, Class&gt; detailClass) &#123;
        JSONObject jsonObject = JSONObject.fromObject(jsonString);
        T mainEntity = JSONHelper.toBean(jsonObject, mainClass);
        for (Object key : detailClass.keySet()) &#123;
            try &#123;
                Class value = (Class) detailClass.get(key);
                BeanUtils.setProperty(mainEntity, key.toString(), value);
            &#125; catch (Exception ex) &#123;
                throw new RuntimeException(&quot;主从关系JSON反序列化实体失败！&quot;);
            &#125;
        &#125;
        return mainEntity;
    &#125;
    
    public static String listtojson(String[] fields, int total, List list) throws Exception &#123;
        Object[] values = new Object[fields.length];
        String jsonTemp = &quot;&#123;\&quot;total\&quot;:&quot; + total + &quot;,\&quot;rows\&quot;:[&quot;;
        for (int j = 0; j &lt; list.size(); j++) &#123;
            jsonTemp = jsonTemp + &quot;&#123;\&quot;state\&quot;:\&quot;closed\&quot;,&quot;;
            for (int i = 0; i &lt; fields.length; i++) &#123;
                String fieldName = fields[i].toString();
                values[i] = org.jeecgframework.tag.core.easyui.TagUtil.fieldNametoValues(fieldName, list.get(j));
                jsonTemp = jsonTemp + &quot;\&quot;&quot; + fieldName + &quot;\&quot;&quot; + &quot;:\&quot;&quot; + values[i] + &quot;\&quot;&quot;;
                if (i != fields.length - 1) &#123;
                    jsonTemp = jsonTemp + &quot;,&quot;;
                &#125;
            &#125;
            if (j != list.size() - 1) &#123;
                jsonTemp = jsonTemp + &quot;&#125;,&quot;;
            &#125; else &#123;
                jsonTemp = jsonTemp + &quot;&#125;&quot;;
            &#125;
        &#125;
        jsonTemp = jsonTemp + &quot;]&#125;&quot;;
        return jsonTemp;
    &#125;

&#125;
</code></pre>
<h2 id="kudu性能调优"><a href="#kudu性能调优" class="headerlink" title="kudu性能调优"></a><strong>kudu性能调优</strong></h2><pre><code class="python">#### 1、Kudu Tablet Server Maintenance Threads**

解释：Kudu后台对数据进行维护操作，如写入数据时的并发线程数，一般设置为4，官网建议的是数据目录的3倍
参数：maintenance_manager_num_threads

#### 2、Kudu Tablet Server Block Cache Capacity Tablet**

解释：分配给Kudu Tablet Server块缓存的最大内存量，建议是2-4G
参数：block_cache_capacity_mb

#### 3、Kudu Tablet Server Hard Memory Limit Kudu**

解释：Tablet Server能使用的最大内存量，有多大，设置多大，tablet Server在批量写入数据时并非实时写入磁盘，而是先Cache在内存中，在flush到磁盘。这个值设置过小时，会造成Kudu数据写入性能显著下降。对于写入性能要求比较高的集群，建议设置更大的值（一般是机器内存的百分之80）
参数：memory_limit_hard_bytes

Cgroup 内存软限制，这个限制并不会阻止进程使用超过限额的内存，只是在系统内存不足时，会优先回收超过限额的进程占用的内存，使之向限定值靠拢,当进程试图占用的内存超过了cgroups的限制，会触发out of memory，导致进程被kill掉
memory.soft_limit_in_bytes：

Cgroup 内存硬限制，限制该组中的进程使用的物理内存总量不超过设定值
memory.limit_in_bytes：
报错：Service unavailable: Soft memory limit exceeded (at 96.35% of capacity)
#### 4、建议每个表50columns左右，不能超过300个**

#### **5、hash分区数量\*range分区数量不能超过60个（1.7.0版本之后没限制了）**

#### 6、设置block的管理器为文件管理器（默认是日志服务器）**

解释：并非所有文件系统格式都需要设置该选项。ext4、xfs格式支持hole punching（打孔），所以不需要设置block_manager=file，但是ext3 格式需要。可以通过df -Th命令来查看文件系统的格式。

参数：--block_manager=file
#### **7、设置ntp服务器的时间误差不超过20s（默认是10s）**

参数：max_clock_sync_error_usec=20000000

#### **8 、设置rpc的连接时长（默认是3s，建议不要设置）**

参数：--rpc_negotiation_timeout_ms=300000

#### **9、设置rpc一致性选择的连接时长（默认为1s，建议不要设置）**

参数：--consensus_rpc_timeout_ms=1000

#### 10、记录kudu的crash的信息**

解释：

Kudu在Kudu遇到崩溃时，使用Google
Breakpad库来生成minidump。这些minidumps的大小通常只有几MB，即使禁用了核心转储生成，也会生成，生成minidumps只能在Linux上建立。
minidump文件包含有关崩溃的进程的重要调试信息，包括加载的共享库及其版本，崩溃时运行的线程列表，处理器寄存器的状态和每个线程的堆栈内存副本，以及CPU和操作系统版本信息。
Minitump可以通过电子邮件发送给Kudu开发人员或附加到JIRA，以帮助Kudu开发人员调试崩溃。为了使其有用，开发人员将需要知道Kudu的确切版本和发生崩溃的操作系统。请注意，虽然minidump不包含堆内存转储，但它确实包含堆栈内存，因此可以将应用程序数据显示在minidump中。如果机密或个人信息存储在群集上，请不要共享minidump文件。
参数： --minidump_path=minidumps              
           --max_minidumps=9
 （默认是在设置的log目录下生成minidumps目录，里边包含最多9个以dmp结尾的文件，无法设置为空值，需要注意的是如果自定义minidump文件，在master不能启动的情况下，需要将该目录中的文件删除）

#### 11、Stack WatchLog**

解释：每个Kudu服务器进程都有一个称为Stack Watchdog的后台线程，它监视服务器中的其他线程，以防它们被阻塞超过预期的时间段。这些跟踪可以指示操作系统问题或瓶颈存储。通过WARN日志信息的跟踪（Trace）可以用于诊断由于Kudu以下的系统（如磁盘控制器或文件系统）引起的根本原因延迟问题。

### 12、kudu表如果不新建的情况下，在表中增加字段，对数据是没有影响的**
kudu对表字段进行操作
 String tableName = &quot;wyh_main&quot;;
    KuduClient client = new KuduClient.KuduClientBuilder(&quot;hadoop4,hadoop5,hadoop6&quot;).defaultAdminOperationTimeoutMs(600000).build();
    try &#123;
        Object o = 0L;
        // 创建非空的列
        client.alterTable(tableName, new AlterTableOptions().addColumn(&quot;device_id&quot;, Type.INT64, o));

        // 创建列为空
        client.alterTable(tableName, new AlterTableOptions().addNullableColumn(&quot;site_id&quot;, Type.INT64));
### 13、cdh设置多master**
        参数：--master_addresses=hadoop4:7051,hadoop5:7051,hadoop6:7051

### 14、kudu出现启动速度特别慢**
解决办法：
        1、取消所有配置参数（除了资源、时间同步）
        2、升级版本到kudu1.6.0
        3、client必须停止（client不占用io的情况，3台机器，每台机器60G，127分区数量，启动速度3分钟）
        4、查看io使用情况 iostat -d -x -k 1 200
### 15、单hash分区最大是60**

### 16、安装kudu过程中，会要求CPU支持ssc4.2指令集，但是我们的虚拟机cpu没有这个执行集，所以无法安装**

### 17、设置client长连接过期时间**
        参数：--authn_token_validity_seconds=12960000（150天）
        注意：设置到tserver的配置文件中
        
### 18、tserver和master的wal和data目录要分隔（或者是目录设置为lvm卷轴）**
原因：wal目录只能设置为1个
参数：--fs_wal_dir_reserved_bytes
用于非kudu都使用的日志目录文件系统的字节数，默认情况下是-1，每个磁盘上的磁盘空间的1%将被保留，指定的任何其他值表示保留的字节数，必须大于或等于0。
        
 ### 19、设置用户权限，能移动tablet**
参数：--superuser_acl=*
        
20、说明文档
          http://kudu.apache.org/releases/1.5.0/docs/known_issues.html
        
### 21、tserver宕掉后，5分钟后没有恢复的情况下，该机器上的tablet会移动到其他机器**
参数：--follower_unavailable_considered_failed_sec=300
        
### 22、超过参数时间的历史数据会被清理，如果是base数据不会被清理。而真实运行时数据大小持续累加，没有被清理。**
参数：--tablet_history_max_age_sec=900
</code></pre>
<h2 id="Hive运行参数优化"><a href="#Hive运行参数优化" class="headerlink" title="Hive运行参数优化"></a><strong>Hive运行参数优化</strong></h2><p>下边介绍一些运行HQL时的设置参数，以此达到一定程度上的优化。</p>
<p><strong>小文件处理的参数</strong></p>
<pre><code class="sql">set mapred.max.split.size=1024000000;  // 每个Map最大输入大小
set mapred.min.split.size=256000000// 每个Map最小输入大小
set mapred.min.split.size.per.node=256000000; //每个节点处理的最小split
set mapred.min.split.size.per.rack=256000000;//每个机架处理的最小slit
//1.注意一般来说这四个参数的配置结果大小要满足如下关系。max.split.size &gt;= min.split.size &gt;= min.size.per.node &gt;= min.size.per.node
set hive.hadoop.supports.splittable.combineinputformat=true;     
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 
hive.merge.mapredfiles=false;   //reduce输出是否合并 
hive.merge.mapfiles=true;     //map输出是否合并
hive.merge.smallfiles.avgsize： //合并后文件的基本阈值，设置大点可以减少小文件个数，需要mapfiles和mapredfiles为true，默认值是16MB；
set hive.merge.size.per.task = 256*1000*1000  //设置合并文件的大小
1.修改参数  hive.merge.mapredfiles=true
2.通过mapreduece的办法生成一张新的分区表,此时生成的文件变成了每个分区一个文件
小结：
正确处理hive小文件 是 控制map数的一个重要环节.处理的不好 会大大影响任务的执行效率.


使用hadoop的archive归档
#用来控制归档是否可用
set hive.archive.enabled=true;
#通知Hive在创建归档时是否可以设置父目录
set hive.archive.har.parentdir.settable=true;
#控制需要归档文件的大小
set har.partfile.size=1099511627776;

#使用以下命令进行归档
ALTER TABLE srcpart ARCHIVE PARTITION(ds=&#39;2008-04-08&#39;, hr=&#39;12&#39;);

#对已归档的分区恢复为原文件
ALTER TABLE srcpart UNARCHIVE PARTITION(ds=&#39;2008-04-08&#39;, hr=&#39;12&#39;);

#::注意，归档的分区不能够INSERT OVERWRITE，必须先unarchive
</code></pre>
<p><strong>数据倾斜参数</strong></p>
<pre><code class="sql">hive.map.aggr=true //map端是否聚合
hive.map.aggr.hash.force.flush.memory.threshold=0.9
hive.map.aggr.hash.min.reduction=0.5 
hive.map.aggr.hash.percentmemory=0.5 
hive.groupby.skewindata=false; //是否开启倾斜优化
set hive.exec.reducers.max=200;
set mapred.reduce.tasks= 200;---增大Reduce个数 
set hive.groupby.mapaggr.checkinterval=100000 ;
--这个是group的键对应的记录条数超过这个值则会进行分拆,
--值根据具体数据量设置 
set hive.groupby.skewindata=true; 
--如果是group by过程出现倾斜 应该设置为true 
set hive.skewjoin.key=100000; 
--这个是join的键对应的记录条数超过这个值则会进行分拆,
--值根据具体数据量设置 
set hive.optimize.skewjoin=true;--如果是join 过程出现倾斜 
-- 应该设置为true 
1.通过修改参数
hive.map.aggr=true // map端聚合,相当于combiner
hive.groupby.skewindata=true //数据倾斜优化,为true时,查询计划生产两个mapreduce,第一个mr随机处理,第二个按照业务主键聚合,
</code></pre>
<p><strong>分区表参数</strong></p>
<pre><code class="sql">hive.exec.dynamic.partition=true; -- 是否允许动态分区 
hive.exec.dynamic.partition.mode=strict; -- strict是避免
-- 全分区字段是动态的，必须有至少一个分区字段是指定有值的.另一个值
-- 为 nonstrict 
//以下是配置阀值 
hive.exec.max.created.files=100000; -- 一个DML操作可以创建的
-- 文件数 
hive.exec.max.dynamic.partitions=1000; -- 一个DML操作可以
-- 创建的最大动态分区数 
hive.exec.max.dynamic.partitions.pernode=100;  
-- each mapper or reducer 可以创建的最大动态分区数  
</code></pre>
<p><strong>并行执行参数</strong></p>
<pre><code class="sql">hive.exec.parallel=false; -- 打开任务并行执行 
set hive.exec.parallel.thread.number=16; 
-- 同一个sql允许最大并行度，默认为8。
</code></pre>
<p><strong>hive提供的文件合并功能</strong></p>
<table>
<thead>
<tr>
<th align="center"><strong>参数名</strong></th>
<th align="center"><strong>作用</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center">hive.merge.mapfiles</td>
<td align="center">是否在纯Map的任务（没有reduce task）后开启小文件合并</td>
</tr>
<tr>
<td align="center">hive.merge.mapredfiles</td>
<td align="center">是否在mapreduce任务后开启小文件合并</td>
</tr>
<tr>
<td align="center">hive.merge.sparkfiles</td>
<td align="center">是否在hive on spark任务后开启小文件合并</td>
</tr>
<tr>
<td align="center">hive.merge.smallfiles.avgsize</td>
<td align="center">如果原先输出的文件平均大小小于这个值，则开启小文件合并。比如输出原本有100个文件，总大小1G，那平均每个文件大小只有10M，如果我们这个参数设置为16M，这时就会开启文件合并</td>
</tr>
<tr>
<td align="center">hive.merge.size.per.task</td>
<td align="center">开启小文件合并后，预期的一个合并文件的大小。比如原先的总大小有1G，我们预期一个文件256M的话，那么最终经过合并会生成4个文件。</td>
</tr>
</tbody></table>
<p><strong>其它一些参数说明</strong></p>
<pre><code class="sql">set hive.auto.convert.join=true;
解释: hive是否会根据输入文件大小将普通的join转为mapjoin，默认是true。
set hive.auto.convert.join.noconditionaltask=true;
set hive.vectorized.execution.enabled=false;   //向量化配置,针对orc文件
set hive.vectorized.execution.reduce.enabled=false; //向量化配置,针对orc文件
</code></pre>
<h2 id="Redis开发规范总结"><a href="#Redis开发规范总结" class="headerlink" title="Redis开发规范总结**"></a>Redis开发规范总结**</h2><h2 id="Hive开发规范总结"><a href="#Hive开发规范总结" class="headerlink" title="Hive开发规范总结"></a><strong>Hive开发规范总结</strong></h2><h2 id="Spark最佳实践"><a href="#Spark最佳实践" class="headerlink" title="Spark最佳实践"></a><strong>Spark最佳实践</strong></h2><h2 id="Docker-es-部署-spark-写入ES"><a href="#Docker-es-部署-spark-写入ES" class="headerlink" title="Docker es 部署 spark 写入ES"></a>Docker es 部署 spark 写入ES</h2><pre><code class="shell">docker search elasticsearch
docker pull elasticsearch:6.5.4
docker images
elasticsearch        6.5.4               93109ce1d590        20 months ago       774MB
#创建容器，并将9200端口和9300端口进行映射到本机
docker run -d -p 9200:9200 -p 9300:9300 --name elasticsearch elasticsearch:6.5.4
docker ps
http://localhost:9200/
&#123;
  &quot;name&quot; : &quot;lY2RmAj&quot;,
  &quot;cluster_name&quot; : &quot;docker-cluster&quot;,
  &quot;cluster_uuid&quot; : &quot;kroqIJSUTICoNJAXHD3tmA&quot;,
  &quot;version&quot; : &#123;
    &quot;number&quot; : &quot;6.5.4&quot;,
    &quot;build_flavor&quot; : &quot;default&quot;,
    &quot;build_type&quot; : &quot;tar&quot;,
    &quot;build_hash&quot; : &quot;d2ef93d&quot;,
    &quot;build_date&quot; : &quot;2018-12-17T21:17:40.758843Z&quot;,
    &quot;build_snapshot&quot; : false,
    &quot;lucene_version&quot; : &quot;7.5.0&quot;,
    &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,
    &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;
  &#125;,
  &quot;tagline&quot; : &quot;You Know, for Search&quot;
&#125;

#安装elasticsearch‐head
#因为我们需要修改Elasticsearch下的elasticsearch.yml文件，而容器并没有vi命令，所以我们使用文件挂载的方式来进行操作
docker stop 启动docker时的镜像的ID
#先查看所有的容器
 docker ps -a
#移除镜像中e s的容器
docker rm a059e6a2bb51
docker run -d -p 9200:9200 -p 9300:9300 --name elasticsearch -e &quot;ES_JAVA_OPTS=-Xms256 -Xmx512m&quot; elasticsearch:6.5.4
#首先进入到容器中，然后进入到指定目录修改elasticsearch.yml文件。
docker exec -it elasticsearch /bin/bash
cd /usr/share/elasticsearch/config/
vi elasticsearch.yml
#在elasticsearch.yml的文件末尾加上:
http.cors.enabled: true
http.cors.allow-origin: &quot;*&quot;
#修改配置后重启容器即可。
exit #退出容器
docker restart elasticsearch
Docker 部署 ElasticSearch-Head
docker pull mobz/elasticsearch-head:5
docker search elasticsearch-head
yanliangzhong/elasticsearch-head         支持es6.X的elasticsearch-head  
docker pull yanliangzhong/elasticsearch-head
docker run -d --name es_admin -p 9100:9100 yanliangzhong/elasticsearch-head:latest
</code></pre>
<pre><code class="scala">val spark: SparkSession = SparkSession.builder()
      .appName(&quot;test&quot;)
      .master(&quot;local[3]&quot;)
      .config(&quot;es.index.auto.create&quot;, &quot;true&quot;)
      .config(&quot;spark.es.batch.size.entries&quot;,&quot;5000&quot;)
      .config(&quot;spark.es.batch.write.refresh&quot;,false)
      .config(&quot;es.nodes&quot;, &quot;127.0.0.1:9200&quot;)
      .config(&quot;es.nodes.wan.only&quot;,&quot;true&quot;)
      .getOrCreate()

    val rdd = spark.sparkContext.makeRDD(Array((1, &quot;闵得志&quot;, &quot;male&quot;), (2, &quot;刘德华&quot;, &quot;femal&quot;), (3, &quot;周润发&quot;, &quot;document&quot;)))
      .map(tp=&gt;&#123;Person(tp._1,tp._2,tp._3)&#125;)
    //数据写入es
    import org.elasticsearch.spark._

    rdd.saveToEs(&quot;sql_command/sql_info&quot;, Map(&quot;es.mapping.id&quot; -&gt; &quot;id&quot;))
    spark.stop()
  &#125;

  case class Person(
                   var id:Int,
                   var name:String,
                   var sex:String
                   )
</code></pre>
<h2 id="java多线程的四种实现方式"><a href="#java多线程的四种实现方式" class="headerlink" title="java多线程的四种实现方式"></a>java多线程的四种实现方式</h2><pre><code class="java">**第一种：	
**1: 继承Thread类**
**2: 重写run方法（线程要做的事，定义在方法中）**
**3：创建一个子类的对象**
**4：使用该对象，调用start方法，开启新线程。**
public class ThreadDemo &#123;
    public static void main(String[] args) &#123;
        Demo d1 = new Demo(&quot;Jack&quot;);
        Demo d2 = new Demo(&quot;Rose&quot;);
        System.out.println(Thread.currentThread().getName());//获取当前线程名字,当前线程是主线程   主线程的名字是main
        d1.setName(&quot;Jack线程&quot;);//修改
        d2.setName(&quot;Rose线程&quot;);
        System.out.println(d1.getName());//获取线程的名称
        System.out.println(d2.getName());
        d2.start();//新线程
        d1.start();
    &#125;
&#125;
class Demo extends Thread&#123;
    String nickName;
    public Demo(String nickName) &#123;
        this.nickName = nickName;
    &#125;
    String name; 
    public void run() &#123;
        for(int i=0;i&lt;30;i++) &#123;
            System.out.println(Thread.currentThread().getName()+&quot;---&quot;+nickName+&quot;------&quot;+i);
        &#125;
    &#125;
&#125;
</code></pre>
<pre><code class="java">**第二种方式**:
**1. 实现Runnable接口**
**2. 重写run方法**
**3. 创建Runnable接口的子类对象** 
**4. 创建Thread类的对象,把第三步的对象传到构造方法中**
**5. 使用Thread类的对象,调用start方法**
public class ThreadTest &#123;

    public static void main(String[] args) &#123;
        new Thread(new PrintUpperCase()).start();
        new Thread(new PrintLowerCase()).start();
    &#125;
&#125;
class PrintUpperCase implements Runnable&#123;
    @Override
    public void run() &#123;
        for(char i=&#39;A&#39;;i&lt;=&#39;Z&#39;;i++) &#123;
            System.out.println(i);
            Thread.yield();
        &#125;
    &#125;
&#125;
class PrintLowerCase implements Runnable&#123;

    @Override
    public void run() &#123;
        for(char i=&#39;a&#39;;i&lt;=&#39;z&#39;;i++) &#123;
            System.out.println(i);
            Thread.yield();
        &#125;
    &#125;	
&#125;
</code></pre>
<pre><code class="java">**第三种方式**:
**1：创建线程池对象**
**2：线程池调用execute()方法**
**3:  execute()方法中传入一个Runable接口的子类的对象（匿名内部类）**
例：ExecutorService pool = Executors.newCachedThreadPool();
           pool.execute(new Runnable() &#123;
            @Override
            public void run() &#123;
              该方法体中定义了线程任务
                &#125;
            &#125;
        &#125;);
</code></pre>
<pre><code class="java">**第四种方式:
通过Callable和FutureTask创建线程**
**a:创建Callable接口的实现类 ，并实现Call方法** 
**b:创建Callable实现类的实现，使用FutureTask类包装Callable对象，该FutureTask对象封装了Callable对象的Call方法的返回值**
**c:使用FutureTask对象作为Thread对象的target创建并启动线程 (start方法）**
**d:调用FutureTask对象的get()来获取子线程执行结束的返回值**
public class Test2 &#123;
    public static void main(String[] args) throws InterruptedException, ExecutionException &#123;
        // 创建线程任务对象
        Demo demo = new Demo();
        //创建FutureTask
        FutureTask&lt;String&gt; f = new FutureTask&lt;String&gt;(demo);
        //开启线程
        new Thread(f).start();
        //获取线程返回的数据
        String res = f.get();
        System.out.println(res);
    &#125;
&#125;
</code></pre>
<h2 id="KUDU-API的操作"><a href="#KUDU-API的操作" class="headerlink" title="KUDU-API的操作"></a><strong>KUDU-API的操作</strong></h2><pre><code class="java">引入pom.xml依赖
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt;
    &lt;artifactId&gt;kudu-client&lt;/artifactId&gt;
    &lt;version&gt;1.6.0&lt;/version&gt;
&lt;/dependency&gt;

import org.apache.kudu.ColumnSchem;
import org.apache.kudu.Schema;
import org.apache.kudu.Type;
import org.apache.kudu.client.CreateTableOptions;
import org.apache.kudu.client.KuduClient;
import org.apache.kudu.client.KuduException;
import java.util.LinkedList;
import java.util.List;

public class CreateTable&#123;
    private static ColumnSchema newColumn(String name,Type type,boolean iskey)&#123;
    ColumnScheama.ColumnSchemaBuilder column=new ColumnSchema.ColumnSchemaBuilder(name,type);
    column.key(iskey);
    return column.build();
&#125;
    public static void main(String[] args)throws KuduException&#123;
    // master 地址
    String masteraddr = &quot;hdp-101,hdp-102,hdp-103&quot;
    //创建kudu的数据库连接
    KuduClient client = new KuduClient.KuduClientBuilder(masteraddr).defaultSocketReadTimeoutMS(6000).build();
    //设置表的schema
    List&lt;ColumnSchema&gt; columns = new LinkedList&lt;&gt;();
    /**
        与RDBMS不同，kudu不提供自动自增功能，因此应用程序必须始终在插入期间提供完整的主键 */
    colums.add(newColumn(&quot;id&quot;,Type.INT32，true));
    colums.add(newColumn(&quot;name&quot;,Type.String，true));
     Schema  schema = new Schema(columns);
       // 创建表时提供的所有选项
    CreateTableOptions options = new CreateTableOptions();
    // 设置表的replication备份和分区规则
    List&lt;String&gt; parcols = new LinkedList&lt;~&gt;();
    parcols.add(&quot;id&quot;);
    // 设置表的备份数
    options.setNumReplicas(1);
    // 设置range分区和数量
    options.setRangePartitionColumns(parcols,3);
   // 设置hash分区和数量
   // options.addHashPartitions(parcols,3);
    try&#123;
        client.createTable(&quot;student&quot;,schema,options)
        &#125;catch(KuduException e)&#123;
            e.printStackTrace();
        &#125;finally&#123;
            client.close()
        &#125;

&#125;
&#125;
</code></pre>
<pre><code class="java">/**

* 向表中加载数据

* @throws KuduException

*/

@Test

public void loadData() throws KuduException &#123;

//打开表

KuduTable kuduTable = kuduClient.openTable(tableName);
   

//创建KuduSession对象 kudu必须通过KuduSession写入数据

KuduSession kuduSession = kuduClient.newSession();
   

//采用flush方式 手动刷新

kuduSession.setFlushMode(SessionConfiguration.FlushMode.MANUAL_FLUSH);

kuduSession.setMutationBufferSpace(3000);
   

//准备数据

for(int i=1; i&lt;=10; i++)&#123;

Insert insert = kuduTable.newInsert();

//设置字段的内容

insert.getRow().addInt(&quot;CompanyId&quot;,i);

insert.getRow().addInt(&quot;WorkId&quot;,i);

insert.getRow().addString(&quot;Name&quot;,&quot;lisi&quot;+i);

insert.getRow().addString(&quot;Gender&quot;,&quot;male&quot;);

insert.getRow().addString(&quot;Photo&quot;,&quot;person&quot;+i);  

kuduSession.flush();

kuduSession.apply(insert);

&#125;
  

kuduSession.close();

kuduClient.close();
&#125;
</code></pre>
<pre><code class="scala">package org.kududb.spark.demo.basic

import org.kududb.client.KuduClient

object ScanTable &#123;
  def main(args:Array[String]): Unit = &#123;
    if (args.length == 0) &#123;
      println(&quot;&lt;kuduMaster&gt; &lt;tableName&gt; &lt;limit&gt;&quot;)
      return
    &#125;
    val kuduMaster = args(0)
    val tableName = args(1)
    val limit = args(2).toInt

    val kuduClient = new KuduClient.KuduClientBuilder(kuduMaster).build()
    val table = kuduClient.openTable(tableName)
    println(&quot;starting scan&quot;)
    val scannerX = kuduClient.newScannerBuilder(table).build()
    while (scannerX.hasMoreRows) &#123;
      val rows = scannerX.nextRows()
      while (rows.hasNext) &#123;
        val row = rows.next()
        println(&quot; - &quot; + row.rowToString())
      &#125;
    &#125;
    println(&quot;finished scan&quot;)
    scannerX.close();
    kuduClient.close()
  &#125;
&#125;
</code></pre>
<h2 id="最受欢迎的JVM参数"><a href="#最受欢迎的JVM参数" class="headerlink" title="最受欢迎的JVM参数"></a><strong>最受欢迎的JVM参数</strong></h2><pre><code class="java">1.-Xms：初始堆大小。只要启动，就占用的堆大小。
2.-Xmx：最大堆大小。java.lang.OutOfMemoryError：Java heap这个错误可以通过配置-Xms和-Xmx参数来设置。
3.-Xss：栈大小分配。栈是每个线程私有的区域，通常只有几百K大小，决定了函数调用的深度，而局部变量、参数都分配到栈上。
当出现大量局部变量，递归时，会发生栈空间OOM（java.lang.StackOverflowError）之类的错误。
4.XX:NewSize：设置新生代大小的绝对值。
5.-XX:NewRatio：设置年轻代和年老代的比值。比如设置为3，则新生代：老年代=1:3，新生代占总heap的1/4。
6.-XX:MaxPermSize：设置持久代大小。
java.lang.OutOfMemoryError:PermGenspace这个OOM错误需要合理调大PermSize和MaxPermSize大小。
7.-XX:SurvivorRatio：年轻代中Eden区与两个Survivor区的比值。注意，Survivor区有form和to两个。比如设置为8时，那么eden:form:to=8:1:1。
8.-XX:HeapDumpOnOutOfMemoryError：发生OOM时转储堆到文件，这是一个非常好的诊断方法。
9.-XX:HeapDumpPath：导出堆的转储文件路径。
10.-XX:OnOutOfMemoryError：OOM时，执行一个脚本，比如发送邮件报警，重启程序。后面跟着一个脚本的路径。
</code></pre>
<h2 id="Spark提交参数说明和常见优化"><a href="#Spark提交参数说明和常见优化" class="headerlink" title="Spark提交参数说明和常见优化"></a><strong>Spark提交参数说明和常见优化</strong></h2><pre><code class="python">./bin/spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 1 \
    --queue thequeue \
    --jars my-other-jar.jar,my-other-other-jar.jar \
    examples/jars/spark-examples*.jar \
    app_arg1 app_arg2
    
在提交任务时的几个重要参数:
executor-cores —— 每个executor使用的内核数，默认为1
num-executors —— 启动executors的数量，默认为2
executor-memory —— executor内存大小，默认1G
driver-cores —— driver使用内核数，默认为1
driver-memory —— driver内存大小，默认512M

executor_cores*num_executors 
表示的是能够并行执行Task的数目 
不宜太小或太大！一般不超过总队列 cores 的 25%，比如队列总 cores 400，最大不要超过100，最小不建议低于 40，除非日志量很小。

executor_cores 
不宜为1！否则 work 进程中线程数过少，一般 2~4 为宜。

executor_memory 
一般 6~10g 为宜，最大不超过20G，否则会导致GC代价过高，或资源浪费严重。

driver-memory 
driver 不做任何计算和存储，只是下发任务与yarn资源管理器和task交互，除非你是 spark-shell，否则一般 1-2g

增加每个executor的内存量，增加了内存量以后，对性能的提升，有三点：
1、如果需要对RDD进行cache，那么更多的内存，就可以缓存更多的数据，将更少的数据写入磁盘， 
甚至不写入磁盘。减少了磁盘IO。
2、对于shuffle操作，reduce端，会需要内存来存放拉取的数据并进行聚合。如果内存不够，也会写入磁盘。如果给executor分配更多内存以后，就有更少的数据，需要写入磁盘，甚至不需要写入磁盘。减少了磁盘IO，提升了性能。
3、对于task的执行，可能会创建很多对象。如果内存比较小，可能会频繁导致JVM堆内存满了，然后频繁GC，垃圾回收，minor GC和full GC。（速度很慢）。内存加大以后，带来更少的GC，垃圾回收，避免了速度变慢，性能提升。

常规注意事项:
预处理数据，丢掉一些不必要的数据
增加Task的数量
过滤掉一些容易导致发生倾斜的key
避免创建重复的RDD
尽可能复用一个RDD
对多次使用的RDD进行持久化
尽量避免使用shuffle算子
在要使用groupByKey算子的时候,尽量用reduceByKey或者aggregateByKey算子替代.因为调用groupByKey时候,按照相同的key进行分组,形成RDD[key,Iterable[value]]的形式,此时所有的键值对都将被重新洗牌,移动,对网络数据传输造成理论上的最大影响.
使用高性能的算子

## spark资源配置
spark on Yarn资源参数配置方式
代码配置：
conf = SparkConf()
conf.set(&quot;spark.executor.instances&quot;, &quot;50&quot;)
conf.set(&quot;spark.executor.memory&quot;, &quot;8g&quot;)
conf.set(&quot;spark.executor.cores&quot;, &quot;2&quot;)
conf.set(&quot;spark.yarn.executor.memoryOverhead&quot;, &quot;2g&quot;)
SparkSession.builder.master(&quot;yarn&quot;).appName(&quot;aml&quot;).config(conf=conf) \
  .enableHiveSupport().getOrCreate()
 
脚本配置：
spark-submit \
 --name aml
 --master yarn \
 --deploy-mode cluster \
 --executor-memory 20G \
  --num-executors 50 \
  --driver-memory 4g \
  --executor-cores 2 \
 /path/to/examples.jar 1000
 
spark.executor.instances
参数说明：设置Spark作业的Executor总个数。
推荐设置：50~100
参数详解：Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。
spark.executor.memory
参数说明：设置每个Executor的内存。
推荐设置：4~8g
参数详解：Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。另外这个参数最大不能超过32G。
spark.executor.cores
参数说明：设置每个Executor占用的CPU core数量。
推荐设置：2~4
参数详解：这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么executors个数 * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。
核数与内存需满足一定比例，通常在1：4左右，如果任务gc时间较长，可以考虑增大内存，资源不够则减少核数。
spark.driver.memory
参数说明：设置Driver的内存。
推荐设置：默认1G
参数详解：Driver端的内存是本地的，通常是存放spark job的执行结果以及运行本地代码使用的。如果需要使用collect、collectAsMap等算子将RDD的大量数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。
spark.default.parallelism
参数说明：设置shuffle后的stage的默认task数量。（仅rdd算子有效）
推荐设置：500~1000
参数详解：这个参数很重要，如果不设置可能会直接影响Spark作业性能。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。
spark.sql.shuffle.partitions
参数说明：设置shuffle后stage的默认task数量。（仅spark SQL有效）
推荐设置：500~1000
参数详解：与spark.default.parallelism一样。不同的是这个参数是对spark SQL的shuffle有效，对rdd的shuffle无效。
spark.yarn.executor.memoryOverhead
参数说明：设置每个executor的对外内存。
推荐设置：默认是executorMemory的10%，但不会低于384M。
参数详解：对于pyspark开发的程序，对executor的对外内存要求会更高。当算子里的自定义函数比较复杂时，有可能使job失败，建议适当调大这个参数。如果无效，需减少executor-cores，减少出错stage的task数量。
</code></pre>
<h2 id="JVM常用调优-x2F-调试参数"><a href="#JVM常用调优-x2F-调试参数" class="headerlink" title="JVM常用调优&#x2F;调试参数:"></a>JVM常用调优&#x2F;调试参数:</h2><table>
<thead>
<tr>
<th><strong>-Xms<size></size></strong></th>
<th>例如:-Xms2G,设置Heap的初始大小</th>
</tr>
</thead>
<tbody><tr>
<td>-Xmx<size></size></td>
<td>设置Heap的最大尺寸,建议为最大物理内存的1&#x2F;3(建议此值不超过12G)</td>
</tr>
<tr>
<td>-Xss<size></size></td>
<td>方法栈大小</td>
</tr>
<tr>
<td>-Xmn<size></size></td>
<td>新生代大小</td>
</tr>
<tr>
<td>-XX:NewSize&#x3D;<value>-XX:MaxNewSize&#x3D;<value>-XX:SurvivorRatio&#x3D;<value></value></value></value></td>
<td>新生代设置.单位为byteSurvivorRatio &#x3D; Eden&#x2F;survivor;例如此值为3,则表示,Eden : from : to &#x3D; 3:1:1;默认值为8</td>
</tr>
<tr>
<td>-XX:PermSize&#x3D;<value>-XX:MaxPermSize&#x3D;<value></value></value></td>
<td>方法区大小,max值默认为64M</td>
</tr>
<tr>
<td>-XX:[+ | -]UseConcMarkSweepGC</td>
<td>打开或关闭基于ParNew + CMS + SerialOld的收集器组合.(-XX:+UseParNewGC)</td>
</tr>
<tr>
<td>-XX:[+ | -]UseParallelGC</td>
<td>在server模式下的默认值(+),表示使用Parallel Scavenge + Serial Old收集器组合</td>
</tr>
<tr>
<td>-XX:[+ | -]UseParallelOldGC</td>
<td>默认关闭,+表示打开&#x2F;使用Parallel Scavenge + Parallel Old收集器组合</td>
</tr>
<tr>
<td>-XX:PretenureSizeThreshold&#x3D;<value></value></td>
<td>直接晋升为旧生代的对象大小.大于此值的将会直接被分配到旧生代,单位byte</td>
</tr>
<tr>
<td>-XX:MaxTenuringThreshold&#x3D;<value></value></td>
<td>晋升到旧生代的对象年龄(已经或者即将被回收的次数);每个对象被minor GC一次,它的年龄+1,如果对象的年龄达到此值,将会进入旧生代.</td>
</tr>
<tr>
<td>-XX:[+ | -]UseAdaptiveSizePolicy</td>
<td>默认开启;是否动态调整java中堆中各个区域大小以及进入旧生代的年龄;此参数可以方便我们对参数调优,找到最终适合配置的参数.</td>
</tr>
<tr>
<td>-XX:[+ | -]HandlePromotionFailure</td>
<td>JDK1.6默认开启,是否支持内存分配失败担保策略;在发生Minor GC时，虚拟机会检测之前每次晋升到老年代的平均大小是否大于老年代的剩余空间大小，如果大于，则改为直接进行一次Full GC。如果小于，则查看HandlePromotionFailure设置是否允许担保失败；如果允许，那只会进行Minor GC；如果不允许，则也要改为进行一次Full GC。</td>
</tr>
<tr>
<td>-XX:ParallelGCThreads&#x3D;<value></value></td>
<td>并行GC时所使用的线程个数.建议保持默认值(和CPU个数有换算关系).</td>
</tr>
<tr>
<td>-XX:GCTimeRatio&#x3D;<value></value></td>
<td>GC占JVM服务总时间比.默认为99,即允许1%的GC时间消耗.此参数只在Parallel Scavenge收集器下生效</td>
</tr>
<tr>
<td>-XX:CMSInitiatingOccupancyFraction&#x3D;<value></value></td>
<td>设置CMS收集器在旧生代空间使用占比达到此值时,触发GC.</td>
</tr>
<tr>
<td>-XX:[+ | -]UseCMSCompactAtFullCollection</td>
<td>默认开启,表示在CMS收集器进行一次Full gc后是否进行一次内存碎片整理,[原因:CMS回收器会带来内存碎片]</td>
</tr>
<tr>
<td>-XX:CMSFullGCSBeforeCompaction&#x3D;<value></value></td>
<td>进行多少次FullGC之后,进行一次内存碎片整理.[原因:CMS回收器会带来内存碎片]</td>
</tr>
</tbody></table>
<h2 id="Elasticsearch性能优化实战指南"><a href="#Elasticsearch性能优化实战指南" class="headerlink" title="Elasticsearch性能优化实战指南"></a><strong>Elasticsearch性能优化实战指南</strong></h2><pre><code class="python">#### **1、索引层面优化配置**

默认情况下，6.x及之前的版本中Elasticsearch索引有5个主分片和1个副本，7.X及之后版本1主1副。 这种配置并不适用于所有业务场景。 需要正确设置分片配置，以便维持索引的稳定性和有效性。

**1.1、分片大小**

分片大小对于搜索查询非常重要。

一方面， 如果分配给索引的分片太多，则Lucene分段会很小，从而导致开销增加。当同时进行多个查询时，许多小分片也会降低查询吞吐量。
另一方面，太大的分片会导致搜索性能下降和故障恢复时间更长。
Elasticsearch官方建议一个分片的大小应该在20到40 GB左右。

例如，如果您计算出索引将存储300 GB的数据，则可以为该索引分配9到15个主分片。 

根据集群大小，假设群集中有10个节点，您可以选择为此索引分配10个主分片，以便在集群节点之间均匀分配分片。

深入原理推荐：
Elasticsearch之如何合理分配索引分片？
https://www.elastic.co/cn/blog/how-many-shards-should-i-have-in-my-elasticsearch-cluster
Elasticsearch究竟要设置多少分片数？
https://qbox.io/blog/optimizing-elasticsearch-how-many-shards-per-index

**1.2、数据动态持续写入场景**

如果存在连续写入到Elasticsearch集群的数据流，如：实时爬虫互联网数据写入ES集群。则应使用基于时间的索引以便更轻松地维护索引。

如果写入数据流的吞吐量随时间而变化，则需要适当地改变下一个索引的配置才能实现数据的动态扩展。

那么，如何查询分散到不同的基于时间索引的所有文档？答案是别名。可以将多个索引放入别名中，并且对该别名进行搜索会使查询就像在单个索引上一样。

当然，需要保持好平衡。注意思考：将多少数据写入别名？别名上写入太多小索引会对性能产生负面影响。

例如，是以周还是以月为单位为单位建立索引是需要结合业务场景平衡考虑的问题？

如果以月为单位建议索引性能最优，那么相同数据以周为单位建立索引势必会因为索引太多导致负面的性能问题。

**1.3、Index Sorting**


注意：索引排序机制是6.X版本才有的特性。

在Elasticsearch中创建新索引时，可以配置每个分片中的分段的排序方式。 默认情况下，Lucene不会应用任何排序。 index.sort.* 定义应使用哪些字段对每个Segment内的文档进行排序。
PUT /twitter
 &#123;
     &quot;settings&quot; : &#123;
         &quot;index&quot; : &#123;
             &quot;sort.field&quot; : &quot;date&quot;, 
             &quot;sort.order&quot; : &quot;desc&quot; 
         &#125;
     &#125;,
     &quot;mappings&quot;: &#123;
        &quot;properties&quot;: &#123;
            &quot;date&quot;: &#123;
                &quot;type&quot;: &quot;date&quot;
            &#125;
        &#125;
    &#125;
&#125;

目的：index sorting是优化Elasticsearch检索性能的非常重要的方式之一。

大白话：index sorting机制通过写入的时候指定了某一个或者多个字段的排序方式，会极大提升检索的性能。

更深原理：推荐阅读：
https://www.elastic.co/cn/blog/index-sorting-elasticsearch-6-0
    
#### **2、分片层面优化配置**

分片是底层基本的读写单元，分片的目的是分割巨大索引，让读写并行执行。写入过程先写入主分片，主分片写入成功后再写入副本分片。

副本分片的出现，提升了集群的高可用性和读取吞吐率。

在优化分片时，分片的大小、节点中有多少分片是主要考虑因素。副本分片对于扩展搜索吞吐量很重要，如果硬件条件允许，则可以小心增加副本分片的数量。

容量规划的一个很好的启动点是分配分片，“《深入理解Elasticsearch》强调：最理想的分片数量应该依赖于节点的数量。”其数量是节点数量的1.5到3倍。

分配副本分片数的公式：max（max_failures，ceil（num_nodes /） num_primaries） -  1）。

原理：如果您的群集具有num_nodes节点，总共有num_primaries主分片，如果您希望最多能够同时处理max_failures节点故障，那么适合您的副本数量为如上公式值。

公式来源：
https://www.elastic.co/guide/en/elasticsearch/reference/master/tune-for-search-speed.html

总的来说：节点数和分片数、副本数的简单计算公式如下：
所需做大节点数=分片数*（副本数+1）。

#### **3、Elasticsearch整体层面配置**

配置Elasticsearch集群时，最主要的考虑因素之一是确保至少有一半的可用内存进入文件系统缓存，以便Elasticsearch可以将索引的hot regions保留在物理内存中。

在设计集群时还应考虑物理可用堆空间。 Elasticsearch建议基于可用堆空间的分片分配最多应为20个分片/ GB，这是一个很好的经验法则。

例如，具有30 GB堆的节点最多应有600个分片，以保持集群的良好状态。

一个节点上的存储可以表述如下：节点可以支持的磁盘空间= 20 （堆大小单位：GB）（以GB为单位的分片大小），由于在高效集群中通常会看到大小在20到40 GB之间的分片，因此最大存储空间可以支持16 GB可用堆空间的节点，最多可达12 TB的磁盘空间（20*16*40=12.8TB）。

边界意识有助于为更好的设计和未来的扩展操作做好准备。

可以在运行时以及初始阶段进行许多配置设置。

在构建Elasticsearch索引和集群本身以获得更好的搜索性能时，了解在运行时哪些配置可以修改以及哪些配不可以修改是至关重要的。

**3.1 动态设置**
**1、设置历史数据索引为只读状态。**

基于时间的动态索引的执行阶段，如果存放历史数据的索引没有写操作，可以将月度索引设置为只读模式，以提高对这些索引的搜索性能。
6.X之后的只读索引实战设置方式：
PUT /twitter/_settings
&#123;
  &quot;index.blocks.read_only_allow_delete&quot;: null
&#125;

**2、对只读状态索引，进行段合并。**
当索引设置为只读时，可以通过强制段合并操作以减少段的数量。

优化段合并将导致更好的搜索性能，因为每个分片的开销取决于段的计数和大小。

注意1：不要将段合并用于读写索引，因为它将导致产生非常大的段（每段&gt; 5Gb）。

注意2：此操作应在非高峰时间进行，因为这是一项非常耗资源的操作。

段合并操作实战方式：
curl -X POST &quot;localhost:9200/kimchy/_forcemerge only_expunge_deletes=false&amp;max_num_segments=100&amp;flush=true&quot;
    
**3、使用preference优化缓存利用率**
有多个缓存可以帮助提高搜索性能，例如文件系统缓存，请求缓存或查询缓存。

然而，所有这些缓存都维护在节点级别，这意味着如果您在拥有1个或更多副本且基于默认路由算法集群上连续两次运行相同的请求，这两个请求将转到不同的分片副本上 ，阻止节点级缓存帮助。

由于搜索应用程序的用户一个接一个地运行类似的请求是常见的，例如为了检索分析索引的部分较窄子集，使用preference标识当前用户或会话的偏好值可以帮助优化高速缓存的使用。

preference实战举例：
GET /_search?preference=xyzabc123
&#123;
    &quot;query&quot;: &#123;
        &quot;match&quot;: &#123;
            &quot;title&quot;: &quot;elasticsearch&quot;
        &#125;
    &#125;
&#125;

**4、禁止交换**

可以在每个节点上禁用交换以确保稳定性，并且应该不惜一切代价避免交换。它可能导致垃圾收集持续数分钟而不是毫秒，并且可能导致节点响应缓慢甚至断开与集群的连接。

在Elasticsearch分布式系统中，让操作系统终止节点更有效。可以通过将bootstrap.memory_lock设置为True来禁用它。

Linux系统级配置：sudo swapoff -a
Elasticsearch配置文件elasticsearch.yml配置：bootstrap.memory_lock: true

**5、增加刷新间隔 refresh_interval**

默认刷新间隔为1秒。这迫使Elasticsearch每秒创建一个分段。实际业务中，应该根据使用情况增加刷新间隔，举例：增加到30秒。
这样之后，30s产生一个大的段，较每秒刷新大大减少未来的段合并压力。最终会提升写入性能并使搜索查询更加稳定。

更新刷新间隔实战：
    PUT /twitter/_settings
&#123;
    &quot;index&quot; : &#123;
        &quot;refresh_interval&quot; : &quot;1s&quot;
    &#125;
&#125;

**6、设置max_thread_count**
index.merge.scheduler.max_thread_count默认设置为
Math.max(1, Math.min(4, Runtime.getRuntime().availableProcessors() / 2))
但这适用于SSD配置。对于HDD，应将其设置为1。

实战：
curl -XPUT &#39;localhost:9200/_settings&#39; -d &#39;&#123; 
     &quot;index.merge.scheduler.max_thread_count&quot; : 1
&#125;

**7、禁止动态分配分片**
有时，Elasticsearch将重新平衡集群中的分片。此操作可能会降低检索的性能。
在生产模式下，需要时，可以通过cluster.routing.rebalance.enable设置将重新平衡设置为none。
PUT /_cluster/settings 
&#123; 
  &quot;transient&quot; : &#123;
    &quot;cluster.routing.allocation.enable&quot; : &quot;none&quot;
  &#125;
&#125;
其中典型的应用场景之包括：
1:集群中临时重启、剔除一个节点；
2:集群逐个升级节点；当您关闭节点时，分配过程将立即尝试将该节点上的分片复制到集群中的其他节点，从而导致大量浪费的IO. 在关闭节点之前禁用分配可以避免这种情况。
更多实践，推荐阅读：
https://www.elastic.co/guide/en/elasticsearch/reference/5.5/restart-upgrade.html

**8、充分利用近似日期缓存效果**
现在使用的日期字段上的查询通常不可缓存，因为匹配的范围一直在变化。
然而，就用户体验而言，切换到近似日期通常是可接受的，并且能更好地使用查询高速缓存带来的益处。

实战如下:
GET index/_search
 &#123;
  &quot;query&quot;: &#123;
     &quot;constant_score&quot;: &#123;
       &quot;filter&quot;: &#123;
         &quot;range&quot;: &#123;
           &quot;my_date&quot;: &#123;
             &quot;gte&quot;: &quot;now-1h/m&quot;,
             &quot;lte&quot;: &quot;now/m&quot;
          &#125;
        &#125;
      &#125;
    &#125;
  &#125;
&#125;

这里可能不大好理解，推荐深入阅读：
https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-search-speed.html
    
#### **3.2 初始设置**

**1、合并多字段提升检索性能**
query_string或multi_match查询所针对的字段越多，检索越慢。
提高多个字段的搜索速度的常用技术是在索引时将其值复制到单个字段中。
对于经常查询的某些字段，请使用Elasticsearch的copy-to功能。

例如，汽车的品牌名称，发动机版本，型号名称和颜色字段可以与复制到指令合并。它将改善在这些字段上进行的搜索查询性能。

PUT movies
&#123;
  &quot;mappings&quot;: &#123;
    &quot;properties&quot;: &#123;
      &quot;cars_infos&quot;: &#123;
        &quot;type&quot;: &quot;text&quot;
      &#125;,
      &quot;brand_name&quot;: &#123;
        &quot;type&quot;: &quot;text&quot;,
        &quot;copy_to&quot;: &quot;cars_infos&quot;
      &#125;,
      &quot;engine_version&quot;: &#123;
        &quot;type&quot;: &quot;text&quot;,
        &quot;copy_to&quot;: &quot;cars_infos&quot;
      &#125;,
   &quot;model &quot;: &#123;
        &quot;type&quot;: &quot;text&quot;,
        &quot;copy_to&quot;: &quot;cars_infos&quot;
      &#125;,
   &quot;color&quot;: &#123;
        &quot;type&quot;: &quot;text&quot;,
        &quot;copy_to&quot;: &quot;cars_infos&quot;
      &#125;
    &#125;
  &#125;
&#125;
**2、设置分片分配到指定节点**

实战业务中经常遇到的业务场景问题：如何将分片设置非均衡分配，有新节点配置极高，能否多分片点过去？
某个 shard 分配在哪个节点上，一般来说，是由 ES 自动决定的。以下几种情况会触发分配动作：
1）新索引生成
2）索引的删除
3）新增副本分片
4）节点增减引发的数据均衡
ES 提供了一系列参数详细控制这部分逻辑，其中之一是：在异构集群的情为具有更好硬件的节点的分片分配分配权重。
为了分配权重，
需要设置cluster.routing.allocation.balance.shard值，默认值为0.45f。
数值越大越倾向于在节点层面均衡分片。

实战：
PUT _cluster/settings
&#123;
“transient” : &#123;
“cluster.routing.allocation.balance.shard” : 0.60
&#125;
&#125;
**3、调整熔断内存比例大小**

查询本身也会对响应的延迟产生重大影响。为了在查询时不触发熔断并导致Elasticsearch集群处于不稳定状态，
可以根据查询的复杂性将indices.breaker.total.limit设置为适合您的JVM堆大小。此设置的默认值是JVM堆的70％。
PUT /_cluster/settings
&#123;
  &quot;persistent&quot; : &#123;
    &quot;indices.breaker.fielddata.limit&quot; : &quot;60%&quot; 
  &#125;
&#125;
最好为断路器设置一个相对保守点的值。更深原理推荐阅读：
https://www.elastic.co/guide/cn/elasticsearch/guide/current/_limiting_memory_usage.html。

《Elastic源码分析》作者张超指出：“Elasticsearch 7.0 增加了 indices.breaker.total.use_real_memory 配置项，可以更加精准的分析当前的内存情况，及时防止 OOM 出现。虽然该配置会增加一点性能损耗，但是可以提高 JVM 的内存使用率，增强了节点的保护机制。”

**4、特定搜索场景，增加搜索线程池配置**

默认情况下，Elasticsearch将主要用例是搜索。在需要增加检索并发性的情况下，可以增加用于搜索设置的线程池，与此同时，可以根据节点上的CPU中的核心数量多少斟酌减少用于索引的线程池。

举例：更改配置文件elasticsearch.yml增加如下内容：
1thread_pool.search.queue_size: 500
2#queue_size允许控制没有线程执行它们的挂起请求队列的初始大小。
官方：
https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html

**5、打开自适应副本选择**

应打开自适应副本选择。该请求将被重定向到响应最快的节点。
当存在多个数据副本时，elasticsearch可以使用一组称为自适应副本选择的标准，根据包含每个分片副本的节点的响应时间，服务时间和队列大小来选择数据的最佳副本。
这样可以提高查询吞吐量并减少搜索量大的应用程序的延迟。
这个配置默认是关闭的，实战打开方法：

PUT /_cluster/settings
&#123;
    &quot;transient&quot;: &#123;
        &quot;cluster.routing.use_adaptive_replica_selection&quot;: true
    &#125;
&#125;

#### **4、小结**

Elasticsearch集群有许多配置设置可以减少响应延迟，提升检索性能。 以上只是冰山一角。更多实践配置推荐阅读官方文档之鼻祖级优化指南：
https://www.elastic.co/guide/en/elasticsearch/reference/6.7/tune-for-search-speed.html
</code></pre>
<h2 id="Elasticsearch性能优化实践"><a href="#Elasticsearch性能优化实践" class="headerlink" title="Elasticsearch性能优化实践"></a><strong>Elasticsearch性能优化实践</strong></h2><pre><code class="python">### **1、集群规划优化实践**

#### **1.1 基于目标数据量规划集群**

在业务初期，经常被问到的问题，要几个节点的集群，内存、CPU要多大，要不要SSD？
最主要的考虑点是：你的`目标存储数据量`是多大？可以针对目标数据量反推节点多少。

#### **1.2 要留出容量Buffer**

注意：Elasticsearch有三个警戒水位线，磁盘使用率达到85%、90%、95%。
不同警戒水位线会有不同的应急处理策略。
这点，磁盘容量选型中要规划在内。控制在`85%之下`是合理的。
当然，也可以通过配置做调整。

#### **1.3 ES集群各节点尽量不要和其他业务功能复用一台机器。**

除非内存非常大。
举例：普通服务器，安装了ES+Mysql+redis，业务数据量大了之后，势必会出现内存不足等问题。

#### **1.4 磁盘尽量选择SSD**

Elasticsearch官方文档肯定`推荐SSD`，考虑到成本的原因。需要结合业务场景，
如果业务对写入、检索速率有较高的速率要求，建议使用SSD磁盘。
阿里的业务场景，SSD磁盘比机械硬盘的速率提升了5倍。
但要因业务场景而异。

#### **1.5 内存配置要合理**

官方建议：堆内存的大小是官方建议是：Min（32GB，机器内存大小/2）。
Medcl和wood大叔都有明确说过，不必要设置32/31GB那么大，建议：`热数据设置：26GB，冷数据：31GB`。
总体内存大小没有具体要求，但肯定是内容越大，检索性能越好。
经验值供参考：每天200GB+增量数据的业务场景，服务器至少要64GB内存。
除了JVM之外的预留内存要充足，否则也会经常OOM。

#### **1.6 CPU核数不要太小**

CPU核数是和ESThread pool关联的。和写入、检索性能都有关联。
建议：`16核+`。

#### **1.7 超大量级的业务场景，可以考虑跨集群检索**

除非业务量级非常大，例如：滴滴、携程的PB+的业务场景，否则基本不太需要跨集群检索。

#### **1.8 集群节点个数无需奇数**

ES内部维护集群通信，不是基于zookeeper的分发部署机制，所以，`无需奇数`。
但是discovery.zen.minimum_master_nodes的值要设置为：候选主节点的个数/2+1，才能有效避免脑裂。

#### **1.9 节点类型优化分配**

集群节点数：&lt;=3，建议：所有节点的master：true， data：true。既是主节点也是路由节点。 集群节点数：&gt;3, 根据业务场景需要，建议：逐步独立出Master节点和协调/路由节点。

#### **1.10 建议冷热数据分离**

`热数据存储SSD`和普通历史数据存储机械磁盘，物理上提高检索效率。

### **2、索引优化实践**

Mysql等关系型数据库要分库、分表。Elasticserach的话也要做好充分的考虑。

#### **2.1 设置多少个索引？**

建议根据业务场景进行存储。

不同通道类型的数据要`分索引存储`。举例：知乎采集信息存储到知乎索引；APP采集信息存储到APP索引。

#### **2.2 设置多少分片？**

建议根据数据量衡量。
经验值：建议每个分片大小`不要超过30GB`。

#### **2.3 分片数设置？**

建议根据集群节点的个数规模，分片个数建议&gt;=集群节点的个数。
5节点的集群，5个分片就比较合理。

注意：除非reindex操作，`分片数是不可以修改`的。

#### **2.4副本数设置？**

除非你对系统的健壮性有异常高的要求，比如：银行系统。可以考虑2个副本以上。
否则，1个副本足够。

注意：`副本数是可以通过配置随时修改`的。

#### **2.5不要再在一个索引下创建多个type**

即便你是5.X版本，考虑到未来版本升级等后续的可扩展性。

建议：一个索引对应一个type。6.x默认对应_doc，5.x你就直接对应type统一为doc。

#### **2.6 按照日期规划索引**

随着业务量的增加，单一索引和数据量激增给的矛盾凸显。
按照日期规划索引是必然选择。

好处1：可以实现历史数据秒删。很对历史索引delete即可。注意：一个索引的话需要借助delete_by_query+force_merge操作，慢且删除不彻底。

好处2：便于冷热数据分开管理，检索最近几天的数据，直接物理上指定对应日期的索引，速度快的一逼！

操作参考：`模板使用+rollover API使用`。

#### **2.7 务必使用别名**

ES不像mysql方面的更改索引名称。使用别名就是一个相对灵活的选择。

## 3、数据模型优化实践**

#### **3.1 不要使用默认的Mapping**

默认Mapping的字段类型是系统`自动识别`的。其中：string类型默认分成：text和keyword两种类型。如果你的业务中不需要分词、检索，仅需要精确匹配，仅设置为keyword即可。

根据业务需要选择合适的类型，有利于节省空间和提升精度，如：浮点型的选择。

#### **3.2 Mapping各字段的选型流程**
![微信图片_20200315214927](D:\Typora_image\微信图片_20200315214927.jpg)

### **3.3 选择合理的分词器**

常见的开源中文分词器包括：ik分词器、ansj分词器、hanlp分词器、结巴分词器、海量分词器、“ElasticSearch最全分词器比较及使用方法” 搜索可查看对比效果。

如果选择ik，建议使用ik_max_word。因为：粗粒度的分词结果基本包含细粒度ik_smart的结果。

### **3.4 date、long、还是keyword**
根据业务需要，如果需要基于时间轴做分析，必须date类型；
如果仅需要秒级返回，建议使用keyword。

###　**4、数据写入优化实践**
###  **4.1 要不要秒级响应？**
Elasticsearch近实时的本质是：最快1s写入的数据可以被查询到。
如果refresh_interval设置为1s，势必会产生大量的segment，检索性能会受到影响。
所以，非实时的场景可以调大，设置为30s，甚至-1。

###  **4.2 减少副本，提升写入性能。
写入前，副本数设置为0，
写入后，副本数设置为原来值。

###  **4.3 能批量就不单条写入
批量接口为bulk，批量的大小要结合队列的大小，而队列大小和线程池大小、机器的cpu核数。

###  **4.4 禁用swap
在Linux系统上，通过运行以下命令临时禁用交换：sudo swapoff -a

### **5、检索聚合优化实战**

### **5.1 禁用 wildcard模糊匹配**

数据量级达到TB+甚至更高之后，wildcard在多字段组合的情况下很容易出现卡死，甚至导致集群节点崩溃宕机的情况。

后果不堪设想。

替代方案：
方案一：针对精确度要求高的方案:两套分词器结合，standard和ik结合，使用match_phrase检索。
方案二：针对精确度要求不高的替代方案：建议ik分词，通过match_phrase和slop结合查询。

###  **5.2极小的概率使用match匹配**
中文match匹配显然结果是不准确的。很大的业务场景会使用短语匹配“match_phrase&quot;。
match_phrase结合合理的分词词典、词库，会使得搜索结果精确度更高，避免噪音数据。

###  **5.3 结合业务场景，大量使用filter过滤器**
对于不需要使用计算相关度评分的场景，无疑filter缓存机制会使得检索更快。
举例：过滤某邮编号码。

### **5.4控制返回字段和结果**

和mysql查询一样，业务开发中，select * 操作几乎是不必须的。
同理，ES中，_source 返回全部字段也是非必须的。
要通过_source 控制字段的返回，只返回业务相关的字段。
网页正文content，网页快照html_content类似字段的批量返回，可能就是业务上的设计缺陷。

显然，摘要字段应该提前写入，而不是查询content后再截取处理。

### **5.5 分页深度查询和遍历**
分页查询使用：from+size;
遍历使用：scroll；
并行遍历使用：scroll+slice。

斟酌集合业务选型使用。

### **5.6 聚合Size的合理设置**

聚合结果是不精确的。除非你设置size为2的32次幂-1，否则聚合的结果是取每个分片的Top size元素后综合排序后的值。

实际业务场景要求精确反馈结果的要注意。
尽量不要获取全量聚合结果——从业务层面取TopN聚合结果值是非常合理的。因为的确排序靠后的结果值意义不大。

### **5.7 聚合分页合理实现**

聚合结果展示的时，势必面临聚合后分页的问题，而ES官方基于性能原因不支持聚合后分页。

如果需要聚合后分页，需要自开发实现。包含但不限于：

方案一：每次取聚合结果，拿到内存中分页返回。

方案二：scroll结合scroll after集合redis实现。

### **6、业务优化**
让Elasticsearch做它擅长的事情，很显然，它更擅长基于倒排索引进行搜索。
业务层面，用户想最快速度看到自己想要的结果，中间的“字段处理、格式化、标准化”等一堆操作，用户是不关注的。
为了让Elasticsearch更高效的检索，建议：
1）要做足“前戏”
字段抽取、倾向性分析、分类/聚类、相关性判定放在写入ES之前的ETL阶段;

2）“睡服”产品经理
产品经理基于各种奇葩业务场景可能会提各种无理需求。

作为技术人员，要“通知以情晓之以理”，给产品经理讲解明白搜索引擎的原理、Elasticsearch的原理，哪些能做，哪些真的“臣妾做不到”。
</code></pre>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image\微信图片_20200315214927.jpg" alt="微信图片_20200315214927"></p>
<h2 id="presto优化实践"><a href="#presto优化实践" class="headerlink" title="presto优化实践"></a><strong>presto优化实践</strong></h2><pre><code>
</code></pre>
<h2 id="hbase参数优化"><a href="#hbase参数优化" class="headerlink" title="hbase参数优化"></a><strong>hbase参数优化</strong></h2><p><strong>zookeeper.session.timeout</strong></p>
<pre><code class="java">默认值：3分钟（180000ms）
说明：RegionServer与Zookeeper间的连接超时时间。当超时时间到后，ReigonServer会被Zookeeper从RS集群清单中移除，HMaster收到移除通知后，会对这台server负责的regions重新balance，让其他存活的RegionServer接管.

调优：这个timeout决定了RegionServer是否能够及时的failover。设置成1分钟或更低，可以减少因等待超时而被延长的failover时间。
不过需要注意的是，对于一些Online应用，RegionServer从宕机到恢复时间本身就很短的（网络闪断，crash等故障，运维可快速介入），如果调低timeout时间，反而会得不偿失。因为当ReigonServer被正式从RS集群中移除时，HMaster就开始做balance了（让其他RS根据故障机器记录的WAL日志进行恢复）。当故障的RS在人工介入恢复后，这个balance动作是毫无意义的，反而会使负载不均匀，给RS带来更多负担。特别是那些固定分配regions的场景。
</code></pre>
<p><strong>hbase.regionserver.handler.count</strong></p>
<pre><code class="java">默认值：10
说明：RegionServer的请求处理IO线程数。

调优：
这个参数的调优与内存息息相关。
较少的IO线程，适用于处理单次请求内存消耗较高的Big PUT场景（大容量单次PUT或设置了较大cache的scan，均属于Big PUT）或ReigonServer的内存比较紧张的场景。
较多的IO线程，适用于单次请求内存消耗低，TPS要求非常高的场景。设置该值的时候，以监控内存为主要参考。
这里需要注意的是如果server的region数量很少，大量的请求都落在一个region上，因快速充满memstore触发flush导致的读写锁会影响全局TPS，不是IO线程数越高越好。
压测时，开启Enabling RPC-level logging，可以同时监控每次请求的内存消耗和GC的状况，最后通过多次压测结果来合理调节IO线程数。
这里是一个案例?Hadoop and HBase Optimization for Read Intensive Search Applications，作者在SSD的机器上设置IO线程数为100，仅供参考。
</code></pre>
<p><strong>hbase.hregion.max.filesize</strong></p>
<pre><code class="java">默认值：256M
说明：在当前ReigonServer上单个Reigon的最大存储空间，单个Region超过该值时，这个Region会被自动split成更小的region。

调优：
1：小region对split和compaction友好，因为拆分region或compact小region里的storefile速度很快，内存占用低。缺点是split和compaction会很频繁。
2：特别是数量较多的小region不停地split, compaction，会导致集群响应时间波动很大，region数量太多不仅给管理上带来麻烦，甚至会引发一些Hbase的bug。
3：一般512以下的都算小region。
4：大region，则不太适合经常split和compaction，因为做一次compact和split会产生较长时间的停顿，对应用的读写性能冲击非常大。此外，大region意味着较大的storefile，compaction时对内存也是一个挑战。
5：当然，大region也有其用武之地。如果你的应用场景中，某个时间点的访问量较低，那么在此时做compact和split，既能顺利完成split和compaction，又能保证绝大多数时间平稳的读写性能。
6：既然split和compaction如此影响性能，有没有办法去掉？
7：compaction是无法避免的，split倒是可以从自动调整为手动。
8：只要通过将这个参数值调大到某个很难达到的值，比如100G，就可以间接禁用自动split（RegionServer不会对未到达100G的region做split）。
9：再配合RegionSplitter这个工具，在需要split时，手动split。
10：手动split在灵活性和稳定性上比起自动split要高很多，相反，管理成本增加不多，比较推荐online实时系统使用。
11：内存方面，小region在设置memstore的大小值上比较灵活，大region则过大过小都不行，过大会导致flush时app的IO wait增高，过小则因store file过多影响读性能。
</code></pre>
<p><strong>hbase.regionserver.global.memstore.upperLimit&#x2F;lowerLimit</strong></p>
<pre><code class="java">默认值：0.4/0.35

upperlimit说明：hbase.hregion.memstore.flush.size 这个参数的作用是当单个Region内所有的memstore大小总和超过指定值时，flush该region的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模式来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。
这个参数的作用是防止内存占用过大，当ReigonServer内所有region的memstores所占用内存总和达到heap的40%时，HBase会强制block所有的更新并flush这些region以释放所有memstore占用的内存。
lowerLimit说明： 同upperLimit，只不过lowerLimit在所有region的memstores所占用内存达到Heap的35%时，不flush所有的memstore。它会找一个memstore内存占用最大的region，做个别flush，此时写更新还是会被block。lowerLimit算是一个在所有region强制flush导致性能降低前的补救措施。在日志中，表现为 “** Flush thread woke up with memory above low water.”

调优：
这是一个Heap内存保护参数，默认值已经能适用大多数场景。
参数调整会影响读写，如果写的压力大导致经常超过这个阀值，则调小读缓存hfile.block.cache.size增大该阀值，或者Heap余量较多时，不修改读缓存大小。
如果在高压情况下，也没超过这个阀值，那么建议你适当调小这个阀值再做压测，确保触发次数不要太多，然后还有较多Heap余量的时候，调大hfile.block.cache.size提高读性能。
还有一种可能性是?hbase.hregion.memstore.flush.size保持不变，但RS维护了过多的region，要知道 region数量直接影响占用内存的大小。
</code></pre>
<p><strong>hfile.block.cache.size</strong></p>
<pre><code class="java">默认值：0.2
说明：storefile的读缓存占用Heap的大小百分比，0.2表示20%。该值直接影响数据读的性能。

调优：
当然是越大越好，如果写比读少很多，开到0.4-0.5也没问题。如果读写较均衡，0.3左右。如果写比读多，果断默认吧。设置这个值的时候，你同时要参考?
hbase.regionserver.global.memstore.upperLimit?，该值是memstore占heap的最大百分比，两个参数一个影响读，一个影响写。如果两值加起来超过80-90%，会有OOM的风险，谨慎设置。
</code></pre>
<p><strong>hbase.hstore.blockingStoreFiles</strong></p>
<pre><code class="java">默认值：
说明：在flush时，当一个region中的Store（Coulmn Family）内有超过7个storefile时，则block所有的写请求进行compaction，以减少storefile数量。

调优：block写请求会严重影响当前regionServer的响应时间，但过多的storefile也会影响读性能。从实际应用来看，为了获取较平滑的响应时间，可将值设为无限大。如果能容忍响应时间出现较大的波峰波谷，那么默认或根据自身场景调整即可。
</code></pre>
<p><strong>hbase.hregion.memstore.block.multiplier</strong></p>
<pre><code class="java">默认值：2
说明：当一个region里的memstore占用内存大小超过hbase.hregion.memstore.flush.size两倍的大小时，block该region的所有请求，进行flush，释放内存。虽然我们设置了region所占用的memstores总内存大小，比如64M，但想象一下，在最后63.9M的时候，我Put了一个200M的数据，此时memstore的大小会瞬间暴涨到超过预期的hbase.hregion.memstore.flush.size的几倍。这个参数的作用是当memstore的大小增至超过hbase.hregion.memstore.flush.size 2倍时，block所有请求，遏制风险进一步扩大。

调优： 
这个参数的默认值还是比较靠谱的。如果你预估你的正常应用场景（不包括异常）不会出现突发写或写的量可控，那么保持默认值即可。如果正常情况下，你的写请求量就会经常暴长到正常的几倍，那么你应该调大这个倍数并调整其他参数值，比如
hfile.block.cache.sizehbase.regionserver.global.memstore.upperLimit/lowerLimit，以预留更多内存，防止HBase server OOM。
</code></pre>
<p><strong>hbase.hregion.memstore.mslab.enabled</strong></p>
<pre><code class="java">默认值：true
说明：减少因内存碎片导致的Full GC，提高整体性能。
</code></pre>
<p><strong>其它参数</strong></p>
<pre><code class="python">hbase.regionserver.hlog.splitlog.writer.threads：默认值是3，建议设为10，日志切割所用的线程数

hbase.snapshot.enabled：快照功能，默认是false(不开启)，建议设为true，特别是对某些关键的表，定时用快照做备份是一个不错的选择。

hbase.hregion.majorcompaction：hbase的region主合并的间隔时间，默认为1天，建议设置为0，禁止自动的major主合并，major合并会把一个store下所有的storefile重写为一个storefile文件，在合并过程中还会把有删除标识的数据删除，在生产集群中，主合并能持续数小时之久，为减少对业务的影响，建议在业务低峰期进行手动或者通过脚本或者api定期进行major合并。

hbase.hregion.memstore.flush.size：默认值128M，单位字节，一旦有memstore超过该值将被flush，如果regionserver的jvm内存比较充足(16G以上)，可以调整为256M。

hbase.hstore.compaction.min：默认值为3，如果任何一个store里的storefile总数超过该值，会触发默认的合并操作，可以设置5~8，在手动的定期major compact中进行storefile文件的合并，减少合并的次数，不过这会延长合并的时间，以前的对应参数为hbase.hstore.compactionThreshold。
HStore的storeFile数量&gt;= compactionThreshold配置的值，则可能会进行compact，默认值为3，可以调大，比如设置为6，在定期的major compact中进行剩下文件的合并。

hbase.hstore.compaction.max：默认值为10,一次最多合并多少个storefile，避免OOM。

hbase.hstore.blockingStoreFiles：默认为7，如果任何一个store(非.META.表里的store)的storefile的文件数大于该值，则在flush memstore前先进行split或者compact，同时把该region添加到flushQueue，延时刷新，这期间会阻塞写操作直到compact完成或者超过hbase.hstore.blockingWaitTime(默认90s)配置的时间，可以设置为30，避免memstore不及时flush。当regionserver运行日志中出现大量的“Region has too many store files; delaying flush up to 90000ms”时，说明这个值需要调整了

hbase.regionserver.thread.compaction.small：默认值为1，regionserver做Minor Compaction时线程池里线程数目,可以设置为5。

hbase.regionserver.thread.compaction.large：默认值为1，regionserver做Major Compaction时线程池里线程数目，可以设置为8。

hbase.regionserver.lease.period：默认值60000(60s)，客户端连接regionserver的租约超时时间，客户端必须在这个时间内汇报，否则则认为客户端已死掉。这个最好根据实际业务情况进行调整

hbase.client.write.buffer：默认为2M，写缓存大小，推荐设置为5M，单位是字节，当然越大占用的内存越多，此外测试过设为10M下的入库性能，反而没有5M好

hbase.client.pause：默认是1000(1s),如果你希望低延时的读或者写，建议设为200，这个值通常用于失败重试，region寻找等

hbase.client.retries.number：默认值是10，客户端最多重试次数,可以设为11，结合上面的参数，共重试时间71s

hbase.ipc.client.tcpnodelay：默认是false，建议设为true，关闭消息缓冲

hbase.client.scanner.caching：scan缓存，默认为1，避免占用过多的client和rs的内存，一般1000以内合理，如果一条数据太大，则应该设置一个较小的值，通常是设置业务需求的一次查询的数据条数
    
export HBASE_OPTS=&quot;$HBASE_OPTS -XX:+UseCompressedOops -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSParallelRemarkEnabled  -XX:CMSInitiatingOccupancyFraction=75 -XX:SoftRefLRUPolicyMSPerMB=0&quot;
    
xport HBASE_REGIONSERVER_OPTS=&quot;-Xms36g -Xmx36g -Xmn1g -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=15 -XX:CMSInitiatingOccupancyFraction=70 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/data/logs/gc-$(hostname)-hbase.log&quot;
    
hfile.block.index.cacheonwrite
在index写入的时候允许put无根（non-root）的多级索引块到block cache里，默认是false，设置为true，或许读性能更好，但是是否有副作用还需调查。

io.storefile.bloom.cacheonwrite
默认为false，需调查其作用。

hbase.regionserver.regionSplitLimit
控制最大的region数量，超过则不可以进行split操作，默认是Integer.MAX，可设置为1，禁止自动的split，通过人工，或者写脚本在集群空闲时执行。如果不禁止自动的split，则当region大小超过hbase.hregion.max.filesize时会触发split操作（具体的split有一定的策略，不仅仅通过该参数控制，前期的split会考虑region数据量和memstore大小），每次flush或者compact之后，regionserver都会检查是否需要Split，split会先下线老region再上线split后的region，该过程会很快，但是会存在两个问题：1、老region下线后，新region上线前client访问会失败，在重试过程中会成功但是如果是提供实时服务的系统则响应时长会增加；2、split后的compact是一个比较耗资源的动作。   
</code></pre>
<h2 id="linux高频命令"><a href="#linux高频命令" class="headerlink" title="linux高频命令"></a><strong>linux高频命令</strong></h2><pre><code class="shell">1、查找文件
find / -name filename.txt 根据名称查找/目录下的filename.txt文件。

find . -name &quot;*.xml&quot; 递归查找所有的xml文件

find . -name &quot;*.xml&quot; |xargs grep &quot;hello world&quot; 递归查找所有文件内容中包含hello world的xml文件

grep -H &#39;spring&#39; *.xml 查找所以有的包含spring的xml文件

find ./ -size 0 | xargs rm -f &amp; 删除文件大小为零的文件

ls -l | grep &#39;.jar&#39; 查找当前目录中的所有jar文件

grep &#39;test&#39; d* 显示所有以d开头的文件中包含test的行。

grep &#39;test&#39; aa bb cc 显示在aa，bb，cc文件中匹配test的行。

grep &#39;[a-z]\&#123;5\&#125;&#39; aa 显示所有包含每个字符串至少有5个连续小写字符的字符串的行。

2、查看一个程序是否运行
ps -ef|grep tomcat 查看所有有关tomcat的进程

3、终止线程
kill -9 19979 终止线程号位19979的进程

4、查看文件，包含隐藏文件
ls -al

5、当前工作目录
pwd

6、复制文件
cp source dest 复制文件

cp -r sourceFolder targetFolder 递归复制整个文件夹

scp sourecFile romoteUserName@remoteIp:remoteAddr 远程拷贝

7、创建目录
mkdir newfolder

8、删除目录
rmdir deleteEmptyFolder 删除空目录

rm -rf deleteFile 递归删除目录中所有内容

9、移动文件
mv /temp/movefile /targetFolder

10、重命名
mv oldNameFile newNameFile

11、切换用户
su -username

12、修改文件权限
chmod 777 file.java file.java 的权限-rwxrwxrwx，r表示读、w表示写、x表示可执行

13、压缩文件
tar -czf test.tar.gz /test1 /test2
zip -r ./xxx.zip  fileDir

14、列出压缩文件列表
tar -tzf test.tar.gz

15、解压文件
tar -xvzf test.tar.gz
unzip xxx.zip

16、查看文件头10行
head -n 10 example.txt

17、查看文件尾10行
tail -n 10 example.txt

18、查看日志类型文件
tail -f exmaple.log 这个命令会自动显示新增内容，屏幕只显示10行内容的（可设置）。

19、使用超级管理员身份执行命令
sudo rm a.txt 使用管理员身份删除文件

20、查看端口占用情况
netstat -tln | grep 8080 查看端口8080的使用情况
netstat -anltp | grep 端口号
netstat -ap | grep ssh

21、查看端口属于哪个程序
lsof -i :8080

22、查看进程
ps aux|grep java 查看java进程
ps aux 查看所有进程


23、以树状图列出目录的内容
tree a

24、文件下载
wget http://file.tgz

curl http://file.tgz

25、网络检测
ping www.just-ping.com

26、 显示每个文件和目录的磁盘使用空间，也就是文件的大小。
du -h 以K  M  G为单位显示，提高可读性（最常用的一个）

27、显示磁盘分区上可以使用的磁盘空间
df -h 使用-h选项以KB、MB、GB的单位来显示，可读性高

28、统计文件命令
wc -l filename 统计文件的行数
wc passwd fileName 同时统计多个文件

29、文本编辑命令
sed -i &#39;1d&#39; fileName   删除第一行 
sed &#39;3ahello&#39; 1.txt    向第三行后面添加hello，3表示行号

30、让某个程序在后台运行
nohup ./start-dishi.sh &gt;output 2&gt;&amp;1 &amp;
nohup ./start-dishi.sh &gt; /dev/null &amp;    /dev/null文件的作用，这是一个无底洞，任何东西都可以定向到这里，但是却无法打开
 00 01 * * * /bin/sh/server/scripts/mysqlbak.sh &gt;/dev/null 2&gt;&amp;1
 
31、
lsof -i:port 查看指定端口有哪些进程在使用
lsof  -a -u root -d txt查看root 用户进程所打开的文件类型为txt的文件
losf -p pid 通过进程号显示该进行打开的文件
losf -p ^pid 列出除了某个进程号，其他进程号所打开的文件信息
lsof -a -u username -i 列出某个用户的所有活跃网络端口

32、执行telnet指令开启终端机阶段作业，并登入远端主机。
telnet 192.168.120.206
telnet www.baidu.com
service xinetd restart 启动telnet命令

33、反向输出，与cat相反
tac test
cat -n test  展示文件内容并显示行号
-n rz ：行号在自己栏位的最右方显示，且加 0 ；   nl -b a -n rz log2014.log

34、比较文件的区别
diff -y x.txt y.txt   以并列的方式显示文件的异同之处；
diff  -e  1.txt  2.txt  &gt; script.txt   生成一个编辑角本，作为ex 或ed 的输入可将文件1 转换成文件2
-r：当diff的参数为文件夹时，diff会遍历整个文件夹对新旧文件夹下同名的文件进行比较
-w：忽略所有空格和制表符，将所有其他空白字符串视为一致。例如，if ( a == b ) 与 if(a==b) 相等。
-i：忽略字母大小写。例如，小写 a 被认为同大写 A 一样。

35 截取字符串
echo 123:456:789 | cut -c3  截取第三个字符
echo 123:456:789 | cut -c-3 截取前三个字符
echo 123:456:789 | cut -c3-6,8-10 提取第三到第六和第八到第十间的字符
date |cut -b 1-6
cat tmp.cc |&gt;&gt;tmp.cc |&gt;&gt;tmp.cc |&gt;&gt;tmp.cc | head -n10 |&gt;tmp.cc| cut -c7-7
cut -c-10 test.txt 截取文件的前10行
 
ss -an | grep:80    查看80端口的tcp状态
cut：将文件的每一行按指定分隔符分割并输出。
split：分割文件为不同的小片段。
paste：按行合并文件内容。
sort：对文件的文本内容排序。
uniq：去除重复行。oldboy
wc：统计文件的行数、单词数或字节数。
iconv：转换文件的编码格式。
dos2unix：将DOS格式文件转换成UNIX格式。
diff：全拼difference，比较文件的差异，常用于文本文件。
vimdiff：命令行可视化文件比较工具，常用于文本文件。
rev：反向输出文件内容。
grep/egrep：过滤字符串，三剑客老三。
join：按两个文件的相同字段合并。
tr：替换或删除字符。
vi/vim：命令行文本编辑器。

36 批量杀死进程
ps -ef|grep check_os.sh | grep -v grep | awk &#39;&#123;print $2&#125;&#39; | xargs kill -9
$2表示第2列,即进程号PID; 
grep -v grep是列出除开grep命令本身的进程,grep iboss2确认进程关键字
kill -9 强杀进程;
xargs 使用上一个操作的结果作为下一个命令的参数使用

### hdfs 命令
hdfs dfs -cat file | wc -l       显示文件行数
hdfs dfs -cat file | head -5     显示文件前5行
hdfs dfs -du -h  file          显示文件大小
hdfs dfs -tail -5 file         显示文件后5行
hdfs dfs -getmerge &lt; src&gt; &lt; localdst&gt; [addnl]   将源目录和目标文件作为输入，并将src中的文件连接到目标本地文件（把两个文件的内容合并起来）
hdfs dfs -cat file | grep 过滤字段   从hdfs上过滤包含某个字符的行内容
hadoop fs -appendToFile /home/dataflair/Desktop/sample /user/dataflair/dir1   将一个或者多个文件添加到HDFS系统中，他也是从标准输入中读取，然后添加到目标文件系统汇总
hdfs fs -appedToFile ./hello.txt /input/hello.txt  ---追加一个文件到另一个文件到末尾
hdfs job -kill -task &lt;task-id&gt;   杀死任务。被杀死的任务不会不利于失败尝试。
hdfs fsck &lt;path&gt; -delete   删除受损文件
hdfs jar file.jar 执行jar包程序
hdfs job -submit &lt;job-file&gt;  提交作业
hdfs job -kill &lt;job-id&gt;  杀死指定作业。
hdfs fsck &lt;path&gt; -locations     打印出每个块的位置信息。
hdfs fsck &lt;path&gt; -blocks     打印出块信息报告
</code></pre>
<h2 id="Hbase性能优化"><a href="#Hbase性能优化" class="headerlink" title="Hbase性能优化"></a><strong>Hbase性能优化</strong></h2><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image\v2-c4b89ad08f557e7a2d231dad007e1ef0_720w.jpg" alt="v2-c4b89ad08f557e7a2d231dad007e1ef0_720w"></p>
<pre><code class="python">### HBase写入优化
客户端优化

## **批量写**
采用批量写，可以减少客户端到RegionServer之间的RPC的次数，提高写入性能。批量写请求要么全部成功返回，要么抛出异常。
HTable.put(List&lt;Put&gt;);

## **异步批量提交**
如果业务可以接受异常情况下丢失少量数据，可以使用异步批量提交方式提交请求。
用户提交写请求之后，数据会先写入客户端缓存，并返回用户写入成功（此时数据并为提交到RegionServer），当客户端缓存达到阈值（默认2M，可通过hbase.client.write.buffer配置）时才会批量提交给RegionServer。需要注意的是，在某些情况下客户端异常的情况下缓存数据有可能丢失。

HTable.setWriteBufferSize(writeBufferSize); // 设置缓存大小
HTable.setAutoFlush(false);

## **多线程并发写**

客户端开启多个HTable写线程，每个写线程负责一个HTable对象的flush操作，这样结合定时flush和写buffer，可以即保证在数据量小的时候，数据可以在较短时间内被flush，同时又保证在数据量大的时候，写buffer一满就即使进行flush。

## **使用BulkLoad写入**
在HBase中数据都是以HFile形式保存在HDFS中的，当有大量数据需要写入到HBase的时候，可以采用BulkLoad方式完成。
使用MapReduce或者Spark直接生成HFile格式的数据文件，然后再通过RegionServer将HFile数据文件移动到相应的Region上去。

写数据表设计调优

## **COMPRESSION**
配置数据的压缩算法，这里的压缩是HFile中block级别的压缩。对于可以压缩的数据，配置压缩算法可以有效减少磁盘的IO，从而达到提高性能的目的。但是并不是所有数据都可以进行有效压缩，如图片，因为图片一般是已经压缩后的数据，所以压缩效果有限。常用的压缩算法是SNAPPY，因为它有较好的压缩和解压速度和可以接受的压缩率。

## **IN_MEMORY**
配置表的数据优先缓存在内存中，这样可以有效提升读取的性能。适合小表，而且需要频繁进行读取操作的。

## **预分区**

在HBase中数据是分布在各个Region中的，每个Region都负责一个起始RowKey和结束Rowkey的范围，在向HBase中写数据的时候，会根据RowKey请求到对应的Region上，如果写请求都集中在某一个Region或某几个Region上的时候，性能肯定不如写请求均匀分布在各个Region上好。默认情况下，创建的HBase的只有一个Region分区，会随着数据量的变大，进行split，拆分成多个Region，最开始的性能肯定会很不好

建议在设计HBase的的时候，进行预分区，并设计一个良好的Rowkey生成规则（关于RowKey设计，可以参考《一篇文章带你快速搞懂HBase RowKey设计》），尽量将数据分散到各个Region上，那样在进行HBase的读写的时候，对性能会有很好的改善。

## **合理设置WAL存储级别**

数据在写入HBase的时候，先写WAL，再写入缓存。通常情况下写缓存延迟很低，WAL机制一方面是为了确保数据即使写入缓存后数据丢失也可以通过WAL恢复，另一方面是为了集群之间的复制。默认WAL机制是开启的，并且使用的是同步机制写WAL。

如果业务不特别关心异常情况下部分数据的丢失，而更关心数据写入吞吐量，可考虑关闭WAL写，这样可以提升2~3倍数据写入的吞吐量。
如果业务不能接受不写WAL，但是可以接受WAL异步写入，这样可以带了1~2倍性能提升。

HBase中可以通过设置WAL的持久化等级决定是否开启WAL机制、以及HLog的落盘方式。
WAL的持久化等级分为如下四个等级：

SKIP_WAL：只写缓存，不写HLog日志。这种方式因为只写内存，因此可以极大的提升写入性能，但是数据有丢失的风险。在实际应用过程中并不建议设置此等级，除非确认不要求数据的可靠性。
ASYNC_WAL：异步将数据写入HLog日志中。
SYNC_WAL：同步将数据写入日志文件中，需要注意的是数据只是被写入文件系统中，并没有真正落盘，默认。
FSYNC_WAL：同步将数据写入日志文件并强制落盘。最严格的日志写入等级，可以保证数据不会丢失，但是性能相对比较差。
同样，除了在创建表的时候直接设置WAL存储级别，也可以通过客户端设置WAL持久化等级，代码：
put.setDurability(Durability.SYNC_WAL);


### HBase读取优化
客户端优化

## **批量get请求**
使用批量请求，可以减少RPC的次数，显著提高吞吐量。需要注意的是，批量get请求要么成功返回所有请求数据，要么抛出异常。
Result[] re= table.get(List&lt;Get&gt; gets);

## **合理何止scan缓存大小**
一次scan可能会返回大量数据，但是实际客户端发起一次scan请求，并不会将所有数据一次性加载到本地，而是分成多次RPC请求进行加载，这样设计一方面是因为大量数据请求可能会导致网络带宽严重消耗进而影响其他业务，另一方面是有可能因为数据量太大导致客户端发生OOM。所以采用先加载一部分数据到本地，然后进行遍历，每次加载一部分数据，如此往复，直至所有数据加载完成。数据加载到本地就存放在scan缓存中，默认100。

增大scan的缓存，可以让客户端减少一次scan的RPC次数，从而从整体上提升数据读取的效率。
scan.setCaching(int caching); //大scan可以设置为1000

## **指定请求列族或者列名**
HBase是列族数据库，同一列族的数据存储在一块，不同列族是分开存储的，如果一个表由多个列族，只是根据RowKey而不指定列族进行检索的话，不同列族的数据需要独立进行检索，性能必然会比指定列族的查询差的多。

此外指定请求的列的话，不需要将整个列族的所有列的数据返回，这样就减少了网路IO。
scan.addColumn();

## **设置只读Rowkey过滤器**
在只需要Rowkey数据时，可以为Scan添加一个只读取Rowkey的filter（FirstKeyOnlyFilter或KeyOnlyFilter）。

## **关闭ResultScanner**
在使用table.getScanner之后，记得关闭，否则它会和服务器端一直保持连接，资源无法释放，从而导致服务端的某些资源不可用。
scanner.close();

## **离线计算访问HBase建议禁用缓存**
当离线访问HBase时，往往会对HBase表进行扫描，此时读取的数据没有必要存放在BlockCache中，否则会降低扫描的效率。
scan.setBlockCache(false);
建议在对HBase表进行扫描时禁用缓存。
对于频繁查询HBase的应用场景不需要禁用缓存，并且可以考虑在应用程序和HBase之间加一层缓存系统（如Redis），先查询缓存，缓存没有命中再去查询HBase。

读数据表设计调优

## **COMPRESSION**
同写性能优化COMPRESSION部分。

## **BLOCKSIZE**
配置HFile中block块的大小，不同的block大小，可以影响HBase读写数据的效率。越大的block块，配置压缩算法，压缩的效率就越好；但是由于HBase的读取数据时以block块为单位的，所以越大的block块，对于随机读的情况，性能可能会比较差，如果要提升写入的性能，一般扩大到128kb或者256kb，可以提升写数据的效率，也不会影响太大的随机读性能。

## **DATA_BLOCK_ENCODING**
配置HFile中block块的编码方法。当一行数据中存在多个列时，一般可以配置为&quot;FAST_DIFF&quot;，可以有效的节省数据存储的空间，从而提升性能。

## **BloomFilter**
优化原理：BloomFilter主要用来过滤不存在待检索RowKey或者Row-Col的HFile文件，避免无用的IO操作。它会告诉你在这个HFile文件中是否可能存在待检索的KeyValue，如果不存在，就可以不用小号IO打开文件进行seek。通过设置BloomFilter可以提升读写的性能。

BloomFilter是一个列族级别的配置属性，如果列族设置了BloomFilter，那么HBase会在生成StoreFile时包含一份BloomFilter的结构的数据，称为MetaBlock（一旦写入就无法更新）。MetaBlock和DataBlock（真实的KeyValue数据）一起由LRUBlockCache维护，所以开启了BloomFilter会有一定的存储即内存cache开销。

HBase利用BloomFilter可以节省必须读磁盘过程，可以提高随机读（get）的性能，但是对于顺序读（scan）而言，设置BloomFilter是没有作用的（0.92版本以后，如果设置了BloomFilter为ROWCOL，对于执行了qualifier的scan有一定的优化）

BloomFilter取值有两个，ROW和ROWCOL，需要根据业务来确定具体使用哪种。

如果业务大多数随机查询仅仅使用row作为查询条件，BloomFilter一定要设置为ROW。
如果大多数随机查询使用row+col作为查询条件，BloomFilter需要设置为ROWCOL。
如果不确定业务查询类型，设置为ROW。

## **预分区**
同写性能优化预分区部分。


HBase服务端调优

##	**GC_OPTS**
HBase是利用内存完成读写操作。提高HBase内存可以有效提高HBase性能。GC_OPTS主要需要调整HeapSize和NewSize的大小。调整HeapSize大小的时候，建议将Xms和Xmx设置成相同的值，这样可以避免JVM动态调整HeapSize大小的时候影响性能。调整NewSize大小的时候，建议把其设置为HeapSize大小的1/9。
当HBase集群规模越大，Region数量越多时，可以适当调大HMaster的GC_OPTS参数
RegionServer需要比HMaster更大的内存，在内存充足的情况下，HeapSize可以相对设置大一些。

HMaster的HeapSize为4G的时候，HBase集群可以支持100000个Region的规模。根据经验值，单个RegionServer的HeapSize不建议超过20GB。
# HMaster、RegionServer GC_OPTS配置如下：
HMaster: -Xms2G -Xmx2G -XX:NewSize=256M -XX:MaxNewSize=256M 
RegionServer: -Xms4G -Xmx4G -XX:NewSize=512M -XX:MaxNewSize=512M
            
## **RegionServer并发请求处理数量**
hbase.regionserver.handler.count表示RegionServer在同一时刻能够并发处理多少请求。如果设置过高会导致激烈的线程竞争，如果设置过小，请求将会在RegionServer长时间等待，降低处理能力。应该根据资源情况，适当增加处理线程数。
建议根据CPU的使用情况，可以设置为100至300之间的值。

## **控制MemStore的大小**
hbase.hregion.memstore.flush.size默认值128M，单位字节，一旦有MemStore超过该值将被flush，如果regionserver的jvm内存比较充足(16G以上)，可以调整为256M。在内存足够put负载大情况下可以调整增大。

## **BlockCache优化**
BlockCache作为读缓存，合理设置对于提高读性能非常重要。默认情况下，BlockCache和MemStore的配置各占40%，可以根据集群业务进行修正，比如读多写少业务可以将BlockCache占比调大。另外BlockCache的策略也很重要，不同策略对读性能来说影响并不大，但是对GC的影响 却很显著。

HBase缓存区大小，主要影响查询性能。根据查询模式以及查询记录分布情况来决定缓存区的大小。如果采用随机查询使得缓存区的命中率较低，可以适当降低缓存大小。
hfile.block.cache.size，默认0.4，用来提高读性能
hbase.regionserver.global.memstore.size，默认0.4，用来提高写性能

## **控制HFile个数**
MemStore在flush之前，会进行StoreFile的文件数量校验（通过hbase.hstore.blockingStoreFiles参数配置），如果大于设定值，系统将会强制执行Compaction操作进行文件合并，在合并的过程中会阻塞MemStore的数据写入，等待其他线程将StoreFile进行合并。通常情况下发生在数据写入很快的情况下。
hbase.hstore.compactionThreshold表示启动Compaction的最低阈值，该值不能太大，否则会积累太多文件，一般建议设置为5～8左右。
hbase.hstore.blockingStoreFiles默认设置为7，可以适当调大一些。

## **Split优化**
hbase.hregion.max.filesize表示HBase中Region的文件总大小的最大值。当Region中的文件大于该参数时，将会导致Region分裂。
如果该参数设置过小时，可能会导致Split操作频繁
如果该参数设置过大时，会导致Compaction操作需要处理的文件个数增大，影响Compaction执行效率

## **Compaction优化**
hbase.hstore.compaction.min当一个Store中文件超过该值时，会进行Compaction，适当增大该值，可以减少文件被重复执行Compaction。但是如果过大，会导致Store中文件数过多而影响读取的性能。
hbase.hstore.compaction.max控制一次Compaction操作时的文件数据量的最大值。
hbase.hstore.compaction.max.size如果一个HFile文件的大小大于该值，那么在Minor Compaction操作中不会选择这个文件进行Compaction操作，除非进行Major Compaction操作。这个值可以防止较大的HFile参与Compaction操作。在禁止Major Compaction后，一个Store中可能存在几个HFile，而不会合并成为一个HFile，这样不会对数据读取造成太大的性能影响。

原则是：尽量要减小Compaction的次数和Compaction的执行时间

### **HBase 的解决方案-MSLAB**
MSLAB，全称是 MemStore-Local Allocation Buffer，是 Cloudera 在 HBase 0.90.1 时提交的一个 patch 里包含的特性。它基于 Arena Allocation 解决了 HBase 因Region flush 导致的内存碎片问题。

MSLAB 的实现原理（对照 Arena Allocation，HBase 实现细节）：
MemstoreLAB 为 Memstore 提供 Allocator。
创建一个 2M（默认）的 Chunk 数组和一个 chunk 偏移量，默认值为 0。
当 Memstore 有新的 KeyValue 被插入时，通过 KeyValue.getBuffer()取得 data bytes数组。将 data 复制到 Chunk 数组起始位置为 chunk 偏移量处，并增加偏移量=偏移量+data.length。
当一个 chunk 满了以后，再创建一个 chunk。
所有操作 lock free，基于CMS 原语。

优势：
KeyValue 原始数据在 minor gc 时被销毁。
数据存放在 2m 大小的 chunk 中，chunk 归属于 memstore。
flush 时，只需要释放多个 2m 的 chunks，chunk 未满也强制释放，从而为 Heap 腾出了多个 2M 大小的内存区间，减少碎片密集程度。
</code></pre>
<h2 id="Hbase之MemStore何时Flush-HFile-？"><a href="#Hbase之MemStore何时Flush-HFile-？" class="headerlink" title="Hbase之MemStore何时Flush HFile ？"></a><strong>Hbase之MemStore何时Flush HFile ？</strong></h2><pre><code class="java">1.Region级别的触发刷写。

（1）hbase.hregion.memstore.flush.size
  单个region内所有的memstore大小总和超过指定值时，flush该region的所有memstore。这里为什么是所有memsotre?因为一张表可能有多个CF，其对应的一个Region自然包含多个CF（即HStore），每个Store都有自己的memstore，这个配置值是所有的store的memstore的总和。当这个总和达到配置值时，即针对每个HSotre，都触发其Memstore，刷写成storefile(HFile的封装)文件。

（2）hbase.hstore.blockingStoreFiles 默认值：7
  说明：在flush时，当一个region中的Store（Coulmn Family）内有超过7个storefile时，则block所有的写请求进行compaction，以减少storefile数量。

  调优：block写请求会严重影响当前regionServer的响应时间，但过多的storefile也会影响读性能。从实际应用来看，为了获取较平滑的响应时间，可将值设为无限大。如果能容忍响应时间出现较大的波峰波谷，那么默认或根据自身场景调整即可。这个值设置比较大，会增加客户端的负载处理能力（即影响读取性能），但是如果你的服务器一直处于一个高的水平，那说明你的机器已经达到性能瓶颈，需要其他方式解决。

（3）hbase.hregion.memstore.block.multiplier默认值：2
说明：当一个region里总的memstore占用内存大小超过hbase.hregion.memstore.flush.size两倍的大小时，block该region的所有请求，进行flush，释放内存。虽然我们设置了region所占用的memstores总内存大小，比如64M，但想象一下，在最后63.9M的时候，我Put了一个200M的数据，此时memstore的大小会瞬间暴涨到超过预期的hbase.hregion.memstore.flush.size的几倍。这个参数的作用是当memstore的大小增至超过hbase.hregion.memstore.flush.size2倍时，block所有请求，遏制风险进一步扩大。

调优：这个参数的默认值还是比较靠谱的。如果你预估你的正常应用场景（不包括异常）不会出现突发写或写的量可控，那么保持默认值即可。如果正常情况下，你的写请求量就会经常暴长到正常的几倍，那么你应该调大这个倍数并调整其他参数值，比如hfile.block.cache.size和hbase.regionserver.global.memstore.upperLimit/lowerLimit，以预留更多内存，防止HBase server OOM。


2.RegionServer全局性的触发刷写。

（1）hbase.regionserver.global.memstore.upperLimit
当ReigonServer内所有region的memstores所占用内存总和达到heap的40%时，HBase会强制block所有的更新并flush这些region以释放所有memstore占用的内存。

（2）hbase.regionserver.global.memstore.lowerLimit
同upperLimit，只不过lowerLimit在所有region的memstores所占用内存达到Heap的35%时，不flush所有的memstore。它会找一个memstore内存占用最大的region，做个别flush，此时写更新还是会被block。lowerLimit算是一个在所有region强制flush导致性能降低前的补救措施。在日志中，表现为“** Flush thread woke up with memory above low water.”。

调优：这是一个Heap内存保护参数，默认值已经能适用大多数场景。

 
3. HLog (WAL)引起的regionserver全局性的触发刷写。

当数据被写入时会默认先写入Write-ahead Log(WAL)。WAL中包含了所有已经写入Memstore但还未Flush到HFile的更改(edits)。在Memstore中数据还没有持久化，当RegionSever宕掉的时候，可以使用WAL恢复数据。

若是关闭WAL，则在hbase-site.xml新增hbase.regionserver.hlog.enabled配置，设为false即可，不建议关闭。

当WAL(在HBase中成为HLog)变得很大的时候，在恢复的时候就需要很长的时间。因此，对WAL的大小也有一些限制，当达到这些限制的时候，就会触发Memstore的flush。Memstore flush会使WAL减少，因为数据持久化之后(写入到HFile)，就没有必要在WAL中再保存这些修改。有两个属性可以配置：
（1）hbase.regionserver.hlog.blocksize
（2）hbase.regionserver.maxlogs

WAL的最大值由hbase.regionserver.maxlogs*hbase.regionserver.hlog.blocksize (2GB by default)决定。一旦达到这个值，Memstore flush就会被触发。所以，当你增加Memstore的大小以及调整其他的Memstore的设置项时，你也需要去调整HLog的配置项。否则，WAL的大小限制可能会首先被触发，因而，你将利用不到其他专门为Memstore而设计的优化。抛开这些不说，通过WAL限制来触发Memstore的flush并非最佳方式，这样做可能会会一次flush很多Region，尽管“写数据”是很好的分布于整个集群，进而很有可能会引发flush“大风暴”。

提示：最好将hbase.regionserver.hlog.blocksize* hbase.regionserver.maxlogs设置为稍微大于hbase.regionserver.global.memstore.lowerLimit* HBASE_HEAPSIZE.
</code></pre>
<h2 id="StringUtils工具类"><a href="#StringUtils工具类" class="headerlink" title="StringUtils工具类"></a><strong>StringUtils工具类</strong></h2><pre><code class="Java">public static boolean isEmpty(String str)    
判断某字符串是否为空，为空的标准是 str==null 或 str.length()==0 

public static boolean isNotEmpty(String str) 
判断某字符串是否非空，等于 !isEmpty(String str) 

public static boolean isBlank(String str) 
判断某字符串是否为空或长度为0或由空白符(whitespace) 构成

public static boolean isNotBlank(String str) 
判断某字符串是否不为空且长度不为0且不由空白符(whitespace) 构成，等于!isBlank(String str)
    
public static String removeEnd(final String str, final String remove) &#123;
        if (isEmpty(str) || isEmpty(remove)) &#123;
            return str;
        &#125;
        if (str.endsWith(remove)) &#123;
            return str.substring(0, str.length() - remove.length());
        &#125;
        return str;
    &#125; 
 
public static String joinWith(final String separator, final Object... objects) &#123;
        if (objects == null) &#123;
            throw new IllegalArgumentException(&quot;Object varargs must not be null&quot;);
        &#125;

        final String sanitizedSeparator = defaultString(separator);

        final StringBuilder result = new StringBuilder();

        final Iterator&lt;Object&gt; iterator = Arrays.asList(objects).iterator();
        while (iterator.hasNext()) &#123;
            final String value = Objects.toString(iterator.next(), &quot;&quot;);
            result.append(value);

            if (iterator.hasNext()) &#123;
                result.append(sanitizedSeparator);
            &#125;
        &#125;

        return result.toString();
    &#125;

    public static boolean isWhitespace(final CharSequence cs) &#123;
        if (cs == null) &#123;
            return false;
        &#125;
        final int sz = cs.length();
        for (int i = 0; i &lt; sz; i++) &#123;
            if (!Character.isWhitespace(cs.charAt(i))) &#123;
                return false;
            &#125;
        &#125;
        return true;
    &#125;
        
  public static boolean isNumericSpace(final CharSequence cs) &#123;
        if (cs == null) &#123;
            return false;
        &#125;
        final int sz = cs.length();
        for (int i = 0; i &lt; sz; i++) &#123;
            if (!Character.isDigit(cs.charAt(i)) &amp;&amp; cs.charAt(i) != &#39; &#39;) &#123;
                return false;
            &#125;
        &#125;
        return true;
    &#125;

 public static boolean isNumeric(final CharSequence cs) &#123;
        if (isEmpty(cs)) &#123;
            return false;
        &#125;
        final int sz = cs.length();
        for (int i = 0; i &lt; sz; i++) &#123;
            if (!Character.isDigit(cs.charAt(i))) &#123;
                return false;
            &#125;
        &#125;
        return true;
    &#125;
 
 public static boolean isBlank(final CharSequence cs) &#123;
        int strLen;
        if (cs == null || (strLen = cs.length()) == 0) &#123;
            return true;
        &#125;
        for (int i = 0; i &lt; strLen; i++) &#123;
            if (!Character.isWhitespace(cs.charAt(i))) &#123;
                return false;
            &#125;
        &#125;
        return true;
    &#125;
        
 public static boolean isEmpty(String str) &#123;
        // 判断字符串是否为空或长度为0
        return str == null || str.length() == 0;
 &#125;

// 首字母转大写
public static String capitalize(final String str) &#123;
        final int strLen = length(str);
        if (strLen == 0) &#123;
            return str;
        &#125;

        final int firstCodepoint = str.codePointAt(0);
        final int newCodePoint = Character.toTitleCase(firstCodepoint);
        if (firstCodepoint == newCodePoint) &#123;
            // already capitalized
            return str;
        &#125;

        final int newCodePoints[] = new int[strLen]; // cannot be longer than the char array
        int outOffset = 0;
        newCodePoints[outOffset++] = newCodePoint; // copy the first codepoint
        for (int inOffset = Character.charCount(firstCodepoint); inOffset &lt; strLen; ) &#123;
            final int codepoint = str.codePointAt(inOffset);
            newCodePoints[outOffset++] = codepoint; // copy the remaining ones
            inOffset += Character.charCount(codepoint);
         &#125;
        return new String(newCodePoints, 0, outOffset);
    &#125;
 
    // 首字母转小写
public static String uncapitalize(final String str) &#123;
        final int strLen = length(str);
        if (strLen == 0) &#123;
            return str;
        &#125;

        final int firstCodepoint = str.codePointAt(0);
        final int newCodePoint = Character.toLowerCase(firstCodepoint);
        if (firstCodepoint == newCodePoint) &#123;
            // already capitalized
            return str;
        &#125;

        final int newCodePoints[] = new int[strLen]; // cannot be longer than the char array
        int outOffset = 0;
        newCodePoints[outOffset++] = newCodePoint; // copy the first codepoint
        for (int inOffset = Character.charCount(firstCodepoint); inOffset &lt; strLen; ) &#123;
            final int codepoint = str.codePointAt(inOffset);
            newCodePoints[outOffset++] = codepoint; // copy the remaining ones
            inOffset += Character.charCount(codepoint);
         &#125;
        return new String(newCodePoints, 0, outOffset);
    &#125;

// 大小写互换
public static String swapCase(final String str) &#123;
        if (isEmpty(str)) &#123;
            return str;
        &#125;

        final int strLen = str.length();
        final int newCodePoints[] = new int[strLen]; // cannot be longer than the char array
        int outOffset = 0;
        for (int i = 0; i &lt; strLen; ) &#123;
            final int oldCodepoint = str.codePointAt(i);
            final int newCodePoint;
            if (Character.isUpperCase(oldCodepoint)) &#123;
                newCodePoint = Character.toLowerCase(oldCodepoint);
            &#125; else if (Character.isTitleCase(oldCodepoint)) &#123;
                newCodePoint = Character.toLowerCase(oldCodepoint);
            &#125; else if (Character.isLowerCase(oldCodepoint)) &#123;
                newCodePoint = Character.toUpperCase(oldCodepoint);
            &#125; else &#123;
                newCodePoint = oldCodepoint;
            &#125;
            newCodePoints[outOffset++] = newCodePoint;
            i += Character.charCount(newCodePoint);
         &#125;
        return new String(newCodePoints, 0, outOffset);
    &#125; 
</code></pre>
<h2 id="Hive行专列和列转行"><a href="#Hive行专列和列转行" class="headerlink" title="Hive行专列和列转行"></a><strong>Hive行专列和列转行</strong></h2><pre><code class="python">### 列转行函数——collect_set和collect_list
hive里通常通过collect_set和collect_list来进行列转行，其中collect_list为不去重转换，collect_set为去重转换。
</code></pre>
<table>
<thead>
<tr>
<th align="center">学号</th>
<th align="center">姓名</th>
<th align="center">科目</th>
<th align="center">分数</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1001</td>
<td align="center">张三</td>
<td align="center">语文</td>
<td align="center">88</td>
</tr>
<tr>
<td align="center">1001</td>
<td align="center">张三</td>
<td align="center">数学</td>
<td align="center">87</td>
</tr>
<tr>
<td align="center">1001</td>
<td align="center">张三</td>
<td align="center">英语</td>
<td align="center">94</td>
</tr>
<tr>
<td align="center">1001</td>
<td align="center">张三</td>
<td align="center">历史</td>
<td align="center">86</td>
</tr>
<tr>
<td align="center">1001</td>
<td align="center">张三</td>
<td align="center">地理</td>
<td align="center">84</td>
</tr>
<tr>
<td align="center">1002</td>
<td align="center">李四</td>
<td align="center">语文</td>
<td align="center">78</td>
</tr>
<tr>
<td align="center">1002</td>
<td align="center">李四</td>
<td align="center">数学</td>
<td align="center">89</td>
</tr>
<tr>
<td align="center">1002</td>
<td align="center">李四</td>
<td align="center">英语</td>
<td align="center">75</td>
</tr>
<tr>
<td align="center">1002</td>
<td align="center">李四</td>
<td align="center">历史</td>
<td align="center">79</td>
</tr>
<tr>
<td align="center">1002</td>
<td align="center">李四</td>
<td align="center">地理</td>
<td align="center">68</td>
</tr>
<tr>
<td align="center">1003</td>
<td align="center">王五</td>
<td align="center">语文</td>
<td align="center">98</td>
</tr>
<tr>
<td align="center">1003</td>
<td align="center">王五</td>
<td align="center">数学</td>
<td align="center">97</td>
</tr>
<tr>
<td align="center">1003</td>
<td align="center">王五</td>
<td align="center">英语</td>
<td align="center">91</td>
</tr>
<tr>
<td align="center">1003</td>
<td align="center">王五</td>
<td align="center">历史</td>
<td align="center">93</td>
</tr>
<tr>
<td align="center">1003</td>
<td align="center">王五</td>
<td align="center">地理</td>
<td align="center">92</td>
</tr>
<tr>
<td align="center">1004</td>
<td align="center">朱六</td>
<td align="center">语文</td>
<td align="center">66</td>
</tr>
<tr>
<td align="center">1004</td>
<td align="center">朱六</td>
<td align="center">数学</td>
<td align="center">63</td>
</tr>
<tr>
<td align="center">1004</td>
<td align="center">朱六</td>
<td align="center">英语</td>
<td align="center">64</td>
</tr>
<tr>
<td align="center">1004</td>
<td align="center">朱六</td>
<td align="center">历史</td>
<td align="center">67</td>
</tr>
<tr>
<td align="center">1004</td>
<td align="center">朱六</td>
<td align="center">地理</td>
<td align="center">68</td>
</tr>
</tbody></table>
<pre><code class="SQL">–使用collect_set函数进行列转行查询
SELECT
stu_id,
stu_name,
concat_ws(’,’,collect_set(course)) as course,
concat_ws(’,’,collect_set(score)) as score
from student_score
group by stu_id,stu_name
</code></pre>
<p>查询出的结果为：<br>序号	stu_id	stu_name	course	score<br>1 1001	张三	语文,数学,英语,历史,地理	88,87,94,86,84<br>2 1002	李四	语文,数学,英语,历史,地理	78,89,75,79,68<br>3 1003	王五	语文,数学,英语,历史,地理	98,97,91,93,92<br>4 1004	朱六	语文,数学,英语,历史,地理	66,63,64,67,68</p>
<pre><code class="sql">建立一个新的学生表，使用上面列转行的结果来作为行转列的例表：
CREATE table student_score_new
as
SELECT
stu_id,
stu_name,
concat_ws(’,’,collect_set(course)) as course,
concat_ws(’,’,collect_set(score)) as score
from student_score
group by stu_id,stu_name;
</code></pre>
<pre><code class="python">### 行转列函数——explode和posexplode
explode函数可以把array或map格式的字段转换成行的形式，当然string形式的字段其实也可以转换，只需要用split函数把字段分割成一个数组的形式即可，比如我们可以把表格中的每个玩家的科目以列的形式查询出来：
</code></pre>
<pre><code class="sql">–使用explode函数进行列转行查询
SELECT stu_id,
stu_name,
ecourse
from student_score_new
lateral view explode(split(course,’,’)) cr as ecourse
</code></pre>
<p>查询结果如下：<br>序号	stu_id	stu_name	ecourse<br>1 1001	张三	语文<br>2 1001	张三	数学<br>3 1001	张三	英语<br>4 1001	张三	历史<br>5 1001	张三	地理<br>6 1002	李四	语文<br>7 1002	李四	数学<br>8 1002	李四	英语<br>9 1002	李四	历史<br>10 1002	李四	地理<br>11 1003	王五	语文<br>12 1003	王五	数学<br>13 1003	王五	英语<br>14 1003	王五	历史<br>15 1003	王五	地理<br>16 1004	朱六	语文<br>17 1004	朱六	数学<br>18 1004	朱六	英语<br>19 1004	朱六	历史<br>20 1004	朱六	地理</p>
<p>但是当我们想要查询每个学生课程对应的分数时，使用explode函数会出现如下结果：</p>
<p>例如使用如下语句：<br>SELECT stu_id,<br>stu_name,<br>ecourse,<br>escore<br>from student_score_new<br>lateral view explode(split(course,’,’)) cr as ecourse<br>lateral view explode(split(score,’,’)) sc as escore;<br>查询的结果如下:</p>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image\微信图片_20200319230153.png" alt="微信图片_20200319230153"></p>
<pre><code class="sql">出现这种情况是因为两个并列的explode的sql没办法识别每个科目对应的成绩是多少，对于多个数组的行转列可以使用posexplode函数，例如使用如下查询语句：
–使用posexplode函数进行列转行查询
SELECT stu_id,
stu_name,
ecourse,
escore
from student_score_new
lateral view posexplode(split(course,’,’)) cr as a,ecourse
lateral view posexplode(split(score,’,’)) sc as b,escore
where a=b;
</code></pre>
<p>查询结果如下：<br>序号	stu_id	stu_name	ecourse	escore<br>1 		1001	张三	语文	88<br>2 		1001	张三	数学	87<br>3 		1001	张三	英语	94<br>4 		1001	张三	历史	86<br>5 		1001	张三	地理	84<br>6 		1002	李四	语文	78<br>7		 1002	李四	数学	89<br>8 		1002	李四	英语	75<br>9 		1002	李四	历史	79<br>10 	  1002	李四	地理	68<br>11 	  1003	王五	语文	98<br>12 	  1003	王五	数学	97<br>13 	  1003	王五	英语	91<br>14       1003	王五	历史	93<br>15       1003	王五	地理	92<br>16       1004	朱六	语文	66<br>17       1004	朱六	数学	63<br>18       1004	朱六	英语	64<br>19 	  1004	朱六	历史	67<br>20       1004	朱六	地理	68</p>
<h2 id="Java工具类"><a href="#Java工具类" class="headerlink" title="Java工具类"></a><strong>Java工具类</strong></h2><pre><code class="java">import java.text.DecimalFormat;
import java.text.FieldPosition;
import java.text.Format;
import java.text.NumberFormat;
import java.text.SimpleDateFormat;
import java.util.Calendar;

/**
 * 根据时间生成唯一序列ID
 * 时间精确到秒，ID最大值为99999且循环使用
 * 
 * @Author:chenssy
 * @date:2016年4月17日
 */
public class GenerateSequenceUtil &#123;
    private static final FieldPosition HELPER_POSITION = new FieldPosition(0);
    
    /** 时间：精确到秒 */
    private final static Format dateFormat = new SimpleDateFormat(&quot;YYYYMMddHHmmss&quot;);
    
    private final static NumberFormat numberFormat = new DecimalFormat(&quot;00000&quot;);
    
    private static int seq = 0;
     
    private static final int MAX = 99999;
    
    public static synchronized String generateSequenceNo() &#123;
         
        Calendar rightNow = Calendar.getInstance();
       
        StringBuffer sb = new StringBuffer();
 
        dateFormat.format(rightNow.getTime(), sb, HELPER_POSITION);
 
        numberFormat.format(seq, sb, HELPER_POSITION);
 
        if (seq == MAX) &#123;
            seq = 0;
        &#125; else &#123;
            seq++;
        &#125;
 
        return sb.toString();
    &#125;
&#125;


import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.InputStreamReader;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;

/**
 *  初始化敏感词库，将敏感词加入到HashMap中，构建DFA算法模型
 *  
 * @Author : chenssy
 * @Date ： 2014年4月20日 下午2:27:06
 */
public class SensitiveWordInit &#123;
    private String ENCODING = &quot;GBK&quot;;    //字符编码
    @SuppressWarnings(&quot;rawtypes&quot;)
    public HashMap sensitiveWordMap;
    
    SensitiveWordInit()&#123;
        super();
    &#125;
    
    /**
     * @author chenssy 
     * @date 2014年4月20日 下午2:28:32
     * @version 1.0
     */
    @SuppressWarnings(&quot;rawtypes&quot;)
    Map initKeyWord()&#123;
        try &#123;
            //读取敏感词库
            Set&lt;String&gt; keyWordSet = readSensitiveWordFile();
            //将敏感词库加入到HashMap中
            addSensitiveWordToHashMap(keyWordSet);
        &#125; catch (Exception e) &#123;
            e.printStackTrace();
        &#125;
        return sensitiveWordMap;
    &#125;

    /**
     * 读取敏感词库，将敏感词放入HashSet中，构建一个DFA算法模型：
     * @author chenssy 
     * @date 2014年4月20日 下午3:04:20
     * @param keyWordSet  敏感词库
     * @version 1.0
     */
    @SuppressWarnings(&#123; &quot;rawtypes&quot;, &quot;unchecked&quot; &#125;)
    private void addSensitiveWordToHashMap(Set&lt;String&gt; keyWordSet) &#123;
        sensitiveWordMap = new HashMap(keyWordSet.size());     //初始化敏感词容器，减少扩容操作
        String key = null;  
        Map nowMap = null;
        Map&lt;String, String&gt; newWorMap = null;
        //迭代keyWordSet
        Iterator&lt;String&gt; iterator = keyWordSet.iterator();
        while(iterator.hasNext())&#123;
            key = iterator.next();    //关键字
            nowMap = sensitiveWordMap;
            for(int i = 0 ; i &lt; key.length() ; i++)&#123;
                char keyChar = key.charAt(i);       //转换成char型
                Object wordMap = nowMap.get(keyChar);       //获取
                
                if(wordMap != null)&#123;        //如果存在该key，直接赋值
                    nowMap = (Map) wordMap;
                &#125;
                else&#123;     //不存在则，则构建一个map，同时将isEnd设置为0，因为他不是最后一个
                    newWorMap = new HashMap&lt;String,String&gt;();
                    newWorMap.put(&quot;isEnd&quot;, &quot;0&quot;);     //不是最后一个
                    nowMap.put(keyChar, newWorMap);
                    nowMap = newWorMap;
                &#125;
                
                if(i == key.length() - 1)&#123;
                    nowMap.put(&quot;isEnd&quot;, &quot;1&quot;);    //最后一个
                &#125;
            &#125;
        &#125;
    &#125;

    /**
     * 读取敏感词库中的内容，将内容添加到set集合中
     * @author chenssy 
     * @date 2014年4月20日 下午2:31:18
     * @return
     * @version 1.0
     * @throws Exception 
     */
    @SuppressWarnings(&quot;resource&quot;)
    private Set&lt;String&gt; readSensitiveWordFile() throws Exception&#123;
        Set&lt;String&gt; set = null;
        
        File file = new File(&quot;D:\\SensitiveWord.txt&quot;);    //读取文件
        InputStreamReader read = new InputStreamReader(new FileInputStream(file),ENCODING);
        try &#123;
            if(file.isFile() &amp;&amp; file.exists())&#123;      //文件流是否存在
                set = new HashSet&lt;String&gt;();
                BufferedReader bufferedReader = new BufferedReader(read);
                String txt = null;
                while((txt = bufferedReader.readLine()) != null)&#123;    //读取文件，将文件内容放入到set中
                    set.add(txt);
                &#125;
            &#125;
            else&#123;         //不存在抛出异常信息
                throw new Exception(&quot;敏感词库文件不存在&quot;);
            &#125;
        &#125; catch (Exception e) &#123;
            throw e;
        &#125;finally&#123;
            read.close();     //关闭文件流
        &#125;
        return set;
    &#125;
&#125;


import java.util.HashSet;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;

/**
 * 铭感词过滤工具类
 * 
 * @Author:chenssy
 * @date:2014年8月5日
 */
public class SensitivewordFilterUtil&#123;
    @SuppressWarnings(&quot;rawtypes&quot;)
    private Map sensitiveWordMap = null;
    public static int minMatchTYpe = 1;      //最小匹配规则
    public static int maxMatchType = 2;      //最大匹配规则
    
    /**
     * 构造函数，初始化敏感词库
     */
    public SensitivewordFilterUtil()&#123;
        sensitiveWordMap = new SensitiveWordInit().initKeyWord();
    &#125;
    
    /**
     * 判断文字是否包含敏感字符
     * @author chenssy 
     * @date 2014年4月20日 下午4:28:30
     * @param txt  文字
     * @param matchType  匹配规则&amp;nbsp;1：最小匹配规则，2：最大匹配规则
     * @return 若包含返回true，否则返回false
     * @version 1.0
     */
    public boolean isContaintSensitiveWord(String txt,int matchType)&#123;
        boolean flag = false;
        for(int i = 0 ; i &lt; txt.length() ; i++)&#123;
            int matchFlag = this.CheckSensitiveWord(txt, i, matchType); //判断是否包含敏感字符
            if(matchFlag &gt; 0)&#123;    //大于0存在，返回true
                flag = true;
            &#125;
        &#125;
        return flag;
    &#125;
    
    /**
     * 获取文字中的敏感词
     * @author chenssy 
     * @date 2014年4月20日 下午5:10:52
     * @param txt 文字
     * @param matchType 匹配规则&amp;nbsp;1：最小匹配规则，2：最大匹配规则
     * @return
     * @version 1.0
     */
    public Set&lt;String&gt; getSensitiveWord(String txt , int matchType)&#123;
        Set&lt;String&gt; sensitiveWordList = new HashSet&lt;String&gt;();
        
        for(int i = 0 ; i &lt; txt.length() ; i++)&#123;
            int length = CheckSensitiveWord(txt, i, matchType);    //判断是否包含敏感字符
            if(length &gt; 0)&#123;    //存在,加入list中
                sensitiveWordList.add(txt.substring(i, i+length));
                i = i + length - 1;    //减1的原因，是因为for会自增
            &#125;
        &#125;
        
        return sensitiveWordList;
    &#125;
    
    /**
     * 替换敏感字字符
     * @author chenssy 
     * @date 2014年4月20日 下午5:12:07
     * @param txt
     * @param matchType
     * @param replaceChar 替换字符，默认*
     * @version 1.0
     */
    public String replaceSensitiveWord(String txt,int matchType,String replaceChar)&#123;
        String resultTxt = txt;
        Set&lt;String&gt; set = getSensitiveWord(txt, matchType);     //获取所有的敏感词
        Iterator&lt;String&gt; iterator = set.iterator();
        String word = null;
        String replaceString = null;
        while (iterator.hasNext()) &#123;
            word = iterator.next();
            replaceString = getReplaceChars(replaceChar, word.length());
            resultTxt = resultTxt.replaceAll(word, replaceString);
        &#125;
        
        return resultTxt;
    &#125;
    
    /**
     * 获取替换字符串
     * @author chenssy 
     * @date 2014年4月20日 下午5:21:19
     * @param replaceChar
     * @param length
     * @return
     * @version 1.0
     */
    private String getReplaceChars(String replaceChar,int length)&#123;
        String resultReplace = replaceChar;
        for(int i = 1 ; i &lt; length ; i++)&#123;
            resultReplace += replaceChar;
        &#125;
        
        return resultReplace;
    &#125;
    
    /**
     * 检查文字中是否包含敏感字符，检查规则如下：&lt;br&gt;
     * @author chenssy 
     * @date 2014年4月20日 下午4:31:03
     * @param txt
     * @param beginIndex
     * @param matchType
     * @return，如果存在，则返回敏感词字符的长度，不存在返回0
     * @version 1.0
     */
    @SuppressWarnings(&#123; &quot;rawtypes&quot;&#125;)
    public int CheckSensitiveWord(String txt,int beginIndex,int matchType)&#123;
        boolean  flag = false;    //敏感词结束标识位：用于敏感词只有1位的情况
        int matchFlag = 0;     //匹配标识数默认为0
        char word = 0;
        Map nowMap = sensitiveWordMap;
        for(int i = beginIndex; i &lt; txt.length() ; i++)&#123;
            word = txt.charAt(i);
            nowMap = (Map) nowMap.get(word);     //获取指定key
            if(nowMap != null)&#123;     //存在，则判断是否为最后一个
                matchFlag++;     //找到相应key，匹配标识+1 
                if(&quot;1&quot;.equals(nowMap.get(&quot;isEnd&quot;)))&#123;       
                    //如果为最后一个匹配规则,结束循环，返回匹配标识数
                    flag = true;       //结束标志位为true   
                    if(SensitivewordFilterUtil.minMatchTYpe == matchType)&#123;   
                        //最小规则，直接返回,最大规则还需继续查找
                        break;
                    &#125;
                &#125;
            &#125;
            else&#123;     //不存在，直接返回
                break;
            &#125;
        &#125;
        if(matchFlag &lt; 2 || !flag)&#123;        //长度必须大于等于1，为词 
            matchFlag = 0;
        &#125;
        return matchFlag;
    &#125;
&#125;


import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.math.BigInteger;
import java.security.MessageDigest;

import com.JUtils.date.DateUtils;
import com.JUtils.math.RandomUtils;

/**
 * @desc:文件工具类
 * @Author:chenssy
 * @date:2014年8月7日
 */
public class FileUtils &#123;
    private static final String FOLDER_SEPARATOR = &quot;/&quot;;
    private static final char EXTENSION_SEPARATOR = &#39;.&#39;;
    
    /**
     * @desc:判断指定路径是否存在，如果不存在，根据参数决定是否新建
     * @autor:chenssy
     * @date:2014年8月7日
     * @param filePath   指定的文件路径
     * @param isNew      true：新建、false：不新建
     * @return 存在返回TRUE，不存在返回FALSE
     */
    public static boolean isExist(String filePath,boolean isNew)&#123;
        File file = new File(filePath);
        if(!file.exists() &amp;&amp; isNew)&#123;    
            return file.mkdirs();    //新建文件路径
        &#125;
        return false;
    &#125;
    
    /**
     * 获取文件名，构建结构为 prefix + yyyyMMddHH24mmss + 10位随机数 + suffix + .type
     * @autor:chenssy
     * @date:2014年8月11日
     * @param type    文件类型
     * @param prefix  前缀
     * @param suffix  后缀
     * @return
     */
    public static String getFileName(String type,String prefix,String suffix)&#123;
        String date = DateUtils.getCurrentTime(&quot;yyyyMMddHH24mmss&quot;);   //当前时间
        String random = RandomUtils.generateNumberString(10);   //10位随机数
        
        //返回文件名  
        return prefix + date + random + suffix + &quot;.&quot; + type;
    &#125;
    
    /**
     * 获取文件名，文件名构成:当前时间 + 10位随机数 + .type
     * @autor:chenssy
     * @date:2014年8月11日
     * @param type  文件类型
     * @return
     */
    public static String getFileName(String type)&#123;
        return getFileName(type, &quot;&quot;, &quot;&quot;);
    &#125;
    
    /**
     * 获取文件名，文件构成：当前时间 + 10位随机数
     * @autor:chenssy
     * @date:2014年8月11日
     * @return
     */
    public static String getFileName()&#123;
        String date = DateUtils.getCurrentTime(&quot;yyyyMMddHH24mmss&quot;);   //当前时间
        String random = RandomUtils.generateNumberString(10);   //10位随机数
        //返回文件名  
        return date + random;
    &#125;
    
    /**
     * 获取指定文件的大小
     * @param file
     * @return
     * @throws Exception
     * @author:chenssy
     * @date : 2016年4月30日 下午9:10:12
     */
    @SuppressWarnings(&quot;resource&quot;)
    public static long getFileSize(File file) throws Exception &#123;
        long size = 0;
        if (file.exists()) &#123;
            FileInputStream fis = null;
            fis = new FileInputStream(file);
            size = fis.available();
        &#125; else &#123;
            file.createNewFile();
        &#125;
        return size;
    &#125;
    
    /**
     * 删除所有文件，包括文件夹
     * @author : chenssy
     * @date : 2016年5月23日 下午12:41:08
     * @param dirpath
     */
    public void deleteAll(String dirpath) &#123;  
         File path = new File(dirpath);  
         try &#123;  
             if (!path.exists())  
                 return;// 目录不存在退出   
             if (path.isFile()) // 如果是文件删除   
             &#123;  
                 path.delete();  
                 return;  
             &#125;  
             File[] files = path.listFiles();// 如果目录中有文件递归删除文件   
             for (int i = 0; i &lt; files.length; i++) &#123;  
                 deleteAll(files[i].getAbsolutePath());  
             &#125;  
             path.delete();  

         &#125; catch (Exception e) &#123;  
             e.printStackTrace();  
         &#125;   
    &#125;
    
    /**
     * 复制文件或者文件夹
     * @author : chenssy
     * @date : 2016年5月23日 下午12:41:59
     * @param inputFile	    源文件
     * @param outputFile    目的文件
     * @param isOverWrite   是否覆盖文件
     * @throws java.io.IOException
     */
    public static void copy(File inputFile, File outputFile, boolean isOverWrite)
            throws IOException &#123;
        if (!inputFile.exists()) &#123;
            throw new RuntimeException(inputFile.getPath() + &quot;源目录不存在!&quot;);
        &#125;
        copyPri(inputFile, outputFile, isOverWrite);
    &#125;
    
    /**
     * 复制文件或者文件夹
     * @author : chenssy
     * @date : 2016年5月23日 下午12:43:24
     * @param inputFile	    源文件
     * @param outputFile    目的文件
     * @param isOverWrite   是否覆盖文件
     * @throws java.io.IOException
     */
    private static void copyPri(File inputFile, File outputFile, boolean isOverWrite) throws IOException &#123;
        if (inputFile.isFile()) &#123;		//文件
            copySimpleFile(inputFile, outputFile, isOverWrite);
        &#125; else &#123;
            if (!outputFile.exists()) &#123;		//文件夹	
                outputFile.mkdirs();
            &#125;
            // 循环子文件夹
            for (File child : inputFile.listFiles()) &#123;
                copy(child, new File(outputFile.getPath() + &quot;/&quot; + child.getName()), isOverWrite);
            &#125;
        &#125;
    &#125;
    
    /**
     * 复制单个文件
     * @author : chenssy
     * @date : 2016年5月23日 下午12:44:07
     * @param inputFile		源文件
     * @param outputFile	目的文件
     * @param isOverWrite	是否覆盖
     * @throws java.io.IOException
     */
    private static void copySimpleFile(File inputFile, File outputFile,
            boolean isOverWrite) throws IOException &#123;
        if (outputFile.exists()) &#123;
            if (isOverWrite) &#123;		//可以覆盖
                if (!outputFile.delete()) &#123;
                    throw new RuntimeException(outputFile.getPath() + &quot;无法覆盖！&quot;);
                &#125;
            &#125; else &#123;
                // 不允许覆盖
                return;
            &#125;
        &#125;
        InputStream in = new FileInputStream(inputFile);
        OutputStream out = new FileOutputStream(outputFile);
        byte[] buffer = new byte[1024];
        int read = 0;
        while ((read = in.read(buffer)) != -1) &#123;
            out.write(buffer, 0, read);
        &#125;
        in.close();
        out.close();
    &#125;
    
    /**
     * 获取文件的MD5
     * @author : chenssy
     * @param file 文件
     * @return
     */
    public static String getFileMD5(File file)&#123;
        if (!file.exists() || !file.isFile()) &#123;  
            return null;  
        &#125;  
        MessageDigest digest = null;  
        FileInputStream in = null;  
        byte buffer[] = new byte[1024];  
        int len;  
        try &#123;  
            digest = MessageDigest.getInstance(&quot;MD5&quot;);  
            in = new FileInputStream(file);  
            while ((len = in.read(buffer, 0, 1024)) != -1) &#123;  
                digest.update(buffer, 0, len);  
            &#125;  
            in.close();  
        &#125; catch (Exception e) &#123;  
            e.printStackTrace();  
            return null;  
        &#125;  
        BigInteger bigInt = new BigInteger(1, digest.digest());  
        return bigInt.toString(16);  
    &#125;
    
    /**
     * 获取文件的后缀
     * @author : chenssy
     * @date : 2016年5月23日 下午12:51:59
     * @param file 文件
     * @return
     */
    public static String getFileSuffix(String file) &#123;
        if (file == null) &#123;
            return null;
        &#125;
        int extIndex = file.lastIndexOf(EXTENSION_SEPARATOR);
        if (extIndex == -1) &#123;
            return null;
        &#125;
        int folderIndex = file.lastIndexOf(FOLDER_SEPARATOR);
        if (folderIndex &gt; extIndex) &#123;
            return null;
        &#125;
        return file.substring(extIndex + 1);
    &#125;
    
    /**
     * 文件重命名
     * @author : chenssy
     * @date : 2016年5月23日 下午12:56:05
     * @param oldPath   老文件
     * @param newPath   新文件
     */
    public boolean renameDir(String oldPath, String newPath) &#123;  
        File oldFile = new File(oldPath);// 文件或目录   
        File newFile = new File(newPath);// 文件或目录   
        
        return oldFile.renameTo(newFile);// 重命名   
    &#125;
&#125;

import java.io.BufferedOutputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.zip.ZipEntry;
import java.util.zip.ZipOutputStream;

/**
 * 文件压缩、解压工具类。文件压缩格式为zip
 *
 * @Author:chenssy
 * @date:2016年5月24日 下午9:16:01
 */
public class ZipUitls &#123;
    /** 文件后缀名 */
    private static final String ZIP_FILE_SUFFIX = &quot;.zip&quot;;
    
    /**
     * 压缩文件
     * @author:chenssy
     * @date : 2016年5月24日 下午9:56:36
     * @param resourcePath  源文件
     * @param targetPath    目的文件,保存文件路径
     */
    public static void zipFile(String resourcePath,String targetPath)&#123;
        File resourcesFile = new File(resourcePath); 
        File targetFile = new File(targetPath);
        
        //目的文件不存在，则新建
        if(!targetFile.exists())&#123;
            targetFile.mkdirs();
        &#125;
        //文件名
        String targetName = resourcesFile.getName() + ZIP_FILE_SUFFIX;
        
        ZipOutputStream out = null;
        try &#123;
            FileOutputStream outputStream = new FileOutputStream(targetPath+&quot;//&quot;+targetName);
            out = new ZipOutputStream(new BufferedOutputStream(outputStream));

            compressedFile(out, resourcesFile, &quot;&quot;);
        &#125; catch (FileNotFoundException e) &#123;
            e.printStackTrace();
        &#125;finally&#123;
            if (out != null) &#123;
                try &#123;
                    out.close();
                &#125; catch (IOException e) &#123;
                    e.printStackTrace();
                &#125; 
            &#125;
        &#125;
    &#125;

    /**
     *
     * @author:chenssy
     * @date : 2016年5月24日 下午10:00:22
     * @param out
     * @param resourcesFile
     * @param dir
     */
    private static void compressedFile(ZipOutputStream out, File file, String dir) &#123;
        FileInputStream fis = null;
        try &#123;
            if (file.isDirectory()) &#123;	//文件夹
                // 得到文件列表信息
                File[] files = file.listFiles();
                // 将文件夹添加到下一级打包目录
                out.putNextEntry(new ZipEntry(dir + &quot;/&quot;));

                dir = dir.length() == 0 ? &quot;&quot; : dir + &quot;/&quot;;

                // 循环将文件夹中的文件打包
                for (int i = 0; i &lt; files.length; i++) &#123;
                    compressedFile(out, files[i], dir + files[i].getName()); // 递归处理
                &#125;
            &#125; else &#123; 	//如果是文件则打包处理
                fis = new FileInputStream(file);

                out.putNextEntry(new ZipEntry(dir));
                // 进行写操作
                int j = 0;
                byte[] buffer = new byte[1024];
                while ((j = fis.read(buffer)) &gt; 0) &#123;
                    out.write(buffer, 0, j);
                &#125;
                // 关闭输入流
            &#125;
        &#125; catch (FileNotFoundException e) &#123;
            e.printStackTrace();
        &#125; catch (IOException e) &#123;
            e.printStackTrace();
        &#125; finally&#123;
            if(fis != null)&#123;
                try &#123;
                    fis.close();
                &#125; catch (IOException e) &#123;
                    e.printStackTrace();
                &#125;
            &#125;
        &#125;
    &#125;
&#125;

import java.text.ParsePosition;
import java.text.SimpleDateFormat;
import java.util.Date;

/**
 * 日期格式化工具类
 * @Author:chenssy
 * @date:2016年5月26日 下午12:39:57
 */
public class DateFormatUtils &#123;
    public static final String DATE_YEAR = &quot;yyyy&quot;;    			/** yyyy:年 */
    public static final String DATE_MONTH = &quot;MM&quot;;    			/** MM：月 */
    public static final String DATE_DAY = &quot;dd&quot;;        	     	 /** DD：日 */
    public static final String DATE_HOUR = &quot;HH&quot;;		    	/** HH：时 */
    public static final String DATE_MINUTE = &quot;mm&quot;;		    	/** mm：分 */
    public static final String DATE_SECONDES = &quot;ss&quot;;	    	/** ss：秒 */
    public static final String DATE_FORMAT1 = &quot;yyyy-MM-dd&quot;;		/** yyyy-MM-dd */
    public static final String DATE_FORMAT2 = &quot;yyyy-MM-dd HH:mm:ss&quot;;    /** yyyy-MM-dd hh:mm:ss */
    public static final String TIME_FORMAT_SSS = &quot;yyyy-MM-dd HH:mm:ss|SSS&quot;;	/** yyyy-MM-dd hh:mm:ss|SSS */
    public static final String DATE_NOFUll_FORMAT = &quot;yyyyMMdd&quot;;			/** yyyyMMdd */
    public static final String TIME_NOFUll_FORMAT = &quot;yyyyMMddHHmmss&quot;;   /** yyyyMMddhhmmss */
    
    /**
     * 
     * 格式转换&lt;br&gt;
     * yyyy-MM-dd hh:mm:ss 和 yyyyMMddhhmmss 相互转换&lt;br&gt;
     * yyyy-mm-dd 和yyyymmss 相互转换
     * @author chenssy
     * @date Dec 26, 2013
     * @param value  日期
     * @return String
     */
    public static String formatString(String value) &#123;
        String sReturn = &quot;&quot;;
        if (value == null || &quot;&quot;.equals(value))
            return sReturn;
        if (value.length() == 14) &#123;   //长度为14格式转换成yyyy-mm-dd hh:mm:ss
            sReturn = value.substring(0, 4) + &quot;-&quot; + value.substring(4, 6) + &quot;-&quot; + value.substring(6, 8) + &quot; &quot;
                    + value.substring(8, 10) + &quot;:&quot; + value.substring(10, 12) + &quot;:&quot; + value.substring(12, 14);
            return sReturn;
        &#125;
        if (value.length() == 19) &#123;   //长度为19格式转换成yyyymmddhhmmss
            sReturn = value.substring(0, 4) + value.substring(5, 7) + value.substring(8, 10) + value.substring(11, 13)
                    + value.substring(14, 16) + value.substring(17, 19);
            return sReturn;
        &#125;
        if(value.length() == 10)&#123;     //长度为10格式转换成yyyymmhh
            sReturn = value.substring(0, 4) + value.substring(5,7) + value.substring(8,10);
        &#125;
        if(value.length() == 8)&#123;      //长度为8格式转化成yyyy-mm-dd
            sReturn = value.substring(0, 4) + &quot;-&quot; + value.substring(4, 6) + &quot;-&quot; + value.substring(6, 8);
        &#125;
        return sReturn;
    &#125;
    
    public static String formatDate(String date, String format) &#123;
        if (date == null || &quot;&quot;.equals(date))&#123;
            return &quot;&quot;;
        &#125;
        Date dt = null;
        SimpleDateFormat inFmt = null;
        SimpleDateFormat outFmt = null;
        ParsePosition pos = new ParsePosition(0);
        date = date.replace(&quot;-&quot;, &quot;&quot;).replace(&quot;:&quot;, &quot;&quot;);
        if ((date == null) || (&quot;&quot;.equals(date.trim())))
            return &quot;&quot;;
        try &#123;
            if (Long.parseLong(date) == 0L)
                return &quot;&quot;;
        &#125; catch (Exception nume) &#123;
            return date;
        &#125;
        try &#123;
            switch (date.trim().length()) &#123;
            case 14:
                inFmt = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);
                break;
            case 12:
                inFmt = new SimpleDateFormat(&quot;yyyyMMddHHmm&quot;);
                break;
            case 10:
                inFmt = new SimpleDateFormat(&quot;yyyyMMddHH&quot;);
                break;
            case 8:
                inFmt = new SimpleDateFormat(&quot;yyyyMMdd&quot;);
                break;
            case 6:
                inFmt = new SimpleDateFormat(&quot;yyyyMM&quot;);
                break;
            case 7:
            case 9:
            case 11:
            case 13:
            default:
                return date;
            &#125;
            if ((dt = inFmt.parse(date, pos)) == null)
                return date;
            if ((format == null) || (&quot;&quot;.equals(format.trim()))) &#123;
                outFmt = new SimpleDateFormat(&quot;yyyy年MM月dd日&quot;);
            &#125; else &#123;
                outFmt = new SimpleDateFormat(format);
            &#125;
            return outFmt.format(dt);
        &#125; catch (Exception ex) &#123;
        &#125;
        return date;
    &#125;

    /**
     * 格式化日期
     * @author chenming
     * @date 2016年5月31日
     * @param date
     * @param format
     * @return
     */
    public static String formatDate(Date date,String format)&#123;
        return formatDate(DateUtils.date2String(date), format);
    &#125;
    
    /**
     * @desc:格式化是时间，采用默认格式（yyyy-MM-dd HH:mm:ss）
     * @autor:chenssy
     * @date:2014年8月6日
     * @param value
     * @return
     */
    public static String formatDate(String value)&#123;
        return getFormat(DATE_FORMAT2).format(DateUtils.string2Date(value, DATE_FORMAT2));
    &#125;
    
    /**
     * 格式化日期
     * @author : chenssy
     * @date : 2016年5月31日 下午5:40:58
     * @param value
     * @return
     */
    public static String formatDate(Date value)&#123;
        return formatDate(DateUtils.date2String(value));
    &#125;
    
    /**
     * 获取日期显示格式，为空默认为yyyy-mm-dd HH:mm:ss
     * @author chenssy
     * @date Dec 30, 2013
     * @param format
     * @return
     * @return SimpleDateFormat
     */
    protected static SimpleDateFormat getFormat(String format)&#123;
        if(format == null || &quot;&quot;.equals(format))&#123;
            format = DATE_FORMAT2;
        &#125;
        return new SimpleDateFormat(format);
    &#125;
&#125;


import java.text.DateFormat;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.Date;

/**
 * @desc:时间处理工具类
 * 
 * @Author:chenssy
 * @date:2014年8月4日
 */
public class DateUtils &#123;
    private static final String[] weeks = &#123;&quot;星期日&quot;,&quot;星期一&quot;,&quot;星期二&quot;,&quot;星期三&quot;,&quot;星期四&quot;,&quot;星期五&quot;,&quot;星期六&quot;&#125;;
    /**
     * 根据指定格式获取当前时间
     * @author chenssy
     * @date Dec 27, 2013
     * @param format
     * @return String
     */
    public static String getCurrentTime(String format)&#123;
        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);
        Date date = new Date();
        return sdf.format(date);
    &#125;
    
    /**
     * 获取当前时间，格式为：yyyy-MM-dd HH:mm:ss
     * @author chenssy
     * @date Dec 27, 2013
     * @return String
     */
    public static String getCurrentTime()&#123;
        return getCurrentTime(DateFormatUtils.DATE_FORMAT2);
    &#125;
    
    /**
     * 获取指定格式的当前时间：为空时格式为yyyy-mm-dd HH:mm:ss
     * @author chenssy
     * @date Dec 30, 2013
     * @param format
     * @return Date
     */
    public static Date getCurrentDate(String format)&#123;
         SimpleDateFormat sdf = DateFormatUtils.getFormat(format);
         String dateS = getCurrentTime(format);
         Date date = null;
         try &#123;
            date = sdf.parse(dateS);
        &#125; catch (ParseException e) &#123;
            e.printStackTrace();
        &#125;
        return date;
    &#125;
    
    /**
     * 获取当前时间，格式为yyyy-MM-dd HH:mm:ss
     * @author chenssy
     * @date Dec 30, 2013
     * @return Date
     */
    public static Date getCurrentDate()&#123;
        return getCurrentDate(DateFormatUtils.DATE_FORMAT2);
    &#125;
    
    /**
     * 给指定日期加入年份，为空时默认当前时间
     * @author chenssy
     * @date Dec 30, 2013
     * @param year 年份  正数相加、负数相减
     * @param date 为空时，默认为当前时间
     * @param format 默认格式为：yyyy-MM-dd HH:mm:ss
     * @return String
     */
    public static String addYearToDate(int year,Date date,String format)&#123;
        Calendar calender = getCalendar(date,format);
        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);
        
        calender.add(Calendar.YEAR, year);
        
        return sdf.format(calender.getTime());
    &#125;
    
    /**
     * 给指定日期加入年份，为空时默认当前时间
     * @author chenssy
     * @date Dec 30, 2013
     * @param year 年份  正数相加、负数相减
     * @param date 为空时，默认为当前时间
     * @param format 默认格式为：yyyy-MM-dd HH:mm:ss
     * @return String
     */
    public static String addYearToDate(int year,String date,String format)&#123;
        Date newDate = new Date();
        if(null != date &amp;&amp; !&quot;&quot;.equals(date))&#123;
            newDate = string2Date(date, format);
        &#125;
        
        return addYearToDate(year, newDate, format);
    &#125;
    
    /**
     * 给指定日期增加月份 为空时默认当前时间
     * @author chenssy
     * @date Dec 30, 2013
     * @param month  增加月份  正数相加、负数相减
     * @param date 指定时间
     * @param format 指定格式 为空默认 yyyy-mm-dd HH:mm:ss
     * @return String
     */
    public static String addMothToDate(int month,Date date,String format) &#123;
        Calendar calender = getCalendar(date,format);
        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);
        
        calender.add(Calendar.MONTH, month);
        
        return sdf.format(calender.getTime());
    &#125;
    
    /**
     * 给指定日期增加月份 为空时默认当前时间
     * @author chenssy
     * @date Dec 30, 2013
     * @param month  增加月份  正数相加、负数相减
     * @param date 指定时间
     * @param format 指定格式 为空默认 yyyy-mm-dd HH:mm:ss
     * @return String
     */
    public static String addMothToDate(int month,String date,String format) &#123;
        Date newDate = new Date();
        if(null != date &amp;&amp; !&quot;&quot;.equals(date))&#123;
            newDate = string2Date(date, format);
        &#125;
        
        return addMothToDate(month, newDate, format);
    &#125;
    
    /**
     * 给指定日期增加天数，为空时默认当前时间
     * @author chenssy
     * @date Dec 31, 2013
     * @param day 增加天数 正数相加、负数相减
     * @param date 指定日期
     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss
     * @return String
     */
    public static String addDayToDate(int day,Date date,String format) &#123;
        Calendar calendar = getCalendar(date, format);
        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);
        
        calendar.add(Calendar.DATE, day);
        
        return sdf.format(calendar.getTime());
    &#125;
    
    /**
     * 给指定日期增加天数，为空时默认当前时间
     * @author chenssy
     * @date Dec 31, 2013
     * @param day 增加天数 正数相加、负数相减
     * @param date 指定日期
     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss
     * @return String
     */
    public static String addDayToDate(int day,String date,String format) &#123;
        Date newDate = new Date();
        if(null != date &amp;&amp; !&quot;&quot;.equals(date))&#123;
            newDate = string2Date(date, format);
        &#125;
        
        return addDayToDate(day, newDate, format);
    &#125;
    
    /**
     * 给指定日期增加小时，为空时默认当前时间
     * @author chenssy
     * @date Dec 31, 2013
     * @param hour 增加小时  正数相加、负数相减
     * @param date 指定日期
     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss
     * @return String
     */
    public static String addHourToDate(int hour,Date date,String format) &#123;
        Calendar calendar = getCalendar(date, format);
        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);
        
        calendar.add(Calendar.HOUR, hour);
        
        return sdf.format(calendar.getTime());
    &#125;
    
    /**
     * 给指定日期增加小时，为空时默认当前时间
     * @author chenssy
     * @date Dec 31, 2013
     * @param hour 增加小时  正数相加、负数相减
     * @param date 指定日期
     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss
     * @return String
     */
    public static String addHourToDate(int hour,String date,String format) &#123;
        Date newDate = new Date();
        if(null != date &amp;&amp; !&quot;&quot;.equals(date))&#123;
            newDate = string2Date(date, format);
        &#125;
        
        return addHourToDate(hour, newDate, format);
    &#125;
    
    /**
     * 给指定的日期增加分钟，为空时默认当前时间
     * @author chenssy
     * @date Dec 31, 2013
     * @param minute 增加分钟  正数相加、负数相减
     * @param date 指定日期 
     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss
     * @return String
     */
    public static String addMinuteToDate(int minute,Date date,String format) &#123;
        Calendar calendar = getCalendar(date, format);
        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);
        
        calendar.add(Calendar.MINUTE, minute);
        
        return sdf.format(calendar.getTime());
    &#125;
    
    /**
     * 给指定的日期增加分钟，为空时默认当前时间
     * @author chenssy
     * @date Dec 31, 2013
     * @param minute 增加分钟  正数相加、负数相减
     * @param date 指定日期 
     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss
     * @return String
     */
    public static String addMinuteToDate(int minute,String date,String format)&#123;
        Date newDate = new Date();
        if(null != date &amp;&amp; !&quot;&quot;.equals(date))&#123;
            newDate = string2Date(date, format);
        &#125;
        
        return addMinuteToDate(minute, newDate, format);
    &#125;
    
    /**
     * 给指定日期增加秒，为空时默认当前时间
     * @author chenssy
     * @date Dec 31, 2013
     * @param second 增加秒 正数相加、负数相减
     * @param date 指定日期
     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss
     * @return String
     */
    public static String addSecondToDate(int second,Date date,String format)&#123;
        Calendar calendar = getCalendar(date, format);
        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);
        
        calendar.add(Calendar.SECOND, second);
        
        return sdf.format(calendar.getTime());
    &#125;
    
    /**
     * 给指定日期增加秒，为空时默认当前时间
     * @author chenssy
     * @date Dec 31, 2013
     * @param second 增加秒 正数相加、负数相减
     * @param date 指定日期
     * @param format 日期格式 为空默认 yyyy-mm-dd HH:mm:ss
     * @return String
     * @throws Exception 
     */
    public static String addSecondToDate(int second,String date,String format)&#123;
        Date newDate = new Date();
        if(null != date &amp;&amp; !&quot;&quot;.equals(date))&#123;
            newDate = string2Date(date, format);
        &#125;
        
        return addSecondToDate(second, newDate, format);
    &#125;
    
    /**
     * 获取指定格式指定时间的日历
     * @author chenssy
     * @date Dec 30, 2013
     * @param date 时间 
     * @param format 格式
     * @return Calendar
     */
    public static Calendar getCalendar(Date date,String format)&#123;
        if(date == null)&#123;
            date = getCurrentDate(format);
        &#125;
        
        Calendar calender = Calendar.getInstance();
        calender.setTime(date);
        
        return calender;
    &#125;
    
    /**
     * 字符串转换为日期，日期格式为
     * 
     * @author : chenssy
     * @date : 2016年5月31日 下午5:20:22
     *
     * @param value
     * @return
     */
    public static Date string2Date(String value)&#123;
        if(value == null || &quot;&quot;.equals(value))&#123;
            return null;
        &#125;
        
        SimpleDateFormat sdf = DateFormatUtils.getFormat(DateFormatUtils.DATE_FORMAT2);
        Date date = null;
        
        try &#123;
            value = DateFormatUtils.formatDate(value, DateFormatUtils.DATE_FORMAT2);
            date = sdf.parse(value);
        &#125; catch (Exception e) &#123;
            e.printStackTrace();
        &#125;
        return date;
    &#125;
    
    /**
     * 将字符串(格式符合规范)转换成Date
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 需要转换的字符串
     * @param format 日期格式 
     * @return Date
     */
    public static Date string2Date(String value,String format)&#123;
        if(value == null || &quot;&quot;.equals(value))&#123;
            return null;
        &#125;
        
        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);
        Date date = null;
        
        try &#123;
            value = DateFormatUtils.formatDate(value, format);
            date = sdf.parse(value);
        &#125; catch (Exception e) &#123;
            e.printStackTrace();
        &#125;
        return date;
    &#125;
    
    /**
     * 将日期格式转换成String
     * @author chenssy
     * @date Dec 31, 2013
     * 
     * @param value 需要转换的日期
     * @param format 日期格式
     * @return String
     */
    public static String date2String(Date value,String format)&#123;
        if(value == null)&#123;
            return null;
        &#125;
        
        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);
        return sdf.format(value);
    &#125;
    
    /**
     * 日期转换为字符串
     * 
     * @author : chenssy
     * @date : 2016年5月31日 下午5:21:38
     *
     * @param value
     * @return
     */
    public static String date2String(Date value)&#123;
        if(value == null)&#123;
            return null;
        &#125;
        
        SimpleDateFormat sdf = DateFormatUtils.getFormat(DateFormatUtils.DATE_FORMAT2);
        return sdf.format(value);
    &#125;
    
    /**
     * 获取指定日期的年份
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 日期
     * @return int
     */
    public static int getCurrentYear(Date value)&#123;
        String date = date2String(value, DateFormatUtils.DATE_YEAR);
        return Integer.valueOf(date);
    &#125;
    
    /**
     * 获取指定日期的年份
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 日期
     * @return int
     */
    public static int getCurrentYear(String value) &#123;
        Date date = string2Date(value, DateFormatUtils.DATE_YEAR);
        Calendar calendar = getCalendar(date, DateFormatUtils.DATE_YEAR);
        return calendar.get(Calendar.YEAR);
    &#125;
    
    /**
     * 获取指定日期的月份
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 日期
     * @return int
     */
    public static int getCurrentMonth(Date value)&#123;
        String date = date2String(value, DateFormatUtils.DATE_MONTH);
        return Integer.valueOf(date);
    &#125;
    
    /**
     * 获取指定日期的月份
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 日期
     * @return int
     */
    public static int getCurrentMonth(String value) &#123;
        Date date = string2Date(value, DateFormatUtils.DATE_MONTH);
        Calendar calendar = getCalendar(date, DateFormatUtils.DATE_MONTH);
        
        return calendar.get(Calendar.MONTH);
    &#125;
    
    /**
     * 获取指定日期的天份
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 日期
     * @return int
     */
    public static int getCurrentDay(Date value)&#123;
        String date = date2String(value, DateFormatUtils.DATE_DAY);
        return Integer.valueOf(date);
    &#125;
    
    /**
     * 获取指定日期的天份
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 日期
     * @return int
     */
    public static int getCurrentDay(String value)&#123;
        Date date = string2Date(value, DateFormatUtils.DATE_DAY);
        Calendar calendar = getCalendar(date, DateFormatUtils.DATE_DAY);
        
        return calendar.get(Calendar.DATE);
    &#125;
    
    /**
     * 获取当前日期为星期几
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 日期
     * @return String
     */
    public static String getCurrentWeek(Date value) &#123;
        Calendar calendar = getCalendar(value, DateFormatUtils.DATE_FORMAT1);
        int weekIndex = calendar.get(Calendar.DAY_OF_WEEK) - 1 &lt; 0 ? 0 : calendar.get(Calendar.DAY_OF_WEEK) - 1;
        
        return weeks[weekIndex];
    &#125;
    
    /**
     * 获取当前日期为星期几
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 日期
     * @return String
     */
    public static String getCurrentWeek(String value) &#123;
        Date date = string2Date(value, DateFormatUtils.DATE_FORMAT1);
        return getCurrentWeek(date);
    &#125;
    
    /**
     * 获取指定日期的小时
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 日期
     * @return int
     */
    public static int getCurrentHour(Date value)&#123;
        String date = date2String(value, DateFormatUtils.DATE_HOUR);
        return Integer.valueOf(date);
    &#125;
    
    /**
     * 获取指定日期的小时
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 日期
     * @return
     * @return int
     */
    public static int getCurrentHour(String value) &#123;
        Date date = string2Date(value, DateFormatUtils.DATE_HOUR);
        Calendar calendar = getCalendar(date, DateFormatUtils.DATE_HOUR);
        
        return calendar.get(Calendar.DATE);
    &#125;
    
    /**
     * 获取指定日期的分钟
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 日期
     * @return int
     */
    public static int getCurrentMinute(Date value)&#123;
        String date = date2String(value, DateFormatUtils.DATE_MINUTE);
        return Integer.valueOf(date);
    &#125;
    
    /**
     * 获取指定日期的分钟
     * @author chenssy
     * @date Dec 31, 2013
     * @param value 日期
     * @return int
     */
    public static int getCurrentMinute(String value)&#123;
        Date date = string2Date(value, DateFormatUtils.DATE_MINUTE);
        Calendar calendar = getCalendar(date, DateFormatUtils.DATE_MINUTE);
        
        return calendar.get(Calendar.MINUTE);
    &#125;
    
    /**  
     * 比较两个日期相隔多少天(月、年) &lt;br&gt;
     * 例：&lt;br&gt;
     * &amp;nbsp;compareDate(&quot;2009-09-12&quot;, null, 0);//比较天 &lt;br&gt;
     * &amp;nbsp;compareDate(&quot;2009-09-12&quot;, null, 1);//比较月 &lt;br&gt; 
     * &amp;nbsp;compareDate(&quot;2009-09-12&quot;, null, 2);//比较年 &lt;br&gt;
     * 
     * @author chenssy
     * @date Dec 31, 2013 
     * @param startDay 需要比较的时间 不能为空(null),需要正确的日期格式 ,如：2009-09-12   
     * @param endDay 被比较的时间  为空(null)则为当前时间    
     * @param stype 返回值类型   0为多少天，1为多少个月，2为多少年    
     * @return int
     */    
    public static int compareDate(String startDay,String endDay,int stype) &#123;     
        int n = 0;     
        startDay = DateFormatUtils.formatDate(startDay, &quot;yyyy-MM-dd&quot;);
        endDay = DateFormatUtils.formatDate(endDay, &quot;yyyy-MM-dd&quot;);
        
        String formatStyle = &quot;yyyy-MM-dd&quot;;
        if(1 == stype)&#123;
            formatStyle = &quot;yyyy-MM&quot;;
        &#125;else if(2 == stype)&#123;
            formatStyle = &quot;yyyy&quot;;
        &#125;   
             
        endDay = endDay==null ? getCurrentTime(&quot;yyyy-MM-dd&quot;) : endDay;     
             
        DateFormat df = new SimpleDateFormat(formatStyle);     
        Calendar c1 = Calendar.getInstance();     
        Calendar c2 = Calendar.getInstance();     
        try &#123;     
            c1.setTime(df.parse(startDay));     
            c2.setTime(df.parse(endDay));   
        &#125; catch (Exception e) &#123;    
            e.printStackTrace();
        &#125;     
        while (!c1.after(c2)) &#123;                   // 循环对比，直到相等，n 就是所要的结果     
            n++;     
            if(stype==1)&#123;     
                c1.add(Calendar.MONTH, 1);          // 比较月份，月份+1     
            &#125;     
            else&#123;     
                c1.add(Calendar.DATE, 1);           // 比较天数，日期+1     
            &#125;     
        &#125;     
        n = n-1;     
        if(stype==2)&#123;     
            n = (int)n/365;     
        &#125;        
        return n;     
    &#125;   
    
    /**
     * 比较两个时间相差多少小时(分钟、秒)
     * @author chenssy
     * @date Jan 2, 2014
     * @param startTime 需要比较的时间 不能为空，且必须符合正确格式：2012-12-12 12:12:
     * @param endTime 需要被比较的时间 若为空则默认当前时间
     * @param type 1：小时   2：分钟   3：秒
     * @return int
     */
    public static int compareTime(String startTime , String endTime , int type) &#123;
        //endTime是否为空，为空默认当前时间
        if(endTime == null || &quot;&quot;.equals(endTime))&#123;
            endTime = getCurrentTime();
        &#125;
        
        SimpleDateFormat sdf = DateFormatUtils.getFormat(&quot;&quot;);
        int value = 0;
        try &#123;
            Date begin = sdf.parse(startTime);
            Date end = sdf.parse(endTime);
            long between = (end.getTime() - begin.getTime()) / 1000;  //除以1000转换成豪秒
            if(type == 1)&#123;   //小时
                value = (int) (between % (24 * 36000) / 3600);
            &#125;
            else if(type == 2)&#123;
                value = (int) (between % 3600 / 60);
            &#125;
            else if(type == 3)&#123;
                value = (int) (between % 60 / 60);
            &#125;
        &#125; catch (ParseException e) &#123;
            e.printStackTrace();
        &#125;
        return value;
    &#125;
    
    /**
     * 比较两个日期的大小。&lt;br&gt;
     * 若date1 &gt; date2 则返回 1&lt;br&gt;
     * 若date1 = date2 则返回 0&lt;br&gt;
     * 若date1 &lt; date2 则返回-1
     * @autor:chenssy
     * @date:2014年9月9日
     *
     * @param date1  
     * @param date2
     * @param format  待转换的格式
     * @return 比较结果
     */
    public static int compare(String date1, String date2,String format) &#123;
        DateFormat df = DateFormatUtils.getFormat(format);
        try &#123;
            Date dt1 = df.parse(date1);
            Date dt2 = df.parse(date2);
            if (dt1.getTime() &gt; dt2.getTime()) &#123;
                return 1;
            &#125; else if (dt1.getTime() &lt; dt2.getTime()) &#123;
                return -1;
            &#125; else &#123;
                return 0;
            &#125;
        &#125; catch (Exception exception) &#123;
            exception.printStackTrace();
        &#125;
        return 0;
    &#125;
    
    /**
     * 获取指定月份的第一天 
     * 
     * @author : chenssy
     * @date : 2016年5月31日 下午5:31:10
     *
     * @param date
     * @return
     */
    public static String getMonthFirstDate(String date)&#123;
        date = DateFormatUtils.formatDate(date);
        return DateFormatUtils.formatDate(date, &quot;yyyy-MM&quot;) + &quot;-01&quot;;
    &#125;
    
    /**
     * 获取指定月份的最后一天
     * 
     * @author : chenssy
     * @date : 2016年5月31日 下午5:32:09
     *
     * @param strdate
     * @return
     */
    public static String getMonthLastDate(String date) &#123;
        Date strDate = DateUtils.string2Date(getMonthFirstDate(date));
        Calendar calendar = Calendar.getInstance();
        calendar.setTime(strDate);
        calendar.add(Calendar.MONTH, 1);
        calendar.add(Calendar.DAY_OF_YEAR, -1);
        return DateFormatUtils.formatDate(calendar.getTime());
    &#125;
    
    /**
     * 获取所在星期的第一天
     * 
     * @author : chenssy
     * @date : 2016年6月1日 下午12:38:53
     *
     * @param date
     * @return
     */
    @SuppressWarnings(&quot;static-access&quot;)
    public static Date getWeekFirstDate(Date date) &#123;
        Calendar now = Calendar.getInstance();
        now.setTime(date);
        int today = now.get(Calendar.DAY_OF_WEEK);
        int first_day_of_week = now.get(Calendar.DATE) + 2 - today; // 星期一
        now.set(now.DATE, first_day_of_week);
        return now.getTime();
    &#125;
    
    /**
     * 获取所在星期的最后一天
     * @author : chenssy
     * @date : 2016年6月1日 下午12:40:31
     * @param date
     * @return
     */
    @SuppressWarnings(&quot;static-access&quot;)
    public static Date geWeektLastDate(Date date) &#123;
        Calendar now = Calendar.getInstance();
        now.setTime(date);
        int today = now.get(Calendar.DAY_OF_WEEK);
        int first_day_of_week = now.get(Calendar.DATE) + 2 - today; // 星期一
        int last_day_of_week = first_day_of_week + 6; // 星期日
        now.set(now.DATE, last_day_of_week);
        return now.getTime();
    &#125;
&#125;


import java.sql.Timestamp;
import java.text.SimpleDateFormat;
import java.util.Date;

/**
 * TimeStamp工具类，提供TimeStamp与String、Date的转换
 * @author chenssy
 * @date 2016-09-24
 * @since 1.0.0
 */
public class TimestampUtils &#123;

/**
 * String转换为TimeStamp
 * @param value 待转换的String，格式必须为 yyyy-mm-dd hh:mm:ss[.f...] 这样的格式，中括号表示可选，否则报错
 * @return java.sql.Timestamp
 * @author chenssy
 * @date 2016-09-24
 * @since v1.0.0
 */
    public static Timestamp string2Timestamp(String value)&#123;
        if(value == null &amp;&amp; !&quot;&quot;.equals(value.trim()))&#123;
            return null;
        &#125;
        Timestamp ts = new Timestamp(System.currentTimeMillis());
        ts = Timestamp.valueOf(value);
        return ts;
    &#125;

    /**
     * 将Timestamp 转换为String类型，format为null则使用默认格式 yyyy-MM-dd HH:mm:ss
     * @param value     待转换的Timestamp
     * @param format    String的格式
     * @return java.lang.String
     * @author chenssy
     * @date 2016-09-24
     * @since v1.0.0
     */
    public static String timestamp2String(Timestamp value,String format)&#123;
        if(null == value)&#123;
            return &quot;&quot;;
        &#125;
        SimpleDateFormat sdf = DateFormatUtils.getFormat(format);
        
        return sdf.format(value);
    &#125;

    /**
     * Date转换为Timestamp
     * @param date    待转换的Date
     * @return java.sql.Timestamp
     * @author chenssy
     * @date 2016-09-24
     * @since v1.0.0
     */
    public static Timestamp date2Timestamp(Date date)&#123;
        if(date == null)&#123;
            return null;
        &#125;
        return new Timestamp(date.getTime());
    &#125;

    /**
     * Timestamp转换为Date
     * @param time  待转换的Timestamp
     * @return java.util.Date
     * @author chenssy
     * @date 2016-09-24
     * @since v1.0.0
     */
    public static Date timestamp2Date(Timestamp time)&#123;
        return time == null ? null : time;
    &#125;
&#125;

mport java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.io.Serializable;
import java.util.Collection;

/**
 * 克隆工具类，进行深克隆,包括对象、集合
 * 
 * @Author:chenssy
 * @date:2014年8月9日
 */
public class CloneUtils &#123;

    /**
     * 采用对象的序列化完成对象的深克隆
     * @autor:chenssy
     * @date:2014年8月9日
     *
     * @param obj
     * 			待克隆的对象
     * @return
     */
    @SuppressWarnings(&quot;unchecked&quot;)
    public static &lt;T extends Serializable&gt; T cloneObject(T obj) &#123;
        T cloneObj = null;
        try &#123;
            // 写入字节流
            ByteArrayOutputStream out = new ByteArrayOutputStream();
            ObjectOutputStream obs = new ObjectOutputStream(out);
            obs.writeObject(obj);
            obs.close();

            // 分配内存，写入原始对象，生成新对象
            ByteArrayInputStream ios = new ByteArrayInputStream(out.toByteArray());
            ObjectInputStream ois = new ObjectInputStream(ios);
            // 返回生成的新对象
            cloneObj = (T) ois.readObject();
            ois.close();
        &#125; catch (Exception e) &#123;
            e.printStackTrace();
        &#125;
        return cloneObj;
    &#125;
    
    /**
     * 利用序列化完成集合的深克隆
     * @autor:chenssy
     * @date:2014年8月9日
     *
     * @param collection
     * 					待克隆的集合
     * @return
     * @throws ClassNotFoundException
     * @throws java.io.IOException
     */
    @SuppressWarnings(&quot;unchecked&quot;)
    public static &lt;T&gt; Collection&lt;T&gt; cloneCollection(Collection&lt;T&gt; collection) throws ClassNotFoundException, IOException&#123;
        ByteArrayOutputStream byteOut = new ByteArrayOutputStream();  
        ObjectOutputStream out = new ObjectOutputStream(byteOut);  
        out.writeObject(collection);
        out.close();
      
        ByteArrayInputStream byteIn = new ByteArrayInputStream(byteOut.toByteArray());  
        ObjectInputStream in = new ObjectInputStream(byteIn);  
        Collection&lt;T&gt; dest = (Collection&lt;T&gt;) in.readObject();  
        in.close();
        
        return dest;  
    &#125;
&#125;


import java.beans.BeanInfo;
import java.beans.IntrospectionException;
import java.beans.Introspector;
import java.beans.PropertyDescriptor;
import java.lang.reflect.InvocationTargetException;
import java.lang.reflect.Method;
import java.util.HashMap;
import java.util.Map;

/**
 * Bean与Map的转换
 *
 * @author chenssy
 * @date 2016-09-24
 * @since 1.0.0
 */
public class BeanMapConvert &#123;
    /**
     * Bean转换为Map
     * @param object
     * @return String-Object的HashMap
     * @author chenssy
     * @date 2016-09-25
     * @since v1.0.0
     */
    public static Map&lt;String,Object&gt; bean2MapObject(Object object)&#123;
        if(object == null)&#123;
            return null;
        &#125;

        Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;();
        try &#123;
            BeanInfo beanInfo = Introspector.getBeanInfo(object.getClass());
            PropertyDescriptor[] propertyDescriptors = beanInfo.getPropertyDescriptors();
            for (PropertyDescriptor property : propertyDescriptors) &#123;
                String key = property.getName();
                // 过滤class属性
                if (!key.equals(&quot;class&quot;)) &#123;
                    // 得到property对应的getter方法
                    Method getter = property.getReadMethod();
                    Object value = getter.invoke(object);

                    map.put(key, value);
                &#125;
            &#125;
        &#125; catch (Exception e) &#123;
           e.printStackTrace();
        &#125;

        return map;
    &#125;

    /**
     * Map转换为Java Bean
     * @param map     待转换的Map
     * @param object  Java Bean
     * @return java.lang.Object
     * @author chenssy
     * @date 2016-09-25
     * @since v1.0.0
     */
    public static Object map2Bean(Map map,Object object)&#123;
        if(map == null || object == null)&#123;
            return null;
        &#125;
        try &#123;
            BeanInfo beanInfo = Introspector.getBeanInfo(object.getClass());
            PropertyDescriptor[] propertyDescriptors = beanInfo.getPropertyDescriptors();

            for (PropertyDescriptor property : propertyDescriptors) &#123;
                String key = property.getName();
                if (map.containsKey(key)) &#123;
                    Object value = map.get(key);
                    // 得到property对应的setter方法
                    Method setter = property.getWriteMethod();
                    setter.invoke(object, value);
                &#125;
            &#125;
        &#125; catch (IntrospectionException e) &#123;
            e.printStackTrace();
        &#125; catch (InvocationTargetException e) &#123;
            e.printStackTrace();
        &#125; catch (IllegalAccessException e) &#123;
            e.printStackTrace();
        &#125;
        return object;
    &#125;
&#125;
</code></pre>
<h2 id="Flink的exactly-once是如何保证的？"><a href="#Flink的exactly-once是如何保证的？" class="headerlink" title="Flink的exactly-once是如何保证的？"></a><strong>Flink的exactly-once是如何保证的？</strong></h2><pre><code class="java">
</code></pre>
<h2 id="Hbase列族设计"><a href="#Hbase列族设计" class="headerlink" title="Hbase列族设计"></a><strong>Hbase列族设计</strong></h2><pre><code class="python">###  列族属性配置
版本数量
最小版本数
存活时间(TTL)
数据块大小(blockSize)
块缓存(BlockCache)
激进缓存(IN_MEMORY)
压缩(compression)
布隆过滤器
数据库编码
复制范围
预分区

版本数量（VERSIONS）
每个列族可以单独设置行版本数，默认是3。这个设置很重要，因为HBase是不会去覆盖一个值的，它只会在后面追加写，用时间戳（版本号）来区分，过早的版本会在执行Major Compaction时删除。这个版本的值可以根据具体的场景来增加或减少。
不推荐将版本最大值设置成一个很高的水平，除非老数据对你也非常重要。过多的版本，会导致存储文件变大，以至于影响查询效率。

最小版本数（MIN_VERSIONS ）
每个列族可以设置最小版本数，最小版本数缺省值是0，表示禁用该特性。最小版本数参数和存活时间是一起使用的，允许配置“如保存最后T秒有价值的数据，最多N个版本，但最少M个版本”（M是最小版本，M&lt;N）。该参数仅在存活时间对列族启用，且必须小于行版本数。

存活时间（TTL）
HBase支持配置版本数据的存活时间（TTL），TTL设置了一个基于时间戳的临界值，HBase会自动检查TTL值是否达到上限，如果TTL达到上限，则该数据会在Major Compaction过程中被删除。

数据块大小（BLOCKSIZE ）
hbase默认的块大小是64kb，不同于HDFS默认64MB的块大小。原因是hbase需要支持随机访问，一旦找到了行键所在的块，接下来就会定位对应的单元格。使用64kb的块扫描的速度显然优于64MB大小的块。
对于不同的业务数据，块大小的合理设置对读写性能有很大的影响。如果业务请求以Get请求为主，可以考虑将块大小设置较小；如果以Scan请求为主，可以将块大小调大；默认的64K块大小是在Scan和Get之间取得的一个平衡。
注意：
默认块大小适用于多种数据使用模式，调整块大小是比较高级的操作。配置错误将对性能产生负面影响。因此建议在调整之后进行测试，根据测试结果决定是否可以线上使用。

块缓存（BLOCKCACHE）
默认是true。缓存是内存存储，hbase使用块缓存将最近使用的块加载到内存中。块缓存会根据最近最久未使用（LRU）”的规则删除数据块。
如果你的使用场景是经常顺序访问或者很少被访问，可以关闭列族的缓存。列族缓存默认是打开的。

激进缓存的配置（IN_MEMORY）
HBase可以选择一个列族赋予更高的优先级缓存，激进缓存（表示优先级更高），IN_MEMORY 默认是false。如果设置为true，hbase会尝试将整个列族保存在内存中，只有在需要保存是才会持久化写入磁盘。但是在运行时hbase会尝试将整张表加载到内存里。
这个参数通常适合较小的列族。

压缩（COMPRESSION）
HBase在写入数据块到HDFS之前会首先对数据进行压缩，再落盘，从而减少磁盘空间使用量。而在读数据的时候首先从HDFS中加载出block块之后进行解压缩，然后再缓存到BlockCache，最后返回给用户。
使用压缩其实就是使用CPU资源换取磁盘空间资源。
HBase支持三种压缩方式：LZO、Snappy和GZIP。
默认为NONE，不适用压缩，
| **压缩算法**  | 压缩比率  | 压缩速度 | 解压速度  |
| ------------ | -------- | -------- | -------- |
| GZIP         | 13.4%    | 21 MB/s  | 118 MB/s |
| LZO          | 20.5%    | 135 MB/s | 410 MB/s |
| Snappy       | 22.2%    | 172 MB/s | 409 MB/s |
| ------------ | -------- | -------- | -------- |
其中：
1：GZIP的压缩率最高，但是其实CPU密集型的，对CPU的消耗比其他算法要多，压缩和解压速度也慢；
LZO的压缩率居中，比GZIP要低一些，但是压缩和解压速度明显要比GZIP快很多，其中解压速度快的更多；
2：Snappy的压缩率最低，而压缩和解压速度要稍微比LZO要快一些。
综合来看，Snappy的压缩率最低，但是编解码速率最高，对CPU的消耗也最小，目前一般建议使用Snappy。

复制范围（REPLICATION_SCOPE ）
HBase提供了跨级群同步的功能，本地集群的数据更新可以及时同步到其他集群。复制范围（replication scope）的参数默认为0，表示复制功能处于关闭状态。

预分区（SPLITS ）
在默认情况下，HBase表在刚刚被创建的时候，只有1个分区（Region），当一个Region的大小达到阈值（通过hbase.hregion.max.filesize参数控制），Region会进行split，分裂成2个Region。但是在进行split的时候，会消耗大量的资源，频繁的split会对HBase的性能造成巨大的影响。
HBase提供了预分区的功能，用户可以在创建表的时候对表按照一定的规则提前进行分区。这样是进行HBase数据读写的时候，会按照Region分区情况，在集群内做数据的负载均衡。
常用分区方法：
create &#39;table&#39;,&#39;cf&#39;, SPLITS =&gt; [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;]
create &#39;table&#39;,&#39;cf&#39;, &#123; NUMREGIONS =&gt; 8 , SPLITALGO =&gt; &#39;UniformSplit&#39; &#125;
create &#39;table&#39;,&#39;cf&#39;, &#123; NUMREGIONS =&gt; 10, SPLITALGO =&gt; &#39;HexStringSplit&#39; &#125; 

BLOOMFILTER
BloomFilter主要用来过滤不存在待检索RowKey或者Row-Col的HFile文件，避免无用的IO操作。它会告诉你在这个HFile文件中是否可能存在待检索的KV，如果不存在，就可以不用消耗IO打开文件进行seek。通过设置BloomFilter可以提升随机读写的性能。
BloomFilter是一个列族级别的配置属性，如果在表中设置了BloomFilter，那么HBase会在生成StoreFile时包含一份BloomFilter结构的数据，称其为MetaBlock和DataBlock(真实KeyValue数据)一起由LRUBlockCache维护。所以开启BloomFilter会有一定的存储即内存Cache的开销。
BloomFilter取值有两个，row和rowcol，需要根据业务来确定具体使用哪种。
如果业务大多数随机查询时仅仅使用row作为查询条件，BloomFilter一定要设置为row；
如果大多数随机查询使用row+cf作为查询条件，BloomFilter需要设置为rowcol；
如果不确定查询类型，建议设置为row。

列族设置

列族数量
不要在一张表中定义太多的列族。目前HBase并不能很好的处理2~3以上的列族，flush和compaction 操作是针对一个Region的。
当一个列族操作大量数据的时候会引发一个flush，它邻近的列族也会因关联效应被触发flush，尽管它没有操作多少数据。compaction操作是根据一个列族下的全部文件的数量触发的，而不是根据文件大小触发的。
当很多的列族在flush和compaction时，会造成很多没用的IO负载。
尽量在模式中只针对一个列族进行操作。将使用率相近的列归为一个列族，这样每次访问就只用访问一个列族，既能提升查询效率，也能保持尽可能少的访问不同的磁盘文件。

列族的基数
如果一个表存在多个列族，要注意列族之间基数（如行数）相差不要太大。例如列族A有100万行，列族B有10亿行，按照RowKey切分后，列族A可能被分散到很多很多Region（及RegionServer），这导致扫描列族A十分低效。
列族名、列名长度

列族名和列名越短越好，冗长的名字虽然可读性好，但是更短的名字在HBase中更好。
一个具体的值由存储该值的行键、对应的列（列族:列）以及该值的时间戳决定。HBase中索引是为了加速随机访问的速度，索引的创建是基于“行键+列族:列+时间戳+值”的，如果行键和列族的大小过大，甚至超过值本身的大小，那么将会增加索引的大小。并且在HBase中数据记录往往非常之多，重复的行键、列将不但使索引的大小过大，也将加重系统的负担
        
总结
根据HBase列族的这些属性配置，结合我们的使用场景，HBase列族可以进行如下优化：
列族不宜过多，将相关性很强的key-value都放在同一个列族下，；
尽量最小化行键和列族的大小；
提前预估数据量，再根据Rowkey规则，提前规划好Region分区，在创建表的时候进行预分区；
在业务上没有特别要求的情况下，只使用一个版本，即最大版本和最小版本一样，均为1；
根据业务需求合理设置好失效时间（存储的时间越短越好）；
根据查询条件，设置合理的BloomFilter配置；
合理设计RowKey，可以参考《一篇文章带你快速搞懂HBase RowKey设计》。
</code></pre>
<h2 id="布谷鸟过滤器"><a href="#布谷鸟过滤器" class="headerlink" title="布谷鸟过滤器"></a><strong>布谷鸟过滤器</strong></h2><pre><code class="python">布隆过滤器有exists方法通过对位数组的hash计算判断某元素是否在集合中，实现去重功能。但布隆过滤器有一下缺点：
   ## 不支持反向删除元素：一旦对位数组进行了赋值，无法将其删除。
   ## 查询性能弱：布隆过滤器使用多个hash函数计算位图多个不同位点，由于多个位点在内存中不连续，CPU寻址花销较大。
   ## 空间利用率低
    
 #** 布谷鸟过滤器 **#
首先要说明布谷鸟过滤器并不是使用位图实现的, 而是一维数组. 它所存储的是数据的指纹(fingerprint).
布谷鸟过滤器使用两个 hash 算法将新来的元素映射到数组的两个位置. 如果两个位置中有一个位置位空, 那么就可以将元素直接放进去. 但是如果这两个位置都满了, 它就会随机踢走一个, 然后自己霸占了这个位置.
正如布谷鸟那样, 把蛋下到其它鸟的窝里. 这也是得名的由来. 但它并不是像布谷鸟那样, 管杀不管埋, 还会为这个被踢走的数据, 找一个新家.
## 但布谷鸟过滤器有一个明显的弱点, 无法对同一个数据连续插入!
https://www.cnblogs.com/chuxiuhong/p/8215719.html

</code></pre>
<h2 id="Python面向对象"><a href="#Python面向对象" class="headerlink" title="Python面向对象"></a><strong>Python面向对象</strong></h2><pre><code class="python">这篇文章用很简单的例子把python类的内置方法串起来梳理，使得知识点之间具有很强关联性，便于理解。

引入
定义一个类并实例化

class info(object):
    # python3中，新定义的类默认都是object的子类，所以如果只写 class info 没有指明它的父类，那么也是可以的，父类就是object。
    # 之所以有如此多内置方法可用，即使我们自己定义的类里根本看不到这些内容，是因为内置方法正是在其父类object中实现的。
    # 例如，内置方法__str__在我们类定义里找不到，这时候就往其父类object中找，于是该方法就能起到object中所定义的作用。
    # 如果内置方法被我们自己亲自定义了，那么显然，我们自己定义的内置方法取代了它默认的行为。
    message = &#39;student&#39;
    # 可以在类里存放共享的数据，它可以直接访问：类名.message，也可以用对象去访问：对象.message。
    # 一般情况下如果没有重写过任何内置方法，对象.message会先寻找属性字典，再找有没有共享数据info.message
    # 注意，这个数据是一个变量。在类的定义体里可以显式地修改:info.message=...
    def __init__(self, name, age):  
    # 构造函数__init__是内置方法，以 m = info(...) 的方式实例化对象时自动执行。
    # 同时还有析构函数__del__，但是程序执行结束会自动删除对象，这样__del__便自动执行了。
    # 默认的__del__只是单纯删除对象，其他什么也不做。
        self.name = name  
        # self.name可以当做一个变量名来赋值。其中self指代对象自己，&quot;.&quot;后面的名字是自己自定义的一个名字。
        # 这个&quot;.&quot;后面的名字，作为key，存储在对象的内置字典__dict__中。字典的key是可hash的，当然，属性名就是一个字符串。
        # 本质上，这里做的事情是self.__dict__[name]=...
        self.age = age
    def get_name(self):
        # self.name能访问到属性的值，本质上做的事情是在self.__dict__字典中搜寻name这个key，返回它的value。
        return self.name
student = info(&#39;poincare&#39;, 23)

1.反射：setattr，getattr，delattr
反射是用属性名字字符串的方式来访问对象的属性或操作对象的属性的一种手段。难道直接用student.name不可以吗？可以，但是现在我有一个特殊的需求，就是让用户自己输入属性的名字，来操作属性。

def operater(student=student):
    attrname = input(&#39;请输入你要获取的student的属性的属性名称：&#39;)
    print(student.attrname)
operater()

报错。解释器根本不知道这里的student.attrname是什么东西。首先，student.attrname不是一个已定义过的变量，其次，student对象并没有一个属性的名字叫&quot;attrname&quot;。
解决方法是，通过对象内置的字典__dict__来找:
def operater(student=student):
    attrname = input(&#39;请输入你要获取的student的属性的属性名称：&#39;)
    print(student.__dict__[attrname])
operater()

确实，因为对象的属性一般存在__dict__中，所以直接调用__dict__做查、改、删操作屡试不爽。但是这样显得太粗暴了。python有反射函数可以完成这个任务：
def operater(student=student):
    attrname = input(&#39;请输入你要获取的student的属性的属性名称：&#39;)
    print(getattr(student, attrname))
operater()

同理，可以使用反射函数setattr,getattr,delattr来替代对对象__dict__的直接操控。

2.内置方法：__setattr__,__getattr__,__delattr__,__getattribute__
有机会重写对属性的赋值、修改、查找、删除行为。

不管在类的内部还是外部，涉及对属性的赋值或修改，不管是采取student.name=&#39;bla&#39;的方式，还是采取反射setattr(student, &#39;name&#39;, &#39;bla&#39;)的方式，只要重写了__setattr__,程序将优先找到自己重写的__setattr__执行。
怎么会有这种奇葩需求？当然有。最简单的例子就是实现功能：任何方式任何时刻student的name属性被改动，都要在屏幕上给出警告&quot;student.name被修改！&quot;
class info(object):
    message = &#39;student&#39;
    def __init__(self, name, age):
        self.name = name
        self.age = age
    def __setattr__(self, key, value):
        if key == &#39;name&#39;：
            print(&#39;student的name属性被修改！&#39;)
        self.key = value
    def get_name(self):
        return self.name
    
看到这里，马上指出问题。因为前面的反射已经提过了，不可以直接self.key=value，这实际上是创建了一个名叫key的属性。key指代一个字符串对象。考虑其他方式:
class info(object):
    message = &#39;student&#39;
    def __init__(self, name, age):
        self.name = name
        self.age = age
    def __setattr__(self, key, value):
        if key == &#39;name&#39;:
            self.name = value  # 方式一
            # setattr(self, key, value)  # 方式二
            print(&#39;student的name属性被修改！&#39;)
        setattr(self, key, value)
    def get_name(self):
        return self.name

实例化对象stundent的过程中，要把name属性进行设置。此时报错。原因是递归溢出。实际上，在实例化时，首先转到__init__，执行self.name=value,因为重写了__setattr__方法，所以转到__setattr__，又准备执行self.name=value,因为重写了__setattr__方法，又要执行self.name=value，无限递归，永不停止。
方式二把self.name=value改成反射setattr(self,name,value),仍然报错，因为本质上都是在修改这个属性的值，都要触发__setattr__。
于是居然陷入了自相矛盾的过程！我这个方法需要实现赋值，但是赋值又会触发这个方法...
解决方案，就是暴力地修改__dict__中的键了。注意！要区分修改__dict__和修改__dict__中的键值的区别。作为一个字典，字典中键值的修改并不会影响到__dict__所指向的字典“容器”本身。
class info(object):
    message = &#39;student&#39;
    def __init__(self, name, age):
        self.name = name
        self.age = age
    def __setattr__(self, key, value):
        if key == &#39;name&#39;:
            print(&#39;student的name属性被修改！&#39;)
        self.__dict__(key) = value  # 直接通过修改__dict__字典的办法操纵对象的属性。__dict__本身也是对象的属性，但是这里修改的是__dict__的键，而不是__dict__，所以不会触发__setattr__。
    def get_name(self):
        return self.name

还有一种好办法。之前曾提到过，如果自己没有实现__setattr__，解释器就会从父类中找__setattr__。因此完全可以在子类中引用父类的__setattr__来完成赋值：
class info(object):
    message = &#39;student&#39;
    def __init__(self, name, age):
        self.name = name
        self.age = age
    def __setattr__(self, key, value):
        if key == &#39;name&#39;:
            print(&#39;student的name属性被修改！&#39;)
        super().__setattr__(key,value)  # 简化的写法，实际上super()内带有默认参数，与如下同理。
        # super(type(self), self).__setattr__(key,value)  # 直接引用父类未经重写的__setattr__方法操纵对象的属性
        # super(info, self).__setattr__(key,value)  # 也可以显式地在super()内用类自己的名称。不建议。因为显式引用失去了通用性。直接type(self)就能表示self所属的类info
    def get_name(self):
        return self.name

再来看__getattr__，任何对属性值的获取，当属性不存在时，就可能会触发这个方法。简单的场景：如果用户想要获取的属性名不存在，不要报错，而是打印信息，并返回None
class info(object):
    message = &#39;student&#39;
    def __init__(self, name, age):
        self.name = name
        self.age = age
    def __setattr__(self, key, value):
        if key == &#39;name&#39;:
            print(&#39;student的name属性被修改！&#39;)
        super().__setattr__(key,value)
    def __getattr__(self,item):
        print(&#39;该属性&#123;&#125;不存在！&#39;.format(item))
        return None
    def get_name(self):
        return self.name
        
再来看__getattribute__，任何对属性值的访问，无论属性是否存在，都要触发这个方法。实现这个方法后如果同时实现了__getattr__，在__getattribute__没报错或没有显式调用__setattr__的情况下，就不会执行__getattr__。
之所以未亲自实现__getattribute__方法时，属性找不到时才会找__getattr__，就是因为解释器已经先调用__getattribute__找过一遍，报了属性未找到的错误，才转到__getattr__的。
简单的场景：任何时刻用户获取name属性时要给出警告&quot;有人访问了name属性！&quot;
class info(object):
    message = &#39;student&#39;
    def __init__(self, name, age):
        self.name = name
        self.age = age
    def __setattr__(self, key, value):
        if key == &#39;name&#39;:
            print(&#39;student的name属性被修改！&#39;)
        super().__setattr__(key,value)
    def __getattr__(self,item):  # 显式调用或__getattribute__方法报错时才调用。在本程序中，未找到属性怎么办这个事情在__getattribute__中实现了，所以此处永远不会被触发。
        print(&#39;我是__getattr__，报警：属性&#123;&#125;不存在！&#39;.format(item))
        return None
    def __getattribute__(self,item):  # 任何对属性值的访问，注意！包括__dict__，都会先触发这个方法。对__dict__中键值的访问，必须先访问__dict__，所以本质上还是访问了属性__dict__！
        if item not in super().__getattribute__(&#39;__dict__&#39;):  # 利用父类未重写的__getattribute__来避免查找属性__dict__时发生无限递归。
            print(&#39;我是__getattribute__,报警：属性&#123;&#125;不存在！&#39;.format(item))
            return None
        return super().__getattribute__(item)  # 利用父类未重写的__getattribute__来实现本身应该有的功能并避免无限递归：返回属性的值
    def get_name(self):
        return self.name

介绍到这，对对象的属性的操纵的触发，已经有了“优先级”的概念。默认的查找： __dict__字典 → (找不到才)向上父类__dict__字典...
对于内置方法们，如果实现了__getattribute__，那么一定先跳转到__getattribute__，按：__getattribute__  →  (报错才)__getattr__  →  (报错才)向上父类...  的顺序进行的。
（注：原始的__getattribute__做的事：先从属性字典找，再从类共享数据（类也有个字典，存了数和函数）找，再看是否实现了__getattr__，有就跳转，没有就报错。）
至于向上父类的顺序，遵循MRO线性表。MRO线性表的计算规则已经在本博客随笔有详细介绍和python算法实现。

3.内置方法：__set__,__get__,__delete__
有机会代理对属性的赋值、修改、查找、删除等行为。

简单的例子：实现功能：当用户调用不同的student对象的age的时候，返回None，并报警。
class des:
    count = dict()
    def __get__(self, instance, owner):
        des.count[instance] = des.count.get(instance,0)+1
        print(&#39;你这是第&#123;&#125;次在对象&#123;&#125;中找不到age属性了！停，不会再去__getattr__了&#39;.format(des.count[instance, default=0],instance))
        return None
class info:
    age = des()  # age成为了共享属性，因为构造函数没给出self.age
    def __init__(self,name):  # 可以看出，构造函数根本没有实现age属性self.age
        self.name = name
    def __setattr__(self,key,value):
        super().__setattr__(key,value)
    def __getattr__(self,item):  # 实在找不到就只返回个None
        return None
    def __getattribute__(self,item):
        return super().__getattribute__(item)  # 其实重写等于没有重写。因为完全借鉴了原始方法
student1,student2=info(&#39;student1&#39;),info(&#39;student2&#39;)
student1.age # 你这是第一次在student1中找不到属性age！
student1.age # 你这是第二次在student1中找不到属性age！
student2.age # 你这是第一次在student2中找不到属性age！

以上程序可以用查找顺序很容易理解：
首先，必然找__getattribute__，无论是否重写了，都要尝试找它。对于默认的__getattribute__，先在对象的属性字典找，没找到age。再在类定义中找共享属性，找到了age。发现age是一个被des类实例化过的对象，本来应该返回这个对象，但是注意内置方法__get__代理了此行为！

再看__set__。简单的例子：实现功能：当用户初始化时，或修改属性值时，如果value值类型不对，就要报警。
class des:
    def __set__(self,instance,value):
        if isinstance(value,str):
            print(&#39;类型正确！&#39;)
            self.__dict__[instance]=value
        else:
            print(&#39;类型错误！没能成功设置值！&#39;)
class info:
    name = des()
    def __init__(self,name):
        self.name=name
    def __setattr__(self,key,value):
        super().__setattr__(key,value)
    def __getattr__(self,item):
        return None
    def __getattribute__(self,item):
        return super().__getattribute__(item)
student = info(&#39;aaa&#39;)  # 类型正确！
student.name = 3  # 类型错误！没能成功设置值！

以上程序的执行顺序是：
首先，构造函数执行到self.name=name这步，必然找__setattr__，无论是否重写了。然后，找属性name准备修改。但是发现此时并不能在对象的属性字典中找到name。再在类的定义中找共享属性，找到了name。name是一个被des类实例化过的对象，本来应该把这个对象直接改成name，但是注意内置方法__set__代理了此行为！
在__set__中，将student对象和name值，传入函数，然后把name值存到了类共享属性name对象的属性字典中，key值为student对象。student对象确实可以当做key值，因为它是可hash的。
为什么这么做？因为下次实例另一个学生：student_2 = info(&#39;aaa&#39;)的时候，这个类共享属性name对象的属性字典中key值就是student_2对象。因此虽然name是它们共有的属性，但是name这个对象的属性字典里，已经把每个student全部区分开了，不同的student，对应不同的name值！

接下来，要访问student的name值：
print(student.name)

显然不行！这样打印出来的，其实是info类里的共享属性：对象name。联想到上面实现__set__时，是将name值存在了对象name的字典里。因此，正确的访问方式是：
print(student.name.__dict__[student])

这也太麻烦了！这时，联想到__get__方法：__get__方法有机会代理__attribute__属性查找中找到类共享属性后的部分行为。所以，应当把__get__和__set__一起用，放在des类的定义里面：
class des:
    def __get__(self,instance,owner):
        return self.__dict__[instance]
    def __set__(self,instance,value):
        if isinstance(value,str):
            print(&#39;类型正确！&#39;)
            self.__dict__[instance]=value
        else:
            print(&#39;类型错误！没能成功设置值！&#39;)
class info:
    name = des()
    def __init__(self,name):
        self.name=name
    def __setattr__(self,key,value):
        super().__setattr__(key,value)
    def __getattr__(self,item):
        return None
    def __getattribute__(self,item):
        return super().__getattribute__(item)
student = info(&#39;aaa&#39;)  # 类型正确！
student.name = 3  # 类型错误！没能成功设置值！
print(student.name)  # 打印出了aaa

这就是很简单的一种理解描述符作用机制的方式。所谓描述符，就是实现了__get__或__set__或__delete__的类。描述符一定要在另一个类的共享属性中实例化，这样就可以作用于另一个类的对象的属性操作。
只实现__get__的，是非数据描述符，当属性字典查无时，会跳转到该非数据描述符的__get__中。
实现__set__或__delete__的，是数据描述符，但是一般至少同时实现__set__和__get__。这样做出的描述符，对值的修改或查询，都会触发。
注意，如果存在删除属性的动作，记得实现__delete__，否则它会找不到属性删。因为属性并非在__dict__里。

数据描述符是把属性值存到另一个类中，而不是属性字典中，所以，直接使用__dict__来查看对象的属性和属性值会出问题，因为属性字典的查找先于__get__。对吗？
实际上，对于数据描述符，一但实现，属性查找的优先级就被悄悄的提升了。原来__get__是在字典里找不到才跳转到它，现在是先于字典查找就要跳转到它了！
优先级：类属性(这里应指直接以类名.属性名来操纵的属性，例如info.des=None就直接把描述符删了。) &gt; 数据描述符 &gt; 实例属性 &gt; 非数据描述符 &gt; __getattr__


使用描述符的优势：可以观察到，描述符定义清晰，可重用，“只做一件事”，甚至还有机会把构造函数引入描述符中，从而实现更深层次定制。所以，描述符已经被大量使用在很多场景。
Learning about descriptors not only provides access to a larger toolset, it creates a deeper understanding of how Python works and an appreciation for the elegance of its design.
使用描述符的劣势：直接调用对象的__dict__，发现并不存在name属性，与常见范式不是很统一。当然，可以利用描述符内定制性极强这一优势解决：在__set__中增加一行代码：
instance.__dict__[&#39;name&#39;] = value  # 保持使用描述符后，__dict__中属性的统一性。当然，__get__并不会去查找它。

杀器祭出：

定制化类型限制：
class Type:
    def __init__(self, attrname, typename):
        self.attrname = attrname
        self.typename = typename

    def __set__(self, instance, value):
        if not isinstance(value, self.typename):
            raise TypeError(
                &#39;&#123;&#125;必须为&#123;&#125;类型，而接收到的&#123;&#125;却是&#123;&#125;类型。&#39;.format(
                    self.attrname,
                    self.typename,
                    value,
                    type(value)))
        else:
            instance.__dict__[self.attrname] = value
            print(&#39;赋值属性&#123;&#125;值为&#123;&#125;&#39;.format(self.attrname, value))

    def __get__(self, instance, cls):
        if instance is None:
            return self
        else:
            return instance.__dict__[self.attrname]

    def __delete__(self, instance):
        del instance.__dict__[self.attrname]


def typeassert(**kwargs):
    def decorator(cls):
        def wrapper(*kw_):
            for attrname, typename in kwargs.items():
                setattr(cls, attrname, Type(attrname, typename))
            return cls(*kw_)
        return wrapper
    return decorator


@typeassert(name=str, age=int, salary=float)
class Info:
    def __init__(self, name, age, salary):
        self.name = name
        self.age = age
        self.salary = salary

    def __str__(self):
        return &#39;name:&#123;&#125;,age:&#123;&#125;,salary:&#123;&#125;&#39;.format(
            self.name, self.age, self.salary)


dai = Info(&#39;poincare&#39;, 23, 1000.0)

描述符实现绑定到类的方法：
class Classmethod:  # 类装饰器写法
    def __init__(self, funcname):
        self.funcname = funcname

    def __get__(self, instance, cls):
        def wrappers(*kw, **kwargs):
            k = self.funcname(cls, *kw, **kwargs)
            return k
        return wrappers


class Info:
    info = [1, 2, 3]

    def __init__(self, name):
        self.name = name

    @Classmethod
    def sayinfo(self, alladd):
        self.info = [k + alladd for k in self.info]
        return self.info


p = Info(&#39;poin&#39;)
print(p.sayinfo(1))
print(Info.sayinfo(1))


描述符实现静态方法：
class Staticmethod:
    def __init__(self, funcname):
        self.funcname = funcname

    def __get__(self, instance, cls):
        def wrappers(*kw, **kwargs):
            k = self.funcname(*kw, **kwargs)
            return k
        return wrappers


class Info:
    info = [1, 2, 3]

    def __init__(self, name):
        self.name = name

    @Staticmethod
    def add(x, y):
        print(x + y)
        return x + y


p = Info(&#39;dai&#39;)  # 实例化
# p.add(1)  # 报错，缺少一个位置参数
p.add(1, 2)  # 可以返回结果
# Info.add(1)  # 报错，缺少一个位置参数
Info.add(1, 2)  # 可以返回结果   实际上，实例.add 和 类.add 是同一函数，有完全一样的地址。
print(p.add)
print(Info.add)
</code></pre>
<h2 id="python配置文件的封装"><a href="#python配置文件的封装" class="headerlink" title="python配置文件的封装"></a><strong>python配置文件的封装</strong></h2><pre><code class="python"># config文件封装
# 对配置文件进行封装
# 导入配置文件模块
from configparser import ConfigParser

# 创建一个配置文件类
class HandleConfig:
    &quot;&quot;&quot;
    处理配置文件
    &quot;&quot;&quot;
    # 定义一个实例属性
    def __init__(self,filename):
        # 定义一个名称：filename实例属性
        self.filename = filename
        # 创建配置解释器config对象
        self.config = ConfigParser()
        # 指定读取的配置文件, 无需变量接收读取内容
        self.config.read(self.filename,encoding=&quot;utf8&quot;)
    # 定义一个获取配置文件数据的实例方法，获取配置文件中对应不同数据类型的值
    # get方法和字典中的get有区别
    # 第一个参数section为区域名，第二个参数option为选项名
    # 从配置文件中，使用索引(方括号)或者使用get方法，读取出来的所有数据都是字符串类型
    def get_value(self,section,option):
        # 通过定义的实例属性self.config对象，调用ConfigParser类中的get方法，并将结果进行返回
        return self.config.get(section,option)
    # 可以使用getint(区域名，选项名)只能读取int类型的数据，否则会报错
    def get_int(self,section,option):
        return self.config.getint(section,option)
    # 可以使用getfloat(区域名，选项名)只能读取int类型和浮点类型的数据，否则会报错
    def get_float(self,section,option):
        return self.config.getfloat(section,option)
    # 可以使用getboolean(区域名，选项名)来读取布尔类似的数据
    def get_boolean(self,section,option):
        return self.config.getboolean(section,option)
    # 通过eval函数对get获取的字符串进行转义，获取配置文件中不同的数据类型
    def get_eval_data(self,section,option):
        return eval(self.config.get(section, option))

    # 定义写入配置文件的静态方法：
    # 为什么不能定义在内属性，内属性是每个对象的公共资源，每个对象都可以访问
    # 读配置和写配置都用相同的对象，读配置和写配置尽量不要用相同的对象
    @staticmethod
    # 定义write_config方法
    # datas里面传入需要写入的数据，往往是一个嵌套字典的格式
    # filename是文件名
    def write_config(datas,filename):
        # 创建一个config对象，可以用来调用ConfigParser类中的方法
        config = ConfigParser()
        # 在空配置config中写入配置
        # config还没有读取数据时，可以类似一个空字典
        # datas是需要写入的数据
        for key in datas:
            config[key] = datas[key]
        with open(filename,&quot;w&quot;,encoding=&quot;utf8&quot;) as file:
            config.write(file)

# 定义do_config对象，用来调用HandleConfig类中的方法
# testcase.conf为对应的文件名称
do_config = HandleConfig(&quot;testcase.conf&quot;)

# 魔法变量
if __name__ == &quot;__main__&quot;:
    # 创建do_config类的对象
    do_config = HandleConfig(&quot;testcase.conf&quot;)
    # 需要写入到配置文件的数据
    datas = &#123;
        &quot;excel&quot;: &#123;
            &quot;cases_path&quot;: &quot;cases.xlsx&quot;
        &#125;,
        &quot;user&quot;: &#123;
            &quot;username&quot;: &quot;hc&quot;,
            &quot;password&quot;: &quot;123456&quot;
        &#125;

    &#125;
    # 通过do_config对象调用write_config静态方法，将datas数据写入到write_cases.ini文件中
    do_config.write_config(datas,&quot;write_cases.ini&quot;)
    pass



# 读取excel文件的封装
import openpyxl

class CaseData:
    &quot;&quot;&quot;测试用例数据类，专门用来创建对象，存放用例数据&quot;&quot;&quot;
    pass

class ReadExcle(object):

    def __init__(self, filename, sheetname):
        self.filename = filename
        self.sheetname = sheetname

    def open(self):
        &quot;&quot;&quot;打开工作表和表单&quot;&quot;&quot;
        self.wb = openpyxl.load_workbook(self.filename)
        self.sh = self.wb[self.sheetname]

    def read_data(self):
        &quot;&quot;&quot;读取数据的方法&quot;&quot;&quot;
        # 打开工作簿和表单
        self.open()
        # 将表单中的内容，按行获取所有的格子
        rows = list(self.sh.rows)
        # 创建一个空列表，用例存放所有的用例数据
        cases = []
        # 获取表头，放到一个列表中
        title = [c.value for c in rows[0]]
        # 获取除表头以外的其他行中的数据
        for r in rows[1:]:
            # 每遍历一行，创建一个列表，用例存放该行的数据
            data = [c.value for c in r]
            # 将表头和该行的数据进行聚合打包，转换字典
            case_data = dict(zip(title, data))
            # 将该行的用例数据加入到cases这个列表中
            cases.append(case_data)
        # 关闭工作簿对象
        self.wb.close()
        # 将读取好的数据返回出去
        return cases

    def read_data_obj(self):
        &quot;&quot;&quot;读取数据的方法,数据返回的是列表嵌套对象的形式&quot;&quot;&quot;
        # 打开工作簿和表单
        self.open()
        # 将表单中的内容，按行获取所有的格子
        rows = list(self.sh.rows)
        # 创建一个空列表，用例存放所有的用例数据
        cases = []
        # 通过列表推导式获取表头，放到一个列表中
        title = [c.value for c in rows[0]]
        # 获取除表头以外的其他行中的数据
        for r in rows[1:]:
            # 通过列表推导式，获取改行的数据，放到一个列表中
            data = [c.value for c in r]
            # 创建一个用例数据对象
            case = CaseData()
            # 将表头和该行的数据进行聚合打包，然后进行遍历
            for i in zip(title, data):
                # 通过反射机制，将表头设为对象属性，对应值设为对象的属性值
                setattr(case, i[0], i[1])
            # 将该行的用例数据加入到cases这个列表中
            cases.append(case)
        # 关闭工作薄
        self.wb.close()
        # 将读取好的数据返回出去
        return cases

    def write_data(self, row, column, value):
        &quot;&quot;&quot;写入数据&quot;&quot;&quot;
        # 打开工作簿和表单
        self.open()
        # 写入内容
        self.sh.cell(row=row, column=column, value=value)
        # 保存文件
        self.wb.save(self.filename)
        # 关闭工作簿
        self.wb.close()


if __name__ == &#39;__main__&#39;:
    read = ReadExcle(&#39;cases.xlsx&#39;, &#39;register&#39;)
    # 读取
    # data = read.read_data_obj()
    # print(data)
    # read.write_data(2, 4, &#39;通过&#39;)
    # read.write_data(3, 4, &#39;未通过&#39;)

    
# 获取日志封装
import logging

from Day17_2020_03_11.Python_1102_handle_config.handle_yaml import do_yaml


class MyLogger(object):

    @classmethod
    def create_logger(cls):
        &quot;&quot;&quot;创建日志收集器&quot;&quot;&quot;
        # 创建一个日志收集器
        my_log = logging.getLogger(do_yaml.read(&quot;log&quot;, &quot;log_name&quot;))
        # 设置日志收集器的收集等级
        my_log.setLevel(do_yaml.read(&quot;log&quot;, &quot;logger_level&quot;))
        # 设置日志输出的格式
        formater = logging.Formatter(do_yaml.read(&quot;log&quot;, &quot;formatter&quot;))
        # 创建一个输出导控制台的日志输出渠道
        sh = logging.StreamHandler()
        sh.setLevel(do_yaml.read(&quot;log&quot;, &quot;stream_level&quot;))
        # 设置输出导控制台的格式
        sh.setFormatter(formater)
        # 将输出渠道添加到日志收集器中
        my_log.addHandler(sh)

        # 创建一个输出导文件的渠道
        fh = logging.FileHandler(filename=do_yaml.read(&quot;log&quot;, &quot;logfile_name&quot;),
                                 encoding=&#39;utf8&#39;)
        fh.setLevel(do_yaml.read(&quot;log&quot;, &quot;logfile_level&quot;))
        # 设置输出导文件的日志格式
        fh.setFormatter(formater)
        # 将输出渠道添加到日志收集器中
        my_log.addHandler(fh)
        return my_log


do_log = MyLogger.create_logger()


# yaml读取配置文件信息封装
import yaml

class HandleYaml:
    def __init__(self, filename):
        with open(filename, encoding=&quot;utf-8&quot;) as one_file:
            self.datas = yaml.full_load(one_file)

    def read(self, section, option):
        &quot;&quot;&quot;
        读数据
        :param section: 区域名
        :param option: 选项名
        :return:
        &quot;&quot;&quot;
        return self.datas[section][option]

    @staticmethod
    def write(datas, filename):
        &quot;&quot;&quot;
        写数据
        :param datas: 嵌套字典的字典
        :param filename: yaml文件路径
        :return:
        &quot;&quot;&quot;
        with open(filename, mode=&quot;w&quot;, encoding=&quot;utf-8&quot;) as one_file:
            yaml.dump(datas, one_file, allow_unicode=True)


do_yaml = HandleYaml(&quot;testcase01.yaml&quot;)

if __name__ == &#39;__main__&#39;:
    do_yaml = HandleYaml(&quot;testcase01.yaml&quot;)
    datas = &#123;
        &quot;excel&quot;: &#123;
            &quot;cases_path&quot;: &quot;cases.xlsx&quot;,
            &quot;result_col&quot;: 5
        &#125;,
        &quot;msg&quot;: &#123;
            &quot;success_result&quot;: &quot;通过&quot;,
            &quot;fail_result&quot;: &quot;Fail&quot;
        &#125;
    &#125;
    do_yaml.write(datas, &quot;write_datas.yaml&quot;)
    pass


# 测试用例类：test_cases
import unittest

from Day17_2020_03_11.Python_1102_handle_config.register import register
from Day17_2020_03_11.Python_1102_handle_config.handle_excel import ReadExcle
from Day17_2020_03_11.Python_1102_handle_config.ddt import ddt, data
from Day17_2020_03_11.Python_1102_handle_config.handle_yaml import do_yaml
from Day17_2020_03_11.Python_1102_handle_config.handle_log import do_log


@ddt
class RegisterTestCase(unittest.TestCase):
    # excle = ReadExcle(&quot;cases.xlsx&quot;, &#39;register&#39;)
    excle = ReadExcle(do_yaml.read(&quot;excel&quot;, &quot;cases_path&quot;), &#39;register&#39;)
    cases = excle.read_data_obj()

    @data(*cases)
    def test_register(self, case):

        # 第一步  准备用例数据
        # 获取用例的行号
        row = case.case_id + 1
        # 获取预期结果
        excepted = eval(case.excepted)
        # 获取用例入参
        data = eval(case.data)

        # 第二步： 调用功能函数，获取实际结果
        res = register(*data)

        # 第三步：比对预期结果和实际结果
        try:
            self.assertEqual(excepted, res)
        except AssertionError as e:
            self.excle.write_data(row=row,
                                  column=do_yaml.read(&quot;excel&quot;, &quot;result_col&quot;),
                                  value=do_yaml.read(&quot;msg&quot;, &quot;fail_result&quot;))
            # do_log.error(&quot;断言异常: &#123;&#125;&quot;.format(e))
            do_log.error(f&quot;断言异常: &#123;e&#125;&quot;)
            raise e
        else:
            self.excle.write_data(row=row,
                                  column=do_yaml.read(&quot;excel&quot;, &quot;result_col&quot;),
                                  value=do_yaml.read(&quot;msg&quot;, &quot;success_result&quot;))

            
# 测试运行程序：run_test
import unittest

# 导入模块的快捷键：alt + enter
from datetime import datetime

from Day17_2020_03_11.Python_1102_handle_config import test_cases
from Day17_2020_03_11.Python_1102_handle_config.HTMLTestRunnerNew import HTMLTestRunner
from Day17_2020_03_11.Python_1102_handle_config.handle_yaml import do_yaml

# 创建测试套件
suite = unittest.TestSuite()

# 加载用例用例到套件
loader = unittest.TestLoader()
suite.addTest(loader.loadTestsFromModule(test_cases))

result_full_path = do_yaml.read(&#39;report&#39;, &#39;name&#39;) + &#39;_&#39; +  \
                   datetime.strftime(datetime.now(), &#39;%Y%m%d%H%M%S&#39;) + &#39;.html&#39;
with open(result_full_path, &#39;wb&#39;) as fb:
    # 创建测试运行程序
    runner = HTMLTestRunner(stream=fb,
                            title=do_yaml.read(&#39;report&#39;, &#39;title&#39;),
                            description=do_yaml.read(&#39;report&#39;, &#39;description&#39;),
                            tester=do_yaml.read(&#39;report&#39;, &#39;tester&#39;))
    # 执行测试套件中的用例
    runner.run(suite)
</code></pre>
<h2 id="pyspark操作hive"><a href="#pyspark操作hive" class="headerlink" title="pyspark操作hive"></a><strong>pyspark操作hive</strong></h2><pre><code class="python">import sys
sys.path.append(&quot;./xxx.zip&quot;)
import os ss
os.environ.get(&quot;SPARK_HOME&quot;)
os.environ.get(&quot;JAVA_HOME&quot;)
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;test_super&quot;).enableHiveSupport().getOrCreate()
</code></pre>
<h2 id="Hbase-RowKey设计的二三事"><a href="#Hbase-RowKey设计的二三事" class="headerlink" title="Hbase RowKey设计的二三事"></a><strong>Hbase RowKey设计的二三事</strong></h2><pre><code class="scala">
</code></pre>
<h2 id="浅谈hbase二级索引"><a href="#浅谈hbase二级索引" class="headerlink" title="浅谈hbase二级索引"></a><strong>浅谈hbase二级索引</strong></h2><pre><code class="scala">
</code></pre>
<h2 id="kafka-api-java版"><a href="#kafka-api-java版" class="headerlink" title="kafka api java版"></a><strong>kafka api java版</strong></h2><pre><code>
</code></pre>
<h2 id="kafka-api-scala版"><a href="#kafka-api-scala版" class="headerlink" title="kafka api scala版"></a><strong>kafka api scala版</strong></h2><pre><code class="scala">package com.linys.scala.KAFKA_producer
import java.util.Properties
import org.apache.kafka.clients.producer.&#123;KafkaProducer, ProducerRecord, RecordMetadata&#125;

/**
  * 实现producer
  */
object KafkaProducerDemo &#123;
  def main(args: Array[String]): Unit = &#123;
    val prop = new Properties
    object ProduceDemo &#123;
  def main(args: Array[String]): Unit = &#123;

    // 指定topic
    val topic = &quot;testTopic&quot;
    val props = new Properties()
    // 必配的参数
    props.setProperty(&quot;bootstrap.servers&quot;, &quot;hdp-01:9092,hdp-02:9092,hdp-03:9092&quot;)
    props.setProperty(&quot;key.serializer&quot;, classOf[StringSerializer].getName)
    props.setProperty(&quot;value.serializer&quot;, classOf[StringSerializer].getName)

    // 其他配置
    props.setProperty(&quot;acks&quot;, &quot;all&quot;)
    props.setProperty(&quot;retries&quot;,0)
    props.setProperty(&quot;batch.size&quot;,10)  
   // 参数配置的第二种方式。使用 props.put(k,v)方法。
   

    // 构造生产者的API
    val producer: KafkaProducer[String, String] = new KafkaProducer[String, String](props)

    for (i &lt;- 1 to 100000) &#123;
      Thread.sleep(500)
      // 利用ProducerRecord 来封装消息
      //    val record = new ProducerRecord[String, String](topic, &quot;123&quot;) // topic  value
      //    new ProducerRecord[String,String](&quot;&quot;,&quot;&quot;,&quot;&quot;) ///topic  key   value
      val part = i % 3 // 取分区编号
      val record = new ProducerRecord[String, String](topic, part, &quot;&quot;, &quot;i&quot; + i)
      producer.send(record)
    &#125;
    producer.close()
  &#125;
&#125;
    


import java.util
import java.util.Properties

import org.apache.kafka.clients.consumer.&#123;ConsumerRecords, KafkaConsumer&#125;
import org.apache.kafka.common.serialization.&#123;StringDeserializer&#125;

/**
  * Created by Huige 
  * Email: 824203453@qq.com 
  * DATE: 2019/3/19
  * Desc: 
  */
object ConsumerDemo &#123;
  def main(args: Array[String]): Unit = &#123;

    val topic = &quot;testTopic&quot;
    val props = new Properties()
    // 必配的参数
    props.setProperty(&quot;bootstrap.servers&quot;, &quot;hdp-01:9092,hdp-02:9092,hdp-03:9092&quot;)
    // key 和value的反序列化
    props.setProperty(&quot;key.deserializer&quot;, classOf[StringDeserializer].getName)
    props.setProperty(&quot;value.deserializer&quot;, classOf[StringDeserializer].getName)
    props.setProperty(&quot;group.id&quot;,&quot;1232&quot;)

    // 其他配置
    // [latest, earliest, none]
    props.setProperty(&quot;auto.offset.reset&quot;,&quot;earliest&quot;)
    // 是否自动提交偏移量管理  默认是true
    //    props.setProperty(&quot;enable.auto.commit&quot;,&quot;false&quot;)
    //    props.setProperty(&quot;auto.commit.interval.ms&quot;,&quot;5000&quot;) // 自定提交时间  默认5000ms
    val consumer: KafkaConsumer[String, String] = new KafkaConsumer[String,String](props)

    val lst = new util.ArrayList[String]()
    lst.add(&quot;testTopic&quot;)
    // 订阅主题
    consumer.subscribe(lst)
     //    consumer.subscribe(util.Arrays.asList(&quot;&quot;))

    while(true)&#123;
      // 如果Kafak中没有消息，会隔timeout这个值读一次。比如上面代码设置了2秒，也是就2秒后会查一次。
      // 如果Kafka中还有消息没有消费的话，会马上去读，而不需要等待。
      val records: ConsumerRecords[String, String] = consumer.poll(2000)
      import scala.collection.JavaConversions._
      for(i &lt;- records)&#123;
        // ConsumerRecord
        // ConsumerRecord(topic = testTopic, partition = 0, offset = 660, CreateTime = 1552984524657, checksum = 3225686705, serialized key size = 0, serialized value size = 4, key = , value = i966)
        println(i)
       //  println(i.partition())
      &#125;
    &#125;
  &#125;
&#125;
</code></pre>
<h2 id="pulsar基础命令"><a href="#pulsar基础命令" class="headerlink" title="pulsar基础命令*"></a><em>pulsar基础命令</em>*</h2><pre><code class="powershell">### stanalone 启动
bin/pulsar standalone ： 当前terminal运行，terminal关闭，服务关闭
pulsar-daemon start/stop standalone ： 后台运行的standalone服务模式

###  client
生产
bin/pulsar-client produce my-topic --messages &quot;hello-pulsar&quot;
向my-topic这个topic生产数据，内容为“hello-pulsar”，如果topic不存在，pulsar会自动创建

消费
bin/pulsar-client consume my-topic -s &quot;first-subscription&quot;
消费my-topic的数据，订阅名称为“first-subscription&quot;, 如果topic不存在，pulsar会自动创建

### tenants
查看所有tenants                /pulsar-admin tenants list
创建tenants                    pulsar-admin tenants create my-tenant
删除tenants                    pulsar-admin tenants delete my-tenant

### broker
查看存活的broker信息            pulsar-admin brokers list use
查看broke如上的namesapce        pulsar-admin brokers namespaces use --url broker1.use.org.com:8080
查看可以动态更新的配置           pulsar-admin brokers list-dynamic-config
查看已经动态更新过的配置         pulsar-admin brokers get-all-dynamic-config
动态更新配置                    pulsar-admin brokers update-dynamic-config brokerShutdownTimeoutMs 100

### namespace
查看tenant下的所有namespace     pulsar-admin namespaces list test-tenant
创建namespace                  pulsar-admin namespaces create test-tenant/test-namespace
查看namespace策略               pulsar-admin namespaces policies test-tenant/test-namespace
删除namespace                  pulsar-admin namespaces delete test-tenant/ns1

### permission
pulsar的权限控制是在namespace级别的，

授权
pulsar-admin namespaces grant-permission test-tenant/ns1 \
--actions produce,consume \
--role admin10

注意： 当broker.conf中的authorizationAllowWildcardsMatching 为true时，支持通配符匹配，例如，
pulsar-admin namespaces grant-permission test-tenant/ns1 \
--actions produce,consume \
--role &#39;my.role.*&#39;

获取授权信息
pulsar-admin namespaces permissions test-tenant/ns1

撤销授权
pulsar-admin namespaces revoke-permission test-tenant/ns1 \
--role admin10

### persistent topics
格式： persistent://tenant/namespace/topic
查看namespace下的topic信息    pulsar-admin persistent list my-tenant/my-namespace
列举persistent topic          pulsar-admin topics list tenant/namespace

给客户端添加针对于某个topic的role（许可）
pulsar-admin persistent grant-permission
--actions produce,consume --role application1
persistent://test-tenant/ns1/topic1

获取许可信息
pulsar-admin persistent permissions \
persistent://test-tenant/ns1/tp1

回滚许可
pulsar-admin persistent revoke-permission \
--role application1 \
persistent://test-tenant/ns1/tp1 \

删除topic
pulsar-admin persistent delete \
persistent://test-tenant/ns1/tp1 \

下线topic
pulsar-admin persistent unload \
persistent://test-tenant/ns1/tp1

查看topic相关的统计信息
pulsar-admin persistent stats \
persistent://test-tenant/ns1/tp1

查看topic内部统计信息
pulsar-admin persistent stats-internal \
persistent://test-tenant/ns1/tp1

peek 消息
pulsar-admin persistent peek-messages \
--count 10 --subscription my-subscription \
persistent://test-tenant/ns1/tp1

跳过消费部分消息
pulsar-admin persistent skip \
--count 10 --subscription my-subscription \
persistent://test-tenant/ns1/tp1

跳过所有数据
pulsar-admin persistent skip-all \
--subscription my-subscription \
persistent://test-tenant/ns1/tp1 \

重置消费cursor到几分钟之前
pulsar-admin persistent reset-cursor \
--subscription my-subscription --time 10 \
persistent://test-tenant/ns1/tp1 \

查找topic所在的broker信息
pulsar-admin persistent lookup \
persistent://test-tenant/ns1/tp1 \

获取topic的bundle信息
pulsar-admin persistent bundle-range \
persistent://test-tenant/ns1/tp1 \
&quot;0x00000000_0xffffffff&quot;

查询topic的订阅信息
pulsar-admin persistent subscriptions \
persistent://test-tenant/ns1/tp1 \

取消订阅
pulsar-admin persistent unsubscribe \
--subscription my-subscription \
persistent://test-tenant/ns1/tp1 \

最后一条消息的MessageID
pulsar-admin topics last-message-id topic-name
non-persistent topics
格式 ： non-persistent://tenant/namespace/topic

获取统计信息
pulsar-admin non-persistent stats \
non-persistent://test-tenant/ns1/tp1 \

获取内存统计信息
pulsar-admin non-persistent stats-internal \
non-persistent://test-tenant/ns1/tp1 \

创建分区topic
bin/pulsar-admin non-persistent create-partitioned-topic \
non-persistent://my-tenant/my-namespace/my-topic \
--partitions 4
注意：需要指明topic名称和分区数量

分区topic的元数据信息
pulsar-admin non-persistent get-partitioned-topic-metadata \
non-persistent://my-tenant/my-namespace/my-topic

下线topic
pulsar-admin non-persistent unload \
non-persistent://test-tenant/ns1/tp1

分区topic
格式： persistent://tenant/namespace/topic

创建topic
bin/pulsar-admin topics create-partitioned-topic \
persistent://my-tenant/my-namespace/my-topic \
--partitions 4

创建非分区topic
$ bin/pulsar-admin topics create persistent://my-tenant/my-namespace/my-topic

获取分区topic的元数据信息
pulsar-admin topics get-partitioned-topic-metadata \
persistent://my-tenant/my-namespace/my-topic

更新topic信息
pulsar-admin topics update-partitioned-topic \
persistent://my-tenant/my-namespace/my-topic \
--partitions 8
注意：修改分区数量时，只能比原来的分区数大

删除topic
bin/pulsar-admin topics delete-partitioned-topic \
persistent://my-tenant/my-namespace/my-topic

获取统计信息
pulsar-admin topics partitioned-stats \
persistent://test-tenant/namespace/topic \
--per-partition

获取内部统计信息
pulsar-admin topics stats-internal \
persistent://test-tenant/namespace/topic

### Schema
上传schema
pulsar-admin schemas upload &lt;topic-name&gt; --filename /path/to/schema-definition-file

获取schema
pulsar-admin schemas get &lt;topic-name&gt;

删除schema
pulsar-admin schemas delete &lt;topic-name&gt;
</code></pre>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1UW411i7Qa?from=search&amp;seid=8467458502881112354">https://www.bilibili.com/video/BV1UW411i7Qa?from=search&amp;seid=8467458502881112354</a></p>
<h2 id="pulsar-api"><a href="#pulsar-api" class="headerlink" title="pulsar api"></a><strong>pulsar api</strong></h2><pre><code>
</code></pre>
<h2 id="Spark自定义外部数据源"><a href="#Spark自定义外部数据源" class="headerlink" title="Spark自定义外部数据源"></a><strong>Spark自定义外部数据源</strong></h2><pre><code class="scala">import org.apache.spark.rdd.RDD
import org.apache.spark.sql.&#123;Row, SQLContext&#125;
import org.apache.spark.sql.sources._
import org.apache.spark.sql.types._

/**
  * Created by rana on 29/9/16.
  */
class CustomDatasourceRelation(override val sqlContext : SQLContext, path : String, userSchema : StructType)
  extends BaseRelation with TableScan with PrunedScan with PrunedFilteredScan with Serializable &#123;

  override def schema: StructType = &#123;
    if (userSchema != null) &#123;
      userSchema
    &#125; else &#123;
      StructType(
        StructField(&quot;id&quot;, IntegerType, false) ::
        StructField(&quot;name&quot;, StringType, true) ::
        StructField(&quot;gender&quot;, StringType, true) ::
        StructField(&quot;salary&quot;, LongType, true) ::
        StructField(&quot;expenses&quot;, LongType, true) :: Nil
      )
    &#125;
  &#125;

  override def buildScan(): RDD[Row] = &#123;
    println(&quot;TableScan: buildScan called...&quot;)

    val schemaFields = schema.fields
    // Reading the file&#39;s content
    val rdd = sqlContext.sparkContext.wholeTextFiles(path).map(f =&gt; f._2)

    val rows = rdd.map(fileContent =&gt; &#123;
      val lines = fileContent.split(&quot;\n&quot;)
      val data = lines.map(line =&gt; line.split(&quot;,&quot;).map(word =&gt; word.trim).toSeq)
      val tmp = data.map(words =&gt; words.zipWithIndex.map&#123;
        case (value, index) =&gt;
          val colName = schemaFields(index).name
          Util.castTo(if (colName.equalsIgnoreCase(&quot;gender&quot;)) &#123;if(value.toInt == 1) &quot;Male&quot; else &quot;Female&quot;&#125; else value,
            schemaFields(index).dataType)
      &#125;)

      tmp.map(s =&gt; Row.fromSeq(s))
    &#125;)

    rows.flatMap(e =&gt; e)
  &#125;

  override def buildScan(requiredColumns: Array[String]): RDD[Row] = &#123;
    println(&quot;PrunedScan: buildScan called...&quot;)

    val schemaFields = schema.fields
    // Reading the file&#39;s content
    val rdd = sqlContext.sparkContext.wholeTextFiles(path).map(f =&gt; f._2)

    val rows = rdd.map(fileContent =&gt; &#123;
      val lines = fileContent.split(&quot;\n&quot;)
      val data = lines.map(line =&gt; line.split(&quot;,&quot;).map(word =&gt; word.trim).toSeq)
      val tmp = data.map(words =&gt; words.zipWithIndex.map&#123;
        case (value, index) =&gt;
          val colName = schemaFields(index).name
          val castedValue = Util.castTo(if (colName.equalsIgnoreCase(&quot;gender&quot;)) &#123;if(value.toInt == 1) &quot;Male&quot; else &quot;Female&quot;&#125; else value,
                                        schemaFields(index).dataType)
          if (requiredColumns.contains(colName)) Some(castedValue) else None
      &#125;)

      tmp.map(s =&gt; Row.fromSeq(s.filter(_.isDefined).map(value =&gt; value.get)))
    &#125;)

    rows.flatMap(e =&gt; e)
  &#125;

  override def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row] = &#123;
    println(&quot;PrunedFilterScan: buildScan called...&quot;)

    println(&quot;Filters: &quot;)
    filters.foreach(f =&gt; println(f.toString))

    var customFilters: Map[String, List[CustomFilter]] = Map[String, List[CustomFilter]]()
    filters.foreach( f =&gt; f match &#123;
      case EqualTo(attr, value) =&gt;
        println(&quot;EqualTo filter is used!!&quot; + &quot;Attribute: &quot; + attr + &quot; Value: &quot; + value)

        /**
          * as we are implementing only one filter for now, you can think that this below line doesn&#39;t mak emuch sense
          * because any attribute can be equal to one value at a time. so what&#39;s the purpose of storing the same filter
          * again if there are.
          * but it will be useful when we have more than one filter on the same attribute. Take the below condition
          * for example:
          * attr &gt; 5 &amp;&amp; attr &lt; 10
          * so for such cases, it&#39;s better to keep a list.
          * you can add some more filters in this code and try them. Here, we are implementing only equalTo filter
          * for understanding of this concept.
          */
        customFilters = customFilters ++ Map(attr -&gt; &#123;
          customFilters.getOrElse(attr, List[CustomFilter]()) :+ new CustomFilter(attr, value, &quot;equalTo&quot;)
        &#125;)
      case _ =&gt; println(&quot;filter: &quot; + f.toString + &quot; is not implemented by us!!&quot;)
    &#125;)

    val schemaFields = schema.fields
    // Reading the file&#39;s content
    val rdd = sqlContext.sparkContext.wholeTextFiles(path).map(f =&gt; f._2)

    val rows = rdd.map(file =&gt; &#123;
      val lines = file.split(&quot;\n&quot;)
      val data = lines.map(line =&gt; line.split(&quot;,&quot;).map(word =&gt; word.trim).toSeq)

      val filteredData = data.map(s =&gt; if (customFilters.nonEmpty) &#123;
        var includeInResultSet = true
        s.zipWithIndex.foreach &#123;
          case (value, index) =&gt;
            val attr = schemaFields(index).name
            val filtersList = customFilters.getOrElse(attr, List())
            if (filtersList.nonEmpty) &#123;
              if (CustomFilter.applyFilters(filtersList, value, schema)) &#123;
              &#125; else &#123;
                includeInResultSet = false
              &#125;
            &#125;
        &#125;
        if (includeInResultSet) s else Seq()
      &#125; else s)

      val tmp = filteredData.filter(_.nonEmpty).map(s =&gt; s.zipWithIndex.map &#123;
        case (value, index) =&gt;
          val colName = schemaFields(index).name
          val castedValue = Util.castTo(if (colName.equalsIgnoreCase(&quot;gender&quot;)) &#123;
            if (value.toInt == 1) &quot;Male&quot; else &quot;Female&quot;
          &#125; else value,
            schemaFields(index).dataType)
          if (requiredColumns.contains(colName)) Some(castedValue) else None
      &#125;)

      tmp.map(s =&gt; Row.fromSeq(s.filter(_.isDefined).map(value =&gt; value.get)))
    &#125;)

    rows.flatMap(e =&gt; e)
  &#125;
&#125;
</code></pre>
<pre><code class="scala">import org.apache.spark.sql.types.StructType

case class CustomFilter(attr : String, value : Any, filter : String)

object CustomFilter &#123;
  def applyFilters(filters : List[CustomFilter], value : String, schema : StructType): Boolean = &#123;
    var includeInResultSet = true

    val schemaFields = schema.fields
    val index = schema.fieldIndex(filters.head.attr)
    val dataType = schemaFields(index).dataType
    val castedValue = Util.castTo(value, dataType)

    filters.foreach(f =&gt; &#123;
      val givenValue = Util.castTo(f.value.toString, dataType)
      f.filter match &#123;
        case &quot;equalTo&quot; =&gt; &#123;
          includeInResultSet = castedValue == givenValue
          println(&quot;custom equalTo filter is used!!&quot;)
        &#125;
        case _ =&gt; throw new UnsupportedOperationException(&quot;this filter is not supported!!&quot;)
      &#125;
    &#125;)

    includeInResultSet
  &#125;
&#125;
</code></pre>
<pre><code class="scala">import org.apache.hadoop.fs.Path
import org.apache.spark.sql.&#123;DataFrame, SQLContext, SaveMode&#125;
import org.apache.spark.sql.sources.&#123;BaseRelation, CreatableRelationProvider, RelationProvider, SchemaRelationProvider&#125;
import org.apache.spark.sql.types.StructType

/**
  * Created by rana on 29/9/16.
  */
class DefaultSource extends RelationProvider with SchemaRelationProvider with CreatableRelationProvider &#123;
  override def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation = &#123;
    createRelation(sqlContext, parameters, null)
  &#125;

  override def createRelation(sqlContext: SQLContext, parameters: Map[String, String], schema: StructType): BaseRelation = &#123;
    val path = parameters.get(&quot;path&quot;)
    path match &#123;
      case Some(p) =&gt; new CustomDatasourceRelation(sqlContext, p, schema)
      case _ =&gt; throw new IllegalArgumentException(&quot;Path is required for custom-datasource format!!&quot;)
    &#125;
  &#125;

  override def createRelation(sqlContext: SQLContext, mode: SaveMode, parameters: Map[String, String],
                              data: DataFrame): BaseRelation = &#123;
    val path = parameters.getOrElse(&quot;path&quot;, &quot;./output/&quot;) //can throw an exception/error, it&#39;s just for this tutorial
    val fsPath = new Path(path)
    val fs = fsPath.getFileSystem(sqlContext.sparkContext.hadoopConfiguration)

    mode match &#123;
      case SaveMode.Append =&gt; sys.error(&quot;Append mode is not supported by &quot; + this.getClass.getCanonicalName); sys.exit(1)
      case SaveMode.Overwrite =&gt; fs.delete(fsPath, true)
      case SaveMode.ErrorIfExists =&gt; sys.error(&quot;Given path: &quot; + path + &quot; already exists!!&quot;); sys.exit(1)
      case SaveMode.Ignore =&gt; sys.exit()
    &#125;

    val formatName = parameters.getOrElse(&quot;format&quot;, &quot;customFormat&quot;)
    formatName match &#123;
      case &quot;customFormat&quot; =&gt; saveAsCustomFormat(data, path, mode)
      case &quot;json&quot; =&gt; saveAsJson(data, path, mode)
      case _ =&gt; throw new IllegalArgumentException(formatName + &quot; is not supported!!!&quot;)
    &#125;
    createRelation(sqlContext, parameters, data.schema)
  &#125;

  private def saveAsJson(data : DataFrame, path : String, mode: SaveMode): Unit = &#123;
    /**
      * Here, I am using the dataframe&#39;s Api for storing it as json.
      * you can have your own apis and ways for saving!!
      */
    data.write.mode(mode).json(path)
  &#125;

  private def saveAsCustomFormat(data : DataFrame, path : String, mode: SaveMode): Unit = &#123;
    /**
      * Here, I am  going to save this as simple text file which has values separated by &quot;|&quot;.
      * But you can have your own way to store without any restriction.
      */
    val customFormatRDD = data.rdd.map(row =&gt; &#123;
      row.toSeq.map(value =&gt; value.toString).mkString(&quot;|&quot;)
    &#125;)
    customFormatRDD.saveAsTextFile(path)
  &#125;
&#125;
</code></pre>
<pre><code class="scala">import org.apache.spark.sql.types.&#123;DataType, IntegerType, LongType, StringType&#125;

/**
  * Created by rana on 30/9/16.
  */
object Util &#123;
  def castTo(value : String, dataType : DataType) = &#123;
    dataType match &#123;
      case _ : IntegerType =&gt; value.toInt
      case _ : LongType =&gt; value.toLong
      case _ : StringType =&gt; value
    &#125;
  &#125;
&#125;
</code></pre>
<pre><code class="scala">import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

/**
  * Created by rana on 29/9/16.
  */
object app extends App &#123;
  println(&quot;Application started...&quot;)

  val conf = new SparkConf().setAppName(&quot;spark-custom-datasource&quot;)
  val spark = SparkSession.builder().config(conf).master(&quot;local&quot;).getOrCreate()

  val df = spark.sqlContext.read.format(&quot;cn.zj.spark.sql.datasource&quot;).load(&quot;1229practice/data/&quot;)

  //print the schema
//  df.printSchema()

  //print the data
//  df.show()

  //save the data
//  df.write.options(Map(&quot;format&quot; -&gt; &quot;customFormat&quot;)).mode(SaveMode.Overwrite).format(&quot;io.dcengines.rana.datasource&quot;).save(&quot;out_custom/&quot;)
//  df.write.options(Map(&quot;format&quot; -&gt; &quot;json&quot;)).mode(SaveMode.Overwrite).format(&quot;io.dcengines.rana.datasource&quot;).save(&quot;out_json/&quot;)
//  df.write.mode(SaveMode.Overwrite).format(&quot;io.dcengines.rana.datasource&quot;).save(&quot;out_none/&quot;)

  //select some specific columns
//  df.createOrReplaceTempView(&quot;test&quot;)
//  spark.sql(&quot;select id, name, salary from test&quot;).show()

  //filter data
  df.createOrReplaceTempView(&quot;test&quot;)
  spark.sql(&quot;select * from test where salary = 50000&quot;).show()

  println(&quot;Application Ended...&quot;)
&#125;
</code></pre>
<h2 id="Hive行级更新和生成代理键"><a href="#Hive行级更新和生成代理键" class="headerlink" title="Hive行级更新和生成代理键"></a><strong>Hive行级更新和生成代理键</strong></h2><pre><code class="python">### hive支持行级更新
1.编辑hive-site.xml文件:
客户端

hive.support.concurrency – true
hive.enforce.bucketing – true (Not required as of Hive 2.0)
hive.exec.dynamic.partition.mode – nonstrict
hive.txn.manager – org.apache.hadoop.hive.ql.lockmgr.DbTxnManager

服务端
hive.compactor.initiator.on – true (See table below for more details)
hive.compactor.worker.threads – a positive number on at least one instance of the Thrift metastore service

2.如果一个表要实现update和delete功能，该表就必须支持ACID，而支持ACID，就必须满足以下条件：
表的存储格式必须是目前只支持ORCFileformat和AcidOutputFormat
表必须进行分桶（CLUSTERED BY (col_name, col_name, …) INTO num_buckets BUCKETS）；
Table property中参数transactional必须设定为True（tblproperties(‘transactional’=‘true’)）；

### hive生成代理键
代理键 ：
      维度表中必须有一个能够唯一标识一行记录的列，通过该列维护维度表与事实表之间的关系，一般在维度表中业务主键符合条件可以当作维度主键。

补充：
      是由数据仓库处理过程中产生的，与业务本身无关的, 唯一标识维度表中一条记录并充当维度表主键的列，也是描述维度表与事实表关系的纽带。
所以在设计有代理键的维度表中，事实表中的关联键是代理键而不是原有的业务主键，即业务关系是靠代理键维护，这样有效避免源系统变化对数据仓库的影响。

在实际业务中，代理键通常是数值型，自增的值。
### 1、用row_number()函数生成代理键
INSERT OVERWRITE TABLE testTable
select row_number() over (order by a.acc_no) id,
a.acc_no
from ba_pay_out.app_intf_web_cli_his_view a
补充：指定自增基数
insert into table User_Attribute select (row_number() over())+1000 as id,customid from tbl_custom;
### 2、用UDFRowSequence生成代理键
add jar hdfs:///user/hive-contrib-2.0.0.jar; 
create temporary function row_sequence as &#39;org.apache.hadoop.hive.contrib.udf.udfrowsequence&#39;; 
INSERT OVERWRITE TABLE testTable
select row_sequence() id,
a.acc_no
from ba_pay_out.app_intf_web_cli_his_view a
hive-contrib-2.0.0.jar中包含一个生成记录序号的自定义函数udfrowsequence。上面的语句先加载JAR包，然后创建一个名为row_sequence()的临时函数作为调用UDF的接口，这样可以为查询的结果集生成一个自增伪列。之后就和row_number()写法类似了，只不过将窗口函数row_number()替换为row_sequence()函数。
以上两种方法，第二种的性能要由于第一种，第一种执行慢，且当数据超过约几千万（本人经验超过4千万）时，就报内存不够的了，这个可能与hadoop的资源配置也有关系，而第二中方法在数据超过1.5亿的情况下依然能够快速运行。
</code></pre>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wzy0623/article/details/53893174">https://blog.csdn.net/wzy0623/article/details/53893174</a></p>
<h2 id="Spark高级操作之JSON和复杂嵌套结构的操作-一）"><a href="#Spark高级操作之JSON和复杂嵌套结构的操作-一）" class="headerlink" title="Spark高级操作之JSON和复杂嵌套结构的操作(一）"></a><strong>Spark高级操作之JSON和复杂嵌套结构的操作(一）</strong></h2><p>本文主要讲spark2.0版本以后存在的Sparksql的一些实用的函数，帮助解决复杂嵌套的json数据格式，比如，map和嵌套结构。Spark2.1在spark 的Structured Streaming也可以使用这些功能函数。</p>
<p>下面几个是本文重点要讲的方法。</p>
<p>A),get_json_object()</p>
<p>B),from_json()</p>
<p>C),to_json()</p>
<p>D),explode()</p>
<p>E),selectExpr()</p>
<pre><code class="scala">##    准备阶段
首先，创建一个没有任何嵌套的JSon Schema
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
val jsonSchema = new StructType().add(&quot;battery_level&quot;, LongType)
.add(&quot;c02_level&quot;, LongType)
.add(&quot;cca3&quot;,StringType)
.add(&quot;cn&quot;, StringType)
.add(&quot;device_id&quot;, LongType)
.add(&quot;device_type&quot;, StringType)
.add(&quot;signal&quot;, LongType)
.add(&quot;ip&quot;, StringType)
.add(&quot;temp&quot;, LongType)
.add(&quot;timestamp&quot;, TimestampType)
使用上面的schema，我在这里创建一个Dataframe，使用的是scala 的case class，同时会产生一些json格式的数据。当然，生产中这些数据也可以来自于kafka。这个case class总共有两个字段：整型(作为device id)和一个字符串(json的数据结构，代表设备的事件)
case class DeviceData (id: Int, device: String)
val eventsDS = Seq (
  (0, &quot;&quot;&quot;&#123;&quot;device_id&quot;: 0, &quot;device_type&quot;: &quot;sensor-ipad&quot;, &quot;ip&quot;: &quot;68.161.225.1&quot;, &quot;cca3&quot;: &quot;USA&quot;, &quot;cn&quot;: &quot;United States&quot;, &quot;temp&quot;: 25, &quot;signal&quot;: 23, &quot;battery_level&quot;: 8, &quot;c02_level&quot;: 917, &quot;timestamp&quot; :1475600496 &#125;&quot;&quot;&quot;),
  (1, &quot;&quot;&quot;&#123;&quot;device_id&quot;: 1, &quot;device_type&quot;: &quot;sensor-igauge&quot;, &quot;ip&quot;: &quot;213.161.254.1&quot;, &quot;cca3&quot;: &quot;NOR&quot;, &quot;cn&quot;: &quot;Norway&quot;, &quot;temp&quot;: 30, &quot;signal&quot;: 18, &quot;battery_level&quot;: 6, &quot;c02_level&quot;: 1413, &quot;timestamp&quot; :1475600498 &#125;&quot;&quot;&quot;),
  (2, &quot;&quot;&quot;&#123;&quot;device_id&quot;: 2, &quot;device_type&quot;: &quot;sensor-ipad&quot;, &quot;ip&quot;: &quot;88.36.5.1&quot;, &quot;cca3&quot;: &quot;ITA&quot;, &quot;cn&quot;: &quot;Italy&quot;, &quot;temp&quot;: 18, &quot;signal&quot;: 25, &quot;battery_level&quot;: 5, &quot;c02_level&quot;: 1372, &quot;timestamp&quot; :1475600500 &#125;&quot;&quot;&quot;),
...).toDF(&quot;id&quot;, &quot;device&quot;).as[DeviceData]

### 如何使用get_json_object()
该方法从spark1.6开始就有了，从一个json 字符串中根据指定的json 路径抽取一个json 对象。从上面的dataset中取出部分数据，然后抽取部分字段组装成新的json 对象。比如，我们仅仅抽取：id，devicetype，ip，CCA3 code.
val eventsFromJSONDF = Seq (
  (0, &quot;&quot;&quot;&#123;&quot;device_id&quot;: 0, &quot;device_type&quot;: &quot;sensor-ipad&quot;, &quot;ip&quot;: &quot;68.161.225.1&quot;, &quot;cca3&quot;: &quot;USA&quot;, &quot;cn&quot;: &quot;United States&quot;, &quot;temp&quot;: 25, &quot;signal&quot;: 23, &quot;battery_level&quot;: 8, &quot;c02_level&quot;: 917, &quot;timestamp&quot; :1475600496 &#125;&quot;&quot;&quot;),
  (1, &quot;&quot;&quot;&#123;&quot;device_id&quot;: 1, &quot;device_type&quot;: &quot;sensor-igauge&quot;, &quot;ip&quot;: &quot;213.161.254.1&quot;, &quot;cca3&quot;: &quot;NOR&quot;, &quot;cn&quot;: &quot;Norway&quot;, &quot;temp&quot;: 30, &quot;signal&quot;: 18, &quot;battery_level&quot;: 6, &quot;c02_level&quot;: 1413, &quot;timestamp&quot; :1475600498 &#125;&quot;&quot;&quot;),
  (2, &quot;&quot;&quot;&#123;&quot;device_id&quot;: 2, &quot;device_type&quot;: &quot;sensor-ipad&quot;, &quot;ip&quot;: &quot;88.36.5.1&quot;, &quot;cca3&quot;: &quot;ITA&quot;, &quot;cn&quot;: &quot;Italy&quot;, &quot;temp&quot;: 18, &quot;signal&quot;: 25, &quot;battery_level&quot;: 5, &quot;c02_level&quot;: 1372, &quot;timestamp&quot; :1475600500 &#125;&quot;&quot;&quot;),
...).toDF(&quot;id&quot;, &quot;json&quot;)

测试及输出
val jsDF = eventsFromJSONDF.select($&quot;id&quot;, get_json_object($&quot;json&quot;, &quot;$.device_type&quot;).alias(&quot;device_type&quot;),get_json_object($&quot;json&quot;, &quot;$.ip&quot;).alias(&quot;ip&quot;),get_json_object($&quot;json&quot;, &quot;$.cca3&quot;).alias(&quot;cca3&quot;))
jsDF.printSchema
jsDF.show

### 如何使用from_json()
与get_json_object不同的是该方法，使用schema去抽取单独列。在dataset的api select中使用from_json()方法，我可以从一个json 字符串中按照指定的schema格式抽取出来作为DataFrame的列。还有，我们也可以将所有在json中的属性和值当做一个devices的实体。我们不仅可以使用device.arrtibute去获取特定值，也可以使用*通配符。
下面的例子，主要实现如下功能：
A),使用上述schema从json字符串中抽取属性和值，并将它们视为devices的独立列。
B),select所有列
C),使用.,获取部分列。
val devicesDF = eventsDS.select(from_json($&quot;device&quot;, jsonSchema) as &quot;devices&quot;).select($&quot;devices.*&quot;).filter($&quot;devices.temp&quot; &gt; 10 and $&quot;devices.signal&quot; &gt; 15)

### 如何使用to_json()
下面使用to_json()将获取的数据转化为json格式。将结果重新写入kafka或者保存partquet文件。
val stringJsonDF = eventsDS.select(to_json(struct($&quot;*&quot;))).toDF(&quot;devices&quot;)
stringJsonDF.show
保存数据到kafka
stringJsonDF.write.format(&quot;kafka&quot;).option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;).option(&quot;topic&quot;, &quot;iot-devices&quot;).save()

### 如何使用selectExpr()
将列转化为一个JSON对象的另一种方式是使用selectExpr()功能函数。例如我们可以将device列转化为一个JSON对象。
val stringsDF = eventsDS.selectExpr(&quot;CAST(id AS INT)&quot;, &quot;CAST(device AS STRING)&quot;)
stringsDF.show

SelectExpr()方法的另一个用法，就是使用表达式作为参数，将它们转化为指定的列。如下：
devicesDF.selectExpr(&quot;c02_level&quot;, &quot;round(c02_level/temp) as ratio_c02_temperature&quot;)
.orderBy($&quot;ratio_c02_temperature&quot; desc).show

使用Sparksql的slq语句是很好写的
首先注册成临时表，然后写sql
devicesDF.createOrReplaceTempView(&quot;devicesDFT&quot;)
spark.sql(&quot;select c02_level,round(c02_level/temp) as ratio_c02_temperature from devicesDFT order by ratio_c02_temperature desc&quot;).show

### 验证
为了验证我们的DataFrame转化为json String是成功的我们将结果写入本地磁盘。
stringJsonDF.write.mode(&quot;overwrite&quot;).format(&quot;parquet&quot;).save(&quot;file:///opt/jules&quot;)
</code></pre>
<h2 id="Spark高级操作之JSON和复杂嵌套结构的操作-二）"><a href="#Spark高级操作之JSON和复杂嵌套结构的操作-二）" class="headerlink" title="Spark高级操作之JSON和复杂嵌套结构的操作(二）"></a><strong>Spark高级操作之JSON和复杂嵌套结构的操作(二）</strong></h2><pre><code class="scala">一，准备阶段
Json格式里面有map结构和嵌套json也是很合理的。本文将举例说明如何用spark解析包含复杂的嵌套数据结构，map。现实中的例子是，一个设备的检测事件，二氧化碳的安全你浓度，高温数据等，需要实时产生数据，然后及时的告警处理。
1，定义schema
import org.apache.spark.sql.types._

val schema = new StructType()
  .add(&quot;dc_id&quot;, StringType)                    // data center where data was posted to Kafka cluster
  .add(&quot;source&quot;,                               // info about the source of alarm
    MapType(                                   // define this as a Map(Key-&gt;value)
      StringType,
      new StructType()
        .add(&quot;description&quot;, StringType)
        .add(&quot;ip&quot;, StringType)
        .add(&quot;id&quot;, LongType)
        .add(&quot;temp&quot;, LongType)
        .add(&quot;c02_level&quot;, LongType)
        .add(&quot;geo&quot;,
          new StructType()
            .add(&quot;lat&quot;, DoubleType)
            .add(&quot;long&quot;, DoubleType)
        )
    )
  )

2，准备数据
val dataDS = Seq(&quot;&quot;&quot;
&#123;
&quot;dc_id&quot;: &quot;dc-101&quot;,
&quot;source&quot;: &#123;
    &quot;sensor-igauge&quot;: &#123;
      &quot;id&quot;: 10,
      &quot;ip&quot;: &quot;68.28.91.22&quot;,
      &quot;description&quot;: &quot;Sensor attached to the container ceilings&quot;,
      &quot;temp&quot;:35,
      &quot;c02_level&quot;: 1475,
      &quot;geo&quot;: &#123;&quot;lat&quot;:38.00, &quot;long&quot;:97.00&#125;                        
    &#125;,
    &quot;sensor-ipad&quot;: &#123;
      &quot;id&quot;: 13,
      &quot;ip&quot;: &quot;67.185.72.1&quot;,
      &quot;description&quot;: &quot;Sensor ipad attached to carbon cylinders&quot;,
      &quot;temp&quot;: 34,
      &quot;c02_level&quot;: 1370,
      &quot;geo&quot;: &#123;&quot;lat&quot;:47.41, &quot;long&quot;:-122.00&#125;
    &#125;,
    &quot;sensor-inest&quot;: &#123;
      &quot;id&quot;: 8,
      &quot;ip&quot;: &quot;208.109.163.218&quot;,
      &quot;description&quot;: &quot;Sensor attached to the factory ceilings&quot;,
      &quot;temp&quot;: 40,
      &quot;c02_level&quot;: 1346,
      &quot;geo&quot;: &#123;&quot;lat&quot;:33.61, &quot;long&quot;:-111.89&#125;
    &#125;,
    &quot;sensor-istick&quot;: &#123;
      &quot;id&quot;: 5,
      &quot;ip&quot;: &quot;204.116.105.67&quot;,
      &quot;description&quot;: &quot;Sensor embedded in exhaust pipes in the ceilings&quot;,
      &quot;temp&quot;: 40,
      &quot;c02_level&quot;: 1574,
      &quot;geo&quot;: &#123;&quot;lat&quot;:35.93, &quot;long&quot;:-85.46&#125;
    &#125;
  &#125;
&#125;&quot;&quot;&quot;).toDS()
  // should only be one item
  dataDS.count()

3，准备处理
val df = spark.read.schema(schema).json(dataDS.rdd)
查看schema
df.printSchema

二，如何使用explode()
Explode()方法在spark1.3的时候就已经存在了，在这里展示一下如何抽取嵌套的数据结构。在一些场合，会结合explode，to_json,from_json一起使用。
Explode为给定的map的每一个元素创建一个新的行。比如上面准备的数据，source就是一个map结构。Map中的每一个key/value对都会是一个独立的行。
val explodedDF = df.select($&quot;dc_id&quot;, explode($&quot;source&quot;))
explodedDF.printSchema

获取内部的数据
case class DeviceAlert(dcId: String, deviceType:String, ip:String, deviceId:Long, temp:Long, c02_level: Long, lat: Double, lon: Double)
val notifydevicesDS = explodedDF.select( $&quot;dc_id&quot; as &quot;dcId&quot;,
  $&quot;key&quot; as &quot;deviceType&quot;,
  &#39;value.getItem(&quot;ip&quot;) as &#39;ip,
  &#39;value.getItem(&quot;id&quot;) as &#39;deviceId,
  &#39;value.getItem(&quot;c02_level&quot;) as &#39;c02_level,
  &#39;value.getItem(&quot;temp&quot;) as &#39;temp,
  &#39;value.getItem(&quot;geo&quot;).getItem(&quot;lat&quot;) as &#39;lat,  //note embedded level requires yet another level of fetching.
  &#39;value.getItem(&quot;geo&quot;).getItem(&quot;long&quot;) as &#39;lon)
  .as[DeviceAlert]  // return as a Dataset
查看schema信息
notifydevicesDS.printSchema

三，再复杂一点
在物联网场景里，通畅物联网设备会将很多json 事件数据发给他的收集器。收集器可以是附近的数据中心，也可以是附近的聚合器，也可以是安装在家里的一个设备，它会有规律的周期的将数据通过加密的互联网发给远程的数据中心。说白一点，数据格式更复杂。
我们下面会有三个map的数据格式：恒温计，摄像机，烟雾报警器。
import org.apache.spark.sql.types._

// a bit longish, nested, and convuloted JSON schema :)
val nestSchema2 = new StructType()
  .add(&quot;devices&quot;,
    new StructType()
      .add(&quot;thermostats&quot;, MapType(StringType,
      new StructType()
        .add(&quot;device_id&quot;, StringType)
        .add(&quot;locale&quot;, StringType)
        .add(&quot;software_version&quot;, StringType)
        .add(&quot;structure_id&quot;, StringType)
        .add(&quot;where_name&quot;, StringType)
        .add(&quot;last_connection&quot;, StringType)
        .add(&quot;is_online&quot;, BooleanType)
        .add(&quot;can_cool&quot;, BooleanType)
        .add(&quot;can_heat&quot;, BooleanType)
        .add(&quot;is_using_emergency_heat&quot;, BooleanType)
        .add(&quot;has_fan&quot;, BooleanType)
        .add(&quot;fan_timer_active&quot;, BooleanType)
        .add(&quot;fan_timer_timeout&quot;, StringType)
        .add(&quot;temperature_scale&quot;, StringType)
        .add(&quot;target_temperature_f&quot;, DoubleType)
        .add(&quot;target_temperature_high_f&quot;, DoubleType)
        .add(&quot;target_temperature_low_f&quot;, DoubleType)
        .add(&quot;eco_temperature_high_f&quot;, DoubleType)
        .add(&quot;eco_temperature_low_f&quot;, DoubleType)
        .add(&quot;away_temperature_high_f&quot;, DoubleType)
        .add(&quot;away_temperature_low_f&quot;, DoubleType)
        .add(&quot;hvac_mode&quot;, StringType)
        .add(&quot;humidity&quot;, LongType)
        .add(&quot;hvac_state&quot;, StringType)
        .add(&quot;is_locked&quot;, StringType)
        .add(&quot;locked_temp_min_f&quot;, DoubleType)
        .add(&quot;locked_temp_max_f&quot;, DoubleType)))
      .add(&quot;smoke_co_alarms&quot;, MapType(StringType,
      new StructType()
        .add(&quot;device_id&quot;, StringType)
        .add(&quot;locale&quot;, StringType)
        .add(&quot;software_version&quot;, StringType)
        .add(&quot;structure_id&quot;, StringType)
        .add(&quot;where_name&quot;, StringType)
        .add(&quot;last_connection&quot;, StringType)
        .add(&quot;is_online&quot;, BooleanType)
        .add(&quot;battery_health&quot;, StringType)
        .add(&quot;co_alarm_state&quot;, StringType)
        .add(&quot;smoke_alarm_state&quot;, StringType)
        .add(&quot;is_manual_test_active&quot;, BooleanType)
        .add(&quot;last_manual_test_time&quot;, StringType)
        .add(&quot;ui_color_state&quot;, StringType)))
      .add(&quot;cameras&quot;, MapType(StringType,
      new StructType()
        .add(&quot;device_id&quot;, StringType)
        .add(&quot;software_version&quot;, StringType)
        .add(&quot;structure_id&quot;, StringType)
        .add(&quot;where_name&quot;, StringType)
        .add(&quot;is_online&quot;, BooleanType)
        .add(&quot;is_streaming&quot;, BooleanType)
        .add(&quot;is_audio_input_enabled&quot;, BooleanType)
        .add(&quot;last_is_online_change&quot;, StringType)
        .add(&quot;is_video_history_enabled&quot;, BooleanType)
        .add(&quot;web_url&quot;, StringType)
        .add(&quot;app_url&quot;, StringType)
        .add(&quot;is_public_share_enabled&quot;, BooleanType)
        .add(&quot;activity_zones&quot;,
          new StructType()
            .add(&quot;name&quot;, StringType)
            .add(&quot;id&quot;, LongType))
        .add(&quot;last_event&quot;, StringType))))
对应的数据
val nestDataDS2 = Seq(&quot;&quot;&quot;&#123;
  &quot;devices&quot;: &#123;
     &quot;thermostats&quot;: &#123;
        &quot;peyiJNo0IldT2YlIVtYaGQ&quot;: &#123;
          &quot;device_id&quot;: &quot;peyiJNo0IldT2YlIVtYaGQ&quot;,
          &quot;locale&quot;: &quot;en-US&quot;,
          &quot;software_version&quot;: &quot;4.0&quot;,
          &quot;structure_id&quot;: &quot;VqFabWH21nwVyd4RWgJgNb292wa7hG_dUwo2i2SG7j3-BOLY0BA4sw&quot;,
          &quot;where_name&quot;: &quot;Hallway Upstairs&quot;,
          &quot;last_connection&quot;: &quot;2016-10-31T23:59:59.000Z&quot;,
          &quot;is_online&quot;: true,
          &quot;can_cool&quot;: true,
          &quot;can_heat&quot;: true,
          &quot;is_using_emergency_heat&quot;: true,
          &quot;has_fan&quot;: true,
          &quot;fan_timer_active&quot;: true,
          &quot;fan_timer_timeout&quot;: &quot;2016-10-31T23:59:59.000Z&quot;,
          &quot;temperature_scale&quot;: &quot;F&quot;,
          &quot;target_temperature_f&quot;: 72,
          &quot;target_temperature_high_f&quot;: 80,
          &quot;target_temperature_low_f&quot;: 65,
          &quot;eco_temperature_high_f&quot;: 80,
          &quot;eco_temperature_low_f&quot;: 65,
          &quot;away_temperature_high_f&quot;: 80,
          &quot;away_temperature_low_f&quot;: 65,
          &quot;hvac_mode&quot;: &quot;heat&quot;,
          &quot;humidity&quot;: 40,
          &quot;hvac_state&quot;: &quot;heating&quot;,
          &quot;is_locked&quot;: true,
          &quot;locked_temp_min_f&quot;: 65,
          &quot;locked_temp_max_f&quot;: 80
          &#125;
        &#125;,
        &quot;smoke_co_alarms&quot;: &#123;
          &quot;RTMTKxsQTCxzVcsySOHPxKoF4OyCifrs&quot;: &#123;
            &quot;device_id&quot;: &quot;RTMTKxsQTCxzVcsySOHPxKoF4OyCifrs&quot;,
            &quot;locale&quot;: &quot;en-US&quot;,
            &quot;software_version&quot;: &quot;1.01&quot;,
            &quot;structure_id&quot;: &quot;VqFabWH21nwVyd4RWgJgNb292wa7hG_dUwo2i2SG7j3-BOLY0BA4sw&quot;,
            &quot;where_name&quot;: &quot;Jane&#39;s Room&quot;,
            &quot;last_connection&quot;: &quot;2016-10-31T23:59:59.000Z&quot;,
            &quot;is_online&quot;: true,
            &quot;battery_health&quot;: &quot;ok&quot;,
            &quot;co_alarm_state&quot;: &quot;ok&quot;,
            &quot;smoke_alarm_state&quot;: &quot;ok&quot;,
            &quot;is_manual_test_active&quot;: true,
            &quot;last_manual_test_time&quot;: &quot;2016-10-31T23:59:59.000Z&quot;,
            &quot;ui_color_state&quot;: &quot;gray&quot;
            &#125;
          &#125;,
       &quot;cameras&quot;: &#123;
        &quot;awJo6rH0IldT2YlIVtYaGQ&quot;: &#123;
          &quot;device_id&quot;: &quot;awJo6rH&quot;,
          &quot;software_version&quot;: &quot;4.0&quot;,
          &quot;structure_id&quot;: &quot;VqFabWH21nwVyd4RWgJgNb292wa7hG_dUwo2i2SG7j3-BOLY0BA4sw&quot;,
          &quot;where_name&quot;: &quot;Foyer&quot;,
          &quot;is_online&quot;: true,
          &quot;is_streaming&quot;: true,
          &quot;is_audio_input_enabled&quot;: true,
          &quot;last_is_online_change&quot;: &quot;2016-12-29T18:42:00.000Z&quot;,
          &quot;is_video_history_enabled&quot;: true,
          &quot;web_url&quot;: &quot;https://home.nest.com/cameras/device_id?auth=access_token&quot;,
          &quot;app_url&quot;: &quot;nestmobile://cameras/device_id?auth=access_token&quot;,
          &quot;is_public_share_enabled&quot;: true,
          &quot;activity_zones&quot;: &#123; &quot;name&quot;: &quot;Walkway&quot;, &quot;id&quot;: 244083 &#125;,
          &quot;last_event&quot;: &quot;2016-10-31T23:59:59.000Z&quot;
          &#125;
        &#125;
      &#125;
     &#125;&quot;&quot;&quot;).toDS

通过创建一个简单的dataset，我们可以使用所有的dataset的方法来进行ETL操作，比如from_json(), to_json(), explode() and selectExpr()。
val nestDF2 = spark                            // spark session 
  .read                             //  get DataFrameReader
  .schema(nestSchema2)             //  use the defined schema above and read format as JSON
  .json(nestDataDS2.rdd)

将整个json对象，转化为一个json string
val stringJsonDF = nestDF2.select(to_json(struct($&quot;*&quot;))).toDF(&quot;nestDevice&quot;)

将三个json object 的map对象转化为三个单独的map列，然后可以是使用explode方法访问其属性
val mapColumnsDF = nestDF2.select($&quot;devices&quot;.getItem(&quot;smoke_co_alarms&quot;).alias (&quot;smoke_alarms&quot;),
  $&quot;devices&quot;.getItem(&quot;cameras&quot;).alias (&quot;cameras&quot;),
  $&quot;devices&quot;.getItem(&quot;thermostats&quot;).alias (&quot;thermostats&quot;))

转化为三个dataframe
val explodedThermostatsDF = mapColumnsDF.select(explode($&quot;thermostats&quot;))
val explodedCamerasDF = mapColumnsDF.select(explode($&quot;cameras&quot;))
//or you could use the original nestDF2 and use the devices.X notation
val explodedSmokedAlarmsDF =  nestDF2.select(explode($&quot;devices.smoke_co_alarms&quot;))

访问三个map内部的元素
val thermostateDF = explodedThermostatsDF.select($&quot;value&quot;.getItem(&quot;device_id&quot;).alias(&quot;device_id&quot;),
  $&quot;value&quot;.getItem(&quot;locale&quot;).alias(&quot;locale&quot;),
  $&quot;value&quot;.getItem(&quot;where_name&quot;).alias(&quot;location&quot;),
  $&quot;value&quot;.getItem(&quot;last_connection&quot;).alias(&quot;last_connected&quot;),
  $&quot;value&quot;.getItem(&quot;humidity&quot;).alias(&quot;humidity&quot;),
  $&quot;value&quot;.getItem(&quot;target_temperature_f&quot;).alias(&quot;target_temperature_f&quot;),
  $&quot;value&quot;.getItem(&quot;hvac_mode&quot;).alias(&quot;mode&quot;),
  $&quot;value&quot;.getItem(&quot;software_version&quot;).alias(&quot;version&quot;))

val cameraDF = explodedCamerasDF.select($&quot;value&quot;.getItem(&quot;device_id&quot;).alias(&quot;device_id&quot;),
  $&quot;value&quot;.getItem(&quot;where_name&quot;).alias(&quot;location&quot;),
  $&quot;value&quot;.getItem(&quot;software_version&quot;).alias(&quot;version&quot;),
  $&quot;value&quot;.getItem(&quot;activity_zones&quot;).getItem(&quot;name&quot;).alias(&quot;name&quot;),
  $&quot;value&quot;.getItem(&quot;activity_zones&quot;).getItem(&quot;id&quot;).alias(&quot;id&quot;))

val smokedAlarmsDF = explodedSmokedAlarmsDF.select($&quot;value&quot;.getItem(&quot;device_id&quot;).alias(&quot;device_id&quot;),
  $&quot;value&quot;.getItem(&quot;where_name&quot;).alias(&quot;location&quot;),
  $&quot;value&quot;.getItem(&quot;software_version&quot;).alias(&quot;version&quot;),
  $&quot;value&quot;.getItem(&quot;last_connection&quot;).alias(&quot;last_connected&quot;),
  $&quot;value&quot;.getItem(&quot;battery_health&quot;).alias(&quot;battery_health&quot;))
</code></pre>
<h2 id="shell-定时脚本"><a href="#shell-定时脚本" class="headerlink" title="shell 定时脚本"></a><strong>shell 定时脚本</strong></h2><pre><code class="shell">1：查看crontab是否开启
ps aux | grep cron
启动命令service crond restart
设置开机自启动 chkconfig crond on
2:使用方法。可以通过用户的crobtab设置（命令的方式），也可以通过系统的crontab的配置文件(可以指定其它用户）。
通过命令行的方式（root用户）：
corntab -e 进入编辑器
* * * * * 执行的任务        （五个*分别表示分时日月周，*/n表示每个n分钟执行一次任务）

参数说明：
星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。
逗号（,）：可以用逗号隔开的值指定一个列表范围，（代表不连续的时间）例如，“1,2,5,7,8,9”
中杠（-）：可以用整数之间的中杠表示一个整数范围（代表连续的时间范围），例如“2-6”表示“2,3,4,5,6”
正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。


命令：
30 21 * * * /etc/init.d/smb restart     每晚的21:30重启smb
45 4 1,10,22 * * /etc/init.d/smb restart    每月1、10、22日的4 : 45重启smb
0,30 18-23 * * * /etc/init.d/smb restart   每天18 : 00至23 : 00之间每隔30分钟重启smb
3,15 8-11 * * * command    在上午8点到11点的第3和第15分钟执行
3,15 8-11 */2 * * command   每隔两天的上午8点到11点的第3和第15分钟执行
0 4 1 jan * /etc/init.d/smb restart 一月一号的4点重启smb

配置文件的corntab命令
</code></pre>
<h2 id="基于shell的azkban容错插件"><a href="#基于shell的azkban容错插件" class="headerlink" title="基于shell的azkban容错插件"></a><strong>基于shell的azkban容错插件</strong></h2><pre><code class="shell">
</code></pre>
<h2 id="shell编程整理"><a href="#shell编程整理" class="headerlink" title="shell编程整理"></a><strong>shell编程整理</strong></h2><pre><code>
</code></pre>
<h2 id="Redis管理kafka的偏移量"><a href="#Redis管理kafka的偏移量" class="headerlink" title="Redis管理kafka的偏移量"></a><strong>Redis管理kafka的偏移量</strong></h2><p>注：想要redis管理kafka的偏移量实现exatly-oncle,需要开启redis的pIpeline机制。本篇幅未做管理。</p>
<pre><code class="scala">import java.util

import cn.huige.spark05.utils.JedisPoolUtils
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.TopicPartition
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.log4j.&#123;Level, Logger&#125;
import org.apache.spark.&#123;SparkConf, TaskContext&#125;
import org.apache.spark.streaming.dstream.InputDStream
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;
import scala.collection.mutable

object StreamingOffsetRedis &#123;

  Logger.getLogger(&quot;org&quot;).setLevel(Level.ERROR)

  def main(args: Array[String]): Unit = &#123;
    val conf: SparkConf = new SparkConf()
      .setAppName(this.getClass.getSimpleName)
      .setMaster(&quot;local[*]&quot;)
    val ssc: StreamingContext = new StreamingContext(conf, Seconds(2))

    val groupId = &quot;aaa&quot;
    val concat_sep = &quot;:_:&quot;

    // kafka消费者的参数
    val kafkaParams = Map[String, Object](
      &quot;bootstrap.servers&quot; -&gt; &quot;hdp-01:9092,hdp-02:9092,hdp-03:9092&quot;,
      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],
      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],
      &quot;group.id&quot; -&gt; groupId,


      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,
      // 手动维护偏移量，这里需要设置为false
      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)
    )

    val topics = Array(&quot;wctopic&quot;)

    // 指定offset的位置
    val offsetMap = mutable.HashMap[TopicPartition, Long]()

    val jedis2 = JedisPoolUtils()
    val values: util.Map[String, String] = jedis2.hgetAll(groupId + concat_sep + topics(0))

    import scala.collection.JavaConversions._
    for(tp &lt;- values)&#123;
      // 赋值
      val topicAndPart = new TopicPartition(topics(0),tp._1.toInt)
      offsetMap(topicAndPart)=tp._2.toLong
    &#125;

    val dstream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream(
      ssc,
      // 任务尽量均匀的分布在各个executor节点
      LocationStrategies.PreferConsistent,
      ConsumerStrategies.Subscribe[String, String](topics, kafkaParams, offsetMap))


    dstream.foreachRDD(rdd =&gt; &#123;
      // 判断rdd是否为空
      if(!rdd.isEmpty())&#123;
      println(rdd.partitions.size)
      // 获取到当前批次的偏移量数据
      val ofr: Array[OffsetRange] = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
      ofr.foreach(println)
          
      // 方式二: 业务逻辑的处理。即处理一条数据就更新一条偏移量。

        rdd.foreachPartition(it =&gt;&#123;
          val jedis = JedisPoolUtils()
          it.foreach(tp =&gt;&#123;
            // 具体业务逻辑的实现
            // 业务逻辑写mysql

            // 方式一:偏移量的维护
            val part = TaskContext.getPartitionId()
            val range: OffsetRange = ofr.filter(_.partition == part)(0)
            val uofset = range.untilOffset
            val groupAndTopic = groupId + concat_sep + topics(0)
            // 把最新的offset 写到redis中
            jedis.hset(groupAndTopic, part + &quot;&quot;, uofset + &quot;&quot;)
          &#125;)
          jedis.close()
        &#125;)
      &#125;
    &#125;)


      // 提交更新偏移量
      val jedis = JedisPoolUtils()
      ofr.foreach(or =&gt; &#123;
        val uofset = or.untilOffset
        val groupAndTopic = groupId + concat_sep + or.topic
        // 把最新的offset 写到redis中
        jedis.hset(groupAndTopic, or.partition + &quot;&quot;, uofset + &quot;&quot;)
      &#125;)
      jedis.close()
      &#125;
    &#125;)

    ssc.start()
    ssc.awaitTermination()
  &#125;
&#125;
</code></pre>
<h2 id="kafka-exactly-once保证—mysql"><a href="#kafka-exactly-once保证—mysql" class="headerlink" title="kafka exactly-once保证—mysql"></a><strong>kafka exactly-once保证—mysql</strong></h2><pre><code class="scala">import java.sql.PreparedStatement

import kafka.common.TopicAndPartition
import kafka.message.MessageAndMetadata
import kafka.serializer.StringDecoder
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.InputDStream
import org.apache.spark.streaming.kafka.&#123;KafkaUtils, OffsetRange&#125;
import streamingtest.&#123;ConfigurationConstants, ConnectPool&#125;
import streamingtest.FlumePollingEvent.logger


object OffsetReadAndSave &#123;
  // 从mysql中读取kafka偏移量，并产生kafka DStream
  def KafkaOffsetRead(ssc: StreamingContext, kafkaParams: Map[String, String], consumerTopics: Set[String]): InputDStream[(String, String)] = &#123;
    val connOffset = ConnectPool.getConnection
    val psOffsetCnt: PreparedStatement = connOffset.prepareStatement(&quot;SELECT SUM(1) FROM `kafka_offset` WHERE `topic`=?&quot;)
    psOffsetCnt.setString(1, ConfigurationConstants.kafkaConsumerTopics)
    val rs = psOffsetCnt.executeQuery()
    var parCount = 0
    while (rs.next()) &#123;
      parCount = rs.getInt(1)
      println(parCount.toString)
    &#125;
    var kafkaStream : InputDStream[(String, String)] = null
    var fromOffsets: Map[TopicAndPartition, Long] = Map()
    val psOffsetRead: PreparedStatement = connOffset.prepareStatement(&quot;SELECT offset FROM `kafka_offset` WHERE `topic`=? AND `partition`=?&quot;)
    if (parCount &gt; 0) &#123;
      for (i &lt;- 0 until parCount) &#123;
        psOffsetRead.setString(1, ConfigurationConstants.kafkaConsumerTopics)
        psOffsetRead.setInt(2, i)
        val rs1 = psOffsetRead.executeQuery()
        while (rs1.next()) &#123;
          val partitionOffset = rs1.getInt(1)
          val tp = TopicAndPartition(ConfigurationConstants.kafkaConsumerTopics, i)
          fromOffsets += (tp -&gt; partitionOffset.toLong)
        &#125;
      &#125;
      val messageHandler = (mmd : MessageAndMetadata[String, String]) =&gt; (mmd.topic, mmd.message())
      kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder, (String, String)](ssc, kafkaParams, fromOffsets, messageHandler)
    &#125;
    else &#123;
      kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, consumerTopics)
    &#125;
    ConnectPool.closeCon(psOffsetCnt,connOffset)
    ConnectPool.closeCon(psOffsetRead,connOffset)
    kafkaStream
  &#125;

  // 将kafka偏移量写入mysql中保存
  def KafkaOffsetSave(offsetRanges: Array[OffsetRange]): Unit = &#123;
    val connOffset = ConnectPool.getConnection
    connOffset.setAutoCommit(false)
    val psOffset: PreparedStatement = connOffset.prepareStatement(&quot;REPLACE INTO `kafka_offset` (`topic`, `partition`, `offset`) VALUES (?,?,?)&quot;)
    for (o &lt;- offsetRanges) &#123;
      println(s&quot;$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;&quot;)
      psOffset.setString(1, o.topic.toString)
      psOffset.setInt(2, o.partition.toInt)
      psOffset.setLong(3, o.fromOffset.toLong)
      psOffset.addBatch()
    &#125;
    psOffset.executeBatch()
    connOffset.commit()
    ConnectPool.closeCon(psOffset,connOffset)
  &#125;

&#125;
</code></pre>
<h2 id="kafka-exactly-once-保证redis"><a href="#kafka-exactly-once-保证redis" class="headerlink" title="kafka exactly-once 保证redis"></a><strong>kafka exactly-once 保证redis</strong></h2><pre><code class="scala">package com.mouse.redisPool
import com.mouse.ExactlyOnce.GetLog.MyRecord
import com.mouse.ExactlyOnce.&#123;GetLog, RedisClient&#125;
import org.apache.kafka.common.TopicPartition
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.SparkConf
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;
import redis.clients.jedis.Pipeline

/**
  * 注释：
  *
  * 在Spark Streaming中消费Kafka数据，保证Exactly-once的核心有三点：
  * 使用Direct方式连接Kafka；自己保存和维护Offset；更新Offset和计算在同一事务中完成；
  *
  *  思路步骤：
  *  1.启动后，先从Redis中获取上次保存的Offset，Redis中的key为”topic_partition”，即每个分区维护一个Offset；
  *  2.使用获取到的Offset，创建DirectStream；
  *  3.在处理每批次的消息时，利用Redis的事务机制，确保在Redis中指标的计算和Offset的更新维护，
  *  在同一事务中完成。只有这两者同步，才能真正保证消息的Exactly-once。
  *
  *  注意事项：
  *   1.在启动Spark Streaming程序时候，有个参数最好指定：
  *   2.spark.streaming.kafka.maxRatePerPartition=20000（每秒钟从topic的每个partition最多消费的消息条数）
  *   如果程序第一次运行，或者因为某种原因暂停了很久重新启动时候，会积累很多消息，
  *   如果这些消息同时被消费，很有可能会因为内存不够而挂掉，因此，需要根据实际的数据量大小，
  *   以及批次的间隔时间来设置该参数，以限定批次的消息量。
  *   3.如果该参数设置20000，而批次间隔时间未10秒，那么每个批次最多从Kafka中消费20万消息。
  *
  *
  */
object SparkStreaming_ExactlyOnce &#123;
  def main(args: Array[String]): Unit = &#123;
    val brokers = &quot;kafka:9092&quot;
    val topic = &quot;save_redis_offset&quot;
    val partition : Int = 0 //测试topic只有一个分区

    //Kafka参数
    val kafkaParams = Map[String, Object](
      &quot;bootstrap.servers&quot; -&gt; brokers,
      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],
      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],
      &quot;group.id&quot; -&gt; &quot;exactly-once&quot;,
      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean),
      &quot;auto.offset.reset&quot; -&gt; &quot;none&quot;   //letest
    )
/* 
* earliest   当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
* latest   当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
* none   topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
*/     

    // Redis configurations
    val maxTotal = 10
    val maxIdle = 10
    val minIdle = 1
    val redisHost = &quot;192.168.1.1&quot;
    val redisPort = 6379
    val redisTimeout = 30000
    //默认db，用户存放Offset和pv数据
    val dbDefaultIndex = 8
    RedisClient.makePool(redisHost, redisPort, redisTimeout, maxTotal, maxIdle, minIdle)


    val conf = new SparkConf().setAppName(&quot;TestSparkStreaming&quot;).setIfMissing(&quot;spark.master&quot;, &quot;local[2]&quot;)
    val ssc = new StreamingContext(conf, Seconds(10))

    //从Redis获取上一次存的Offset
    val jedis = RedisClient.getPool.getResource
    jedis.select(dbDefaultIndex)
    val topic_partition_key = topic + &quot;_&quot; + partition
    var lastOffset = 0L
    val lastSavedOffset = jedis.get(topic_partition_key)

    if(null != lastSavedOffset) &#123;
      try &#123;
        lastOffset = lastSavedOffset.toLong
      &#125; catch &#123;
        case ex : Exception =&gt; println(ex.getMessage)
          println(&quot;get lastSavedOffset error, lastSavedOffset from redis [&quot; + lastSavedOffset + &quot;] &quot;)
          System.exit(1)
      &#125;
    &#125;
    RedisClient.getPool.returnResource(jedis)

    println(&quot;lastOffset from redis -&gt; &quot; + lastOffset)

    //设置每个分区起始的Offset
    val fromOffsets = Map&#123;new TopicPartition(topic, partition) -&gt; lastOffset&#125;

    //使用Direct API 创建Stream
    val stream = KafkaUtils.createDirectStream[String, String](
      ssc,
      LocationStrategies.PreferConsistent,
      ConsumerStrategies.Assign[String, String](fromOffsets.keys.toList, kafkaParams, fromOffsets)
    )

    //开始处理批次消息
    stream.foreachRDD &#123;
      rdd =&gt;

        /**
          * 获取 RDD 每一个分区里的offset;
          * OffsetRange里面核心字段&#123;
            val topic: String,    主题
            val partition: Int,   分区
            val fromOffset: Long, 该拉去数据的开始偏移量
            val untilOffset: Long 该分区拉去数据的最后偏移量
            &#125;

          */
        val offsetRanges: Array[OffsetRange] = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
        val result: Array[MyRecord] = GetLog.processLogs(rdd)
        println(&quot;=============== Total &quot; + result.length + &quot; events in this batch ..&quot;)
        val jedis = RedisClient.getPool.getResource
        val p1 : Pipeline = jedis.pipelined();
        p1.select(dbDefaultIndex)
        p1.multi() //开启事务


        //逐条处理消息
        result.foreach &#123;
          record =&gt;
            //增加小时总pv
            val pv_by_hour_key = &quot;pv_&quot; + record.hour
            p1.incr(pv_by_hour_key)

            //增加网站小时pv
            val site_pv_by_hour_key = &quot;site_pv_&quot; + record.site_id + &quot;_&quot; + record.hour
            p1.incr(site_pv_by_hour_key)

            //使用set保存当天的uv
            val uv_by_day_key = &quot;uv_&quot; + record.hour.substring(0, 10)
            p1.sadd(uv_by_day_key, record.user_id)
        &#125;

        //更新Offset
        offsetRanges.foreach &#123; offsetRange =&gt;
          println(&quot;partition : &quot; + offsetRange.partition + &quot; fromOffset:  &quot; + offsetRange.fromOffset + &quot; untilOffset: &quot; + offsetRange.untilOffset)
          val topic_partition_key = offsetRange.topic + &quot;_&quot; + offsetRange.partition
          p1.set(topic_partition_key, offsetRange.untilOffset.toString)
        &#125;

        p1.exec();//提交事务
        p1.sync();//关闭pipeline

        RedisClient.getPool.returnResource(jedis)
    &#125;
    ssc.start()
    ssc.awaitTermination()
  &#125;
&#125;
</code></pre>
<h2 id="redis常用命令及其含义"><a href="#redis常用命令及其含义" class="headerlink" title="redis常用命令及其含义"></a><strong>redis常用命令及其含义</strong></h2><pre><code class="scala">
</code></pre>
<h2 id="Spark性能优化总结"><a href="#Spark性能优化总结" class="headerlink" title="Spark性能优化总结"></a><strong>Spark性能优化总结</strong></h2><pre><code class="python">0. Overview
1. 开发调优
   - 避免创建重复的RDD
   - 尽可能复用同一个RDD
   - 对多次使用的RDD进行持久化
   - 尽量避免使用shuffle类算子
   - 使用map-side预聚合的shuffle操作
   - 使用高性能的算子
   - 广播大变量
   - 使用Kryo优化序列化性能
   - 优化数据结构
2. 资源参数调优
   - 运行时架构
   - 运行流程
   - 调优
     - executor配置
     - driver配置
     - 并行度
     - 网络超时
     - 数据本地化
     - JVM/gc配置
3. 数据倾斜调优
   - 使用Hive ETL预处理数据
   - 过滤少数导致倾斜的key
   - 提高shuffle操作的并行度
   - 两阶段聚合
   - 将reduce join转为map join
   - 使用随机前缀和扩容RDD进行join
4. Shuffle调优
   - shuffle原理
   - shuffle演进
   - 调优
   - join类型
5. 其他优化项
   - 使用DataFrame/DataSet
</code></pre>
<h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a><strong>Overview</strong></h4><p>Spark的瓶颈一般来自于集群(standalone, yarn, mesos, k8s)的资源紧张，CPU，网络带宽，内存。通过都会将数据序列化，降低其内存memory和网络带宽shuffle的消耗。</p>
<blockquote>
<p>Spark的性能，想要它快，就得充分利用好系统资源，尤其是内存和CPU：核心思想就是能用内存cache就别spill落磁盘，CPU 能并行就别串行，数据能local就别shuffle。</p>
</blockquote>
<h4 id="开发调优"><a href="#开发调优" class="headerlink" title="开发调优"></a><strong>开发调优</strong></h4><ol>
<li><p>避免创建重复的RDD</p>
</li>
<li><ul>
<li>比如多次读可以persist；但如果input太大，persist可能得不偿失</li>
</ul>
</li>
<li><p>尽可能复用同一个RDD</p>
</li>
<li><ul>
<li>但是如果rdd的lineage太长，最好checkpoint下来，避免长重建</li>
</ul>
</li>
<li><p>对多次使用的RDD进行持久化</p>
</li>
<li><ul>
<li>持久化级别（SER，MEM，DISK，_N）</li>
</ul>
</li>
<li><p>尽量避免使用shuffle类算子</p>
</li>
<li><ul>
<li>shuffle算子如distinct（实际调用reduceByKey）、reduceByKey、aggregateByKey、sortByKey、groupByKey、join、cogroup、repartition等，入参中会有一个并行度参数numPartitions</li>
<li>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key</li>
</ul>
</li>
<li><p>使用map-side预聚合的shuffle操作</p>
</li>
<li><ul>
<li><p>reduceByKey(combiner)，groupByKey(没有combiner)</p>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/202041-91701.jpg"></p>
</li>
</ul>
</li>
<li><p>使用高性能的算子</p>
</li>
<li><ul>
<li>一边进行重分区的shuffle操作，一边进行排序</li>
</ul>
</li>
<li><ul>
<li>减少小文件数量</li>
</ul>
</li>
<li><ul>
<li>特别是在写DB的时候，避免每条写记录都new一个connection；推荐是每个partition new一个connection；更好的是new connection池，每个partition从中取即可，减少partitionNum个new的消耗</li>
</ul>
</li>
<li><ul>
<li>使用reduceByKey&#x2F;aggregateByKey替代groupByKey</li>
<li>使用mapPartitions替代普通map</li>
<li>使用foreachPartitions替代foreach</li>
<li>使用filter之后进行coalesce操作</li>
<li>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</li>
</ul>
</li>
<li><p>广播大变量</p>
</li>
<li><ul>
<li>广播变量是executor内所有task共享的，避免了每个task自己维护一个变量，OOM</li>
</ul>
</li>
<li><p>使用Kryo优化序列化性能</p>
</li>
<li><p>优化数据结构</p>
</li>
<li><ul>
<li>原始类型(Int, Long)</li>
<li>字符串，每个字符串内部都有一个字符数组以及长度等额外信息</li>
<li>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间</li>
<li>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry</li>
<li>尽量使用<code>字符串</code>替代<code>对象</code>，使用<code>原始类型</code>（比如Int、Long）替代<code>字符串</code>，使用<code>数组</code>替代<code>集合类型</code>，这样尽可能地减少内存占用，从而降低GC频率，提升性能</li>
</ul>
</li>
</ol>
<h4 id="资源参数调优"><a href="#资源参数调优" class="headerlink" title="资源参数调优"></a><strong>资源参数调优</strong></h4><p>运行时架构</p>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/202041-92929.jpg" alt="202041-92929"></p>
<ul>
<li><p>Client：客户端进程，负责提交作业</p>
</li>
<li><p>Driver&#x2F;SC：运行应用程序&#x2F;业务代码的main()函数并且创建SparkContext，其中创建SparkContext的目的是为了准备Spark应用程序的运行环境。在Spark中由SparkContext负责和ClusterManager&#x2F;ResourceManager通信，进行资源的申请、任务的分配和监控等；当Executor部分运行完毕后，Driver负责将SparkContext关闭。通常用SparkContext代表Drive</p>
</li>
<li><ul>
<li>SparkContext：整个应用程序的上下文，控制应用的生命周期</li>
<li>DAGScheduler：实现将Spark作业分解成一到多个Stage，每个Stage根据RDD的Partition个数决定Task的个数，然后生成相应的Task set放到TaskScheduler中</li>
<li>TaskScheduler：分配Task到Executor上执行，并维护Task的运行状态</li>
</ul>
</li>
<li><p>Executor：应用程序Application运行在Worker节点上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上。在Spark on Yarn模式下，其进程名称为CoarseGrainedExecutorBackend，负责将Task包装成taskRunner，并从线程池中抽取出一个空闲线程运行Task。每个CoarseGrainedExecutorBackend能并行运行Task的数量就取决于分配给它的CPU的个数</p>
</li>
<li><p>Job：一个job包含多个RDD及作用于相应RDD上的各种Operation。每执行一个action算子（foreach, count, collect, take, saveAsTextFile）就会生成一个 job</p>
</li>
<li><p>Stage：每个Job会被拆分很多组Task，每组Task被称为Stage，亦称TaskSet。一个作业job分为多个阶段stages（shuffle，串行），一个stage包含一系列的tasks（并行）</p>
</li>
<li><p>Task：被送往各个Executor上的执行的内容，task之间无状态传递，可以并行执行</p>
<p><strong>运行流程</strong></p>
<ol>
<li><p>client向YARN的ResourceManager&#x2F;RM申请启动ApplicationMaster&#x2F;AM（单个应用程序&#x2F;作业的资源管理和任务监控）</p>
</li>
<li><p>RM收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，spark在此启动其AM，其中AM进行SparkContext&#x2F;SC&#x2F;Driver初始化启动并创建RDD Object、DAGScheduler、TASKScheduler</p>
</li>
<li><p>SC根据RDD的依赖关系构建DAG图，并将DAG提交给DAGScheduler解析为<strong>stage</strong>。Stages以TaskSet的形式提交给TaskScheduler，TaskScheduler维护所有TaskSet，当Executor向Driver发送心跳时，TaskScheduler会根据其资源剩余情况分配相应的Task，另外TaskScheduler还维护着所有Task的运行状态，重试失败了的Task</p>
</li>
<li><p>AM向RM申请container资源，资源到位后便与NodeManager通信，要求它在获得的Container中(executor)启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向AM中的SC注册并申请Task</p>
</li>
<li><p>AM中的SC分配Task给CoarseGrainedExecutorBackend&#x2F;executor执行，CoarseGrainedExecutorBackend运行Task并向AM汇报运行的状态和进度，以便让AM随时掌握各个task的运行状态，从而可以在任务失败时重新启动任务或者推测执行</p>
</li>
<li><p>应用程序运行完成后，AM向RM申请注销并关闭自己</p>
<pre><code class="python">调优
### executor配置
spark.executor.memory
spark.executor.instances
spark.executor.cores
### driver配置
spark.driver.memory（如果没有collect操作，一般不需要很大，1~4g即可）
spark.driver.cores
### 并行度
spark.default.parallelism (used for RDD API)
spark.sql.shuffle.partitions (usef for DataFrame/DataSet API)
### 网络超时
spark.network.timeout (所有网络交互的默认超时)
### 数据本地化
spark.locality.wait
### JVM/gc配置
spark.executor.extraJavaOptions
spark.driver.extraJavaOptions
</code></pre>
</li>
</ol>
</li>
<li><pre><code class="java">调优
executor配置
spark.executor.memory
spark.executor.instances
spark.executor.cores
    
driver配置
spark.driver.memory（如果没有collect操作，一般不需要很大，1~4g即可）
spark.driver.cores
    
并行度
spark.default.parallelism (used for RDD API)
spark.sql.shuffle.partitions (usef for DataFrame/DataSet API)
    
网络超时
spark.network.timeout (所有网络交互的默认超时)
    
数据本地化
spark.locality.wait
    
JVM/gc配置
spark.executor.extraJavaOptions
spark.driver.extraJavaOptions
</code></pre>
</li>
</ul>
<h4 id="数据倾斜调优"><a href="#数据倾斜调优" class="headerlink" title="数据倾斜调优"></a><strong>数据倾斜调优</strong></h4><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/202041-93654.jpg" alt="202041-93654"></p>
<p>使用Hive ETL预处理数据<br>            治标不治本（利用了mr的走disk特性），还多了一条skew pipeline<br>过滤少数导致倾斜的key<br>            但有些场景倾斜是常态<br>提高shuffle操作的并行度<br>            让每个task处理比原来更少的数据（之前可能task会%parNum分到2个key），但是如果单key倾斜，方法失效</p>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/202041-93827.jpg" alt="202041-93827"></p>
<p>两阶段聚合（局部聚合+全局聚合）<br>            附加随机前缀 -&gt; 局部聚合 -&gt; 去除随机前缀 -&gt; 全局聚合<br>            适用于聚合类shuffle（计算sum，count），但是对于join类shuffle不适用</p>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/微信图片_20200401094020.jpg" alt="微信图片_20200401094020"></p>
<p>将reduce join转为map join<br>            适用于join类shuffle，因为shuffle变成map操作了<br>            只适用于一个大表和一个小表，将小表广播，并不适合两个都是大表<br>使用随机前缀和扩容RDD进行join<br>            leftDf添加随机前缀(1<del>N的)；复制rightDf每条record至N条并依次打上前缀(1</del>N)<br>            缺点是复制后的rightDf增大了N-1倍</p>
<h4 id="Shuffle调优"><a href="#Shuffle调优" class="headerlink" title="Shuffle调优"></a><strong>Shuffle调优</strong></h4><p><strong>shuffle原理</strong></p>
<ul>
<li><p>Spark在DAG阶段以宽依赖shuffle为界，划分stage，上游stage做map task，每个map task将计算结果数据分成多份，每一份对应到下游stage的每个partition中，并将其临时写到磁盘，该过程叫做shuffle write</p>
</li>
<li><p>下游stage做reduce task，每个reduce task通过网络拉取上游stage中所有map task的指定分区结果数据，该过程叫做shuffle read，最后完成reduce的业务逻辑</p>
</li>
<li><p>下图中，上游stage有3个map task，下游stage有4个reduce task，那么这3个map task中<strong>每个map task都会产生4份数据</strong>。而4个reduce task中的每个reduce task都会拉取上游3个map task对应的那份数据</p>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/202041-100128.jpg" alt="202041-100128"></p>
<p><strong>shuffle演进</strong></p>
<ol>
<li><p><strong>&lt;0.8</strong> hashBasedShuffle</p>
</li>
<li><ul>
<li>每个map端的task为每个reduce端的partition&#x2F;task生成一个文件，通常会产生大量的文件，伴随大量的随机磁盘IO操作与大量的内存开销<code>M*R</code></li>
</ul>
</li>
<li><p><strong>0.8.1</strong> 引入文件合并File Consolidation机制</p>
</li>
<li><ul>
<li>每个executor为每个reduce端的partition生成一个文件<code>E*R</code></li>
</ul>
</li>
<li><p><strong>0.9</strong> 引入External AppendOnlyMap</p>
</li>
<li><ul>
<li>combine时可以将数据spill到磁盘，然后通过堆排序merge</li>
</ul>
</li>
<li><p><strong>1.1</strong> 引入sortBasedShuffle</p>
</li>
<li><ul>
<li>每个map task不会为每个reducer task生成一个单独的文件，而是会将所有的结果写到一个文件里，同时会生成一个index文件，reducer可以通过这个index文件取得它需要处理的数据<code>M</code></li>
</ul>
</li>
<li><p><strong>1.4</strong> 引入Tungsten-Sort Based Shuffle</p>
</li>
<li><ul>
<li>亦称unsafeShuffle，将数据记录用序列化的二进制方式存储，把排序转化成指针数组的排序，引入堆外内存空间和新的内存管理模型</li>
</ul>
</li>
<li><p><strong>1.6</strong> Tungsten-sort并入Sort Based Shuffle</p>
</li>
<li><ul>
<li>由SortShuffleManager自动判断选择最佳Shuffle方式，如果检测到满足Tungsten-sort条件会自动采用Tungsten-sort Based Shuffle，否则采用Sort Based Shuffle</li>
</ul>
</li>
<li><p><strong>2.0</strong> hashBasedShuffle退出历史舞台</p>
</li>
<li><ul>
<li><p>从此Spark只有sortBasedShuffle</p>
<pre><code class="python">调优

shuffle是一个涉及到CPU（序列化反序列化）、网络IO（跨节点数据传输）以及磁盘IO（shuffle中间结果落盘）的操作。所以用户在编写Spark应用程序的过程中应当尽可能避免shuffle算子和考虑shuffle相关的优化，提升spark应用程序的性能。
要减少shuffle的开销，主要有两个思路，
减少shuffle次数，尽量不改变key，把数据处理在local完成
减少shuffle的数据规模
先去重，再合并
A.union(B).distinct() vs. A.distinct().union(B.distinct()).distinct()
用broadcast + filter来代替join
spark.shuffle.file.buffer
设置shuffle write task的buffer大小，将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘
spark.reducer.maxSizeInFlight
设置shuffle read task的buffer大小，决定了每次能够拉取pull多少数据。减少拉取数据的次数，也就减少了网络传输的次数
spark.shuffle.sort.bypassMergeThreshold
shuffle read task的数量小于这个阈值（默认是200），则map-side/shuffle write过程中不会进行排序操作
</code></pre>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4 id="Spark的join类型"><a href="#Spark的join类型" class="headerlink" title="Spark的join类型"></a><strong>Spark的join类型</strong></h4><p>Shuffled Hash Join<br>Sort Merge Join<br>Broadcast Join</p>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Typora_image/微信图片_20200401100531.jpg" alt="微信图片_20200401100531"></p>
<h4 id="其他优化项"><a href="#其他优化项" class="headerlink" title="其他优化项"></a><strong>其他优化项</strong></h4><ol>
<li><p>使用DataFrame&#x2F;DataSet</p>
</li>
<li><ul>
<li><p>spark sql 的catalyst优化器，</p>
</li>
<li><p>堆外内存（有了Tungsten后，感觉off-head没有那么明显的性能提升了）</p>
<table>
<thead>
<tr>
<th>**Type **</th>
<th>**RDD **</th>
<th><em><strong>*DataFrame*</strong></em></th>
<th><em><strong>*DataSet*</strong></em></th>
</tr>
</thead>
<tbody><tr>
<td>definition</td>
<td>RDD是分布式的Java对象的集合</td>
<td>DataFrame是分布式的Row对象的集合</td>
<td>DataSet是分布式的Java对象的集合 ds &#x3D; df.as[ElementType] df &#x3D; Dataset[Row]</td>
</tr>
<tr>
<td>pros</td>
<td>* 编译时类型安全 * 面向对象的编程风格</td>
<td>* 引入schema结构信息 * 减少数据读取，优化执行计划，如filter下推，剪裁 * off-heap堆外存储</td>
<td>* Encoder序列化 * 支持结构与非结构化数据 * 和rdd一样，支持自定义对象存储 * 和dataframe一样，支持结构化数据的sql查询 * 采用堆外内存存储，gc友好 * 类型转化安全，代码有好</td>
</tr>
<tr>
<td>cons</td>
<td>* 对于结构化数据不友好 * 默认采用的是java序列化方式，序列化结果比较大，而且数据存储在java堆内存中，导致gc比较频繁</td>
<td>* rdd内部数据直接以java对象存储，dataframe内存存储的是Row对象而不能是自定义对象 * 编译时不能类型转化安全检查，运行时才能确定是否有问题</td>
<td>* 可能需要额外定义Encoder</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ol>
<h2 id="Kafka知识补充"><a href="#Kafka知识补充" class="headerlink" title="Kafka知识补充"></a>Kafka知识补充</h2><pre><code class="python">session.timeout.ms
    默认是10000ms，会话超时时间。当我们使用consumer_group的模式进行消费时，kafka如果检测到某个consumer挂掉，就会记性rebalance。consumer每隔一段时间(heartbeat.interval.ms)给broker发送心跳消息，如果超过这个时间没有发送，broker就会认为这个consumer挂了。这个参数的有效取值范围是broker端的设group.min.session.timeout.ms(6000)和group.max.session.timeout.ms(300000)之间。
    
max.poll.interval.ms
    在使用消费者组管理时，调用poll（）之间的最大延迟。这提出了消费者在获取更多记录之前可以闲置的时间量的上界。如果在此超时到期之前未调用poll（），则认为使用者失败，并且组将重新平衡以将分区重新分配给其他成员。
    
max.partition.fetch.bytes
    一次fetch请求，从一个partition中取得的records最大大小。如果在从topic中第一个非空的partition取消息时，如果取到的第一个record的大小就超过这个配置时，仍然会读取这个record，也就是说在这片情况下，只会返回这一条record。 broker、topic都会对producer发给它的message size做限制。所以在配置这值时，可以参考broker的message.max.bytes 和 topic的max.message.bytes的配置.
    session.timeout.ms是针对这个线程到底能不能按时发送心跳的。但是如果这个线程运行正常，但是消费线程挂了呢？这就无法检测了啊。所以就引进了max.poll.interval.ms，用来解决这个问题。
    
heartbeat.interval.ms
    在使用Kafka的团队管理设施时，心跳与团队协调员之间的预期时间。心跳信号用于确保工作人员的会话保持活动状态，并便于在新成员加入或离开组时重新平衡。该值必须设置为低于session.timeout.ms，但通常应设置为不高于该值的1/3。它可以调整得更低，以控制正常再平衡的预期时间。
    
### timestamp_index文件的作用和理解？？？##

</code></pre>
<h2 id="Flink-WaterMark的理解"><a href="#Flink-WaterMark的理解" class="headerlink" title="Flink WaterMark的理解"></a><strong>Flink WaterMark的理解</strong></h2><pre><code class="java">一种延迟触发窗口机制。当前数据流中最大时间-延迟时间  &gt;= 窗口最大时间则触发窗口执行。是一种衡量Event Time进展的机制，它是数据本身的一个隐藏属性。通常基于Event Time的数据，自身都包含一个timestamp.watermark是用于处理乱序事件的，而正确的处理乱序事件，通常用watermark机制结合window来实现。
watermark+window机制。window中可以对input进行按照Event Time排序，使得完全按照Event Time发生的顺序去处理数据，以达到处理乱序数据的目的。
    
基于Event Time的事件处理，Flink默认的事件触发条件为:
对于out-of-order及正常的数据而言
    watermark的时间戳 &gt; = window endTime
    在 [window_start_time,window_end_time] 中有数据存在。

对于late element太多的数据而言
    Event Time &gt; watermark的时间戳

AssignerWithPeriodicWatermarks   
定时提取watermark，这种方式会定时提取更新wartermark。
    
AssignerWithPunctuatedWatermarks  
伴随event的到来就提取watermark，就是每一个event到来的时候，就会提取一次Watermark。这样的方式当然设置watermark更为精准，但是当数据量大的时候，频繁的更新wartermark会比较影响性能。
通常情况下采用定时提取就足够了。
</code></pre>
<h2 id="Hbase的Region划分"><a href="#Hbase的Region划分" class="headerlink" title="Hbase的Region划分"></a><strong>Hbase的Region划分</strong></h2><h2 id="ES-的Setting参数和优化"><a href="#ES-的Setting参数和优化" class="headerlink" title="ES 的Setting参数和优化"></a><strong>ES 的Setting参数和优化</strong></h2><pre><code class="java">&#123;-
    &quot;app_centurily_cbest_order_data_2020-06-12&quot;:&#123;
        &quot;settings&quot;:&#123;-
            &quot;index&quot;:&#123;-
                &quot;routing&quot;:&#123;-
                    &quot;reblance&quot;:&#123;-
                        &quot;enable&quot;:&quot;none&quot;
                    &#125;,
                    &quot;allocation&quot;:&#123;-
                        &quot;total_shards_per_node&quot;:&quot;4&quot;
                    &#125;
                &#125;,
                &quot;refresh_interval&quot;:&quot;120s&quot;,
                --主分片数，默认为5.只能在创建索引时设置，不能修改
                &quot;number_of_shards&quot;: &quot;20&quot;,
                
                &quot;translog&quot;:&#123;-
                    &quot;flush_threshold_size&quot;:&quot;1024mb&quot;,
                    &quot;sync_interval&quot;:&quot;120s&quot;,
                    &quot;durability&quot;:&quot;async&quot;
                &#125;,
                --减少磁盘争用（ssd可以忽略该条设置）
                &quot;merge&quot;:&#123;-
                    &quot;scheduler&quot;:&#123;-
                        &quot;max_thread_count&quot;:&quot;1&quot;
                    &#125;
                &#125;
                &quot;provided_name&quot;:&quot;app_centurily_cbest_order_data_2020-06-12&quot;,
                &quot;max_result_window&quot;:&quot;100000000&quot;,
                &quot;create_date&quot;:&quot;1591923600447&quot;,
                -- 每个主分片的副本数。默认为 1。
                &quot;number_of_replicas&quot;:&quot;1&quot;,
                &quot;uuid&quot;:&quot;wGzBij23445454VpQ&quot;,
                &quot;version&quot;:&#123;-
                    &quot;created&quot;:&quot;7070099&quot;
                &#125;			
            &#125;
        &#125;
    &#125;
&#125;
</code></pre>
<p>1：静态设置：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>index.number_of_shards</td>
<td>主分片数，默认为5.只能在创建索引时设置，不能修改</td>
</tr>
<tr>
<td>index.shard.check_on_startup</td>
<td>是否应在索引打开前检查分片是否损坏，当检查到分片损坏将禁止分片被打开。false：默认值；checksum：检查物理损坏；true:检查物理和逻辑损坏，这将消耗大量内存和CPU；fix：检查物理和逻辑损坏。有损坏的分片将被集群自动删除，这可能导致数据丢失</td>
</tr>
<tr>
<td>index.routing_partition_size</td>
<td>自定义路由值可以转发的目的分片数。默认为 1，只能在索引创建时设置。此值必须小于index.number_of_shards</td>
</tr>
<tr>
<td>index.codec</td>
<td>默认使用LZ4压缩方式存储数据，也可以设置为 best_compression，它使用 DEFLATE 方式以牺牲字段存储性能为代价来获得更高的压缩比例</td>
</tr>
</tbody></table>
<p>2.动态设置</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>index.number_of_replicas</td>
<td>每个主分片的副本数。默认为 1。</td>
</tr>
<tr>
<td>index.auto_expand_replicas</td>
<td>基于可用节点的数量自动分配副本数量,默认为 false（即禁用此功能）</td>
</tr>
<tr>
<td>index.refresh_interval</td>
<td>执行刷新操作的频率，这使得索引的最近更改可以被搜索。默认为 1s。可以设置为 -1 以禁用刷新</td>
</tr>
<tr>
<td>index.max_result_window</td>
<td>用于索引搜索的 from+size 的最大值。默认为 10000</td>
</tr>
<tr>
<td>index.max_rescore_window</td>
<td>在搜索此索引中 rescore 的 window_size 的最大值</td>
</tr>
<tr>
<td>index.blocks.read_only</td>
<td>设置为 true 使索引和索引元数据为只读，false 为允许写入和元数据更改。</td>
</tr>
<tr>
<td>index.blocks.read</td>
<td>设置为 true 可禁用对索引的读取操作</td>
</tr>
<tr>
<td>index.blocks.write</td>
<td>设置为 true 可禁用对索引的写入操作</td>
</tr>
<tr>
<td>index.blocks.metadata</td>
<td>设置为 true 可禁用对索引元数据的读取和写入</td>
</tr>
<tr>
<td>index.max_refresh_listeners</td>
<td>索引的每个分片上可用的最大刷新侦听器数</td>
</tr>
</tbody></table>
<h2 id="ES-Mapping的参数说明和优化"><a href="#ES-Mapping的参数说明和优化" class="headerlink" title="ES Mapping的参数说明和优化"></a><strong>ES Mapping的参数说明和优化</strong></h2><p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/20180123193245289.png" alt="20180123193245289"></p>
<pre><code class="python">Mapping类似数据库中的表结构定义，主要作用如下：

    定义Index下的字段名（Field Name）
    定义字段的类型，比如数据型、字符串型、布尔型等
    定义倒排索引相关配置，比如是否索引、记录position等
    
自定义mapping：

Mapping中字段类型一旦设定后，禁止直接修改（Lucene实现的倒排索引生成后不允许修改）
重新建立新的索引，然后做reindex操作
允许新增字段
## 通过dynamic参数来控制字段的新增 ## 
    true:默认值，允许自动新增字段
    false：不允许自动新增字段，但是文档可以正常写入，但无法对字段进行查询等操作
    strict：文档不能写入，报错
    
    &#123;
    &quot;zhidao_index&quot;: &#123;
        &quot;mappings&quot;: &#123;
            &quot;zhidao_type&quot;: &#123;
                &quot;_ttl&quot;: &#123;
                    &quot;enabled&quot;: false
                &#125;,
                &quot;properties&quot;: &#123;
                    &quot;answer&quot;: &#123;
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;index&quot;: &quot;not_analyzed&quot;
                    &#125;,
                    &quot;answerAuthor&quot;: &#123;
                        &quot;type&quot;: &quot;string&quot;
                    &#125;,
                    &quot;answerDate&quot;: &#123;
                        &quot;type&quot;: &quot;date&quot;,
                        &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot;//这里出现了复合类型
                    &#125;,
                    &quot;answer_author&quot;: &#123;
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;index&quot;: &quot;not_analyzed&quot;
                    &#125;,
                    &quot;answer_date&quot;: &#123;
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;index&quot;: &quot;not_analyzed&quot;
                    &#125;,
                    &quot;author&quot;: &#123;
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;index&quot;: &quot;not_analyzed&quot;
                    &#125;,
                    &quot;category&quot;: &#123;
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;index&quot;: &quot;not_analyzed&quot;
                    &#125;,
                    &quot;date&quot;: &#123;
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;index&quot;: &quot;not_analyzed&quot;
                    &#125;,
                    &quot;description&quot;: &#123;
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;index&quot;: &quot;not_analyzed&quot;
                    &#125;,
                    &quot;id&quot;: &#123;
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;index&quot;: &quot;not_analyzed&quot;
                    &#125;,
                    &quot;keywords&quot;: &#123;
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;index&quot;: &quot;not_analyzed&quot;
                    &#125;,
                    &quot;list&quot;: &#123;
                        &quot;type&quot;: &quot;object&quot;
                    &#125;,
                    &quot;question&quot;: &#123;
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;index&quot;: &quot;not_analyzed&quot;
                    &#125;,
                    &quot;readCount&quot;: &#123;
                        &quot;type&quot;: &quot;long&quot;
                    &#125;,
                    &quot;read_count&quot;: &#123;
                        &quot;type&quot;: &quot;integer&quot;
                    &#125;,
                    &quot;title&quot;: &#123;
                        &quot;type&quot;: &quot;string&quot;
                    &#125;
                &#125;
            &#125;
        &#125;
    &#125;
&#125;
</code></pre>
<p>1.基本数据类型</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>字符串类型</td>
<td>string,text,keyword</td>
</tr>
<tr>
<td>整数类型</td>
<td>integer,long,short,byte</td>
</tr>
<tr>
<td>浮点类型</td>
<td>double,float,half_float,scaled_float</td>
</tr>
<tr>
<td>逻辑类型</td>
<td>boolean</td>
</tr>
<tr>
<td>日期类型</td>
<td>date</td>
</tr>
<tr>
<td>范围类型</td>
<td>range</td>
</tr>
<tr>
<td>二进制类型</td>
<td>binary</td>
</tr>
</tbody></table>
<p>分片查询</p>
<p>Es会将数据均衡的存储在分片中，我们可以指定es去具体的分片或节点钟查询从而进一步的实现es极速查询。</p>
<p><strong>1：randomizeacross shards</strong></p>
<p>随机选择分片查询数据，es的默认方式</p>
<p><strong>2：_local</strong></p>
<p>优先在本地节点上的分片查询数据然后再去其他节点上的分片查询，本地节点没有IO问题但有可能造成负载不均问题。数据量是完整的。</p>
<p><strong>3：_primary</strong></p>
<p>只在主分片中查询不去副本查，一般数据完整。</p>
<p><strong>4：_primary_first</strong></p>
<p>优先在主分片中查，如果主分片挂了则去副本查，一般数据完整。</p>
<p><strong>5：_only_node</strong></p>
<p>只在指定id的节点中的分片中查询，数据可能不完整。</p>
<p><strong>6：_prefer_node</strong></p>
<p>优先在指定你给节点中查询，一般数据完整。</p>
<p><strong>7：_shards</strong></p>
<p>在指定分片中查询，数据可能不完整。</p>
<p><strong>8：_only_nodes</strong></p>
<p>可以自定义去指定的多个节点查询，es不提供此方式需要改源码。</p>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000019057909">https://segmentfault.com/a/1190000019057909</a></p>
<h2 id="Kylin的性能优化"><a href="#Kylin的性能优化" class="headerlink" title="Kylin的性能优化"></a><strong>Kylin的性能优化</strong></h2><h2 id="Flink性能调优"><a href="#Flink性能调优" class="headerlink" title="Flink性能调优"></a>Flink性能调优</h2><h2 id="HiveSQL常用优化方法全面总结"><a href="#HiveSQL常用优化方法全面总结" class="headerlink" title="HiveSQL常用优化方法全面总结"></a>HiveSQL常用优化方法全面总结</h2><p><strong>优化手段</strong></p>
<pre><code class="python">## 列裁剪和分区裁剪
所谓列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。以我们的日历记录表为例：
select uid,event_type,record_data
from calendar_record_log
where pt_date &gt;= 20190201 and pt_date &lt;= 20190224
and status = 0;
select *或者不指定分区，全列扫描和全表扫描效率都很低。
Hive中与列裁剪优化相关的配置项是hive.optimize.cp，与分区裁剪优化相关的则是hive.optimize.pruner，默认都是true。在HiveQL解析阶段对应的则是ColumnPruner逻辑优化器。

## 谓词下推
它就是将SQL语句中的where谓词逻辑都尽可能提前执行，减少下游处理的数据量。
例如以下HiveQL语句
select a.uid,a.event_type,b.topic_id,b.title
from calendar_record_log a
left outer join (
  select uid,topic_id,title from forum_topic
  where pt_date = 20190224 and length(content) &gt;= 100
) b on a.uid = b.uid
where a.pt_date = 20190224 and status = 0;
对forum_topic做过滤的where语句写在子查询内部，而不是外部。Hive中有谓词下推优化的配置项hive.optimize.ppd，默认值true，与它对应的逻辑优化器是PredicatePushDown。该优化器就是将OperatorTree中的FilterOperator向上提，见下图。
</code></pre>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200615231829.jpg"></p>
<pre><code class="python">上面的链接中是一篇讲解HiveQL解析与执行过程的好文章，前文提到的优化器、OperatorTree等概念在其中也有详细的解释，非常推荐。

## sort by代替order by
HiveQL中的order by与其他SQL方言中的功能一样，就是将结果按某字段全局排序，这会导致所有map端数据都进入一个reducer中，在数据量大时可能会长时间计算不完。
如果使用sort by，那么还是会视情况启动多个reducer进行排序，并且保证每个reducer内局部有序。为了控制map端数据分配到reducer的key，往往还要配合distribute by一同使用。如果不加distribute by的话，map端数据就会随机分配到reducer。
举个例子，假如要以UID为key，以上传时间倒序、记录类型倒序输出记录数据：
select uid,upload_time,event_type,record_data
from calendar_record_log
where pt_date &gt;= 20190201 and pt_date &lt;= 20190224
distribute by uid
sort by upload_time desc,event_type desc;.

## group by代替distinct
当要统计某一列的去重数时，如果数据量很大，count(distinct)就会非常慢，原因与order by类似，count(distinct)逻辑只会有很少的reducer来处理。这时可以用group by来改写：
select count(1) from (
  select uid from calendar_record_log
  where pt_date &gt;= 20190101
  group by uid
) t;
但是这样写会启动两个MR job（单纯distinct只会启动一个），所以要确保数据量大到启动job的overhead远小于计算耗时，才考虑这种方法。当数据集很小或者key的倾斜比较明显时，group by还可能会比distinct慢。
那么如何用group by方式同时统计多个列？下面是解决方法：
select t.a,sum(t.b),count(t.c),count(t.d) from (
  select a,b,null c,null d from some_table
  union all
  select a,0 b,c,null d from some_table group by a,c
  union all
  select a,0 b,null c,d from some_table group by a,d
) t;

## group by配置调整
    # - map端预聚合
    group by时，如果先起一个combiner在map端做部分预聚合，可以有效减少shuffle数据量。预聚合的配置项是hive.map.aggr，默认值true，对应的优化器为GroupByOptimizer，简单方便。
通过hive.groupby.mapaggr.checkinterval参数也可以设置map端预聚合的行数阈值，超过该值就会分拆job，默认值100000。

    # - 倾斜均衡配置项
    group by时如果某些key对应的数据量过大，就会发生数据倾斜。Hive自带了一个均衡数据倾斜的配置项hive.groupby.skewindata，默认值false。
其实现方法是在group by时启动两个MR job。第一个job会将map端数据随机输入reducer，每个reducer做部分聚合，相同的key就会分布在不同的reducer中。第二个job再将前面预处理过的数据按key聚合并输出结果，这样就起到了均衡的效果。
但是，配置项毕竟是死的，单纯靠它有时不能根本上解决问题，因此还是建议自行了解数据倾斜的细节，并优化查询语句。

## join基础优化
    # - build table（小表）前置
    在最常见的hash join方法中，一般总有一张相对小的表和一张相对大的表，小表叫build table，大表叫probe table。如下图所示：
</code></pre>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200615233237.jpg"></p>
<pre><code class="python">Hive在解析带join的SQL语句时，会默认将最后一个表作为probe table，将前面的表作为build table并试图将它们读进内存。如果表顺序写反，probe table在前面，引发OOM的风险就高了。
在维度建模数据仓库中，事实表就是probe table，维度表就是build table。假设现在要将日历记录事实表和记录项编码维度表来join：
select a.event_type,a.event_code,a.event_desc,b.upload_time
from calendar_event_code a
inner join (
  select event_type,upload_time from calendar_record_log
  where pt_date = 20190225
) b on a.event_type = b.event_type;

# - 多表join时key相同
这种情况会将多个join合并为一个MR job来处理，例如：
select a.event_type,a.event_code,a.event_desc,b.upload_time
from calendar_event_code a
inner join (
  select event_type,upload_time from calendar_record_log
  where pt_date = 20190225
) b on a.event_type = b.event_type
inner join (
  select event_type,upload_time from calendar_record_log_2
  where pt_date = 20190225
) c on a.event_type = c.event_type;
如果上面两个join的条件不相同，比如改成a.event_code = c.event_code，就会拆成两个MR job计算。
负责这个的是相关性优化器CorrelationOptimizer，它的功能除此之外还非常多，逻辑复杂，参考Hive官方的文档可以获得更多细节：https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer
    
    # - 利用map join特性
 map join特别适合大小表join的情况。Hive会将build table和probe table在map端直接完成join过程，消灭了reduce，效率很高。
    select /*+mapjoin(a)*/ a.event_type,b.upload_time
    from calendar_event_code a
    inner join (
          select event_type,upload_time from calendar_record_log
          where pt_date = 20190225
    ) b on a.event_type &lt; b.event_type;
上面的语句中加了一条map join hint，以显式启用map join特性。早在Hive 0.8版本之后，就不需要写这条hint了。map join还支持不等值连接，应用更加灵活。
map join的配置项是hive.auto.convert.join，默认值true，对应逻辑优化器是MapJoinProcessor。
还有一些参数用来控制map join的行为，比如hive.mapjoin.smalltable.filesize，当build table大小小于该值就会启用map join，默认值25000000（25MB）。还有hive.mapjoin.cache.numrows，表示缓存build table的多少行数据到内存，默认值25000。

    # - 分桶表map join
map join对分桶表还有特别的优化。由于分桶表是基于一列进行hash存储的，因此非常适合抽样（按桶或按块抽样）。
它对应的配置项是hive.optimize.bucketmapjoin，优化器是BucketMapJoinOptimizer。但我们的业务中用分桶表较少，所以就不班门弄斧了，只是提一句。
sort merge bucket join

    # - 倾斜均衡配置项
    这个配置与上面group by的倾斜均衡配置项异曲同工，通过hive.optimize.skewjoin来配置，默认false。
如果开启了，在join过程中Hive会将计数超过阈值hive.skewjoin.key（默认100000）的倾斜key对应的行临时写进文件中，然后再启动另一个job做map join生成结果。通过hive.skewjoin.mapjoin.map.tasks参数还可以控制第二个job的mapper数量，默认10000。
再重复一遍，通过自带的配置项经常不能解决数据倾斜问题。join是数据倾斜的重灾区，后面还要介绍在SQL层面处理倾斜的各种方法。

## 优化SQL处理join数据倾斜
上面已经多次提到了数据倾斜，包括已经写过的sort by代替order by，以及group by代替distinct方法，本质上也是为了解决它。join操作更是数据倾斜的重灾区，需要多加注意。

    # - 空值或无意义值
    这种情况很常见，比如当事实表是日志类数据时，往往会有一些项没有记录到，我们视情况会将它置为null，或者空字符串、-1等。如果缺失的项很多，在做join时这些空值就会非常集中，拖累进度。
因此，若不需要空值数据，就提前写where语句过滤掉。需要保留的话，将空值key用随机方式打散，例如将用户ID为null的记录随机改为负值：
select a.uid,a.event_type,b.nickname,b.age
from (
  select 
  (case when uid is null then cast(rand()*-10240 as int) else uid end) as uid,
  event_type from calendar_record_log
  where pt_date &gt;= 20190201
) a left outer join (
  select uid,nickname,age from user_info where status = 4
) b on a.uid = b.uid;

    # - 单独处理倾斜key
    这其实是上面处理空值方法的拓展，不过倾斜的key变成了有意义的。一般来讲倾斜的key都很少，我们可以将它们抽样出来，对应的行单独存入临时表中，然后打上一个较小的随机数前缀（比如0~9），最后再进行聚合。SQL语句与上面的相仿，不再赘述。
    
    # - 不同数据类型
    这种情况不太常见，主要出现在相同业务含义的列发生过逻辑上的变化时。
举个例子，假如我们有一旧一新两张日历记录表，旧表的记录类型字段是(event_type int)，新表的是(event_type string)。为了兼容旧版记录，新表的event_type也会以字符串形式存储旧版的值，比如&#39;17&#39;。当这两张表join时，经常要耗费很长时间。其原因就是如果不转换类型，计算key的hash值时默认是以int型做的，这就导致所有“真正的”string型key都分配到一个reducer上。所以要注意类型转换：
elect a.uid,a.event_type,b.record_data
from calendar_record_log a
left outer join (
  select uid,event_type from calendar_record_log_2
  where pt_date = 20190228
) b on a.uid = b.uid and b.event_type = cast(a.event_type as string)
where a.pt_date = 20190228;

    # - build table过大
    有时，build table会大到无法直接使用map join的地步，比如全量用户维度表，而使用普通join又有数据分布不均的问题。这时就要充分利用probe table的限制条件，削减build table的数据量，再使用map join解决。代价就是需要进行两次join。举个例子：
    select /*+mapjoin(b)*/ a.uid,a.event_type,b.status,b.extra_info
    from calendar_record_log a
    left outer join (
          select /*+mapjoin(s)*/ t.uid,t.status,t.extra_info
          from (select distinct uid from calendar_record_log where pt_date = 20190228) s
          inner join user_info t on s.uid = t.uid
    ) b on a.uid = b.uid
where a.pt_date = 20190228;

## MapReduce优化
</code></pre>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200615234331.jpg"></p>
<pre><code class="python">    # -调整mapper数
mapper数量与输入文件的split数息息相关，在Hadoop源码org.apache.hadoop.mapreduce.lib.input.FileInputFormat类中可以看到split划分的具体逻辑。这里不贴代码，直接叙述mapper数是如何确定的。
    可以直接通过参数mapred.map.tasks（默认值2）来设定mapper数的期望值，但它不一定会生效，下面会提到。
    设输入文件的总大小为total_input_size。HDFS中，一个块的大小由参数dfs.block.size指定，默认值64MB或128MB。在默认情况下，mapper数就是：
default_mapper_num = total_input_size / dfs.block.size。
    参数mapred.min.split.size（默认值1B）和mapred.max.split.size（默认值64MB）分别用来指定split的最小和最大大小。split大小和split数计算规则是：
split_size = MAX(mapred.min.split.size, MIN(mapred.max.split.size, dfs.block.size))；
split_num = total_input_size / split_size。
    得出mapper数：
mapper_num = MIN(split_num, MAX(default_num, mapred.map.tasks))。
可见，如果想减少mapper数，就适当调高mapred.min.split.size，split数就减少了。如果想增大mapper数，除了降低mapred.min.split.size之外，也可以调高mapred.map.tasks。
一般来讲，如果输入文件是少量大文件，就减少mapper数；如果输入文件是大量非小文件，就增大mapper数；至于大量小文件的情况，得参考下面“合并小文件”一节的方法处理。

    # -调整reducer数
reducer数量的确定方法比mapper简单得多。使用参数mapred.reduce.tasks可以直接设定reducer数量，不像mapper一样是期望值。但如果不设这个参数的话，Hive就会自行推测，逻辑如下：
    参数hive.exec.reducers.bytes.per.reducer用来设定每个reducer能够处理的最大数据量，默认值1G（1.2版本之前）或256M（1.2版本之后）。
    参数hive.exec.reducers.max用来设定每个job的最大reducer数量，默认值999（1.2版本之前）或1009（1.2版本之后）。
    得出reducer数：
reducer_num = MIN(total_input_size / reducers.bytes.per.reducer, reducers.max)。
reducer数量与输出文件的数量相关。如果reducer数太多，会产生大量小文件，对HDFS造成压力。如果reducer数太少，每个reducer要处理很多数据，容易拖慢运行时间或者造成OOM。

    # -合并小文件
输入阶段合并
    需要更改Hive的输入文件格式，即参数hive.input.format，默认值是org.apache.hadoop.hive.ql.io.HiveInputFormat，我们改成org.apache.hadoop.hive.ql.io.CombineHiveInputFormat。
这样比起上面调整mapper数时，又会多出两个参数，分别是mapred.min.split.size.per.node和mapred.min.split.size.per.rack，含义是单节点和单机架上的最小split大小。如果发现有split大小小于这两个值（默认都是100MB），则会进行合并。具体逻辑可以参看Hive源码中的对应类。
输出阶段合并
    直接将hive.merge.mapfiles和hive.merge.mapredfiles都设为true即可，前者表示将map-only任务的输出合并，后者表示将map-reduce任务的输出合并。
另外，hive.merge.size.per.task可以指定每个task输出后合并文件大小的期望值，hive.merge.size.smallfiles.avgsize可以指定所有输出文件大小的均值阈值，默认值都是1GB。如果平均大小不足的话，就会另外启动一个任务来进行合并。

    # -启用压缩
压缩job的中间结果数据和输出数据，可以用少量CPU时间节省很多空间。压缩方式一般选择Snappy，效率最高。
要启用中间压缩，需要设定hive.exec.compress.intermediate为true，同时指定压缩方式hive.intermediate.compression.codec为org.apache.hadoop.io.compress.SnappyCodec。另外，参数hive.intermediate.compression.type可以选择对块（BLOCK）还是记录（RECORD）压缩，BLOCK的压缩率比较高。
输出压缩的配置基本相同，打开hive.exec.compress.output即可。

    # -JVM重用
在MR job中，默认是每执行一个task就启动一个JVM。如果task非常小而碎，那么JVM启动和关闭的耗时就会很长。可以通过调节参数mapred.job.reuse.jvm.num.tasks来重用。例如将这个参数设成5，那么就代表同一个MR job中顺序执行的5个task可以重复使用一个JVM，减少启动和关闭的开销。但它对不同MR job中的task无效。

## 并行执行与本地模式
并行执行
    Hive中互相没有依赖关系的job间是可以并行执行的，最典型的就是多个子查询union all。在集群资源相对充足的情况下，可以开启并行执行，即将参数hive.exec.parallel设为true。另外hive.exec.parallel.thread.number可以设定并行执行的线程数，默认为8，一般都够用。
本地模式
    Hive也可以不将任务提交到集群进行运算，而是直接在一台节点上处理。因为消除了提交到集群的overhead，所以比较适合数据量很小，且逻辑不复杂的任务。
设置hive.exec.mode.local.auto为true可以开启本地模式。但任务的输入数据总量必须小于hive.exec.mode.local.auto.inputbytes.max（默认值128MB），且mapper数必须小于hive.exec.mode.local.auto.tasks.max（默认值4），reducer数必须为0或1，才会真正用本地模式执行。

## 严格模式
所谓严格模式，就是强制不允许用户执行3种有风险的HiveQL语句，一旦执行会直接失败。这3种语句是：
查询分区表时不限定分区列的语句；
    两表join产生了笛卡尔积的语句；
    用order by来排序但没有指定limit的语句。
    要开启严格模式，需要将参数hive.mapred.mode设为strict
## 采用合适的存储格式
在HiveQL的create table语句中，可以使用stored as ...指定表的存储格式。Hive表支持的存储格式有TextFile、SequenceFile、RCFile、Avro、ORC、Parquet等。
存储格式一般需要根据业务进行选择，在我们的实操中，绝大多数表都采用TextFile与Parquet两种存储格式之一。
TextFile是最简单的存储格式，它是纯文本记录，也是Hive的默认格式。虽然它的磁盘开销比较大，查询效率也低，但它更多地是作为跳板来使用。RCFile、ORC、Parquet等格式的表都不能由文件直接导入数据，必须由TextFile来做中转。
Parquet和ORC都是Apache旗下的开源列式存储格式。列式存储比起传统的行式存储更适合批量OLAP查询，并且也支持更好的压缩和编码。我们选择Parquet的原因主要是它支持Impala查询引擎，并且我们对update、delete和事务性操作需求很低。
这里就不展开讲它们的细节，可以参考各自的官网：
https://parquet.apache.org/
https://orc.apache.org/
</code></pre>
<h2 id="spark数据倾斜"><a href="#spark数据倾斜" class="headerlink" title="spark数据倾斜"></a><strong>spark数据倾斜</strong></h2><p>场景1：大量不同的Key被分配到了相同的Task造成该Task数据量过大。<br>        ***	案例	***<br>现有一张测试表，名为student_external，内有10.5亿条数据，每条数据有一个唯一的id值。现从中取出id取值为9亿到10.5亿的共1.5条数据，并通过一些处理，使得id为9亿到9.4亿间的所有数据对12取模后余数为8（即在Shuffle并行度为12时该数据集全部被HashPartition分配到第8个Task），其它数据集对其id除以100取整，从而使得id大于9.4亿的数据在Shuffle时可被均匀分配到所有Task中，而id小于9.4亿的数据全部分配到同一个Task中。处理过程如</p>
<pre><code class="sql">INSERT OVERWRITE TABLE test
SELECT CASE WHEN id &lt; 940000000 THEN (9500000  + (CAST (RAND() * 8 AS INTEGER)) * 12 )
       ELSE CAST(id/100 AS INTEGER)
       END,
       name
FROM student_external
WHERE id BETWEEN 900000000 AND 1050000000;
</code></pre>
<p>这种方式可能会造成数据倾斜。接下来，使用Spark读取该测试数据，并通过*groupByKey(12)*对id分组处理，且Shuffle并行度为12。代码如下：</p>
<pre><code class="python">public class SparkDataSkew &#123;
  public static void main(String[] args) &#123;
    SparkSession sparkSession = SparkSession.builder()
      .appName(&quot;SparkDataSkewTunning&quot;)
      .config(&quot;hive.metastore.uris&quot;, &quot;thrift://hadoop1:9083&quot;)
      .enableHiveSupport()
      .getOrCreate();

    Dataset&lt;Row&gt; dataframe = sparkSession.sql( &quot;select * from test&quot;);
    dataframe.toJavaRDD()
      .mapToPair((Row row) -&gt; new Tuple2&lt;Integer, String&gt;(row.getInt(0),row.getString(1)))
      .groupByKey(12)
      .mapToPair((Tuple2&lt;Integer, Iterable&lt;String&gt;&gt; tuple) -&gt; &#123;
        int id = tuple._1();
        AtomicInteger atomicInteger = new AtomicInteger(0);
        tuple._2().forEach((String name) -&gt; atomicInteger.incrementAndGet());
        return new Tuple2&lt;Integer, Integer&gt;(id, atomicInteger.get());
      &#125;).count();

      sparkSession.stop();
      sparkSession.close();
  &#125;
  
&#125;
提交：   
spark-submit --queue ambari --num-executors 4 --executor-cores 12 --executor-memory 12g --class com.jasongj.spark.driver.SparkDataSkew --master yarn --deploy-mode client SparkExample-with-dependencies-1.0.jar
    
# 自定义Partitioner
使用自定义的Partitioner（默认为HashPartitioner），将原本被分配到同一个Task的不同Key分配到不同Task。
以上述数据集为例，继续将并发度设置为12，但是在groupByKey算子上，使用自定义的Partitioner（实现如下）

.groupByKey(new Partitioner() &#123;
  @Override
  public int numPartitions() &#123;
    return 12;
  &#125;

  @Override
  public int getPartition(Object key) &#123;
    int id = Integer.parseInt(key.toString());
    if(id &gt;= 9500000 &amp;&amp; id &lt;= 9500084 &amp;&amp; ((id - 9500000) % 12) == 0) &#123;
      return (id - 9500000) / 12;
    &#125; else &#123;
      return id % 12;
    &#125;
  &#125;
&#125;)
</code></pre>
<p>场景二：小数据集join大数据集</p>
<p>解决方案：在Java&#x2F;Scala代码中将小数据集数据拉取到Driver，然后通过Broadcast方案将小数据集的数据广播到各Executor。或者在使用SQL前，将Broadcast的阈值调整得足够多，从而使用Broadcast生效。进而将Reduce侧Join替换为Map侧Join。</p>
<p>*<strong>案例*</strong></p>
<p>通过如下SQL创建一张具有倾斜Key且总记录数为1.5亿的大表test。</p>
<pre><code class="sql">INSERT OVERWRITE TABLE test
SELECT CAST(CASE WHEN id &lt; 980000000 THEN (95000000  + (CAST (RAND() * 4 AS INT) + 1) * 48 )
       ELSE CAST(id/10 AS INT) END AS STRING),
       name
FROM student_external
WHERE id BETWEEN 900000000 AND 1050000000;
</code></pre>
<p>使用如下SQL创建一张数据分布均匀且总记录数为50万的小表test_new。</p>
<pre><code class="sql">INSERT OVERWRITE TABLE test_new
SELECT CAST(CAST(id/10 AS INT) AS STRING),
       name
FROM student_delta_external
WHERE id BETWEEN 950000000 AND 950500000;
</code></pre>
<p>直接通过Spark Thrift Server提交如下SQL将表test与表test_new进行Join并将Join结果存于表test_join中。</p>
<pre><code class="sql">INSERT OVERWRITE TABLE test_join
SELECT test_new.id, test_new.name
FROM test
JOIN test_new
ON test.id = test_new.id;
</code></pre>
<p>以上sql执行会发生数据倾斜。</p>
<p>接下来，尝试通过Broadcast实现Map侧Join。实现Map侧Join的方法，并非直接通过<em>CACHE TABLE test_new</em>将小表test_new进行cache。现通过如下SQL进行Join。</p>
<pre><code class="sql">CACHE TABLE test_new;
INSERT OVERWRITE TABLE test_join
SELECT test_new.id, test_new.name
FROM test
JOIN test_new
ON test.id = test_new.id;
</code></pre>
<p>该操作仍分为三个Stage，且仍然有Shuffle存在，唯一不同的是，小表的读取不再直接扫描Hive表，而是扫描内存中缓存的表。并且数据倾斜仍然存在。</p>
<p><strong>正确的使用Broadcast实现Map侧Join的方式是</strong>，通过*SET spark.sql.autoBroadcastJoinThreshold&#x3D;104857600;*将Broadcast的阈值设置得足够大。</p>
<p>最终版：</p>
<pre><code class="sql">SET spark.sql.autoBroadcastJoinThreshold=104857600;
INSERT OVERWRITE TABLE test_join
SELECT test_new.id, test_new.name
FROM test
JOIN test_new
ON test.id = test_new.id;
</code></pre>
<p>场景三：两张表都比较大，无法使用Map则Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p>
<p>解决：为skew的key增加随机前&#x2F;后缀。<br>为数据量特别大的Key增加随机前&#x2F;后缀，从而使倾斜的数据集分散到不同的Task中，彻底解决数据倾斜问题。Join另一则的数据中，与倾斜Key对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜Key如何加前缀，都能与之正常Join。</p>
<h3 id="案例"><a href="#案例" class="headerlink" title="*案例*"></a>*<strong>案例*</strong></h3><p>通过如下SQL，将id为9亿到9.08亿共800万条数据的id转为9500048或者9500096，其它数据的id除以100取整。从而该数据集中，id为9500048和9500096的数据各400万，其它id对应的数据记录数均为100条。这些数据存于名为test的表中。</p>
<p>对于另外一张小表test_new，取出50万条数据，并将id（递增且唯一）除以100取整，使得所有id都对应100条数据。</p>
<pre><code class="SQL">INSERT OVERWRITE TABLE test
SELECT CAST(CASE WHEN id &lt; 908000000 THEN (9500000  + (CAST (RAND() * 2 AS INT) + 1) * 48 )
  ELSE CAST(id/100 AS INT) END AS STRING),
  name
FROM student_external
WHERE id BETWEEN 900000000 AND 1050000000;

INSERT OVERWRITE TABLE test_new
SELECT CAST(CAST(id/100 AS INT) AS STRING),
  name
FROM student_delta_external
WHERE id BETWEEN 950000000 AND 950500000;
</code></pre>
<p>通过如下代码，读取test表对应的文件夹内的数据并转换为JavaPairRDD存于leftRDD中，同样读取test表对应的数据存于rightRDD中。通过RDD的join算子对leftRDD与rightRDD进行Join，并指定并行度为48。</p>
<pre><code class="java">public class SparkDataSkew&#123;
  public static void main(String[] args) &#123;
    SparkConf sparkConf = new SparkConf();
    sparkConf.setAppName(&quot;DemoSparkDataFrameWithSkewedBigTableDirect&quot;);
    sparkConf.set(&quot;spark.default.parallelism&quot;, String.valueOf(parallelism));
    JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf);

    JavaPairRDD&lt;String, String&gt; leftRDD = javaSparkContext.textFile(&quot;hdfs://hadoop1:8020/apps/hive/warehouse/default/test/&quot;)
      .mapToPair((String row) -&gt; &#123;
        String[] str = row.split(&quot;,&quot;);
        return new Tuple2&lt;String, String&gt;(str[0], str[1]);
      &#125;);

    JavaPairRDD&lt;String, String&gt; rightRDD = javaSparkContext.textFile(&quot;hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/&quot;)
      .mapToPair((String row) -&gt; &#123;
        String[] str = row.split(&quot;,&quot;);
          return new Tuple2&lt;String, String&gt;(str[0], str[1]);
      &#125;);

    leftRDD.join(rightRDD, parallelism)
      .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2()))
      .foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;
        AtomicInteger atomicInteger = new AtomicInteger();
          iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());
      &#125;);

    javaSparkContext.stop();
    javaSparkContext.close();
  &#125;
&#125;
</code></pre>
<p>此代码会产生数据倾斜。<br>现通过如下操作，实现倾斜Key的分散处理</p>
<ul>
<li><p>将leftRDD中倾斜的key（即9500048与9500096）对应的数据单独过滤出来，且加上1到24的随机前缀，并将前缀与原数据用逗号分隔（以方便之后去掉前缀）形成单独的leftSkewRDD</p>
</li>
<li><p>将rightRDD中倾斜key对应的数据抽取出来，并通过flatMap操作将该数据集中每条数据均转换为24条数据（每条分别加上1到24的随机前缀），形成单独的rightSkewRDD</p>
</li>
<li><p>将leftSkewRDD与rightSkewRDD进行Join，并将并行度设置为48，且在Join过程中将随机前缀去掉，得到倾斜数据集的Join结果skewedJoinRDD</p>
</li>
<li><p>将leftRDD中不包含倾斜Key的数据抽取出来作为单独的leftUnSkewRDD</p>
</li>
<li><p>对leftUnSkewRDD与原始的rightRDD进行Join，并行度也设置为48，得到Join结果unskewedJoinRDD</p>
</li>
<li><p>通过union算子将skewedJoinRDD与unskewedJoinRDD进行合并，从而得到完整的Join结果集</p>
<pre><code class="java">public class SparkDataSkew&#123;
    public static void main(String[] args) &#123;
      int parallelism = 48;
      SparkConf sparkConf = new SparkConf();
      sparkConf.setAppName(&quot;SolveDataSkewWithRandomPrefix&quot;);
      sparkConf.set(&quot;spark.default.parallelism&quot;, parallelism + &quot;&quot;);
      JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf);

      JavaPairRDD&lt;String, String&gt; leftRDD = javaSparkContext.textFile(&quot;hdfs://hadoop1:8020/apps/hive/warehouse/default/test/&quot;)
        .mapToPair((String row) -&gt; &#123;
          String[] str = row.split(&quot;,&quot;);
            return new Tuple2&lt;String, String&gt;(str[0], str[1]);
        &#125;);

        JavaPairRDD&lt;String, String&gt; rightRDD = javaSparkContext.textFile(&quot;hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/&quot;)
          .mapToPair((String row) -&gt; &#123;
            String[] str = row.split(&quot;,&quot;);
              return new Tuple2&lt;String, String&gt;(str[0], str[1]);
          &#125;);

        String[] skewedKeyArray = new String[]&#123;&quot;9500048&quot;, &quot;9500096&quot;&#125;;
        Set&lt;String&gt; skewedKeySet = new HashSet&lt;String&gt;();
        List&lt;String&gt; addList = new ArrayList&lt;String&gt;();
        for(int i = 1; i &lt;=24; i++) &#123;
            addList.add(i + &quot;&quot;);
        &#125;
        for(String key : skewedKeyArray) &#123;
            skewedKeySet.add(key);
        &#125;

        Broadcast&lt;Set&lt;String&gt;&gt; skewedKeys = javaSparkContext.broadcast(skewedKeySet);
        Broadcast&lt;List&lt;String&gt;&gt; addListKeys = javaSparkContext.broadcast(addList);

        JavaPairRDD&lt;String, String&gt; leftSkewRDD = leftRDD
          .filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1()))
          .mapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;((new Random().nextInt(24) + 1) + &quot;,&quot; + tuple._1(), tuple._2()));

        JavaPairRDD&lt;String, String&gt; rightSkewRDD = rightRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1()))
          .flatMapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; addListKeys.value().stream()
          .map((String i) -&gt; new Tuple2&lt;String, String&gt;( i + &quot;,&quot; + tuple._1(), tuple._2()))
          .collect(Collectors.toList())
          .iterator()
        );

        JavaPairRDD&lt;String, String&gt; skewedJoinRDD = leftSkewRDD
          .join(rightSkewRDD, parallelism)
          .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1().split(&quot;,&quot;)[1], tuple._2()._2()));

        JavaPairRDD&lt;String, String&gt; leftUnSkewRDD = leftRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; !skewedKeys.value().contains(tuple._1()));
        JavaPairRDD&lt;String, String&gt; unskewedJoinRDD = leftUnSkewRDD.join(rightRDD, parallelism).mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2()));

        skewedJoinRDD.union(unskewedJoinRDD).foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;
          AtomicInteger atomicInteger = new AtomicInteger();
          iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());
        &#125;);
        javaSparkContext.stop();
        javaSparkContext.close();
    &#125;
    &#125;
</code></pre>
<h2 id="kafka-分区的选择"><a href="#kafka-分区的选择" class="headerlink" title="kafka 分区的选择"></a><strong>kafka 分区的选择</strong></h2><pre><code class="python">## 分区多的优点 ##
kafka使用分区将topic的消息打散到多个分区分布保存在不同的broker上，实现了producer和consumer消息处理的高吞吐量。Kafka的producer和consumer都可以多线程地并行操作，而每个线程处理的是一个分区的数据。因此分区实际上是调优Kafka并行度的最小单元。对于producer而言，它实际上是用多个线程并发地向不同分区所在的broker发起Socket连接同时给这些分区发送消息；而consumer，同一个消费组内的所有consumer线程都被指定topic的某一个分区进行消费。
所以说，如果一个topic分区越多，理论上整个集群所能达到的吞吐量就越大。

## 分区不是越多越好 ##
分区是否越多越好呢？显然也不是，因为每个分区都有自己的开销:

# 一、客户端/服务器端需要使用的内存就越多 #
Kafka0.8.2之后，在客户端producer有个参数batch.size，默认是16KB。它会为每个分区缓存消息，一旦满了就打包将消息批量发出。看上去这是个能够提升性能的设计。不过很显然，因为这个参数是分区级别的，如果分区数越多，这部分缓存所需的内存占用也会更多。假设你有10000个分区，按照默认设置，这部分缓存需要占用约157MB的内存。而consumer端呢？我们抛开获取数据所需的内存不说，只说线程的开销。如果还是假设有10000个分区，同时consumer线程数要匹配分区数(大部分情况下是最佳的消费吞吐量配置)的话，那么在consumer client就要创建10000个线程，也需要创建大约10000个Socket去获取分区数据。这里面的线程切换的开销本身已经不容小觑了。
服务器端的开销也不小，如果阅读Kafka源码的话可以发现，服务器端的很多组件都在内存中维护了分区级别的缓存，比如controller，FetcherManager等，因此分区数越多，这种缓存的成本就越大。
# 二、文件句柄的开销 #
每个分区在底层文件系统都有属于自己的一个目录。该目录下通常会有两个文件：base_offset.log和base_offset.index。Kafak的controller和ReplicaManager会为每个broker都保存这两个文件句柄(file handler)。很明显，如果分区数越多，所需要保持打开状态的文件句柄数也就越多，最终可能会突破你的ulimit -n的限制。
# 三、降低高可用性 #
Kafka通过副本(replica)机制来保证高可用。具体做法就是为每个分区保存若干个副本(replica_factor指定副本数)。每个副本保存在不同的broker上。其中的一个副本充当leader 副本，负责处理producer和consumer请求。其他副本充当follower角色，由Kafka controller负责保证与leader的同步。如果leader所在的broker挂掉了，contorller会检测到然后在zookeeper的帮助下重选出新的leader——这中间会有短暂的不可用时间窗口，虽然大部分情况下可能只是几毫秒级别。但如果你有10000个分区，10个broker，也就是说平均每个broker上有1000个分区。此时这个broker挂掉了，那么zookeeper和controller需要立即对这1000个分区进行leader选举。比起很少的分区leader选举而言，这必然要花更长的时间，并且通常不是线性累加的。如果这个broker还同时是controller情况就更糟了。
</code></pre>
<p><strong>如何确定分区数量呢？</strong>　　</p>
<p>可以遵循一定的步骤来尝试确定分区数：创建一个只有1个分区的topic，然后测试这个topic的producer吞吐量和consumer吞吐量。假设它们的值分别是Tp和Tc，单位可以是MB&#x2F;s。然后假设总的目标吞吐量是Tt，<strong>那么分区数 &#x3D; Tt &#x2F; max(Tp, Tc)</strong></p>
<p>说明：Tp表示producer的吞吐量。测试producer通常是很容易的，因为它的逻辑非常简单，就是直接发送消息到Kafka就好了。Tc表示consumer的吞吐量。测试Tc通常与应用的关系更大， 因为Tc的值取决于你拿到消息之后执行什么操作，因此Tc的测试通常也要麻烦一些。</p>
<p><strong>一条消息如何知道要被发送到哪个分区？</strong></p>
<p><strong>1，</strong> <em><strong>*如果指定了分区，就写入到指定的分区中。*</strong></em></p>
<p><strong>2，</strong> <em><strong>*如果没有指定分区，指定了key，按照key的hashcode，取模，写入对应的分区*</strong></em></p>
<p>def partition(key: Any, numPartitions: Int): Int &#x3D; {   Utils.abs(key.hashCode) % numPartitions }</p>
<p> 如果<strong>key为null时，从缓存中取分区id或者随机取一个。</strong>如果你没有指定key，那么Kafka是如何确定这条消息去往哪个分区的呢？</p>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200625083640.png"></p>
<p><strong>3</strong>，<em><strong>没有指定分区和key，轮询机制*</strong></em></p>
<pre><code class="python">## Consumer个数与分区数有什么关系？## 
topic下的一个分区只能被同一个consumer group下的一个consumer线程来消费，但反之并不成立，即一个consumer线程可以消费多个分区的数据，比如Kafka提供的ConsoleConsumer，默认就只是一个线程来消费所有分区的数据。
所以，如果你的分区数是N，那么最好线程数也保持为N，这样通常能够达到最大的吞吐量。超过N的配置只是浪费系统资源，因为多出的线程不会被分配到任何分区。
</code></pre>
</li>
</ul>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200625084324.png"></p>
<pre><code class="python">## Consumer消费Partition的分配策略 ##
Kafka提供的两种分配策略：range和roundrobin，由参数partition.assignment.strategy指定，默认是range策略。
当以下事件发生时，Kafka 将会进行一次分区分配：
    #同一个 Consumer Group 内新增消费者
    #消费者离开当前所属的Consumer Group，包括shuts down 或 crashes
    #订阅的主题新增分区
    
Range策略是对每个主题而言的，首先对同一个主题里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。
然后将partitions的个数除于消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区.
例如：排完序的分区将会是0, 1, 2, 3, 4, 5, 6, 7, 8, 9，10；消费者线程序号排完序将会是C1-0, C2-0, C2-1
最后分区分配的结果看起来是这样的.
C1-0 将消费 0, 1, 2, 3 分区
C2-0 将消费 4, 5, 6, 7 分区
C2-1 将消费 8, 9, 10 分区

使用RoundRobin策略有两个前提条件必须满足：
同一个Consumer Group里面的所有消费者的num.streams必须相等；
每个消费者订阅的主题必须相同。
所以这里假设前面提到的2个消费者的num.streams = 2。RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序，最后按照round-robin风格将分区分别分配给不同的消费者线程。
目前我们还不能自定义分区分配策略，只能通过partition.assignment.strategy参数选择 range 或 roundrobin。
</code></pre>
<p><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200625085037.png"></p>
<h2 id="StructedStreaming-的outputMode注意事项："><a href="#StructedStreaming-的outputMode注意事项：" class="headerlink" title="StructedStreaming 的outputMode注意事项："></a><strong>StructedStreaming 的outputMode注意事项：</strong></h2><p>Append 模式（只输出新添加的（原来没有的））</p>
<ol>
<li>Append模式用WaterMark删除旧的聚合状态。</li>
<li>仅在WaterMark &gt;&#x3D; Window End Time时，输出一次最终结果到结果表并写入到接收器。</li>
<li>如果有聚合操作，必须添加waterMark，否则不支持。</li>
<li>Join查询，目前只支持Append模式。</li>
<li>Append模式下，在flatMapGroupsWithState之后，可再有聚合操作。</li>
</ol>
<p>Update 模式（只输出变化的部分）</p>
<ol>
<li>Update模式用WaterMark删除旧的聚合状态。</li>
<li>只要WaterMark &lt; Window End Time, 就会触发聚合计算并输出。</li>
<li>Update模式下, 在flatMapGroupsWithState之后，不允许再有聚合操作。</li>
<li>支持MapGroupsWithState</li>
</ol>
<p>Complete 模式（全部输出）</p>
<ol>
<li>Complete模式不会删除旧的聚合状态。</li>
<li>不论数据迟到多久，都会触发聚合计算。</li>
<li>必须要聚合操作，否则报错。</li>
</ol>
<h2 id="StructedStreaming-DataFrame-x2F-Streaming-DataSet的注意事项"><a href="#StructedStreaming-DataFrame-x2F-Streaming-DataSet的注意事项" class="headerlink" title="StructedStreaming DataFrame&#x2F;Streaming DataSet的注意事项"></a><strong>StructedStreaming DataFrame&#x2F;Streaming DataSet的注意事项</strong></h2><pre><code class="python">## 1 Event time must be defined on a window or a timestamp 时间时间必须为时间戳

## 2-Sorting is not supported on streaming DataFrames/Datasets, unless it is on aggregated DataFrame/Dataset in Complete output mode;; 排序在流式处理的时候不支持

## 3 输出模式必须是append或update. 在输出模式是complete的时候(必须有聚合), 要求每次输出所有的聚合结果. 我们使用 watermark 的目的是丢弃一些过时聚合数据, 所以complete模式使用wartermark无效也无意义

## 4 在输出模式是append时, 必须设置 watermask 才能使用聚合操作. 其实, watermask 定义了 append 模式中何时输出聚合聚合结果(状态), 并清理过期状态

## 5 在输出模式是update时, watermask 主要用于过滤过期数据并及时清理过期状态.

## 6 watermask 会在处理当前批次数据时更新, 并且会在处理下一个批次数据时生效使用. 但如果节点发送故障, 则可能延迟若干批次生效

## 7 withWatermark 必须使用与聚合操作中的时间戳列是同一列.df.withWatermark(“time”, “1 min”).groupBy(“time2”).count() 无效

## 8 withWatermark 必须在聚合之前调用f.groupBy(“time”).count().withWatermark(“time”, “1 min”) 无效

$$ streaming Datasets/DataFrame不支持一些Datasets/DataFrames操作。其中一些如下。
streaming Datasets尚不支持多个流聚合（即流DF上的聚合链）。
streaming Datasets不支持Limit 和take first N 。
不支持对流数据集的去重操作。
仅在聚合之后且在“complete”下，流数据集才支持排序操作。
有条件地支持流数据集和静态数据集之间的Outer joins。
不支持流数据集的Full outer join
不支持使用右侧流数据集的Left outer join
不支持使用左侧流数据集的Right outer join
尚不支持两个流数据集之间的任何类型的连接。
此外，还有一些数据集方法无法处理流式数据集。它们是将立即运行查询并返回结果的操作，这在流式数据集上没有意义。相反，这些功能可以通过显式启动流式查询来实现
count（）-无法从流数据集返回单个计数。而是使用ds.groupBy（）.count（）返回包含运行计数的流数据集。
foreach（）-而是使用ds.writeStream.foreach（…）
show（）-而是使用console sink（请参阅下一节）。
如果尝试这些操作中的任何一个，您将看到一个AnalysisException，
例如“流数据帧/数据集不支持操作XYZ”。尽管将来的Spark版本可能会支持其中的某些功能，
但从根本上讲，还有一些功能很难有效地在流数据上实现。例如，不支持对输入流进行排序，
因为它需要跟踪流中接收到的所有数据。因此，从根本上讲，这很难有效执行。
</code></pre>
<h2 id="StructedStreaming窗口的划分逻辑"><a href="#StructedStreaming窗口的划分逻辑" class="headerlink" title="StructedStreaming窗口的划分逻辑"></a><strong>StructedStreaming窗口的划分逻辑</strong></h2><pre><code class="scala">maxNumOverlapping &lt;- ceil(windowDuration / slideDuration)
   for (i &lt;- 0 until maxNumOverlapping)
    windowId &lt;- ceil((timestamp - startTime) / slideDuration)
       windowStart &lt;- windowId * slideDuration + (i - maxNumOverlapping) * slideDuration + startTime
    windowEnd &lt;- windowStart + windowDuration
        return windowStart, windowEnd

计算逻辑：
输入数据：2019-08-14 10:55:00_dog,hello,word
 window($&quot;tm&quot;, &quot;4 minutes&quot;, &quot;2 minutes&quot;), //设置的窗口参数
 startTime 没传默认是0
 windowDuration =4  slideDuration=2
 
 maxNumOverlapping =2  //计算出最大的窗口数为 2
 windowId &lt;- ceil((timestamp - startTime) / slideDuration)    55/2 向上取整28
   windowStart &lt;- windowId * slideDuration + (i - maxNumOverlapping) * slideDuration + startTime
   56+（0-2）*2 = 52 
  windowStart：2019-08-14 10:52:00 
 windowEnd &lt;- windowStart + windowDuration
windowEnd ：2019-08-14 10:56:00 
 最终计算完的窗口如下：
 [2019-08-14 10:52:00  2019-08-14 10:56:00 ]
 [2019-08-14 10:54:00  2019-08-14 10:58:00 ]
</code></pre>
<h2 id="structured-streaming的WaterMark"><a href="#structured-streaming的WaterMark" class="headerlink" title="structured-streaming的WaterMark"></a><strong>structured-streaming的WaterMark</strong></h2><pre><code class="scala">水印计算公式：
watermark 计算: watermark = MaxEventTime - Threshhod
而且, watermark只能逐渐增加, 不能减少
初始水印值为0

watermark 在用于基于时间的状态聚合操作时, 该时间可以基于窗口, 也可以基于 event-timeb本身.
输出模式必须是append或update. 在输出模式是complete的时候(必须有聚合), 要求每次输出所有的聚合结果. 我们使用 watermark 的目的是丢弃一些过时聚合数据, 所以complete模式使用wartermark无效也无意义.
在输出模式是append时, 必须设置 watermark 才能使用聚合操作. 其实, watermark 定义了 append 模式中何时输出聚合聚合结果(状态), 并清理过期状态.
在输出模式是update时, watermark 主要用于过滤过期数据并及时清理过期状态.
watermark 会在处理当前批次数据时更新, 并且会在处理下一个批次数据时生效使用. 但如果节点发送故障, 则可能延迟若干批次生效.
withWatermark 必须使用与聚合操作中的时间戳列是同一列.df.withWatermark(“time”, “1 min”).groupBy(“time2”).count() 无效
withWatermark 必须在聚合之前调用. f.groupBy(“time”).count().withWatermark(“time”, “1 min”) 无效
</code></pre>
<h2 id="Zookeeper与redis分布式锁的实现"><a href="#Zookeeper与redis分布式锁的实现" class="headerlink" title="Zookeeper与redis分布式锁的实现"></a><strong>Zookeeper与redis分布式锁的实现</strong></h2><p><strong>zk的分布式锁：</strong></p>
<p>其实可以做的比较简单，就是某个节点尝试创建临时znode，此时创建成功了就获取了这个锁；这个时候别的客户端来创建锁会失败，只能注册个监听器监听这个锁。释放锁就是删除这个znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新枷锁。</p>
<pre><code class="java">import org.apache.zookeeper.*;
import org.apache.zookeeper.data.Stat;
import org.springframework.beans.factory.annotation.Autowired;
 
import java.io.IOException;
import java.util.Collections;
import java.util.List;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;
 
public class ZooKeeperDistributedLock implements Watcher &#123;
 
    private ZooKeeper zk;
    private String locksRoot= &quot;/locks&quot;;
    private String productId;
    private String waitNode;
    private String lockNode;
    private CountDownLatch latch;
    private CountDownLatch connectedLatch = new CountDownLatch(1);
    private int sessionTimeout = 30000;
 
    public ZooKeeperDistributedLock(String productId)&#123;
        this.productId = productId;
        try &#123;
            String address = &quot;192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181&quot;;
            zk = new ZooKeeper(address, sessionTimeout, this);
            connectedLatch.await();
        &#125; catch (IOException e) &#123;
            throw new LockException(e);
        &#125; catch (InterruptedException e) &#123;
            throw new LockException(e);
        &#125;
    &#125;
 
    @Autowired
    public void process(WatchedEvent event) &#123;
        if(event.getState()== Event.KeeperState.SyncConnected)&#123;
            connectedLatch.countDown();
            return;
        &#125;
 
        if(this.latch != null) &#123;
            this.latch.countDown();
        &#125;
    &#125;
 
    public void acquireDistributedLock() &#123;
        try &#123;
            if(this.tryLock())&#123;
                return;
            &#125;
            else&#123;
                waitForLock(waitNode, sessionTimeout);
            &#125;
        &#125; catch (KeeperException e) &#123;
            throw new LockException(e);
        &#125; catch (InterruptedException e) &#123;
            throw new LockException(e);
        &#125;
    &#125;
 
    public boolean tryLock() &#123;
        try &#123;
            // 传入进去的locksRoot + “/” + productId
            // 假设productId代表了一个商品id，比如说1
            // locksRoot = locks
            // /locks/10000000000，/locks/10000000001，/locks/10000000002
            // EPHEMERAL_SEQUENTIAL 临时顺序编号目录节点
            lockNode = zk.create(locksRoot + &quot;/&quot; + productId, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);
 
            // 看看刚创建的节点是不是最小的节点
            // locks：10000000000，10000000001，10000000002
            List&lt;String&gt; locks = zk.getChildren(locksRoot, false);
            Collections.sort(locks);
 
            if(lockNode.equals(locksRoot+&quot;/&quot;+ locks.get(0)))&#123;
                //如果是最小的节点,则表示取得锁
                return true;
            &#125;
 
            //如果不是最小的节点，找到比自己小1的节点
            int previousLockIndex = -1;
            for(int i = 0; i &lt; locks.size(); i++) &#123;
                if(lockNode.equals(locksRoot + &quot;/&quot; + locks.get(i))) &#123;
                    previousLockIndex = i - 1;
                    break;
                &#125;
            &#125;
 
            this.waitNode = locks.get(previousLockIndex);
        &#125; catch (KeeperException e) &#123;
            throw new LockException(e);
        &#125; catch (InterruptedException e) &#123;
            throw new LockException(e);
        &#125;
        return false;
    &#125;
 
    private boolean waitForLock(String waitNode, long waitTime) throws InterruptedException, KeeperException &#123;
        Stat stat = zk.exists(locksRoot + &quot;/&quot; + waitNode, true);
        if(stat != null)&#123;
            this.latch = new CountDownLatch(1);
            this.latch.await(waitTime, TimeUnit.MILLISECONDS);                 this.latch = null;
        &#125;
        return true;
    &#125;
 
    public void unlock() &#123;
        try &#123;
            // 删除/locks/10000000000节点
            // 删除/locks/10000000001节点
            System.out.println(&quot;unlock &quot; + lockNode);
            zk.delete(lockNode,-1);
            lockNode = null;
            zk.close();
        &#125; catch (InterruptedException e) &#123;
            e.printStackTrace();
        &#125; catch (KeeperException e) &#123;
            e.printStackTrace();
        &#125;
    &#125;
 
 
    public class LockException extends RuntimeException &#123;
        private static final long serialVersionUID = 1L;
        public LockException(String e)&#123;
            super(e);
        &#125;
        public LockException(Exception e)&#123;
            super(e);
        &#125;
    &#125;
// 如果有一把锁，被多个人给竞争，此时多个人会排队，第一个拿到锁的人会执行，然后释放锁，后面的每个人都会去监听排在自己前面的那个人创建的node上，一旦某个人释放了锁，排在自己后面的人就会被zookeeper给通知，一旦被通知了之后，就ok了，自己就获取到了锁，就可以执行代码了
</code></pre>
<p><strong>Redlock算法</strong></p>
<p>在算法的分布式版本中，我们假设我们有N个Redis母版。这些节点是完全独立的，因此我们不使用复制或任何其他隐式协调系统。我们已经描述了如何在单个实例中安全地获取和释放锁。我们认为该算法将使用此方法在单个实例中获取和释放锁，这是理所当然的。在我们的示例中，我们将N &#x3D; 5设置为一个合理的值，因此我们需要在不同的计算机或虚拟机上运行5个Redis主服务器，以确保它们将以大多数独立的方式发生故障。</p>
<p>为了获取锁，客户端执行以下操作：</p>
<ol>
<li><p>它以毫秒为单位获取当前时间。</p>
</li>
<li><p>它尝试在所有N个实例中顺序使用所有实例中相同的键名和随机值来获取锁定。在第2步中，在每个实例中设置锁时，客户端使用的超时时间小于总锁自动释放时间，以便获取该超时时间。例如，如果自动释放时间为10秒，则超时时间可能在5到50毫秒之间。这样可以防止客户端长时间与处于故障状态的Redis节点进行通信：如果某个实例不可用，我们应该尝试与下一个实例尽快进行通信。</p>
</li>
<li><p>客户端通过从当前时间中减去在步骤1中获得的时间戳，来计算获取锁所花费的时间。当且仅当客户端能够在大多数实例（至少3个）中获取锁时， ，并且获取锁所花费的总时间小于锁有效时间，则认为已获取锁。</p>
</li>
<li><p>如果获取了锁，则将其有效时间视为初始有效时间减去经过的时间，如步骤3中所计算。</p>
</li>
<li><p>如果客户端由于某种原因（无法锁定N &#x2F; 2 + 1实例或有效时间为负数）而未能获得该锁，它将尝试解锁所有实例（即使它认为不是该实例）能够锁定）。</p>
<pre><code class="java">@Component
public class RedisLockHelper &#123;
     private long sleepTime = 100;
    // 1. 配置文件
    Config config = new Config();
    config.useSingleServer()
            .setAddress(&quot;redis://127.0.0.1:6379&quot;)
            .setPassword(RedisConfig.PASSWORD)
            .setDatabase(0);
    //2. 构造RedissonClient
    RedissonClient redissonClient = Redisson.create(config);

    //3. 设置锁定资源名称
    RLock lock = redissonClient.getLock(&quot;redlock&quot;);
    lock.lock();
    try &#123;
        System.out.println(&quot;获取锁成功，实现业务逻辑&quot;);
        Thread.sleep(10000);
    &#125; catch (InterruptedException e) &#123;
        e.printStackTrace();
    &#125; finally &#123;
        lock.unlock();
    &#125;

    /**
     * 直接使用setnx + expire方式获取分布式锁
     * 非原子性
     *
     * @param key
     * @param value
     * @param timeout
     * @return
     */
    public boolean lock_setnx(Jedis jedis,String key, String value, int timeout) &#123;
        Long result = jedis.setnx(key, value);
        // result = 1时，设置成功，否则设置失败
        if (result == 1L) &#123;
            return jedis.expire(key, timeout) == 1L;
        &#125; else &#123;
            return false;
        &#125;
    &#125;

    /**
     * 使用Lua脚本，脚本中使用setnex+expire命令进行加锁操作
     *
     * @param jedis
     * @param key
     * @param UniqueId
     * @param seconds
     * @return
     */
    public boolean Lock_with_lua(Jedis jedis,String key, String UniqueId, int seconds) &#123;
        String lua_scripts = &quot;if redis.call(&#39;setnx&#39;,KEYS[1],ARGV[1]) == 1 then&quot; +
                &quot;redis.call(&#39;expire&#39;,KEYS[1],ARGV[2]) return 1 else return 0 end&quot;;
        List&lt;String&gt; keys = new ArrayList&lt;&gt;();
        List&lt;String&gt; values = new ArrayList&lt;&gt;();
        keys.add(key);
        values.add(UniqueId);
        values.add(String.valueOf(seconds));
        Object result = jedis.eval(lua_scripts, keys, values);
        //判断是否成功
        return result.equals(1L);
    &#125;

    /**
     * 在Redis的2.6.12及以后中,使用 set key value [NX] [EX] 命令
     *
     * @param key
     * @param value
     * @param timeout
     * @return
     */
    public boolean lock(Jedis jedis,String key, String value, int timeout, TimeUnit timeUnit) &#123;
        long seconds = timeUnit.toSeconds(timeout);
        return &quot;OK&quot;.equals(jedis.set(key, value, &quot;NX&quot;, &quot;EX&quot;, seconds));
    &#125;

    /**
     * 自定义获取锁的超时时间
     *
     * @param jedis
     * @param key
     * @param value
     * @param timeout
     * @param waitTime
     * @param timeUnit
     * @return
     * @throws InterruptedException
     */
    public boolean lock_with_waitTime(Jedis jedis,String key, String value, int timeout, long waitTime,TimeUnit timeUnit) throws InterruptedException &#123;
        long seconds = timeUnit.toSeconds(timeout);
        while (waitTime &gt;= 0) &#123;
            String result = jedis.set(key, value, &quot;nx&quot;, &quot;ex&quot;, seconds);
            if (&quot;OK&quot;.equals(result)) &#123;
                return true;
            &#125;
            waitTime -= sleepTime;
            Thread.sleep(sleepTime);
        &#125;
        return false;
    &#125;
    /**
     * 错误的解锁方法—直接删除key
     *
     * @param key
     */
    public void unlock_with_del(Jedis jedis,String key) &#123;
        jedis.del(key);
    &#125;

    /**
     * 使用Lua脚本进行解锁操纵，解锁的时候验证value值
     *
     * @param jedis
     * @param key
     * @param value
     * @return
     */
    public boolean unlock(Jedis jedis,String key,String value) &#123;
        String luaScript = &quot;if redis.call(&#39;get&#39;,KEYS[1]) == ARGV[1] then &quot; +
                &quot;return redis.call(&#39;del&#39;,KEYS[1]) else return 0 end&quot;;
        return jedis.eval(luaScript, Collections.singletonList(key), Collections.singletonList(value)).equals(1L);
    &#125;
&#125;
总结：
    1： 直接使用setnx + expire方式获取分布式锁
    SET my:lock 随机值 NX PX 30000，这个命令就ok，这个的NX的意思就是只有key不存在的时候才会设置成功，PX 30000的意思是30秒后锁自动释放。别人创建的时候如果发现已经有了就不能加锁了。
        
    2： 使用Lua脚本，脚本中使用setnex+expire命令进行加锁操作
        if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then
            return redis.call(&quot;del&quot;,KEYS[1])
        else
            return 0
        end
        
    3：使用 set key value [NX] [EX] 命令,加锁
     
    4:使用Lua脚本进行解锁操作，解锁的时候验证value值（释放锁）
    
</code></pre>
<h2 id="kafka分段日志"><a href="#kafka分段日志" class="headerlink" title="kafka分段日志"></a><strong>kafka分段日志</strong></h2><p><strong>Kafka的存储结构</strong></p>
<p>总所周知，Kafka的Topic可以有多个分区，分区其实就是最小的读取和存储结构，即Consumer看似订阅的是Topic，实则是从Topic下的某个分区获得消息，Producer也是发送消息也是如此。</p>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaDFQc2V4RWFpYjU5Z0I0NHNjdVN6Q1dZTFg3VndoS3VKMEk4aFdoZlBXdDI0aDdZRXdrdTROTEEvNjQw?x-oss-process=image/format,png" alt="img" style="zoom:50%;">

<p>上图是总体逻辑上的关系，映射到实际代码中在磁盘上的关系则是如下图所示：</p>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaHFpYnNVMUxkOWZ2aG1pYVZhVXdLTnBzcGRZNlZ5V0FEUzZXSFRwQzU1UG9kampEeXJwRm9jcWljUS82NDA?x-oss-process=image/format,png" alt="img" style="zoom:50%;">

<p>每个分区对应一个Log对象，在磁盘中就是一个子目录，子目录下面会有多组日志段即多Log Segment，每组日志段包含：消息日志文件(以log结尾)、位移索引文件(以index结尾)、时间戳索引文件(以timeindex结尾)。其实还有其它后缀的文件，例如.txnindex、.deleted等等。篇幅有限，暂不提起<br><strong>以下为日志的定义</strong></p>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaEFYWGlhaWNnRjI2Wk1OT2IzRFRTQ2ljQ3dqSkR5bE1VemwyWmhyd2x5azR0OXpHdjdpYmd1aWJUb3NRLzY0MA?x-oss-process=image/format,png" alt="img" style="zoom:80%;">

<h3 id="以下为日志段的定义"><a href="#以下为日志段的定义" class="headerlink" title="以下为日志段的定义"></a><strong>以下为日志段的定义</strong></h3><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaHJCOEtKcXVobERtcmlhZ3JmY2NoVlBGajRIQm4yczBZZ1VJYVpGaWFOTlRwekZnUkFpYlBCN21XUS82NDA?x-oss-process=image/format,png" alt="img"></p>
</li>
</ol>
<p>***<u>indexIntervalBytes</u><em><strong>可以理解为插了多少消息之后再建一个索引，由此可以看出Kafka的索引其实是</strong></em><u>稀疏索引</u><em><strong>，这样可以</strong></em><u>避免索引文件占用过多的内存，从而可以在内存中保存更多的索引</u>***。对应的就是Broker 端参数log.index.interval.bytes 值，默认4KB。</p>
<p>实际的通过***<u>索引查找</u><em><strong>消息过程是先</strong></em><u>通过offset找到索引所在的文件，然后通过二分法找到离目标最近的索引，再顺序遍历消息文件找到目标文件</u>***。这波操作时间复杂度为O(log2n)+O(m),n是索引文件里索引的个数，m为稀疏程度。</p>
<p>这就***<u>是空间和时间的互换，又经过数据结构与算法的平衡</u>***，妙啊！</p>
<p>再说下***<u>rollJitterMs,</u><em><strong>这其实是个扰动值，对应的参数是log.roll.jitter.ms,这其实就要说到日志段的切分了，<u><em><strong>log.segment.bytes,这个参数控制着日志段文件的大小，默认是1G</strong></em></u>，即当文件存储超过1G之后就新起一个文件写入。这是以</strong></em><u>大小为维度</u><em><strong>的，还有一个参数是</strong></em><u>log.segment.ms,以时间为维度切分</u>***。</p>
<p>那配置了这个参数之后如果有很多很多分区，然后因为这个参数是全局的，<u><em><strong>因此同一时刻需要做很多文件的切分，这磁盘IO就顶不住了啊，因此需要设置个rollJitterMs</strong></em></u>，来岔开它们。</p>
<p><em><strong><u>怎么样有没有联想到redis缓存的过期时间？过期时间加个随机数，防止同一时刻大量缓存过期导致缓存击穿数据库。看看知识都是通的啊！</u></strong></em></p>
<p><em><strong>*日志段的写入*</strong></em></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaHg1bmhIemRKWGJHYkh0RUpGUVdoemNoaWNabThZdW85NXk1enQ1WHBJZ3hGa2tXcHRhdVozb1EvNjQw?x-oss-process=image/format,png" alt="img"></p>
<pre><code class="python">## 1、判断下当前日志段是否为空，空的话记录下时间，来作为之后日志段的切分依据
## 2、确保位移值合法，最终调用的是AbstractIndex.toRelative(..)方法，即使判断offset是否小于0，是否大于int最大值。
## 3、append消息，实际上就是通过FileChannel将消息写入，当然只是写入内存中及页缓存，是否刷盘看配置。
## 4、更新日志段最大时间戳和最大时间戳对应的位移值。这个时间戳其实用来作为定期删除日志的依据
## 5、更新索引项，如果需要的话(bytesSinceLastIndexEntry &gt; indexIntervalBytes)
</code></pre>
<p>最后再来个流程图</p>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaFF2VmswaG9qRUMyamswS2UxODlzZjhVYk5ZZDJCVFNpYzhYWGcydXgyYVY0cldUc0FQSE5pYnNBLzY0MA?x-oss-process=image/format,png" alt="img" style="zoom: 50%;">

<p><em><strong>*日志段的读取*</strong></em></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaEtlMWJ3R2M0ZEpuVXhzOWZTaWI0cjduRVJ3WnVHQmZQY2hxSWd3M2dFdTFRZlFjMmlhV2d3SWljZy82NDA?x-oss-process=image/format,png" alt="img"></p>
<pre><code class="python">## 1、根据第一条消息的offset，通过OffsetIndex找到对应的消息所在的物理位置和大小。
## 2、获取LogOffsetMetadata,元数据包含消息的offset、消息所在segment的起始offset和物理位置
## 3、判断minOneMessage是否为true,若是则调整为必定返回一条消息大小，其实就是在单条消息大于maxSize的情况下得以返回，防止消费者饿死
## 4、再计算最大的fetchSize,即（最大物理位移-此消息起始物理位移）和adjustedMaxSize的最小值(这波我不是很懂，因为以上一波操作adjustedMaxSize已经最小为一条消息的大小了)
## 5、调用 FileRecords 的 slice 方法从指定位置读取指定大小的消息集合，并且构造FetchDataInfo返回
</code></pre>
<p>再来个流程图：</p>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy82ZnVUM2VtV0k1SU1WMmV5b09zQXc3cG1xU203VE9yaEsydHZiT0FxemNIbnJ3NkpvWU16aWN3aDhyQkYzYUpFeTN2WkhBbXdOaWE4dzhGREdCQlA4MjZBLzY0MA?x-oss-process=image/format,png" alt="img" style="zoom:50%;">

<h2 id="面试题：如何保证kafka消息的顺序性"><a href="#面试题：如何保证kafka消息的顺序性" class="headerlink" title="面试题：如何保证kafka消息的顺序性"></a><strong>面试题：如何保证kafka消息的顺序性</strong></h2><pre><code class="python">## 1：一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。

## 2: 每个partition对应一个消费者消费;每个消费者内写 N 个内存 queue，具有相同 key 的数据经过hash都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。
</code></pre>
<img src="https://img-blog.csdnimg.cn/20190519230306612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE0Mzk4Mzk=,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;">

<h2 id="kafka日志留存策略"><a href="#kafka日志留存策略" class="headerlink" title="kafka日志留存策略"></a><strong>kafka日志留存策略</strong></h2><pre><code class="python">## 留存策略类型

与日志留存方式相关的策略类型主要有两种：delete和compact。这两种留存方式的机制完全不同.
delete类型的留存策略
    用户可以通过设置broker端参数log.cleanup.policy来指定集群上所有topic默认的策略类型。另外也可以通过topic级别参数cleanup.policy来为某些topic设置不同于默认值的策略类型。当前log.cleanup.policy参数的默认值是[delete,compact]，这是一个list类型的参数，表示集群上所有topic会同时开启delete和compact两种留存策略——这是0.10.1.0新引入的功能，在0.10.1.0之前，该参数只能两选一，不能同时兼顾，但在实际使用中很多用户都抱怨compact类型的topic存在过期key消息未删除的情况，故社区修改了该参数配置，允许一个topic同时开启两种留存策略。
    
## Kafka是如何处理日志留存的

每个Kafka broker启动时，都会在后台开启一个定时任务，定期地去检查并执行所有topic日志留存，这个定时任务触发的时间周期由broker端参数log.retention.check.interval.ms控制，默认是5分钟，即每台broker每5分钟都会尝试去检查一下是否有可以删除的日志。因此如果你要缩短这个间隔，只需要调小log.retention.check.interval.ms即可。

鉴于日志留存和日志删除实际上是一个问题的两个方面，因而我们下面讨论的是关于Kafka根据什么规则来删除日志。但有一点要强调一下，待删除的标的是日志段，即LogSegment，也就是以.log结尾的一个个文件，而非整个文件夹。另外还有一点也很重要，当前日志段（active logsegment）是永远不会被删除的，不管用户配置了哪种留存机制。
</code></pre>
<p><strong>当前留存机制共有3种：</strong></p>
<ol>
<li><p><em><strong><u>基于空间维度</u></strong></em></p>
<p>也称size-based retention，指的是Kafka定期为那些超过磁盘空间阈值的topic进行日志段的删除。这个阈值由broker端参数<u><strong>log.retention.bytes</strong></u>和topic级别参数**<u>retention.bytes</u>**控制，默认是-1，表示Kafka当前未开启这个留存机制，即不管topic日志量涨到多少，Kafka都不视其为“超过阈值”。如果用户要开启这种留存机制，必须显式设置log.retention.bytes（或retention.bytes）。 </p>
<p>一旦用户设置了阈值，那么Kafka就会在定时任务中尝试比较当前日志量总大小是否超过阈值至少一个日志段的大小。这里所说的总大小是指所有日志段文件的大小，不包括索引文件的大小！如果是则会尝试从最老的日志段文件开始删起。注意这里的“**<u><em>超过阈值至少一个日志段的大小</em></u>**”，这就是说超过阈值的部分必须要大于一个日志段的大小，否则不会进行删除的，原因就是因为删除的标的是日志段文件——即文件只能被当做一个整体进行删除，无法删除部分内容。</p>
<p>举个例子来说明，假设日志段大小是700MB，当前分区共有4个日志段文件，大小分别是700MB，700MB，700MB和1234B——显然1234B那个文件就是active日志段。此时该分区总的日志大小是3*700MB+1234B&#x3D;2100MB+1234B，如果阈值设置为2000MB，那么超出阈值的部分就是100MB+1234B，小于日志段大小700MB，故Kafka不会执行任何删除操作，即使总大小已经超过了阈值；反之如果阈值设置为1400MB，那么超过阈值的部分就是700MB+1234B &gt; 700MB，此时Kafka会删除最老的那个日志段文件。</p>
</li>
<li><p><em><strong><u>基于时间维度</u></strong></em></p>
<p>也称time-based retention，指的是Kafka定期未那些超过时间阈值的topic进行日志段删除操作。这个阈值由broker端参数<u><strong>log.retention.ms</strong></u>、**<u>log.retention.mintues</u><strong>、</strong><u>log.retention.hours</u><strong>以及topic级别参数</strong><u>retention.ms</u>**控制。如果同时设置了log.retention.ms、log.retention.mintues、log.retention.hours，以log.retention.ms优先级为最高，log.retention.mintues次之，log.retention.hours最次。当前这三个参数的默认值依次是null, null和168，<u><strong>故Kafka为每个topic默认保存7天的日志。</strong></u></p>
<p>这里需要讨论下这“7天”是如何界定的？在0.10.0.0之前，Kafka每次检查时都会将当前时间与每个日志段文件的最新修改时间做比较，如果两者的差值超过了上面设定的阈值（比如上面说的7天），那么Kafka就会尝试删除该文件。不过这种界定方法是有问题的，因为文件的最新修改时间是可变动的——比如用户在终端通过touch命令查看该日志段文件或Kafka对该文件切分时都可能导致最新修改时间的变化从而扰乱了该规则的判定，因此自0.10.0.0版本起，**<u><em>Kafka在消息体中引入了时间戳字段(当然不是单纯为了修复这个问题)，并且为每个日志段文件都维护一个最大时间戳字段。通过将当前时间与该最大时间戳字段进行比较来判定是否过期。使用当前最大时间戳字段的好处在于它对用户是透明的，用户在外部无法直接修改它，故不会造成判定上的混乱。</em></u>**</p>
<p>最大时间戳字段的更新机制也很简单，每次日志段写入新的消息时，都会尝试更新该字段。因为消息时间戳通常是递增的，故每次写入操作时都会保证最大时间戳字段是会被更新的，而一旦一个日志段写满了被切分之后它就不再接收任何新的消息，其最大时间戳字段的值也将保持不变。倘若该值距离当前时间超过了设定的阈值，那么该日志段文件就会被删除。</p>
</li>
<li><p><em><strong><u>基于起始位移维度</u></strong></em></p>
<pre><code class="python">## 基于日志起始位移（log start offset)。这实际上是0.11.0.0版本新增加的功能。其实增加这个功能的初衷主要是为了Kafka流处理应用——在流处理应用中存在着大量的中间消息，这些消息可能已经被处理过了，但依然保存在topic日志中，占用了大量的磁盘空间。如果通过设置基于时间维度的机制来删除这些消息就需要用户设置很小的时间阈值，这可能导致这些消息尚未被下游操作算子（operator）处理就被删除；如果设置得过大，则极大地增加了空间占用。故社区在0.11.0.0引入了第三种留存机制：基于起始位移


## 所谓起始位移，就是指分区日志的当前起始位移——注意它是分区级别的值，而非日志段级别。故每个分区都只维护一个起始位移值。该值在初始化时被设置为最老日志段文件的基础位移(base offset)，随着日志段的不断删除，该值会被更新当前最老日志段的基础位移。另外Kafka提供提供了一个脚本命令帮助用户设置指定分区的起始位移：kafka-delete-records.sh。

 
该留存机制是默认开启的，不需要用户任何配置。Kafka会为每个日志段做这样的检查：1. 获取日志段A的下一个日志段B的基础位移；2. 如果该值小于分区当前起始位移则删除此日志段A。

依然拿例子还说明，假设我有一个topic，名字是test，该topic只有1个分区，该分区下有5个日志段文件，分别是A1.log, A2.log, A3.log, A4.log和A5.log，其中A5.log是active日志段。这5个日志段文件中消息范围分别是0~9999,10000~19999,20000~29999,30000~39999和40000~43210（A5未写满）。如果此时我确信前3个日志段文件中的消息已经被处理过了，于是想删除这3个日志段，此时我应该怎么做呢？由于我无法预知这些日志段文件产生的速度以及被消费的速度，因此不管是基于时间的删除机制还是基于空间的删除机制都是不适用的。此时我便可以使用kafka-delete-records.sh脚本将该分区的起始位移设置为A4.log的起始位移，即40000。为了做到这点，我需要首先创建一个JSON文件a.json，内容如下：

&#123;&quot;partitions&quot;:[&#123;&quot;topic&quot;: &quot;test&quot;, &quot;partition&quot;: 0,&quot;offset&quot;: 40000&#125;],&quot;version&quot;:1&#125;

然后执行下列命令：

bin/kafka-delete-records.sh --bootstrap-server localhost:9092 --offset-json-file a.json 

如果一切正常，应该可以看到类似于这样的输出：
Executing records delete operation
Records delete operation completed
partition: test-0 low_watermark: 40000
</code></pre>
<h2 id="kafka最佳实践"><a href="#kafka最佳实践" class="headerlink" title="kafka最佳实践"></a><strong>kafka最佳实践</strong></h2><h2 id="通过Spark生成HFile，并以BulkLoad方式将数据导入到HBase"><a href="#通过Spark生成HFile，并以BulkLoad方式将数据导入到HBase" class="headerlink" title="通过Spark生成HFile，并以BulkLoad方式将数据导入到HBase"></a>通过Spark生成HFile，并以BulkLoad方式将数据导入到HBase</h2><p>在实际生产环境中，将计算和存储进行分离，是我们提高集群吞吐量、确保集群规模水平可扩展的主要方法之一，并且通过集群的扩容、性能的优化，确保在数据大幅增长时，存储不能称为系统的瓶颈。</p>
<p>具体到我们实际的项目需求中，有一个典型的场景，通常会将Hive中的部分数据，比如热数据，存入到HBase中，进行冷热分离处理。</p>
<p>我们采用Spark读取Hive表数据存入HBase中，这里主要有两种方式：</p>
<ol>
<li>通过HBase的put API进行数据的批量写入</li>
<li>通过生成HFile文件，然后通过BulkLoad方式将数据存入HBase</li>
</ol>
<p>HBase的原生put方式，通过HBase集群的region server向HBase插入数据，但是当数据量非常大时，region会进行split、compact等处理，并且这些处理非常占用计算资源和IO开销，影响性能和集群的稳定性。</p>
<p>HBase的数据最终是以HFile的形式存储到HDFS上的，如果我们能直接将数据生成为HFile文件，然后将HFile文件保存到HBase对应的表中，可以避免上述的很多问题，效率会相对更高。</p>
<p>本篇文章主要介绍如何使用Spark生成HFile文件，然后通过BulkLoad方式将数据导入到HBase中，并附批量put数据到HBase以及直接存入数据到HBase中的实际应用示例。</p>
<p><strong>1. 生成HFile，BulkLoad导入</strong></p>
<pre><code class="scala">## 1.1 数据样例
&#123;&quot;id&quot;:&quot;1&quot;,&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:&quot;18&quot;&#125;
&#123;&quot;id&quot;:&quot;2&quot;,&quot;name&quot;:&quot;mike&quot;,&quot;age&quot;:&quot;19&quot;&#125;
&#123;&quot;id&quot;:&quot;3&quot;,&quot;name&quot;:&quot;kilos&quot;,&quot;age&quot;:&quot;20&quot;&#125;
&#123;&quot;id&quot;:&quot;4&quot;,&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:&quot;21&quot;&#125;
...
## 1.2 示例代码
/**
  * @Author bigdatalearnshare
  */
object App &#123;

  def main(args: Array[String]): Unit = &#123;
    System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;)

    val sparkSession = SparkSession
      .builder()
      .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)
      .master(&quot;local[*]&quot;)
      .getOrCreate()
    
    val rowKeyField = &quot;id&quot;
    
    val df = sparkSession.read.format(&quot;json&quot;).load(&quot;/people.json&quot;)

    val fields = df.columns.filterNot(_ == &quot;id&quot;).sorted

    val data = df.rdd.map &#123; row =&gt;
      val rowKey = Bytes.toBytes(row.getAs(rowKeyField).toString)

      val kvs = fields.map &#123; field =&gt;
        new KeyValue(rowKey, Bytes.toBytes(&quot;hfile-fy&quot;), Bytes.toBytes(field), Bytes.toBytes(row.getAs(field).toString))
      &#125;

      (new ImmutableBytesWritable(rowKey), kvs)
    &#125;.flatMapValues(x =&gt; x).sortByKey()
    
    val hbaseConf = HBaseConfiguration.create(sparkSession.sessionState.newHadoopConf())
    hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;linux-1:2181,linux-2:2181,linux-3:2181&quot;)
    hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, &quot;hfile&quot;)
    val connection = ConnectionFactory.createConnection(hbaseConf)

    val tableName = TableName.valueOf(&quot;hfile&quot;)

    //没有HBase表则创建
    creteHTable(tableName, connection)

    val table = connection.getTable(tableName)

    try &#123;
      val regionLocator = connection.getRegionLocator(tableName)

      val job = Job.getInstance(hbaseConf)

      job.setMapOutputKeyClass(classOf[ImmutableBytesWritable])
      job.setMapOutputValueClass(classOf[KeyValue])

      HFileOutputFormat2.configureIncrementalLoad(job, table, regionLocator)

      val savePath = &quot;hdfs://linux-1:9000/hfile_save&quot;
      delHdfsPath(savePath, sparkSession)

      job.getConfiguration.set(&quot;mapred.output.dir&quot;, savePath)

      data.saveAsNewAPIHadoopDataset(job.getConfiguration)

      val bulkLoader = new LoadIncrementalHFiles(hbaseConf)
      bulkLoader.doBulkLoad(new Path(savePath), connection.getAdmin, table, regionLocator)

    &#125; finally &#123;
      //WARN LoadIncrementalHFiles: Skipping non-directory hdfs://linux-1:9000/hfile_save/_SUCCESS 不影响,直接把文件移到HBASE对应HDFS地址了
      table.close()
      connection.close()
    &#125;

    sparkSession.stop()
  &#125;

  def creteHTable(tableName: TableName, connection: Connection): Unit = &#123;
    val admin = connection.getAdmin

    if (!admin.tableExists(tableName)) &#123;
      val tableDescriptor = new HTableDescriptor(tableName)
      tableDescriptor.addFamily(new HColumnDescriptor(Bytes.toBytes(&quot;hfile-fy&quot;)))
      admin.createTable(tableDescriptor)
    &#125;
  &#125;

  def delHdfsPath(path: String, sparkSession: SparkSession) &#123;
    val hdfs = FileSystem.get(sparkSession.sessionState.newHadoopConf())
    val hdfsPath = new Path(path)

    if (hdfs.exists(hdfsPath)) &#123;
      //val filePermission = new FsPermission(FsAction.ALL, FsAction.ALL, FsAction.READ)
      hdfs.delete(hdfsPath, true)
    &#125;
  &#125;
&#125;
## 1.3 注意事项
上述示例代码可以根据实际业务需求作相应调整，但有一个问题需要特别注意：
通过Spark读取过来的数据生成HFile时，要确保HBase的主键、列族、列按照有序排列。否则，会抛出以下异常:
Caused by: java.io.IOException: Added a key not lexically larger than previous. Current cell = 1/hfile-fy:age/1588230543677/Put/vlen=2/seqid=0, lastCell = 1/hfile-fy:name/1588230543677/Put/vlen=4/seqid=0
</code></pre>
<p><strong>2. 批量put</strong>**</p>
<pre><code class="scala">val rowKeyField = &quot;id&quot;
val df = sparkSession.read.format(&quot;json&quot;).load(&quot;/stats.json&quot;)
val fields = df.columns.filterNot(_ == &quot;id&quot;)

df.rdd.foreachPartition &#123; partition =&gt;
      val hbaseConf = HBaseConfiguration.create()
      hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;linux-1:2181,linux-2:2181,linux-3:2181&quot;)
      hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, &quot;batch_put&quot;)

      val conn = ConnectionFactory.createConnection(hbaseConf)
      val table = conn.getTable(TableName.valueOf(&quot;batch_put&quot;))

      val res = partition.map &#123; row =&gt;
        val rowKey = Bytes.toBytes(row.getAs(rowKeyField).toString)
        val put = new Put(rowKey)
        val family = Bytes.toBytes(&quot;hfile-fy&quot;)

        fields.foreach &#123; field =&gt;
          put.addColumn(family, Bytes.toBytes(field), Bytes.toBytes(row.getAs(field).toString))
        &#125;

        put
      &#125;.toList

      Try(table.put(res)).getOrElse(table.close())

      table.close()
      conn.close()
&#125;
</code></pre>
<p>在实际应用中，我们也可以将经常一起查询的数据拼接在一起存入一个列中，比如将上述的pv和uv拼接在一起使用，可以降低KeyValue带来的结构化开销。</p>
<p><strong>3.saveAsNewAPIHadoopDataset</strong></p>
<pre><code class="scala">val hbaseConf = sparkSession.sessionState.newHadoopConf()
hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;linux-1:2181,linux-2:2181,linux-3:2181&quot;)

hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, &quot;direct&quot;)
val job = Job.getInstance(hbaseConf)
job.setMapOutputKeyClass(classOf[ImmutableBytesWritable])
job.setMapOutputValueClass(classOf[Result])
job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])

val rowKeyField = &quot;id&quot;

val df = sparkSession.read.format(&quot;json&quot;).load(&quot;/stats.json&quot;)

val fields = df.columns.filterNot(_ == &quot;id&quot;)

df.rdd.map &#123; row =&gt;
    val put = new Put(Bytes.toBytes(row.getAs(rowKeyField).toString))

    val family = Bytes.toBytes(&quot;hfile-fy&quot;)

    fields.foreach &#123; field =&gt;
      put.addColumn(family, Bytes.toBytes(field), Bytes.toBytes(row.getAs(field).toString))
    &#125;

    (new ImmutableBytesWritable(), put)
&#125;.saveAsNewAPIHadoopDataset(job.getConfiguration)
</code></pre>
<h2 id="sparkSQL技术指北"><a href="#sparkSQL技术指北" class="headerlink" title="sparkSQL技术指北"></a><strong>sparkSQL技术指北</strong></h2><pre><code class="python">## 1:SparkSQL 中的 hint 
SparkSQL 2.2 增加了 Hint Framework 的支持，允许在查询中加入注释，让查询优化器优化逻辑计划。目前支持的 hint 有三个：COALESCE、REPARTITION、BROADCAST，其中 COALESCE、REPARTITION 这两个是 SparkSQL 2.4 开始支持。
SELECT /*+ COALESCE(2) */ ...
SELECT /*+ REPARTITION(10) */ ...
# 这两个 hint 是从 SparkSQL 2.4 开始支持

SELECT /*+ MAPJOIN(a) */ ...
SELECT /*+ BROADCASTJOIN(a) */ ...
SELECT /*+ BROADCAST(a) */ ...
# 该 hint 是从 SparkSQL 2.2 开始支持
</code></pre>
<pre><code class="python">## SparkSQL 性能调优参数

# 1，spark.hadoopRDD.ignoreEmptySplits
默认是false，如果是true，则会忽略那些空的splits，减小task的数量。

# 2，spark.hadoop.mapreduce.input.fileinputformat.split.minsize
是用于聚合input的小文件，用于控制每个mapTask的输入文件，防止小文件过多时候，产生太多的task。

# 3，spark.sql.autoBroadcastJoinThreshold &amp;&amp; spark.sql.broadcastTimeout
用于控制在 spark sql 中使用 BroadcastJoin 时候表的大小阈值，适当增大可以让一些表走 BroadcastJoin，提升性能，但是如果设置太大又会造成 driver 内存压力，而 broadcastTimeout 是用于控制 Broadcast 的 Future 的超时时间，默认是 300s，可根据需求进行调整。

# 4，spark.sql.adaptive.enabled &amp;&amp; spark.sql.adaptive.shuffle.targetPostShuffleInputSize
该参数是用于开启 spark 的自适应执行，后面的 targetPostShuffleInputSize 是用于控制之后的 shuffle 阶段的平均输入数据大小，防止产生过多的task。

# 5，spark.sql.parquet.mergeSchema
默认 false。当设为 true，parquet 会聚合所有 parquet 文件的 schema，否则是直接读取 parquet summary 文件，或者在没有 parquet summary 文件时候随机选择一个文件的 schema 作为最终的 schema。

# 6，spark.sql.files.opencostInBytes
该参数默认 4M，表示小于 4M 的小文件会合并到一个分区中，用于减小小文件，防止太多单个小文件占一个分区情况。

# 7，spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version
1 或者 2，默认是 1. MapReduce-4815 详细介绍了 fileoutputcommitter 的原理，实践中设置了 version=2 的比默认 version=1 的减少了70%以上的 commit 时间，但是1更健壮，能处理一些情况下的异常。
</code></pre>
<h2 id="Spark内存管理详解"><a href="#Spark内存管理详解" class="headerlink" title="Spark内存管理详解"></a><strong>Spark内存管理详解</strong></h2><p>我们知道在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p>
<p>另外，Spark 1.6 之前使用的是静态内存管理 (StaticMemoryManager) 机制，</p>
<p>StaticMemoryManager 也是 Spark 1.6 之前唯一的内存管理器。在 Spark1.6 之后引入了统一内存管理</p>
<p>(UnifiedMemoryManager) 机制，UnifiedMemoryManager 是 Spark 1.6 之后默认的内存管理器，1.6 之前采用的静态管理（StaticMemoryManager）方式仍被保留，可通过配置 spark.memory.useLegacyMode 参数启用。</p>
<p>这里仅对<u><em><strong>统一内存管理模块</strong></em></u> (UnifiedMemoryManager) 机制进行分析。</p>
<h3 id="一、Executor内存总体布局"><a href="#一、Executor内存总体布局" class="headerlink" title="一、Executor内存总体布局"></a><strong>一、Executor内存总体布局</strong></h3><p>默认情况下，Executor不开启堆外内存，因此整个 Executor 端内存布局如下图所示：</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712101630401.png" alt="image-20200712101630401" style="zoom:50%;">

<p>我们可以看到在Yarn集群管理模式中，Spark 以 Executor Container 的形式在 NodeManager 中运行，其可使用的内存上限由</p>
<p>yarn.scheduler.maximum-allocation-mb 指定，我们称之为 MonitorMemory。</p>
<p>整个Executor内存区域分为两块：</p>
<pre><code class="python">## 1. JVM堆外内存

大小由 spark.yarn.executor.memoryOverhead 参数指定。默认大小为 executorMemory * 0.10, with minimum of 384m。

此部分内存主要用于JVM自身，字符串, NIO Buffer（Driect Buffer）等开销。此部分为用户代码及Spark 不可操作的内存，不足时可通过调整参数解决。

## 2. 堆内内存（ExecutorMemory）

大小由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置，即JVM最大分配的堆内存 (-Xmx)。Spark为了更高效的使用这部分内存，对这部分内存进行了逻辑上的划分管理。我们在下面的统一内存管理会详细介绍。
# NOTES:
对于Yarn集群，存在: ExecutorMemory + MemoryOverhead &lt;= MonitorMemory，若应用提交之时，指定的 ExecutorMemory 与 MemoryOverhead 之和大于 MonitorMemory，则会导致 Executor 申请失败；若运行过程中，实际使用内存超过上限阈值，Executor 进程会被 Yarn 终止掉 (kill)。
</code></pre>
<h3 id="二、统一内存管理"><a href="#二、统一内存管理" class="headerlink" title="二、统一内存管理"></a><strong>二、统一内存管理</strong></h3><p>Spark 1.6之后引入了统一内存管理，包括了堆内内存 (On-heap Memory) 和堆外内存 (Off-heap Memory) 两大区域，下面对这两块区域进行详细的说明。</p>
<p><strong>1. 堆内内存 (On-heap Memory)</strong></p>
<p>默认情况下，Spark 仅仅使用了堆内内存。Spark 对堆内内存的管理是一种逻辑上的“规划式”的管理，Executor 端的堆内内存区域在逻辑上被划分为以下四个区域：</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712102003104.png" alt="image-20200712102003104" style="zoom:50%;">

<pre><code class="python">#1 执行内存 (Execution Memory) : 主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据；
#2 存储内存 (Storage Memory) : 主要用于存储 spark 的 cache 数据，例如RDD的缓存、unroll数据；
#3 用户内存（User Memory）: 主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息；
#4 预留内存（Reserved Memory）: 系统预留内存，会用来存储Spark内部对象。

Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前 记录 这些内存，我们来看其具体流程：

    申请内存 ：
        Spark 在代码中 new 一个对象实例
        JVM 从堆内内存分配空间，创建对象并返回对象引用
        Spark 保存该对象的引用，记录该对象占用的内存

    释放内存 ：
        Spark 记录该对象释放的内存，删除该对象的引用
        等待 JVM 的垃圾回收机制释放该对象占用的堆内内存
</code></pre>
<p>下面的图对这个四个内存区域的分配比例做了详细的描述：</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712102136759.png" alt="image-20200712102136759" style="zoom:50%;"></li>
</ol>
<pre><code class="python">## （1）预留内存 (Reserved Memory)
系统预留内存，会用来存储Spark内部对象。其大小在代码中是写死的，其值等于 300MB，这个值是不能修改的（如果在测试环境下，我们可以通过 spark.testing.reservedMemory 参数进行修改）；如果Executor分配的内存小于 1.5 * 300 = 450M 时，Executor将无法执行。

##（2）存储内存 (Storage Memory)
主要用于存储 spark 的 cache 数据，例如 RDD 的缓存、广播（Broadcast）数据、和 unroll 数据。内存占比为 UsableMemory * spark.memory.fraction * spark.memory.storageFraction，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的30%（1 * 0.6 * 0.5 = 0.3）。在 UnifiedMemory 管理中，这两部分内存可以相互借用，具体借用机制我们下一小节会详细介绍。

##（3）执行内存 (Execution Memory)
主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据。内存占比为 UsableMemory * spark.memory.fraction * (1 - spark.memory.storageFraction)，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的30%（1 * 0.6 * (1 - 0.5) = 0.3）。在 UnifiedMemory 管理中，这两部分内存可以相互借用，具体借用机制我们下一小节会详细介绍。

##（4）其他/用户内存 (Other/User Memory) 
主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息。内存占比为 UsableMemory * (1 - spark.memory.fraction)，在Spark2+ 中，默认占可用内存的40%（1 * (1 - 0.6) = 0.4）。

其中，usableMemory = executorMemory - reservedMemory，这个就是 Spark 可用内存。

NOTES:
# （1）为什么设置300M预留内存
统一内存管理最初版本other这部分内存没有固定值 300M 设置，而是和静态内存管理相似，设置的百分比，最初版本占 25%。百分比设置在实际使用中出现了问题，若给定的内存较低时，例如 1G，会导致 OOM，具体讨论参考这里 Make unified memory management work with small heaps。因此，other这部分内存做了修改，先划出 300M 内存。

#（2）spark.memory.fraction 由 0.75 降至 0.6
spark.memory.fraction 最初版本的值是 0.75，很多分析统一内存管理这块的文章也是这么介绍的，同样的，在使用中发现这个值设置的偏高，导致了 gc 时间过长，spark 2.0 版本将其调整为 0.6，详细谈论参见 Reduce spark.memory.fraction default to avoid overrunning old gen in JVM default config。
</code></pre>
<p><strong>2. 堆外内存 (Off-heap Memory)</strong></p>
<p>Spark 1.6 开始引入了 Off-heap memory 。这种模式不在 JVM 内申请内存，而是调用 Java 的 unsafe 相关 API 进行诸如 C 语言里面的 malloc() 直接向操作系统申请内存。这种方式下 Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。另外，堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。，缺点是必须自己编写内存申请和释放的逻辑。</p>
<p>默认情况下Off-heap模式的内存并不启用，我们可以通过 <u><em><strong>spark.memory.offHeap.enabled</strong></em></u> 参数开启，并由 <em><strong><u>spark.memory.offHeap.size</u></strong></em> 指定堆外内存的大小，单位是字节（占用的空间划归 JVM OffHeap 内存）。</p>
<p>如果堆外内存被启用，那么 Executor 内将同时存在堆内和堆外内存，两者的使用互补影响，这个时候 Executor 中的 Execution 内存是堆内的 Execution 内存和堆外的 Execution 内存之和，同理，Storage 内存也一样。其内存分布如下图所示：</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712102544470.png" alt="image-20200712102544470" style="zoom:50%;">

<pre><code class="python">相比堆内内存，堆外内存只区分 Execution 内存和 Storage 内存：

#（1）存储内存 (Storage Memory)
内存占比为 maxOffHeapMemory * spark.memory.storageFraction，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的50%（1 * 0.5 = 0.5）。在 UnifiedMemory 管理中，这两部分内存可以相互借用，具体借用机制我们下一小节会详细介绍。

#（2）执行内存 (Execution Memory)
内存占比为 maxOffHeapMemory * (1 - spark.memory.storageFraction)，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的50%（1 * (1 - 0.5) = 0.5）。在 UnifiedMemory 管理中，这两部分内存可以相互借用，具体借用机制我们下一小节会详细介绍。
</code></pre>
<p><strong>3. Execution 内存和 Storage 内存动态占用机制</strong></p>
<p>在 Spark 1.5 之前，Execution 内存和 Storage 内存分配是静态的，换句话说就是如果 Execution 内存不足，即使 Storage 内存有很大空闲程序也是无法利用到的；反之亦然。</p>
<p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成”一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。</p>
<p>统一内存管理机制，与静态内存管理最大的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域：</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712103204824.png" alt="image-20200712103204824" style="zoom:50%;">

<pre><code class="python">其中最重要的优化在于动态占用机制，其规则如下：
## 程序提交的时候我们都会设定基本的 Execution 内存和 Storage 内存区域（通过 spark.memory.storageFraction 参数设置）。我们用 onHeapStorageRegionSize 来表示 spark.storage.storageFraction 划分的存储内存区域。这部分内存是不可以被驱逐(Evict)的存储内存（但是如果空闲是可以被占用的）。

## 当计算内存不足时，可以借用 onHeapStorageRegionSize 中未使用部分，且 Storage 内存的空间被对方占用后，需要等待执行内存自己释放，不能抢占。

## 若实际 StorageMemory 使用量超过 onHeapStorageRegionSize，那么当计算内存不足时，可以驱逐并借用 StorageMemory – onHeapStorageRegionSize 部分，而 onHeapStorageRegionSize 部分不可被抢占。

## 反之，当存储内存不足时（存储空间不足是指不足以放下一个完整的 Block），也可以借用计算内存空间；但是 Execution 内存的空间被存储内存占用后，是可让对方将占用的部分转存到硬盘，然后“归还”借用的空间。

## 如果双方的空间都不足时，则存储到硬盘；将内存中的块存储到磁盘的策略是按照 LRU 规则进行的。

说明
（1）出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。可通过配置
spark.memory.useLegacyMode 参数启用。

（2）spark.memory.storageFraction 是不可被驱逐的内存空间。只有空闲的时候能够被执行内存占用，但是不能被驱逐抢占。

（3）Storage 内存的空间被对方占用后，目前的实现是无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂；而且 Shuffle 过程产生的文件在后面一定会被使用到，而 Cache 在内存的数据不一定在后面使用。在 Unified Memory Management in Spark 1.6 中详细讲解了为何选择这种策略，简单总结如下:

数据清除的开销 : 驱逐storage内存的开销取决于 storage level，MEMORY_ONLY 可能是最昂贵的，因为需要重新计算，MEMORY_AND_DISK_SER 正好相反，只涉及到磁盘IO。溢写 execution 内存到磁盘的开销并不昂贵，因为 execution 存储的数据格式紧凑(compact format)，序列化开销低。并且，清除的 storage 内存可能不会被用到，但是，可以预见的是，驱逐的 execution 内存是必然会再被读到内存的，频繁的驱除重读 execution 内存将导致昂贵的开销。

实现的复杂度 : storage 内存的驱逐是容易实现的，只需要使用已有的方法，drop 掉 block。execution 则复杂的多，首先，execution 以 page 为单位管理这部分内存，并且确保相应的操作至少有 one page ，如果把这 one page 内存驱逐了，对应的操作就会处于饥饿状态。此外，还需要考虑 execution 内存被驱逐的情况下，等待 cache 的 block 如何处理。

（4）上面说的借用对方的内存需要借用方和被借用方的内存类型都一样，都是堆内内存或者都是堆外内存，不存在堆内i内存不够去借用堆外内存的空间。
</code></pre>
<h3 id="小结："><a href="#小结：" class="headerlink" title="小结："></a><strong>小结：</strong></h3><pre><code class="python">## 1：内存组成部分
总的内存  =  预留内存(300Mb) +  可用内存
可用内存 =   统一内存（60%）  +   其他（40%）
统一内存 =  存储内存（Storeage）（50%）  +  执行内存(Execution)（50%）

--executor-memory  1g
可用内存  =  1g – 300mb  = 724mb
统一内存  = （1g-300） *0.6 = 434.4mb
存储内存 =  （1g-300） *0.6 *0.5 = 217.2mb
执行内存 =  （1g-300） *0.6 *0.5 = 217.2mb

## 2:动态占用机制
1，如果双方的空间都满了，溢出到磁盘。
2，如果对方有空闲，都可以占用   双向占用
3，单向收回，Execution收回 自己被Storage占用的空间。
</code></pre>
<p><strong>4. 任务内存管理（Task Memory Manager）</strong></p>
<pre><code class="python">Executor 中任务以线程的方式执行，各线程共享JVM的资源（即 Execution 内存），任务之间的内存资源没有强隔离（任务没有专用的Heap区域）。因此，可能会出现这样的情况：先到达的任务可能占用较大的内存，而后到的任务因得不到足够的内存而挂起。

在 Spark 任务内存管理中，使用 HashMap 存储任务与其消耗内存的映射关系。每个任务可占用的内存大小为潜在可使用计算内存（ 潜在可使用计算内存为: 初始计算内存 + 可抢占存储内存）的 1/2n ~ 1/n，当剩余内存为小于 1/2n 时，任务将被挂起，直至有其他任务释放执行内存，而满足内存下限 1/2n，任务被唤醒。其中 n 为当前 Executor 中活跃的任务树。

比如如果 Execution 内存大小为 10GB，当前 Executor 内正在运行的 Task 个数为5，则该 Task 可以申请的内存范围为 10 / (2 * 5) ~ 10 / 5，也就是 1GB ~ 2GB 的范围。

任务执行过程中，如果需要更多的内存，则会进行申请，如果存在空闲内存，则自动扩容成功，否则，将抛出 OutOffMemroyError。

每个 Executor 中可同时运行的任务数由 Executor 分配的 CPU 的核数 N 和每个任务需要的 CPU 核心数 C 决定。其中：
N = spark.executor.cores
C = spark.task.cpus
## 由此每个 Executor 的最大任务并行度可表示为 : TP = N / C 。

其中，C 值与应用类型有关，大部分应用使用默认值 1 即可，因此，影响 Executor 中最大任务并行度（最大活跃task数）的主要因素是 N。

依据 Task 的内存使用特征，前文所述的 Executor 内存模型可以简单抽象为下图所示模型：
</code></pre>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712104935038.png" alt="image-20200712104935038" style="zoom:50%;">

<p>其中，Executor 向 yarn 申请的总内存可表示为 : M &#x3D; M1 + M2 。<br>如果考虑堆外内存则大概是如下结构：</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712105012761.png" alt="image-20200712105012761" style="zoom:50%;">

<p><strong>5. 一个示例</strong></p>
<p><strong>(1）只用了堆内内存</strong></p>
<p>现在我们提交的 Spark 作业关于内存的配置如下：–executor-memory 18g<br>由于没有设置spark.memory.fraction 和 spark.memory.storageFraction 参数，我们可以看到 Spark UI 关于 Storage Memory 的显示如下：</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712105156727.png" alt="image-20200712105156727" style="zoom:50%;">

<p>上图很清楚地看到 Storage Memory 的可用内存是 10.1GB，这个数是咋来的呢？根据前面的规则，我们可以得出以下的计算：</p>
<pre><code class="python">systemMemory = spark.executor.memory
reservedMemory = 300MB
usableMemory = systemMemory - reservedMemory
StorageMemory= usableMemory * spark.memory.fraction * spark.memory.storageFraction

#1 如果我们把数据代进去，得出以下的结果：
systemMemory = 18Gb = 19327352832 字节
reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
usableMemory = systemMemory - reservedMemory = 19327352832 - 314572800 = 19012780032
StorageMemory = usableMemory * spark.memory.fraction * spark.memory.storageFraction
              = 19012780032 * 0.6 * 0.5 = 5703834009.6 = 5.312109375GB
#2 和上面的 10.1GB 对不上啊。为什么呢？这是因为 Spark UI 上面显示的 Storage Memory 可用内存其实等于 Execution 内存和 Storage 内存之和，也就是 usableMemory * spark.memory.fraction :
StorageMemory = usableMemory * spark.memory.fraction
              = 19012780032 * 0.6 = 11407668019.2 = 10.62421GB
#3 还是不对，这是因为我们虽然设置了 --executor-memory 18g ，但是 Spark 的 Executor 端通过 Runtime.getRuntime.maxMemory 拿到的内存其实没这么大，只有 17179869184 字节，所以 systemMemory=17179869184，然后计算的数据如下：
ystemMemory = 17179869184 字节
reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384
StorageMemory= usableMemory * spark.memory.fraction
             = 16865296384 * 0.6 = 9.42421875 GB
#4 我们通过将上面的 16865296384 * 0.6 字节除于 1024 * 1024 * 1024 转换成 9.42421875 GB，和 UI 上显示的还是对不上，这是因为 Spark UI 是通过除于 1000 * 1000 * 1000 将字节转换成 GB，如下：
systemMemory = 17179869184 字节
reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384
StorageMemory = usableMemory * spark.memory.fraction
              = 16865296384 * 0.6 字节 =  16865296384 * 0.6 / (1000 * 1000 * 1000) = 10.1GB
现在终于对上了。
</code></pre>
<p>具体将字节转换成 GB 的计算逻辑如下(core 模块下面的 &#x2F;core&#x2F;src&#x2F;main&#x2F;resources&#x2F;org&#x2F;apache&#x2F;spark&#x2F;ui&#x2F;static&#x2F;utils.js)：</p>
<pre><code class="javascript">functionformatBytes(bytes, type) &#123;
    if(type !==&#39;display&#39;)returnbytes;
    if(bytes == 0)return&#39;0.0 B&#39;;
    vark = 1000;
    vardm = 1;
    varsizes = [&#39;B&#39;,&#39;KB&#39;,&#39;MB&#39;,&#39;GB&#39;,&#39;TB&#39;,&#39;PB&#39;,&#39;EB&#39;,&#39;ZB&#39;,&#39;YB&#39;];
    vari = Math.floor(Math.log(bytes) / Math.log(k));
    returnparseFloat((bytes / Math.pow(k, i)).toFixed(dm)) +&#39; &#39;+ sizes[i];
&#125;
</code></pre>
<p>我们设置了 –executor-memory 18g，但是 Spark 的 Executor 端通过</p>
<p>Runtime.getRuntime.maxMemory 拿到的内存其实没这么大，只有 17179869184 字节，这个数据是怎么计算的？</p>
<p>**<u><em>Runtime.getRuntime.maxMemory 是程序能够使用的最大内存，其值会比实际配置的执行器内存的值小。这是因为内存分配池的堆部分划分为 Eden，Survivor 和 Tenured 三部分空间，而这里面一共包含了两个 Survivor 区域，</em></u>**而这两个 Survivor 区域在任何时候我们只能用到其中一个，所以我们可以使用下面的公式进行描述 :</p>
<pre><code class="scala">ExecutorMemory = Eden + 2 * Survivor + Tenured
Runtime.getRuntime.maxMemory = Eden + Survivor + Tenured
上面的 17179869184 字节可能因为你的 GC 配置不一样得到的数据不一样，但是上面的计算公式是一样的。
</code></pre>
<p><strong>（2）用了堆内和堆外内存</strong></p>
<p>现在如果我们启用了堆外内存，情况会怎样呢？我们的内存相关配置如下：</p>
<pre><code class="scala">spark.executor.memory           18g
spark.memory.offHeap.enabled   true
spark.memory.offHeap.size       10737418240
</code></pre>
<p>从上面可以看出，堆外内存为 10GB，现在 Spark UI 上面显示的 Storage Memory 可用内存为 20.9GB，如下：</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712110238920.png" alt="image-20200712110238920" style="zoom:50%;">

<p>其实 Spark UI 上面显示的 Storage Memory 可用内存等于堆内内存和堆外内存之和，计算公式如下：</p>
<p>堆内:</p>
<pre><code class="scala">systemMemory = 17179869184 字节
reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384
totalOnHeapStorageMemory = usableMemory * spark.memory.fraction
                         = 16865296384 * 0.6 = 10119177830
</code></pre>
<p>堆外:</p>
<pre><code class="scala">totalOffHeapStorageMemory = spark.memory.offHeap.size = 10737418240
 
总 Storage 内存:
StorageMemory = totalOnHeapStorageMemory + totalOffHeapStorageMemory
              = (10119177830 + 10737418240) 字节
              = (20856596070 / (1000 * 1000 * 1000)) GB
              = 20.9 GB
</code></pre>
<p><strong>6. Executor内存参数调优</strong></p>
<p>（1<u><em>） <strong>Executor JVM Used Memory Heuristic</strong></em>**</u></p>
<ul>
<li><p><strong>现象：</strong>配置的executor内存比实际使用的JVM最大使用内存还要大很多。</p>
</li>
<li><p><strong>原因：</strong>这意味着 executor 内存申请过多了，实际上并不需要使用这么多内存。</p>
</li>
<li><p><strong>解决方案：</strong>将 spark.executor.memory 设置为一个比较小的值。</p>
<pre><code class="scala">spark.executor.memory : 16 GB
Max executor peak JVM used memory : 6.6 GB 
Suggested spark.executor.memory : 7 GB
</code></pre>
</li>
</ul>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712110643913.png" alt="image-20200712110643913" style="zoom:50%;">

<p><em><strong><u>（2） Executor Unified Memory Heuristic</u></strong></em></p>
<ul>
<li><p><strong>现象：</strong>分配的统一内存 (Unified Memory &#x3D; Storage Memory + Execution Memory) 比 executor 实际使用的统一内存大的多。</p>
</li>
<li><p><strong>原因：</strong>这意味着不需要这么大的统一内存。</p>
</li>
<li><p><strong>解决方案：</strong>降低 spark.memory.fraction 的比例。</p>
<pre><code class="scala">spark.executor.memory : 10 GB
spark.memory.fraction : 0.6
Allocated unified memory : 6 GB
Max peak JVM userd memory : 7.2 GB
Max peak unified memory : 1.2 GB 
Suggested spark.memory.fraction : 0.2
</code></pre>
</li>
</ul>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712111944876.png" alt="image-20200712111944876" style="zoom:50%;">

<p><strong>（3）Executor OOM类错误 （错误代码 137、143等）</strong></p>
<p>该类错误一般是由于 Heap（M2）已达上限，Task 需要更多的内存，而又得不到足够的内存而导致。因此，解决方案要从增加每个 Task 的内存使用量，满足任务需求 或 降低单个 Task 的内存消耗量，从而使现有内存可以满足任务运行需求两个角度出发。因此有如下解决方案：</p>
<pre><code class="python">## 法一：增加单个task的内存使用量

增加最大 Heap值，即上图中 M2 的值，使每个 Task 可使用内存增加。

降低 Executor 的可用 Core 的数量 N , 使 Executor 中同时运行的任务数减少，在总资源不变的情况下，使每个 Task 获得的内存相对增加。当然，这会使得 Executor 的并行度下降。可以通过调高 spark.executor.instances 参数来申请更多的 executor 实例（或者通过 
spark.dynamicAllocation.enabled 启动动态分配），提高job的总并行度。


## 法二：降低单个Task的内存消耗量

降低单个Task的内存消耗量可从配置方式和调整应用逻辑两个层面进行优化：

配置方式
减少每个 Task 处理的数据量，可降低 Task 的内存开销，在 Spark 中，每个 partition 对应一个处理任务 Task，因此，在数据总量一定的前提下，可以通过增加 partition 数量的方式来减少每个 Task 处理的数据量，从而降低 Task 的内存开销。针对不同的 Spark 应用类型，存在不同的 partition 配置参数 :
P = spark.default.parallism (非SQL应用)
P = spark.sql.shuffle.partition (SQL 应用)

通过增加 P 的值，可在一定程度上使 Task 现有内存满足任务运行。注: 当调整一个参数不能解决问题时，上述方案应进行协同调整。
</code></pre>
<p><strong>a.调整应用逻辑</strong></p>
<p>Executor OOM 一般发生 Shuffle 阶段，该阶段需求计算内存较大，且应用逻辑对内存需求有较大影响，下面举例就行说明：</p>
<p>选择合适的算子，如 groupByKey 转换为 reduceByKey。</p>
<p>一般情况下，groupByKey 能实现的功能使用 reduceByKey 均可实现，而 ReduceByKey 存在 Map 端的合并，可以有效减少传输带宽占用及 Reduce 端内存消耗。</p>
<p><strong>b.避免数据倾斜 (data skew)</strong></p>
<p>Data Skew 是指任务间处理的数据量存大较大的差异。</p>
<p>如左图所示，key 为 010 的数据较多，当发生 shuffle 时，010 所在分区存在大量数据，不仅拖慢 Job 执行（Job 的执行时间由最后完成的任务决定）。而且导致 010 对应 Task 内存消耗过多，可能导致 OOM。</p>
<p>右图，经过预处理（加盐，此处仅为举例说明问题，解决方法不限于此）可以有效减少 Data Skew 导致的问题。</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712112334330.png" alt="image-20200712112334330" style="zoom:50%;">

<p><em><u><strong>（4）Execution Memory Spill Heuristic</strong></u></em></p>
<ul>
<li><p><strong>现象：</strong>在 stage 3 发现执行内存溢出。Shuffle read bytes 和 spill 分布均匀。这个 stage 有 200 个 tasks。</p>
</li>
<li><p><strong>原因：</strong>执行内存溢出，意味着执行内存不足。跟上面的 OOM 错误一样，只是执行内存不足的情况下不会报 OOM 而是会将数据溢出到磁盘。但是整个性能很难接受。</p>
</li>
<li><p><strong>解决方案：</strong>同 3。</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200712112421393.png" alt="image-20200712112421393" style="zoom:50%;">

<p><em><u><strong>5） Executor GC Heuristic</strong></u></em></p>
<ul>
<li><p><strong>现象：</strong>Executor 花费很多时间在 GC。</p>
</li>
<li><p><strong>原因：</strong>可以通过-verbose:gc</p>
<p>-XX:+PrintGCDetails</p>
<p>-XX:+PrintGCTimeStamps 查看 GC 情况</p>
</li>
<li><p><strong>解决方案：</strong> Garbage Collection Tuning</p>
</li>
</ul>
<p><strong>（6）Beyond … memory, killed by yarn.</strong></p>
<p>出现该问题原因是由于实际使用内存上限超过申请的内存上限而被 Yarn 终止掉了, 首先说明 Yarn 中 Container 的内存监控机制：</p>
<ul>
<li>Container 进程的内存使用量 : 以 Container 进程为根的进程树中所有进程的内存使用总量。</li>
<li>Container 被杀死的判断依据 : 进程树总内存（物理内存或虚拟内存）使用量超过向 Yarn 申请的内存上限值，则认为该 Container 使用内存超量，可以被“杀死”。</li>
</ul>
<p>因此，对该异常的分析要从是否存在子进程两个角度出发。</p>
<p><strong>a. 不存在子进程</strong></p>
<p>Overhead) 不足，依据 Yarn 内存使用情况有如下两种方案:</p>
<p><strong>法一：</strong>如果，M (spark.executor.memory) 未达到 Yarn 单个 Container 允许的上限时，可仅增加 M1（spark.yarn.executor.memoryOverhead），从而增加 M；如果，M 达到 Yarn 单个 Container 允许的上限时，增加 M1，降低 M2。</p>
<p>注意二者之各要小于 Container 监控内存量，否则伸请资源将被 yarn 拒绝。</p>
<p><strong>法二：</strong>减少可用的 Core 的数量 N，使并行任务数减少，从而减少 Overhead 开销</p>
<p><strong>b. 存在子进程</strong></p>
<p>Spark 应用中 Container 以 Executor（JVM进程）的形式存在，因此根进程为 Executor 对应的进程，而 Spark 应用向Yarn申请的总资源 M &#x3D; M1 + M2，都是以 Executor(JVM) 进程（非进程树）可用资源的名义申请的。</p>
<p>申请的资源并非一次性全量分配给 JVM 使用，而是先为 JVM 分配初始值，随后内存不足时再按比率不断进行扩容，直致达到 Container 监控的最大内存使用量 M。当 Executor 中启动了子进程（如调用 shell 等）时，子进程占用的内存（记为 S）就被加入 Container 进程树，此时就会影响 Executor 实际可使用内存资源（Executor 进程实际可使用资源变为: M - S），然而启动 JVM 时设置的可用最大资源为 M，且 JVM 进程并不会感知 Container 中留给自己的使用量已被子进程占用。因此，当 JVM 使用量达到 M - S，还会继续开劈内存空间，这就会导致 Executor 进程树使用的总内存量大于 M 而被 Yarn 杀死。</p>
<p>典形场景有:</p>
<ul>
<li>PySpark（Spark已做内存限制，一般不会占用过大内存）</li>
<li>自定义Shell调用</li>
</ul>
<p>其解决方案分别为:</p>
<p>a. PySpark场景：</p>
<ul>
<li>如果，M 未达到 Yarn 单个 Container 允许的上限时，可仅增加 M1 ，从而增加 M；如果，M 达到 Yarn 单个 Container 允许的上限时，增加 M1，降低 M2。</li>
<li>减少可用的 Core 的数量 N，使并行任务数减少，从而减少 Overhead 开销</li>
</ul>
<p>b. 自定义 Shell 场景:（OverHead 不足为假象）</p>
<p>调整子进程可用内存量 (通过单机测试，内存控制在 Container 监控内存以内，且为 Spark 保留内存等留有空间）。</p>
<h3 id="7-淘汰和罗盘"><a href="#7-淘汰和罗盘" class="headerlink" title="7. 淘汰和罗盘"></a><strong>7. 淘汰和罗盘</strong></h3><pre><code class="python">由于同一个Executor的所有的计算任务共享有限的存储内存空间，当有新的Block需要缓存但是剩余空间不足且无法动态占用时，就要对LinkedHashMap中的旧Block进行淘汰（Eviction)，而被淘汰的Block如果其存储级别中同时包含存储到磁盘的要求，则要对其进行落盘（Drop），否则直接删除该Block。
存储内存的淘汰规则为：

    #1:被淘汰的旧Block要与新Block的MemoryMode相同，即同属于堆外或堆内内存
    #2:新旧Block不能属于同一个RDD，避免循环淘汰
    #3:旧Block所属RDD不能处于被读状态，避免引发一致性问题
    #4:遍历LinkedHashMap中Block，按照最近最少使用（LRU）的顺序淘汰，直到满足新Block所需的空间。其中LRU是LinkedHashMap的特性。

落盘的流程则比较简单，如果其存储级别符合_useDisk为true的条件，再根据其_deserialized判断是否是非序列化的形式，若是则对其进行序列化，最后将数据存储到磁盘，在Storage模块中更新其信息。
</code></pre>
<h3 id="8-Shuffle的内存占用"><a href="#8-Shuffle的内存占用" class="headerlink" title="8: Shuffle的内存占用"></a><strong>8: Shuffle的内存占用</strong></h3><pre><code class="python">执行内存主要用来存储任务在执行Shuffle时占用的内存，Shuffle是按照一定规则对RDD数据重新分区的过程，我们来看Shuffle的Write和Read两阶段对执行内存的使用：

    ## Shuffle Write
        若在map端选择普通的排序方式，会采用ExternalSorter进行外排，在内存中存储数据时主要占用堆内执行空间。
        若在map端选择Tungsten的排序方式，则采用ShuffleExternalSorter直接对以序列化形式存储的数据排序，在内存中存储数据时可以占用堆外或堆内执行空间，取决于用户是否开启了堆外内存以及堆外执行内存是否足够。
    ## Shuffle Read
        在对reduce端的数据进行聚合时，要将数据交给Aggregator处理，在内存中存储数据时占用堆内执行空间。
        如果需要进行最终结果排序，则要将再次将数据交给ExternalSorter处理，占用堆内执行空间。

在ExternalSorter和Aggregator中，Spark会使用一种叫AppendOnlyMap的哈希表在堆内执行内存中存储数据，但在Shuffle过程中所有数据并不能都保存到该哈希表中，当这个哈希表占用的内存会进行周期性地采样估算，当其大到一定程度，无法再从MemoryManager申请到新的执行内存时，Spark就会将其全部内容存储到磁盘文件中，这个过程被称为溢存(Spill)，溢存到磁盘的文件最后会被归并(Merge)。

Shuffle Write阶段中用到的Tungsten是Databricks公司提出的对Spark优化内存和CPU使用的计划[4]，解决了一些JVM在性能上的限制和弊端。Spark会根据Shuffle的情况来自动选择是否采用Tungsten排序。Tungsten采用的页式内存管理机制建立在MemoryManager之上，即Tungsten对执行内存的使用进行了一步的抽象，这样在Shuffle过程中无需关心数据具体存储在堆内还是堆外。每个内存页用一个MemoryBlock来定义，并用Object obj和long offset这两个变量统一标识一个内存页在系统内存中的地址。堆内的MemoryBlock是以long型数组的形式分配的内存，其obj的值为是这个数组的对象引用，offset是long型数组的在JVM中的初始偏移地址，两者配合使用可以定位这个数组在堆内的绝对地址；堆外的MemoryBlock是直接申请到的内存块，其obj为null，offset是这个内存块在系统内存中的64位绝对地址。Spark用MemoryBlock巧妙地将堆内和堆外内存页统一抽象封装，并用页表(pageTable)管理每个Task申请到的内存页。

Tungsten页式管理下的所有内存用64位的逻辑地址表示，由页号和页内偏移量组成：

#1. 页号：占13位，唯一标识一个内存页，Spark在申请内存页之前要先申请空闲页号。
2. 页内偏移量：占51位，是在使用内存页存储数据时，数据在页内的偏移地址。

有了统一的寻址方式，Spark可以用64位逻辑地址的指针定位到堆内或堆外的内存，整个Shuffle Write排序的过程只需要对指针进行排序，并且无需反序列化，整个过程非常高效，对于内存访问效率和CPU使用效率带来了明显的提升。
</code></pre>
<h2 id="SparkSQL用UDAF实现Bitmap函数"><a href="#SparkSQL用UDAF实现Bitmap函数" class="headerlink" title="SparkSQL用UDAF实现Bitmap函数"></a><strong>SparkSQL用UDAF实现Bitmap函数</strong></h2><pre><code class="SQL">使用phoenix在HBase中创建测试表，字段使用VARBINARY类型

CREATE TABLE IF NOT EXISTS test_binary (
date VARCHAR NOT NULL,
dist_mem VARBINARY
 CONSTRAINT test_binary_pk PRIMARY KEY (date)
 ) SALT_BUCKETS=6;
</code></pre>
<h3 id="实现自定义聚合函数bitmap"><a href="#实现自定义聚合函数bitmap" class="headerlink" title="实现自定义聚合函数bitmap"></a>实现自定义聚合函数bitmap</h3><pre><code class="java">import org.apache.spark.sql.Row;
import org.apache.spark.sql.expressions.MutableAggregationBuffer;
import org.apache.spark.sql.expressions.UserDefinedAggregateFunction;
import org.apache.spark.sql.types.DataType;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import org.roaringbitmap.RoaringBitmap;
 
import java.io.*;
import java.util.ArrayList;
import java.util.List;
 
/**
 * 实现自定义聚合函数Bitmap
 */
public class UdafBitMap extends UserDefinedAggregateFunction &#123;
    @Override
    public StructType inputSchema() &#123;
        List&lt;StructField&gt; structFields = new ArrayList&lt;&gt;();
        structFields.add(DataTypes.createStructField(&quot;field&quot;, DataTypes.BinaryType, true));
        return DataTypes.createStructType(structFields);
    &#125;
 
    @Override
    public StructType bufferSchema() &#123;
        List&lt;StructField&gt; structFields = new ArrayList&lt;&gt;();
        structFields.add(DataTypes.createStructField(&quot;field&quot;, DataTypes.BinaryType, true));
        return DataTypes.createStructType(structFields);
    &#125;
 
    @Override
    public DataType dataType() &#123;
        return DataTypes.LongType;
    &#125;
 
    @Override
    public boolean deterministic() &#123;
        //是否强制每次执行的结果相同
        return false;
    &#125;
 
    @Override
    public void initialize(MutableAggregationBuffer buffer) &#123;
        //初始化
        buffer.update(0, null);
    &#125;
 
    @Override
    public void update(MutableAggregationBuffer buffer, Row input) &#123;
        // 相同的executor间的数据合并
        // 1. 输入为空直接返回不更新
        Object in = input.get(0);
        if(in == null)&#123;
            return ;
        &#125;
        // 2. 源为空则直接更新值为输入
        byte[] inBytes = (byte[]) in;
        Object out = buffer.get(0);
        if(out == null)&#123;
            buffer.update(0, inBytes);
            return ;
        &#125;
        // 3. 源和输入都不为空使用bitmap去重合并
        byte[] outBytes = (byte[]) out;
        byte[] result = outBytes;
        RoaringBitmap outRR = new RoaringBitmap();
        RoaringBitmap inRR = new RoaringBitmap();
        try &#123;
            outRR.deserialize(new DataInputStream(new ByteArrayInputStream(outBytes)));
            inRR.deserialize(new DataInputStream(new ByteArrayInputStream(inBytes)));
            outRR.or(inRR);
            ByteArrayOutputStream bos = new ByteArrayOutputStream();
            outRR.serialize(new DataOutputStream(bos));
            result = bos.toByteArray();
        &#125; catch (IOException e) &#123;
            e.printStackTrace();
        &#125;
        buffer.update(0, result);
    &#125;
 
    @Override
    public void merge(MutableAggregationBuffer buffer1, Row buffer2) &#123;
        //不同excutor间的数据合并
        update(buffer1, buffer2);
    &#125;
 
    @Override
    public Object evaluate(Row buffer) &#123;
        //根据Buffer计算结果
        long r = 0l;
        Object val = buffer.get(0);
        if (val != null) &#123;
            RoaringBitmap rr = new RoaringBitmap();
            try &#123;
                rr.deserialize(new DataInputStream(new ByteArrayInputStream((byte[]) val)));
                r = rr.getLongCardinality();
            &#125; catch (IOException e) &#123;
                e.printStackTrace();
            &#125;
        &#125;
        return r;
    &#125;
&#125;
</code></pre>
<h3 id="调用示例"><a href="#调用示例" class="headerlink" title="调用示例"></a>调用示例</h3><pre><code class="java">/**
     * 使用自定义函数解析bitmap
     *
     * @param sparkSession
     * @return
     */
    private static void udafBitmap(SparkSession sparkSession) &#123;
        try &#123;
            Properties prop = PropUtil.loadProp(DB_PHOENIX_CONF_FILE);
            // JDBC连接属性
            Properties connProp = new Properties();
            connProp.put(&quot;driver&quot;, prop.getProperty(DB_PHOENIX_DRIVER));
            connProp.put(&quot;user&quot;, prop.getProperty(DB_PHOENIX_USER));
            connProp.put(&quot;password&quot;, prop.getProperty(DB_PHOENIX_PASS));
            connProp.put(&quot;fetchsize&quot;, prop.getProperty(DB_PHOENIX_FETCHSIZE));
            // 注册自定义聚合函数
            sparkSession.udf().register(&quot;bitmap&quot;,new UdafBitMap());
            sparkSession
                    .read()
                    .jdbc(prop.getProperty(DB_PHOENIX_URL), &quot;test_binary&quot;, connProp)
                    // sql中必须使用global_temp.表名，否则找不到
                    .createOrReplaceGlobalTempView(&quot;test_binary&quot;);
            //sparkSession.sql(&quot;select YEAR(TO_DATE(date)) year,bitmap(dist_mem) memNum from global_temp.test_binary group by YEAR(TO_DATE(date))&quot;).show();
            sparkSession.sql(&quot;select date,bitmap(dist_mem) memNum from global_temp.test_binary group by date&quot;).show();
        &#125; catch (Exception e) &#123;
            e.printStackTrace();
        &#125;
    &#125;
</code></pre>
<h2 id="ETL常见数据质量监控"><a href="#ETL常见数据质量监控" class="headerlink" title="ETL常见数据质量监控"></a><strong>ETL常见数据质量监控</strong></h2><img src="https://img-blog.csdnimg.cn/20190528154230375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mjg5MzY1MA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:80%;">

<h2 id="数据质量监控方法"><a href="#数据质量监控方法" class="headerlink" title="数据质量监控方法"></a>数据质量监控方法</h2><h3 id="1、校验每天的记录数"><a href="#1、校验每天的记录数" class="headerlink" title="1、校验每天的记录数"></a>1、校验每天的记录数</h3><p>分析师遇到的最常见数据异常是其报告的输出突然降至0。</p>
<p>我们通常会发现最后的罪魁祸首是当天没有将新记录添加到相应的表中。</p>
<p>一种简单的检查方法是确保每天一个表中的新记录数&gt;0。</p>
<h3 id="2、NULL和0值校验"><a href="#2、NULL和0值校验" class="headerlink" title="2、NULL和0值校验"></a>2、NULL和0值校验</h3><p>分析师常遇到的第二个问题是NULL或0值。我们要保证每天增量数据中的NULL或0值不能超过新增数据的99%。要检查这一点，只需将一个循环脚本设置为每天用NULL或0计数一个表中的新记录数。如果看到记录数急剧增加，则可能存在转换错误或源业务系统就存在异常。</p>
<h3 id="3、每天新增的记录数波动范围"><a href="#3、每天新增的记录数波动范围" class="headerlink" title="3、每天新增的记录数波动范围"></a>3、每天新增的记录数波动范围</h3><p>某一天你发现数据量出现大幅增长或下降，而规则1和2都已校验通过。这种波动可能是正常的，比如电商行业某天的大促活动，或者社交软件的营销活动。但是也可能这就是异常的，是因为从源系统抽取了重复的记录。所以针对此种情况，我们也要制定数据质量规则，检查这些波动何时发生，并主动进行诊断。比如自动执行的一个简单的SQL过程，每天检查COUNT个新记录是否在7天跟踪平均值的误差范围内。阈值和误差范围可能因公司和产品而异，经验值一般是加减25％。当然，你可也可以直接和前一天的数据对比，增量不超过前一天的1倍。</p>
<h3 id="4、重复记录数据校验"><a href="#4、重复记录数据校验" class="headerlink" title="4、重复记录数据校验"></a>4、重复记录数据校验</h3><p>不管是电商系统或者是社交系统或者是物联网设备上报的数据，正常情况下都不会出现两条完全一样的记录（包括ID，时间，值都一样）。笔者曾遇到一个终端上报的两条数据完全一样的场景，导致我在做时间分段时候，划分不正确。所以，对数据值唯一性校验是有必要的。</p>
<h3 id="5、数据时间校验"><a href="#5、数据时间校验" class="headerlink" title="5、数据时间校验"></a>5、数据时间校验</h3><p>一般我们业务系统的数据都是带有时间戳的，这个时间戳肯定比当前的时间要小。但是由于采集数据设备异常（业务系统异常），我们会碰到“未来时间”的数据，那如果我们以时间作为分区，后期可能就会出现异常的分析结果。当然，如果你的公司业务是跨国的，你需要考虑时差因素。</p>
<h2 id="Hive实现数据抽样的三种方式"><a href="#Hive实现数据抽样的三种方式" class="headerlink" title="Hive实现数据抽样的三种方式"></a><strong>Hive实现数据抽样的三种方式</strong></h2><p>Hive提供了数据取样（SAMPLING）的功能，能够根据一定的规则进行数据抽样，目前支持数据块抽样，分桶抽样和随机抽样，具体如下所示：</p>
<pre><code class="SQL">1. 数据块抽样（tablesample()函数）
1&gt;   tablesample(n percent)  根据hive表数据的大小按比例抽取数据，并保存到新的hive表中。如：抽取原hive表中10%的数据
（注意：测试过程中发现，select语句不能带where条件且不支持子查询，可通过新建中间表或使用随机抽样解决）
create table xxx_new as select * from xxx tablesample(10 percent)
2&gt;  tablesample(n M)  指定抽样数据的大小，单位为M。
3&gt;  tablesample(n rows)  指定抽样数据的行数，其中n代表每个map任务均取n行数据，map数量可通过hive表的简单查询语句确认（关键词：number of mappers: x)

2.分桶抽样
hive中分桶其实就是根据某一个字段Hash取模，放入指定数据的桶中，比如将表table_1按照ID分成100个桶，其算法是hash(id) % 100，这样，hash(id) % 100 = 0的数据被放到第一个桶中，hash(id) % 100 = 1的记录被放到第二个桶中。创建分桶表的关键语句为：CLUSTER BY语句。
分桶抽样语法：
TABLESAMPLE (BUCKET x OUT OF y [ON colname])

其中x是要抽样的桶编号，桶编号从1开始，colname表示抽样的列，y表示桶的数量。
例如：将表随机分成10组，抽取其中的第一个桶的数据
select * from table_01 tablesample(bucket 1 out of 10 on rand())

3. 随机抽样（rand()函数）
1&gt;  使用rand()函数进行随机抽样，limit关键字限制抽样返回的数据，其中rand函数前的distribute和sort关键字可以保证数据在mapper和reducer阶段是随机分布的，案例如下：
select * from table_name where col=xxx distribute by rand() sort by rand() limit num;

2&gt;  使用order 关键词
案例如下：
select * from table_name where col=xxx order by rand() limit num;
经测试对比，千万级数据中进行随机抽样 order by方式耗时更长，大约多30秒左右。
</code></pre>
<h2 id="Mysql使用规范"><a href="#Mysql使用规范" class="headerlink" title="Mysql使用规范"></a><strong>Mysql使用规范</strong></h2><h2 id="从B-树到LSM树，及LSM树在HBase中的应用"><a href="#从B-树到LSM树，及LSM树在HBase中的应用" class="headerlink" title="从B+树到LSM树，及LSM树在HBase中的应用"></a>从B+树到LSM树，及LSM树在HBase中的应用</h2><h2 id="Sparksql-的-catalyst"><a href="#Sparksql-的-catalyst" class="headerlink" title="Sparksql 的 catalyst"></a><strong>Sparksql 的 catalyst</strong></h2><h2 id="SparkSQL读写部数据源——csv文件的读写"><a href="#SparkSQL读写部数据源——csv文件的读写" class="headerlink" title="SparkSQL读写部数据源——csv文件的读写"></a><strong>SparkSQL读写部数据源——csv文件的读写</strong></h2><p><code>1、sep 和 delimiter的功能都是一样，都是表示csv的切割符，(默认是,)(读写参数)</code></p>
<pre><code class="scala">spark.read.option(&quot;sep&quot;, &quot; &quot;).csv(Seq(&quot;jeffy&quot;, &quot;katy&quot;).toDS()).show()
spark.read.option(&quot;delimiter&quot;, &quot; &quot;).csv(Seq(&quot;jeffy&quot;, &quot;katy&quot;).toDS()).show()sca
ds.write.mode(SaveMode.Overwrite).option(&quot;sep&quot;, &quot;|&quot;).csv(s&quot;$&#123;path&#125;&quot;)
</code></pre>
<p><code>2、header(默认是false) 表示是否将csv文件中的第一行作为schema(读写参数)</code></p>
<pre><code class="scala">spark.read.option(&quot;header&quot;, true).csv(s&quot;$&#123;path&#125;&quot;)
</code></pre>
<p><code>3.inferSchema 表示是否支持从数据中推导出schema(只读参数)</code></p>
<pre><code class="scala">spark.read.option(&quot;header&quot;, true).option(&quot;inferSchema&quot;, true).csv(s&quot;$&#123;path&#125;&quot;)
</code></pre>
<p><code>4.charset和encoding(默认是UTF-8)，根据指定的编码器对csv文件进行解码(只读参数)</code></p>
<pre><code class="scala">spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;encoding&quot;, &quot;iso-8859-1&quot;).option(&quot;sep&quot;, &quot;þ&quot;).csv(s&quot;$&#123;path&#125;&quot;).show()
</code></pre>
<p><code>5.quote(默认值是&quot; ) 表示将不需要切割的字段值用quote标记起来(读写参数)</code></p>
<pre><code> var optMap = Map(&quot;quote&quot; -&gt; &quot;\&#39;&quot;, &quot;delimiter&quot; -&gt; &quot; &quot;)
    spark.read.options(optMap).csv(Seq(&quot;23 &#39;jeffy tang&#39;&quot;, &quot;34 katy&quot;).toDS()).show()
</code></pre>
<p>6.escape(默认值是<code>\</code>) 如果在quote标记的字段值中还含有quote,则用escape来避免(读写参数)</p>
<pre><code class="scala">val optMap = Map(&quot;quote&quot; -&gt; &quot;\&#39;&quot;, &quot;delimiter&quot; -&gt; &quot; &quot;, &quot;escape&quot; -&gt; &quot;\&quot;&quot;)
spark.read.options(optMap).csv(Seq(&quot;23 &#39;jeffy \&quot;&#39;tang&#39;&quot;, &quot;34 katy&quot;).toDS()).show()
</code></pre>
<p><code>7.comment(默认是空字符串，表示关闭这个功能) 表示csv中的注释的标记符(读写参数)</code></p>
<pre><code class="scala">val optMap = Map(&quot;comment&quot; -&gt; &quot;~&quot;, &quot;header&quot; -&gt; &quot;false&quot;)
spark.read.options(optMap).csv(s&quot;$&#123;BASE_PATH&#125;/comments.csv&quot;).show()
</code></pre>
<p><code>8.(读写参数)ignoreLeadingWhiteSpace(默认是false) 表示是否忽略字段值前面的空格 /ignoreTrailingWhiteSpace(默认是false) 表示是否忽略字段值后面的空格</code></p>
<pre><code class="scala"> val optMap = Map(&quot;ignoreLeadingWhiteSpace&quot; -&gt; &quot;true&quot;, &quot;ignoreTrailingWhiteSpace&quot; -&gt; &quot;true&quot;)
 spark.read.options(optMap).csv(Seq(&quot; a,b  , c &quot;).toDS()).show()
</code></pre>
<p><code>9. multiLine(默认是false) 是否支持一条记录被拆分成了多行的csv的读取解析(类似于execl单元格多行)(只读参数)</code></p>
<pre><code class="scala">spark.read.option(&quot;header&quot;, true).option(&quot;multiLine&quot;, true).csv(s&quot;$&#123;path&#125;&quot;).show()
</code></pre>
<p><code>10 、mode(默认是PERMISSIVE) (只读参数)</code></p>
<p><code>\``1) PERMISSIVE 表示碰到解析错误的时候，将字段都置为null</code></p>
<p><code>\``2) DROPMALFORMED 表示忽略掉解析错误的记录</code></p>
<pre><code class="scala">3) FAILFAST 当有解析错误的时候，立马抛出异常
 spark.read.option(&quot;mode&quot;, &quot;PERMISSIVE&quot;).schema(schema).csv(s&quot;$&#123;path&#125;&quot;)
</code></pre>
<p><code>11. nullValue(默认是空字符串)， 表示需要将nullValue指定的字符串解析成null(读写参数)</code></p>
<pre><code class="scala">spark.read.option(&quot;nullValue&quot;, &quot;--&quot;).csv(Seq(&quot;0,2013-11-11,--&quot;, &quot;1,1983-08-04,3&quot;).toDS()).show()
</code></pre>
<p><code>12.nanValue(默认值为NaN) (只读参数)</code></p>
<p><code>1) positiveInf</code></p>
<p><code>2) negativeInf</code></p>
<pre><code class="scala">   spark.read.format(&quot;csv&quot;).schema(StructType(List(
        StructField(&quot;int&quot;, IntegerType, true),
        StructField(&quot;long&quot;, LongType, true),
        StructField(&quot;float&quot;, FloatType, true),
        StructField(&quot;double&quot;, DoubleType, true)
      ))).options(Map(
        &quot;header&quot; -&gt; &quot;true&quot;,
        &quot;mode&quot; -&gt; &quot;DROPMALFORMED&quot;,
        &quot;nullValue&quot; -&gt; &quot;--&quot;,
        &quot;nanValue&quot; -&gt; &quot;NAN&quot;,
        &quot;negativeInf&quot; -&gt; &quot;-INF&quot;,
        &quot;positiveInf&quot; -&gt; &quot;INF&quot;)).load(s&quot;$&#123;BASE_PATH&#125;/numbers.csv&quot;)
</code></pre>
<p><code>13.codec和compression 压缩格式，支持的压缩格式有：</code></p>
<p><code>none 和 uncompressed表示不压缩;</code></p>
<p><code>bzip2、deflate、gzip、lz4、snappy (只写参数)</code></p>
<pre><code class="scala">inferSchemaDF.write.mode(SaveMode.Overwrite).option(&quot;compression&quot;, &quot;gzip&quot;).csv(s&quot;$&#123;path&#125;&quot;)
</code></pre>
<p><code>14.maxColumns(默认是20480) 规定一个csv的一条记录最大的列数 (只读参数)</code></p>
<pre><code class="scala"> spark.read.option(&quot;maxColumns&quot;, &quot;3&quot;).csv(Seq(&quot;test,as,g&quot;, &quot;h,bm,s&quot;).toDS()).show() //会报错
</code></pre>
</li>
</ul>
<h2 id="Spark-Kudu的广告业务项目实战笔记"><a href="#Spark-Kudu的广告业务项目实战笔记" class="headerlink" title="Spark+Kudu的广告业务项目实战笔记"></a>Spark+Kudu的广告业务项目实战笔记</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p><code>本项目需要实现：将广告数据的json文件放置在HDFS上，并利用spark进行ETL操作、分析操作，之后存储在kudu上，最后设定每天凌晨三点自动执行广告数据的分析存储操作。</code></p>
<h3 id="2-项目需求"><a href="#2-项目需求" class="headerlink" title="2.项目需求"></a>2.项目需求</h3><p><code>数据ETL：原始文件为JSON格式数据，需原始文件与IP库中数据进行解析</code></p>
<p><code>统计各省市的地域分布情况</code></p>
<p><code>统计广告投放的地域分布情况</code></p>
<p><code>统计广告投放APP分布情况</code></p>
<h3 id="3-项目架构"><a href="#3-项目架构" class="headerlink" title="3.项目架构"></a>3.项目架构</h3><img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image-20200823135336754.png" alt="image-20200823135336754" style="zoom:50%;">

<h3 id="4-日志字段"><a href="#4-日志字段" class="headerlink" title="4.日志字段"></a>4.日志字段</h3><pre><code class="JSON">&#123;
  &quot;sessionid&quot;: &quot;qld2dU4cfhEa3yhADzgphOf3ySv9vMml&quot;,
  &quot;advertisersid&quot;: 66,
  &quot;adorderid&quot;: 142848,
  &quot;adcreativeid&quot;: 212312,
  &quot;adplatformproviderid&quot;: 174663,
  &quot;sdkversion&quot;: &quot;Android 5.0&quot;,
  &quot;adplatformkey&quot;: &quot;PLMyYnDKQgOPL55frHhxkUIQtBThHfHq&quot;,
  &quot;putinmodeltype&quot;: 1,
  &quot;requestmode&quot;: 1,
  &quot;adprice&quot;: 8410.0,
  &quot;adppprice&quot;: 5951.0,
  &quot;requestdate&quot;: &quot;2018-10-07&quot;,
  &quot;ip&quot;: &quot;182.91.190.221&quot;,
  &quot;appid&quot;: &quot;XRX1000014&quot;,
  &quot;appname&quot;: &quot;支付宝 - 让生活更简单&quot;,
  &quot;uuid&quot;: &quot;QtxDH9HUueM2IffUe8z2UqLKuZueZLqq&quot;,
  &quot;device&quot;: &quot;HUAWEI GX1手机&quot;,
  &quot;client&quot;: 1,
  &quot;osversion&quot;: &quot;&quot;,
  &quot;density&quot;: &quot;&quot;,
  &quot;pw&quot;: 1334,
  &quot;ph&quot;: 750,
  &quot;lang&quot;: &quot;&quot;,
  &quot;lat&quot;: &quot;&quot;,
  &quot;provincename&quot;: &quot;&quot;,
  &quot;cityname&quot;: &quot;&quot;,
  &quot;ispid&quot;: 46007,
  &quot;ispname&quot;: &quot;移动&quot;,
  &quot;networkmannerid&quot;: 1,
  &quot;networkmannername&quot;: &quot;4G&quot;,
  &quot;iseffective&quot;: 1,
  &quot;isbilling&quot;: 1,
  &quot;adspacetype&quot;: 3,
  &quot;adspacetypename&quot;: &quot;全屏&quot;,
  &quot;devicetype&quot;: 1,
  &quot;processnode&quot;: 3,
  &quot;apptype&quot;: 0,
  &quot;district&quot;: &quot;district&quot;,
  &quot;paymode&quot;: 1,
  &quot;isbid&quot;: 1,
  &quot;bidprice&quot;: 6812.0,
  &quot;winprice&quot;: 89934.0,
  &quot;iswin&quot;: 0,
  &quot;cur&quot;: &quot;rmb&quot;,
  &quot;rate&quot;: 0.0,
  &quot;cnywinprice&quot;: 0.0,
  &quot;imei&quot;: &quot;&quot;,
  &quot;mac&quot;: &quot;52:54:00:41:ba:02&quot;,
  &quot;idfa&quot;: &quot;&quot;,
  &quot;openudid&quot;: &quot;FIZHDPIKQYVNHOHOOAWMTQDFTPNWAABZTAFVHTEL&quot;,
  &quot;androidid&quot;: &quot;&quot;,
  &quot;rtbprovince&quot;: &quot;&quot;,
  &quot;rtbcity&quot;: &quot;&quot;,
  &quot;rtbdistrict&quot;: &quot;&quot;,
  &quot;rtbstreet&quot;: &quot;&quot;,
  &quot;storeurl&quot;: &quot;&quot;,
  &quot;realip&quot;: &quot;182.92.196.236&quot;,
  &quot;isqualityapp&quot;: 0,
  &quot;bidfloor&quot;: 0.0,
  &quot;aw&quot;: 0,
  &quot;ah&quot;: 0,
  &quot;imeimd5&quot;: &quot;&quot;,
  &quot;macmd5&quot;: &quot;&quot;,
  &quot;idfamd5&quot;: &quot;&quot;,
  &quot;openudidmd5&quot;: &quot;&quot;,
  &quot;androididmd5&quot;: &quot;&quot;,
  &quot;imeisha1&quot;: &quot;&quot;,
  &quot;macsha1&quot;: &quot;&quot;,
  &quot;idfasha1&quot;: &quot;&quot;,
  &quot;openudidsha1&quot;: &quot;&quot;,
  &quot;androididsha1&quot;: &quot;&quot;,
  &quot;uuidunknow&quot;: &quot;&quot;,
  &quot;userid&quot;: &quot;vtUO8pPXfwdsPnvo6ttNGhAAnHi8NVbA&quot;,
  &quot;reqdate&quot;: null,
  &quot;reqhour&quot;: null,
  &quot;iptype&quot;: 1,
  &quot;initbidprice&quot;: 0.0,
  &quot;adpayment&quot;: 175547.0,
  &quot;agentrate&quot;: 0.0,
  &quot;lomarkrate&quot;: 0.0,
  &quot;adxrate&quot;: 0.0,
  &quot;title&quot;: &quot;中信建投首次公开发行股票发行结果 本次发行价格为5.42元/股&quot;,
  &quot;keywords&quot;: &quot;IPO,中信建投证券,股票,投资,财经&quot;,
  &quot;tagid&quot;: &quot;rBRbAEQhkcAaeZ6XlTrGXOxyw6w9JQ7x&quot;,
  &quot;callbackdate&quot;: &quot;2018-10-07&quot;,
  &quot;channelid&quot;: &quot;123528&quot;,
  &quot;mediatype&quot;: 2,
  &quot;email&quot;: &quot;e4aqd67bo@263.net&quot;,
  &quot;tel&quot;: &quot;13105823726&quot;,
  &quot;age&quot;: &quot;29&quot;,
  &quot;sex&quot;: &quot;0&quot;
&#125;
</code></pre>
<h3 id="5-IP规则库解析"><a href="#5-IP规则库解析" class="headerlink" title="5.IP规则库解析"></a>5.IP规则库解析</h3><p><code>本项目利用IP规则库进行解析，在生产中应该需要专门的公司提供的IP服务。IP规则库中的一条如下：</code></p>
<pre><code class="python">1.0.1.0|1.0.3.255|16777472|16778239|亚洲|中国|福建|福州||电信|350100|China|CN|119.306239|26.075302
其中第三列是该段ip起始地址（十进制），第四列是ip终止地址（十进制）。
</code></pre>
<h3 id="6-代码实现"><a href="#6-代码实现" class="headerlink" title="6.代码实现"></a>6.代码实现</h3><p><code>LogETLApp.scala代码</code></p>
<pre><code class="scala">import com.imooc.bigdata.cp08.utils.IPUtils
import org.apache.spark.sql.SparkSession

object LogETLApp &#123;

  def main(args: Array[String]): Unit = &#123;

    //启动本地模式的spark
    val spark = SparkSession.builder()
      .master(&quot;local[2]&quot;)
      .appName(&quot;LogETLApp&quot;)
      .getOrCreate()

    //使用DataSourceAPI直接加载json数据
    var jsonDF = spark.read.json(&quot;data-test.json&quot;)
    //jsonDF.printSchema()
    //jsonDF.show(false)

    //导入隐式转换
    import spark.implicits._
    //加载IP库,建议将RDD转成DF
    val ipRowRDD = spark.sparkContext.textFile(&quot;ip.txt&quot;)
    val ipRuleDF = ipRowRDD.map(x =&gt; &#123;
      val splits = x.split(&quot;\\|&quot;)
      val startIP = splits(2).toLong
      val endIP = splits(3).toLong
      val province = splits(6)
      val city = splits(7)
      val isp = splits(9)

      (startIP, endIP, province, city, isp)
    &#125;).toDF(&quot;start_ip&quot;, &quot;end_ip&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)
    //ipRuleDF.show(false)

    //利用Spark SQL UDF转换json中的ip
    import org.apache.spark.sql.functions._
    def getLongIp() = udf((ip:String)=&gt;&#123;
      IPUtils.ip2Long(ip)
    &#125;)

    //添加字段传入十进制IP
    jsonDF = jsonDF.withColumn(&quot;ip_long&quot;,
      getLongIp()($&quot;ip&quot;))

    //将日志每一行的ip对应省份、城市、运行商进行解析
    //两个DF进行join，条件是：json中的ip在规则ip中的范围内
    val result =jsonDF.join(ipRuleDF,jsonDF(&quot;ip_long&quot;)
      .between(ipRuleDF(&quot;start_ip&quot;),ipRuleDF(&quot;end_ip&quot;)))
   // 等价于sql
   //  &quot; select * from logs left join &quot; +
   // &quot;ips on logs.ip_long between ips.start_ip and ips.end_ip &quot;
      
    val tableName = &quot;ods&quot;
    val partitionId = &quot;ip&quot;
    val schema = SchemaUtils.ODSSchema

    KuduUtils.sink(result,tableName,masterAddresses,schema,partitionId)

    spark.stop()
  &#125;
&#125;
</code></pre>
<p><code>工具类中将字符串转成十进制的IPUtils.scala：</code></p>
<pre><code class="scala">object IPUtils &#123;

  //字符串-&gt;十进制
  def ip2Long(ip:String)=&#123;
    val splits = ip.split(&quot;[.]&quot;)
    var ipNum = 0L

    for(i&lt;-0 until(splits.length))&#123;
      //“|”是按位或操作，有1即1，全0则0
      //“&lt;&lt;”是整体左移
      //也就是说每一个数字算完向前移动8位接下一个数字
      ipNum = splits(i).toLong | ipNum &lt;&lt; 8L
    &#125;
    ipNum
  &#125;

  def main(args: Array[String]): Unit = &#123;
    println(ip2Long(&quot;1.1.1.1&quot;))
  &#125;
&#125;
</code></pre>
<p><code>SchemaUtils.scala</code></p>
<pre><code class="scala">lazy val ODSSchema: Schema = &#123;
    val columns = List(
      new ColumnSchemaBuilder(&quot;ip&quot;, Type.STRING).nullable(false).key(true).build(),
      new ColumnSchemaBuilder(&quot;sessionid&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;advertisersid&quot;,Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;adorderid&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;adcreativeid&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;adplatformproviderid&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;sdkversion&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;adplatformkey&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;putinmodeltype&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;requestmode&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;adprice&quot;, Type.DOUBLE).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;adppprice&quot;, Type.DOUBLE).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;requestdate&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;appid&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;appname&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;uuid&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;device&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;client&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;osversion&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;density&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;pw&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;ph&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;provincename&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;cityname&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;ispid&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;ispname&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;isp&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;networkmannerid&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;networkmannername&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;iseffective&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;isbilling&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;adspacetype&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;adspacetypename&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;devicetype&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;processnode&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;apptype&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;district&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;paymode&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;isbid&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;bidprice&quot;, Type.DOUBLE).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;winprice&quot;, Type.DOUBLE).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;iswin&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;cur&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;rate&quot;, Type.DOUBLE).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;cnywinprice&quot;, Type.DOUBLE).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;imei&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;mac&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;idfa&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;openudid&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;androidid&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;rtbprovince&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;rtbcity&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;rtbdistrict&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;rtbstreet&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;storeurl&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;realip&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;isqualityapp&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;bidfloor&quot;, Type.DOUBLE).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;aw&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;ah&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;imeimd5&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;macmd5&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;idfamd5&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;openudidmd5&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;androididmd5&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;imeisha1&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;macsha1&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;idfasha1&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;openudidsha1&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;androididsha1&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;uuidunknow&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;userid&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;iptype&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;initbidprice&quot;, Type.DOUBLE).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;adpayment&quot;, Type.DOUBLE).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;agentrate&quot;, Type.DOUBLE).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;lomarkrate&quot;, Type.DOUBLE).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;adxrate&quot;, Type.DOUBLE).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;title&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;keywords&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;tagid&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;callbackdate&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;channelid&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;mediatype&quot;, Type.INT64).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;email&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;tel&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;sex&quot;, Type.STRING).nullable(false).build(),
      new ColumnSchemaBuilder(&quot;age&quot;, Type.STRING).nullable(false).build()
    ).asJava
    new Schema(columns)
  &#125;

lazy val APPSchema: Schema = &#123;
    val columns = List(
new ColumnSchemaBuilder(&quot;appid&quot;, Type.STRING).nullable(false).key(true).build(),
new ColumnSchemaBuilder(&quot;appname&quot;, Type.STRING).nullable(false).key(true).build(),
new ColumnSchemaBuilder(&quot;origin_request&quot;, Type.INT64).nullable(false).build(),
new ColumnSchemaBuilder(&quot;valid_request&quot;, Type.INT64).nullable(false).build(),
new ColumnSchemaBuilder(&quot;ad_request&quot;, Type.INT64).nullable(false).build(),
new ColumnSchemaBuilder(&quot;bid_cnt&quot;, Type.INT64).nullable(false).build(),
new ColumnSchemaBuilder(&quot;bid_success_cnt&quot;, Type.INT64).nullable(false).build(),
new ColumnSchemaBuilder(&quot;bid_success_rate&quot;, Type.DOUBLE).nullable(false).build(),
new ColumnSchemaBuilder(&quot;ad_display_cnt&quot;, Type.INT64).nullable(false).build(),
new ColumnSchemaBuilder(&quot;ad_click_cnt&quot;, Type.INT64).nullable(false).build(),
new ColumnSchemaBuilder(&quot;ad_click_rate&quot;, Type.DOUBLE).nullable(false).build(),
new ColumnSchemaBuilder(&quot;ad_consumption&quot;, Type.DOUBLE).nullable(false).build(),
new ColumnSchemaBuilder(&quot;ad_cost&quot;, Type.DOUBLE).nullable(false).build()
    ).asJava
new Schema(columns)
  &#125;
</code></pre>
<p><code>KuduUtils.scala</code></p>
<pre><code class="scala">import java.util

import com.imooc.bigdata.chapter08.utils.SchemaUtils
import org.apache.kudu.Schema
import org.apache.kudu.client.&#123;CreateTableOptions, KuduClient&#125;
import org.apache.kudu.client.KuduClient.KuduClientBuilder
import org.apache.spark.sql.&#123;DataFrame, SaveMode&#125;
  
object KuduUtils &#123;

  /**
    * 将DF数据落地到Kudu
    * @param data DF结果集
    * @param tableName  Kudu目标表
    * @param master Kudu的Master地址
    * @param schema Kudu的schema信息
    * @param partitionId  Kudu表的分区字段
    */
  def sink(data:DataFrame,
           tableName:String,
           master:String,
           schema:Schema,
           partitionId:String)=&#123;
    val client = new KuduClientBuilder(master).build()

    if(client.tableExists(tableName))&#123;
      client.deleteTable(tableName)
    &#125;

    val options = new CreateTableOptions()
    options.setNumReplicas(1)

    val parcols = new util.LinkedList[String]()
    parcols.add(partitionId)
    options.addHashPartitions(parcols,3)

    client.createTable(tableName,schema,options)

    //数据写入Kudu
    data.write.mode(SaveMode.Append)
      .format(&quot;org.apache.kudu.spark.kudu&quot;)
      .option(&quot;kudu.table&quot;,tableName)
      .option(&quot;kudu.master&quot;,master)
      .save()
 
//    spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)
//      .option(&quot;kudu.master&quot;,master)
//      .option(&quot;kudu.table&quot;,tableName)
//      .load().show()
  &#125;
&#125;
</code></pre>
<h3 id="7-功能二开发"><a href="#7-功能二开发" class="headerlink" title="7.功能二开发"></a><strong>7.功能二开发</strong></h3><h3 id="统计省份、城市数量分布情况，按照provincename与cityname分组统计"><a href="#统计省份、城市数量分布情况，按照provincename与cityname分组统计" class="headerlink" title="统计省份、城市数量分布情况，按照provincename与cityname分组统计"></a><code>统计省份、城市数量分布情况，按照provincename与cityname分组统计</code></h3><p>ProvinceCityStatApp。scala</p>
<pre><code class="scala">import com.imooc.bigdata.cp08.utils.SQLUtils
import org.apache.spark.sql.SparkSession

object ProvinceCityStatApp &#123;

  def main(args: Array[String]): Unit = &#123;
    val spark = SparkSession.builder()
      .master(&quot;local[2]&quot;)
      .appName(&quot;ProvinceCityStatApp&quot;)
      .getOrCreate()

    //从Kudu的ods表中读取数据，然后按照省份和城市分组即可
    val sourceTableName = &quot;ods&quot;
    val masterAddress = &quot;hadoop000&quot;

    val odsDF = spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)
      .option(&quot;kudu.table&quot;, sourceTableName)
      .option(&quot;kudu.master&quot;, masterAddress)
      .load()
    //odsDF.show(false)

    odsDF.createOrReplaceTempView(&quot;ods&quot;)
    val result = spark.sql(SQLUtils.PROVINCE_CITY_SQL)
    result.show(false)

    spark.stop()

  &#125;

&#125;
</code></pre>
<p>SQLUtils.scala</p>
<pre><code class="scala">lazy val PROVINCE_CITY_SQL = &quot;select provincename,cityname,count(1) as cnt from ods group by provincename,cityname&quot; 
lazy val PROVINCE_CITY_SQL = &quot;select provincename,cityname,count(1) as cnt from ods group by provincename,cityname&quot;
</code></pre>
<h3 id="8-代码重构"><a href="#8-代码重构" class="headerlink" title="**8:代码重构 **"></a>**8:代码重构 **</h3><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2O2R2sWRGyQHreSAGqmpRwtXN0gP4jqCyFr7X6ibkI7EMqhv399ADbBv6aHnMnKMicELEjGpPWuwDhA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img" style="zoom:50%;">

<h3 id="trait"><a href="#trait" class="headerlink" title="trait"></a><code>trait</code></h3><!--Scala Trait(特征) 相当于 Java 的接口，实际上它比接口还功能强大。与接口不同的是，它还可以定义属性和方法的实现。一般情况下Scala的类只能够继承单一父类，但是如果是 Trait(特征) 的话就可以继承多个，从结果来看就是实现了多重继承。-->

<pre><code class="scala">import org.apache.spark.sql.SparkSession

//顶层数据处理接口
trait DataProcess &#123;

  def process(spark:SparkSession)

&#125;
</code></pre>
<h3 id="Processor"><a href="#Processor" class="headerlink" title="Processor"></a><code>Processor</code></h3><h3 id="3-1-需求一：ETL的Processor"><a href="#3-1-需求一：ETL的Processor" class="headerlink" title="3.1 需求一：ETL的Processor"></a><code>3.1 需求一：ETL的Processor</code></h3><pre><code class="scala">import com.imooc.bigdata.cp08.`trait`.DataProcess
import com.imooc.bigdata.cp08.utils.&#123;IPUtils, KuduUtils, SQLUtils, SchemaUtils&#125;
import org.apache.spark.sql.SparkSession

object LogETLProcessor extends DataProcess&#123;
  override def process(spark: SparkSession): Unit = &#123;

    //使用DataSourceAPI直接加载json数据
    var jsonDF = spark.read.json(&quot;D:\\Hadoop基础与电商行为日志分析\\spark\\coding385\\sparksql-train\\data\\data-test.json&quot;)
    //jsonDF.printSchema()
    //jsonDF.show(false)

    //导入隐式转换
    import spark.implicits._
    //加载IP库,建议将RDD转成DF
    val ipRowRDD = spark.sparkContext.textFile(&quot;D:\\Hadoop基础与电商行为日志分析\\spark\\coding385\\sparksql-train\\data\\ip.txt&quot;)
    val ipRuleDF = ipRowRDD.map(x =&gt; &#123;
      val splits = x.split(&quot;\\|&quot;)
      val startIP = splits(2).toLong
      val endIP = splits(3).toLong
      val province = splits(6)
      val city = splits(7)
      val isp = splits(9)

      (startIP, endIP, province, city, isp)
    &#125;).toDF(&quot;start_ip&quot;, &quot;end_ip&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)
    //ipRuleDF.show(false)

    //利用Spark SQL UDF转换json中的ip
    import org.apache.spark.sql.functions._
    def getLongIp() = udf((ip:String)=&gt;&#123;
      IPUtils.ip2Long(ip)
    &#125;)

    //添加字段传入十进制IP
    jsonDF = jsonDF.withColumn(&quot;ip_long&quot;,
      getLongIp()($&quot;ip&quot;))

    //将日志每一行的ip对应省份、城市、运行商进行解析
    //两个DF进行join，条件是：json中的ip在规则ip中的范围内
    //    val result = jsonDF.join(ipRuleDF, jsonDF(&quot;ip_long&quot;)
    //      .between(ipRuleDF(&quot;start_ip&quot;), ipRuleDF(&quot;end_ip&quot;)))
    //      //.show(false)

    //用SQL的方式完成
    jsonDF.createOrReplaceTempView(&quot;logs&quot;)
    ipRuleDF.createOrReplaceTempView(&quot;ips&quot;)
    val sql = SQLUtils.SQL
    val result = spark.sql(sql)
    //.show(false)

    //Kudu
    val masterAddresses = &quot;hadoop000&quot;
    val tableName = &quot;ods&quot;
    val partitionId = &quot;ip&quot;
    val schema = SchemaUtils.ODSSchema

    KuduUtils.sink(result,tableName,masterAddresses,schema,partitionId)
    spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)
      .option(&quot;kudu.master&quot;,masterAddresses)
     .option(&quot;kudu.table&quot;,tableName)
      .load().show()

  &#125;

&#125;
</code></pre>
<h3 id="3-2-需求二：ProvinceCityStatProcessor"><a href="#3-2-需求二：ProvinceCityStatProcessor" class="headerlink" title="3.2 需求二：ProvinceCityStatProcessor"></a><code>3.2 需求二：ProvinceCityStatProcessor</code></h3><pre><code class="scala">import com.imooc.bigdata.cp08.`trait`.DataProcess
import com.imooc.bigdata.cp08.utils.&#123;KuduUtils, SQLUtils, SchemaUtils&#125;
import org.apache.spark.sql.SparkSession

object ProvinceCityStatProcessor extends DataProcess&#123;
  override def process(spark: SparkSession): Unit = &#123;

    //从Kudu的ods表中读取数据，然后按照省份和城市分组即可
    val sourceTableName = &quot;ods&quot;
    val masterAddress = &quot;hadoop000&quot;

    val odsDF = spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)
      .option(&quot;kudu.table&quot;, sourceTableName)
      .option(&quot;kudu.master&quot;, masterAddress)
      .load()
    //odsDF.show(false)

    odsDF.createOrReplaceTempView(&quot;ods&quot;)
    val result = spark.sql(SQLUtils.PROVINCE_CITY_SQL)
    //result.show(false)

    //Kudu
    val sinkTableName = &quot;province_city_stat&quot;
    val partitionId = &quot;provincename&quot;
    val schema = SchemaUtils.ProvinceCitySchema


    KuduUtils.sink(result,sinkTableName,masterAddress,schema,partitionId)
    spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)
      .option(&quot;kudu.master&quot;,masterAddress)
      .option(&quot;kudu.table&quot;,sinkTableName)
      .load().show()
  &#125;
&#125;
</code></pre>
<h3 id="项目入口"><a href="#项目入口" class="headerlink" title="项目入口"></a>项目入口</h3><pre><code class="scala">import com.imooc.bigdata.cp08.business.&#123;LogETLProcessor, ProvinceCityStatProcessor&#125;
import org.apache.spark.sql.SparkSession

//整个项目的入口
object SparkApp &#123;

  def main(args: Array[String]): Unit = &#123;
    val spark = SparkSession.builder()
      .master(&quot;local[2]&quot;)
      .appName(&quot;SparkApp&quot;)
      .getOrCreate()

    //ETL
    LogETLProcessor.process(spark)
    //省份
    ProvinceCityStatProcessor.process(spark)

    spark.stop()
  &#125;

&#125;
</code></pre>
<p><code>9 、实现需求四：APP统计。需求如下：</code></p>
<img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2O2R2sWRGyQHreSAGqmpRwta1qiaJBqc9MolImXDQRB6dTMfWgD5yv6Dfl9qQ4iaonWUsfv4g6ibpDOg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img" style="zoom:50%;">

<p>AreaStatProcessor.scala</p>
<pre><code class="scala">import com.imooc.bigdata.cp08.`trait`.DataProcess
import com.imooc.bigdata.cp08.utils.&#123;KuduUtils, SQLUtils, SchemaUtils&#125;
import org.apache.spark.sql.SparkSession
 
object AreaStatProcessor extends DataProcess&#123;
  override def process(spark: SparkSession): Unit = &#123;
    val sourceTableName = &quot;ods&quot;
    val masterAddresses = &quot;hadoop000&quot;
 
    val odsDF = spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)
      .option(&quot;kudu.table&quot;,sourceTableName)
      .option(&quot;kudu.master&quot;,masterAddresses)
      .load()
 
    odsDF.createOrReplaceTempView(&quot;ods&quot;)
 
    val resultTmp = spark.sql(SQLUtils.AREA_SQL_STEP1)
    resultTmp.show()
      
    val sinkTableName = &quot;app_stat&quot;
    val partitionId = &quot;appid&quot;
    val schema = SchemaUtils.APPSchema

    KuduUtils.sink(result,sinkTableName,masterAddresses,schema,partitionId)
    spark.read.format(&quot;org.apache.kudu.spark.kudu&quot;)
      .option(&quot;kudu.master&quot;,masterAddresses)
      .option(&quot;kudu.table&quot;,sinkTableName)
      .load().show()
 
  &#125;
&#125;
</code></pre>
<p>SQLUtils.scala</p>
<pre><code class="scala">lazy val AREA_SQL_STEP1 = &quot;select provincename,cityname, &quot; +
    &quot;sum(case when requestmode=1 and processnode &gt;=1 then 1 else 0 end) origin_request,&quot; +
    &quot;sum(case when requestmode=1 and processnode &gt;=2 then 1 else 0 end) valid_request,&quot; +
    &quot;sum(case when requestmode=1 and processnode =3 then 1 else 0 end) ad_request,&quot; +
    &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and isbid=1 and adorderid!=0 then 1 else 0 end) bid_cnt,&quot; +
    &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 then 1 else 0 end) bid_success_cnt,&quot; +
    &quot;sum(case when requestmode=2 and iseffective=1 then 1 else 0 end) ad_display_cnt,&quot; +
    &quot;sum(case when requestmode=3 and processnode=1 then 1 else 0 end) ad_click_cnt,&quot; +
    &quot;sum(case when requestmode=2 and iseffective=1 and isbilling=1 then 1 else 0 end) medium_display_cnt,&quot; +
    &quot;sum(case when requestmode=3 and iseffective=1 and isbilling=1 then 1 else 0 end) medium_click_cnt,&quot; +
    &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 and adorderid&gt;20000  then 1*winprice/1000 else 0 end) ad_consumption,&quot; +
    &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 and adorderid&gt;20000  then 1*adpayment/1000 else 0 end) ad_cost &quot; +
    &quot;from ods group by provincename,cityname&quot;

lazy val APP_SQL_STEP2 = &quot;select appid,appname, &quot; +
    &quot;origin_request,&quot; +
    &quot;valid_request,&quot; +
    &quot;ad_request,&quot; +
    &quot;bid_cnt,&quot; +
    &quot;bid_success_cnt,&quot; +
    &quot;bid_success_cnt/bid_cnt bid_success_rate,&quot; +
    &quot;ad_display_cnt,&quot; +
    &quot;ad_click_cnt,&quot; +
    &quot;ad_click_cnt/ad_display_cnt ad_click_rate,&quot; +
    &quot;ad_consumption,&quot; +
    &quot;ad_cost from app_tmp &quot; +
    &quot;where bid_cnt!=0 and ad_display_cnt!=0&quot;
</code></pre>
<h2 id="Flink-CEP-原理和案例详解"><a href="#Flink-CEP-原理和案例详解" class="headerlink" title="Flink CEP 原理和案例详解"></a>Flink CEP 原理和案例详解</h2><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1 概念"></a>1 概念</h3><p><em><strong><code>（1）定义</code></strong></em></p>
<!--复合事件处理（Complex Event Processing，CEP）是一种基于动态环境中事件流的分析技术，事件在这里通常是有意义的状态变化，通过分析事件间的关系，利用过滤、关联、聚合等技术，根据事件间的时序关系和聚合关系制定检测规则，持续地从事件流中查询出符合要求的事件序列，最终分析得到更复杂的复合事件。-->

<p><em><strong><code>(2）特征</code></strong></em><br>CEP的特征如下:<br>   <!--目标：从有序的简单事件流中发现一些高阶特征；--><br>   <!--输入：一个或多个简单事件构成的事件流；--><br>   <!--处理：识别简单事件之间的内在联系，多个符合一定规则的简单事件构成复杂事件；--><br>  <!--输出：满足规则的复杂事件。--></p>
<h2 id><a href="#" class="headerlink" title></a></h2><h2 id="Spark-封装代码："><a href="#Spark-封装代码：" class="headerlink" title="Spark 封装代码："></a><strong>Spark 封装代码：</strong></h2><p><em><strong>1：单例模式：</strong></em></p>
<pre><code class="scala">object SQLContextSingleton &#123;
  @transient  private var instance: SQLContext = _
  def getInstance(sparkContext: SparkContext): SQLContext = &#123;
    if (instance == null) &#123;
      instance = new SQLContext(sparkContext)
    &#125;
    instance


  &#125;
&#125;
</code></pre>
<h2 id="Hive数仓"><a href="#Hive数仓" class="headerlink" title="Hive数仓"></a><strong>Hive数仓</strong></h2><p><em><strong>建模理论</strong></em></p>
<pre><code class="python">/**
ER模型
*/

        此建模方法是从全企业的高度设计一个符合第三范式(3NF)模型，用实体关系(ER)模型描述企业业务。数据仓库中3NF和OLTP系统中的3NF的区别是它站在全企业角度面向主题的抽象，而不是针对某个具体的业务流程的实体对象关系的抽象。换句话说，设计数据仓库既不面向功能也不面向应用，是为了满足全企业的数据需求，而不是为了满足某个部门的特定分析需求。

        采用ER模型设计数据仓库模型的出发点是为了整合或集成数据，将各个系统中的数据，以整个企业角度按照主题进行相似性组合和合并，并进行一致性处理，为数据分析决策服务，但不能直接用于分析决策（因为分析决策的功能不在数仓层）。数据建模分为三个层次：高层建模（称为实体关系图，或ERD），中间层建模（数据项集或DIS）和底层建模（物理模型）。

高层模型：是实体的最高抽象层模型，描述主要的主题或主题域的关系，用于描述企业的业务总体概况。

中间层模型：为高层模型标示出的每个主要主题或主题域再进一步细化数据项，是每个主题域的进一步扩展。

底层模型：在中间层模型的基础上，考虑物理存储和物理属性设计，可做一些表合并、分区设计，使模型中包含关键字(主键)和物理特性。

        ER模型的最佳实践是Teradata基于金融行业发布的FS-LDM逻辑数据模型，它通过对金融业务高度抽象和总结，将金融业务划分为10大金融主题（当事人、产品、协议、事件、资产、财务、机构、地域、营销和渠道），并以设计面向金融数仓模型的核心为基础，适当调整和扩展即可快速落地实施。
    
/**
维度模型
*/

        此建模方法是数据仓库大师Ralph Kimball所倡导的。是数据仓库工程领域最流行的数据仓库建模经典。

        维度建模从数据分析决策的需求出发构建模型，为分析需求服务，因此其重点是关注业务用户如何更快地完成需求分析，同时具有较好的查询性能

选择过程：识别出主要的业务过程。它们会充当事实表的源，用数值的、可累加的事实来填充事实表。

选择粒度：粒度的选择意味着准确确定事实表中的记录代表什么，记录细分程度。只有选择了维度的情况下，才能进行业务过程对应维度有条理探讨。

识别和一致化维度：选择了粒度后进行维度表设计。维度是事实表入口，是用户分析进行分组、筛选和检索的关键，需符合企业数据仓库总线前提下，一致化维度和一致化事实设计。

选择事实：确定分析需要衡量的指标。事实尽可能具有累加性。

        这些只是描述建模设计主要步骤，当然还有其他的步骤，如在事实表中存储预处理算法（事实表可累加事实之间的预处理显示存储），缓慢变化维度设计和物理设计等。
    
/**
常见分层架构
*/
## 操作数据层（ODS）：把源系统数据几乎无处理地存放到数据仓库中。
同步：结构化数据增量或全量同步到数据仓库Hive
结构化：把流式、批式半结构或非结构化数据经过结构化处理存储数据仓库Hive

## 公共维度模型层（CDM）：存放明细事实数据、维度数据及公共统一指标汇总数据，其中明细事实数据、维度表数据一般根据ODS层数据加工生成，公共统一指标汇总数据一般根据维度表和明细事实表加工生成。
CDM层又细分为DWD层和DWS层，分别是明细数据层和总汇层，用维度建模理论为基础，将维度退化到事实表中，减少了事实表和维度表的关联提高查询性能；同时汇总数据层，加强指标维度退化，采取构建大宽表构建公共统一指标数据层 ，提升指标的易用性和查询性能。主要功能如下：

组合和合并相似数据：采用明细宽表，复用关联计算，减少表之间的关联和数据扫描，合并不同业务为统一过程
统一指标加工：统一命名规范、统一数据类型、统一计算口径，为数据产品、应用和服务提供统一指标体系；建立逻辑汇总宽表。
建立一致性维度和事实：降低了粒度、计算口径、筛选和分组不一致的风险。
数据清洗：去重、去噪、空值转化、格式统一等数据清洗

## 数据标签层（TDM）：通过客户信息标签化，存放客户属性、行为、消费、社交和风险控制维度刻画客户的全貌信息。主要功能如下：
客户ID打通：ID-MApping，即把客户不同来源等身份标识通过数据手段识别为同一主体。完成对客户信息的全面刻画。
统一标签加工：统一标签分类、命名规范、数据类型、计算口径等，为数据分析与应用服务。

数据标签体系按照标签类型可分为统计类、规则类和机器学习挖掘类三大类型；亦可从用户属性、用户行为、用户消费、风险控制和社交属性共五大维度划分归类。如下：
</code></pre>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200924234141.jpg" alt="微信图片_20200924234141" style="zoom:50%;">

<pre><code class="python"> 数据标签体系建设涉及标签分类、命名规范和计算口径等元数据管理、标签开发、标签存储、标签应用、数据质量管理和数据价值管理内容较多，笔者会单独写文章详细分享。

## 应用数据层（ADS）：存放数据产品个性化的统计指标数据，CDM层和ODS层加工生成。
个性化指标加工：不公用型，复杂性（指数型、比值型、排名指标等）

基于应用数据组织：大宽表数据集市、横表转纵表等。
建模：
1：确定主题域
2：确定总线架构
3：划分主题域，构建总线矩阵
4：dw模型设计
5：维度建模
 选择业务过程--&gt;声明粒度--&gt;标识维度--&gt;标识事实
</code></pre>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200927230650.png" alt="微信图片_20200927230650" style="zoom:50%;">

<h4 id="事实和维度"><a href="#事实和维度" class="headerlink" title="事实和维度"></a><em><strong>事实和维度</strong></em></h4><pre><code class="python">##事实表：我们可以简单地将事实理解为现实中发生的一次操作型事件。比如订单表，我们就可以理解为一张事实表，我们每完成一个订单，就会在订单事实表中增加一条记录。
事实表产生于业务过程，存储了业务活动或事件提炼出来的性能度量。从最低的粒度级别来看，事实表行对应一个度量事件。
事实表根据粒度的角色划分不同，可分为事务事实表、周期快照事实表、累积快照事实表。

（1）事务事实表，用于承载事务数据，通常粒度比较低，它是面向事务的，其粒度是每一行对应一个事务，它是最细粒度的事实表，例如产品交易事务事实、ATM交易事务事实。

（2）周期快照事实表，按照一定的时间周期间隔(每天，每月)来捕捉业务活动的执行情况，一旦装入事实表就不会再去更新，它是事务事实表的补充。用来记录有规律的、固定时间间隔的业务累计数据，通常粒度比较高，例如账户月平均余额事实表。

（3）累积快照事实表，用来记录具有时间跨度的业务处理过程的整个过程的信息，每个生命周期一行，通常这类事实表比较少见。

注意：这里需要值得注意的是，在事实表的设计时，一定要注意一个事实表只能有一个粒度，不能将不同粒度的事实建立在同一张事实表中。

## 维度表：我们可以简单地理解维度表包含了事实表中指定属性的相关详细信息。比如商品维度表表和用户维度表。
维度表，一致性维度，业务过程的发生或分析角度，我们主要关注下退化维度和缓慢变化维。

（1）退化维度（DegenerateDimension）
在维度类型中，有一种重要的维度称作为退化维度，亦维度退化一说。这种维度指的是直接把一些简单的维度放在事实表中。退化维度是维度建模领域中的一个非常重要的概念，它对理解维度建模有着非常重要的作用，退化维度一般在分析中可以用来做分组使用。

（2）缓慢变化维（Slowly Changing Dimensions）
维度的属性并不是始终不变的，它会随着时间的流逝发生缓慢的变化，这种随时间发生变化的维度我们一般称之为缓慢变化维（SCD）。
</code></pre>
<p>SCD常用的三种处理方式：</p>
<p>① <strong>TYPE1</strong> 直接覆盖原值</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/1OYP1AZw0W1UwPYvStthXYOIVIJMRPHgR0hXEic2ooibCQcliandOomZCkS90JesBrE3XfK9FT0L4mzloqeiaBLGiaQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p>
<p>② <strong>TYPE2</strong> 增加维度行</p>
<p>   <em>在为维度成员增加新行时，需为其分配新的主代理键。<strong>并且，至少需要在维度行再增加三列：</strong>有效日期、截止日期、行标识。**这个地方可联想拉链表设计。</em></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/1OYP1AZw0W1UwPYvStthXYOIVIJMRPHgJQjR5Q6icbWznia9JUHr1DA4I6nNictPvXmgNayXhgcBcL96FOJGmY8kw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p>
<p>③ <strong>TYPE3</strong> 增加属性列 </p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/1OYP1AZw0W1UwPYvStthXYOIVIJMRPHgB52FtYIJ0gs20XVBtSFsiaMFCtqsDZjAEkR03vFibnATtpNEBzkn3hnA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p>
<p>④ 混合方式</p>
<p>可根据实际业务场景，混合或选择使用以上三种方式，以快速方便而又准确的分析历史变化情况。</p>
<p><em><strong>星形模型，雪花模型，星座模型的详解</strong></em></p>
<pre><code class="python">
</code></pre>
<p><em><strong>元数据</strong></em></p>
<p> 元数据是对潜在信息的信息，是关于数据的更高层次抽象，是对数据的描述。 </p>
<pre><code class="python">##  业务元数据：

指标名称、计算口径、业务术语解释、衍生指标等

数据概念模型和逻辑模型

业务规则引擎的规则、数据质量检测规则、数据挖掘算法等

数据血缘和影响分析

数据的安全或敏感级别等

## 技术元数据：

物理数据库表名称、列名称、列属性、备注、约束信息等

数据存储类型、位置、数据存储文件格式或数据压缩类型等

数据访问权限、组和角色

字段级血缘关系、ETL抽取加载转换信息

调度依赖关系、进度和数据更新频率

## 操作元数据：

系统执行日志

访问模式、访问频率和执行时间

程序名称和描述

版本维护等

备份、归档时间、归档存储信息

        上述只是大致的分为三类，简单地列举常用的元数据信息，其实还包括结构性元数据、保存性和权限元数据等等这里就不一一列举了。
</code></pre>
<p> <strong>元数据管理</strong> </p>
<p> 元数据也是数据，同样适用数据生命周期管理。元数据生命周期可分为采集、整合、存储、分析、应用、价值和服务几个阶段。 </p>
<pre><code class="python">## 元数据架构

         元数据战略是关于企业元数据管理目标的说明，也是开发团队的参考框架。元数据战略决定了企业元数据架构。元数据架构可分为三类：集中式元数据架构、分布式元数据架构和混合元数据架构。

## 集中式元数据架构：

        集中式架构包括一个集中的元数据存储，在这里保存了来自各个元数据来源的元数据最新副本。保证了其独立于源系统的元数据高可用性；加强了元数据存储的统一性和一致性；通过结构化、标准化元数据及其附件的元数据信息，提升了元数据数据质量。集中式元数据架构有利于元数据标准化统一管理与应用。
分布式元数据架构：

        分布式架构包括一个完整的分布式系统架构只维护一个单一访问点，元数据获取引擎响应用户的需求，从元数据来源系统实时获取元数据，而不存在统一集中元数据存储。虽然此架构保证了元数据始终是最新且有效的，但是源系统的元数据没有经过标准化或附加元数据的整合，且查询能力直接受限于相关元数据来源系统的可用性。
        
## 混合式元数据架构：

        这是一种折中的架构方案，元数据依然从元数据来源系统进入存储库。但是存储库的设计只考虑用户增加的元数据、高度标准化的元数据以及手工获取的元数据。

        这三类各有千秋，但为了更好发挥数据价值，就需要对元数据标准化、集中整合化、统一化管理。如果企业做功能较为完善的数据资产管理平台可采用集中式元数据架构。
</code></pre>
<p> <strong>元数据生命周期</strong> </p>
<p>  笔者这里以集中式元数据架构为例讲解，通过对数据源系统的元数据信息采集，发送Kafka消息系统进行解耦合，再使用Antlr4开发各版SQL解析器，对元数据信息新增、修改和删除操作进行标准化集中整合存储。在元数据集中存储的基础上或过程中，可提供元数据服务与应用，如数据资产目录、数据地图、集成IDE、统一SQL多处理引擎、字段级血缘关系、影响度分析、下线分析、版本管理和数据价值分析等（这些元数据应用可根据产品经理设计理念进行优化组合，笔者这里拉平排列各功能应用，为了方便讲解各元数据应用模块）。这里就包括了元数据采集、整合、存储、分析、应用等阶段的生命周期。 </p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200924235456.jpg" alt="微信图片_20200924235456" style="zoom:50%;">

<p>例：</p>
<p>技术元数据</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200924235712.png" alt="微信图片_20200924235712" style="zoom:50%;">



<p>业务元数据</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200924235755.jpg" alt="微信图片_20200924235755" style="zoom: 80%;">

<p><strong>数据质量监控系统的设计与实现</strong></p>
<p><strong>总体介绍</strong></p>
<p>​    此数据质量监控系统是基于Spark计算引擎，通过界面配置对Hive数据仓库各层表，离线批数据质量监控系统（流式数据质量监模块近期实现后再做分享）。用户可通过前端界面选择哪个数据源（哪个集群），数据库、表或表中字段、配置监控规则，存放到Mysql库（下文有表结构设计），程序通过规则大类，监控规则等元数据信息，动态生成SQL片段集合，在进行优化组合，尽量减少对表读取次数，将执行结果存放监控结果表。调度根据当前任务执行结果判断是否熔断告警。再根据执行结果形成数据质量报告。</p>
<p><strong>功能</strong></p>
<ul>
<li>丰富可扩展数据质量监控规则库</li>
<li>自定义数据质量监控规则及语法检查</li>
<li>任务熔断、电话、短信、邮件多级告警</li>
<li>清晰定位质量问题业务和技术数据Owner</li>
<li>数据质量问题汇总与明细展示</li>
<li>监控对象表结构变更动态感知</li>
<li>数据质量问题订阅</li>
</ul>
<p><strong>设计实现</strong></p>
<p>系统框架图</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Users\14352\AppData\Roaming\Typora\typora-user-images\1600965150300.png" alt="1600965150300" style="zoom:50%;">

<p> <strong>数据质量监控执行流程图</strong> </p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/Users\14352\AppData\Roaming\Typora\typora-user-images\1600965581908.png" alt="1600965581908" style="zoom: 33%;">

<p> <strong>表结构设计</strong> </p>
<pre><code class="python">## 1、监控任务与监控对象表的对应关系表

    说明：
source_id:制定对哪个集群数据进行质量监控
databaseName：指定数据库名称
table_name:指定表名称
owner：数据owner，表创建人，是从HiveMeta元数据中取得

## 2、监控规则配置表
 说明：
task_id:即上一张表id，调度时使用
databaseName：指定数据库名称
table_name: 指定表名称
part_flag: 是否为分区表标志
part_format: 分区格式
task_desc: 用户对此条监控规则的描述
rule_name: 监控规则大分类，如有效性、唯一性、准确性等等
term：具体的监控指标名称
field_name: 监控字段，如果是表级别，则填写“表级规则”或自定义SQL规则
warn_grad：告警级别 1:熔断电话告警；2:电话告警；3：邮件告警等等
rule_logic_monitor:监控规则，需要配置逻辑信息，是个json字符串，程序执行对对json的key进行解析
data_owner: 数据owner
is_usable: 此条监控规则是否可用，前端配置人员可停启规则
    
## 3、监控规则库表
    说明：
rule_name: 监控规则大分类，如有效性、唯一性、准确性等等
rule_desc: 规则大类的说明
term：具体的监控指标名称
term_desc: 规则说明
logic_remark: 监控规则的详细说明
term_level: 规则级别，字段级别、表级别或自定义SQL规则
is_useDefine: 是否为自定义规则
datatype_scope: 此条监控规则的是否范围，比如：同环比波动监测，只能数值型字段
stats: 状态，是否可用

## 4、监控规则元数据表
    说明：
rule_name: 监控规则大分类，如有效性、唯一性、准确性等等
term：具体的监控指标名称
param_name:  参数名称，为rule_logic_monitor监控规则，需要配置逻辑信息，是个json字符串，程序执行对对json的key进行解析
param_desc: 界面展示
value_type: 界面输入框还是下拉框等说明
constrints: 界面输入框输入内容限制，正则表达式
param_weight: 界面参数展示顺序
 
## 5、监控结果表
    说明：
task_id：任务id
uuid: 调用的批次
databaseName：指定数据库名称
table_name: 指定表名称
rule_name: 监控规则大分类，如有效性、唯一性、准确性等等
term：具体的监控指标名称
field_name: 监控字段，如果是表级别，则填写“表级规则”或自定义SQL规则
term_value：监控结果值
result_code：结果返回码，配合告警级别一起使用
warn_grad：告警级别 1:熔断电话告警；2:电话告警；3：邮件告警等等
data_owner: 数据owner
stats_date: 统计日期
    
## 6、监测结果码表
    说明：
    对监控结果表的term_value结果值说明
</code></pre>
<p> <strong>规则指标</strong> </p>
<pre><code class="python">数据质量监控指标从有效性、唯一性、完整性、准确性、一致性、有效性、时效性和自定义监控规则八大类，约20条监控规则指标。以下对各个监控指标做出解释。

## 有效性

 字段长度有效

对字段内容长度是否在有效性范围的监控指标，可配置[最小长度，最        大长度]。如mobile手机号11位；身份证18位或15位是否满足配置监控长度，不等于配置长度范围，视为脏数据。

 字段内容有效

对字段内容是否在满足正则表达式指定内容格式的监控指标。如对name姓名含有中英文结合；身份证号含有中文；手机号11111111111等异常数据监控。

 字段数值范围有效

对数值类型字段是否在有效性值范围的监控指标，可根据业务场景配置该字段值范围[最小值，最大值]。如age年龄，超过1000岁等。

 枚举值个数有效

对枚举值字段的可枚举值种类个数的监控指标，可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]与期望值的比较。如银行储值卡在“消费、转账、提现”三种业务类型，枚举值个数某天少了一种或多种业务类型，可能是上游业务系统出现问题，或数据采集时丢失数据。

 枚举值集合有效

对枚举值字段的可枚举值种类内容集合的监控指标，可配置“包含、相等、不包含”与期望值集合的比较。如银行储值卡在“消费、转账、提现”三种业务类型，出现了“消费、转账、贷款”三种业务类型，虽然枚举值个数也是3种，但是枚举值内容有误。

## 唯一性

 是否重复

对主键是否存在重复数据的监控指标。出现重复数据导致重复计算等问题，也支持联合主键唯一性监控。

完整性

 字段是否为空或NULL

对字段内容是否存NULL的监控指标。

 记录数是否丢失

表级别质量监控指标，判断是否记录是否丢失或无数据，可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]与期望值的比较。

 记录数环比波动

表级别质量监控指标，判断记录数环比波动范围的指标监控，如可配置[最小波动值,最大波动值]可接受记录数波动范围。

 记录数方差检验

表级别质量监控指标，判断记录数方差检验波动范围的指标监控，如可配置[最小波动值,最大波动值]可接受记录数波动范围。

## 准确性

 数值同比波动监测

对数值类型字段值同比年、季度或月度值是否在波动范围内的指标监控。可配置[最小波动值,最大波动值]可接受记录数波动范围。一般用于报表指标等监控。

 数值环比波动监测

对数值类型字段值环比[1-30]天值是否在波动范围内的指标监控。可配置[最小波动值,最大波动值]可接受记录数波动范围。一般用于报表指标等监控。

 数值方差波动检验

对数值类型字段值方差检验是否在波动范围内的指标监控。可配置[最小波动值,最大波动值]可接受记录数波动范围。一般用于报表指标等监控。

 表逻辑检查

表级别质量监控指标，对表两个字段存在逻辑关系是否准确的监控指标。如信用卡当前剩余可用额度&lt;=此次消费金额；还如贷款，起息日应早于贷款放款日期等异常监控。

## 一致性

 表级别一致性检查

表级别质量监控指标，根据提前定义的数据标准，基础元数据字段命名规范，术语命名规则、字段comennt规范、数据类型规范；指标元数据字段命名规范，术语命名规范、字段comennt规范、数据类型规范，计算口径是否统一等规范。对表结构字段、字段comment、数据类型等的是否一致的监控检查。

 交叉验证

表级别质量监控指标，判断两张表的主体对象是否一致。		

## 时效性

数据是否准时产出

表级别质量监控指标，数据是否按时产出。如用于决策的报表是否领导们上班前准时看到准确无误的报表。

## 数据剖析

 最大值检查

对数值类型字段的最大值与期望值可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]比较的监控指标。支持Where条件的自定义谓词条件限制。

最小值检查

对数值类型字段的最小值与期望值可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]比较的监控指标。支持Where条件的自定义谓词条件限制。

平均值检查

对数值类型字段的平均值与期望值可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]比较的监控指标。支持Where条件的自定义谓词条件限制。

汇总值检查

对数值类型字段的汇总值与期望值可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]比较的监控指标。支持Where条件的自定义谓词条件限制。



## 自定义规则检查

 自定义SQL规则
用户写自定义SQL实现的监控规则，但这段SQL结果必须一行一列值，即监测结果是一个值。可配置[&gt;、&gt;=、=、&lt;=、&lt;、!=]与期望值的比较，判断监测结果是否异常。
</code></pre>
<p> 常见数据质量监控规则如下图：</p>
<img src="https://img-blog.csdnimg.cn/20190528154230375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mjg5MzY1MA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 50%;">

<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200927231751.jpg" alt="微信图片_20200927231751" style="zoom:50%;">

<p>表结构变更动态感知** </p>
<pre><code class="python">1:
</code></pre>
<p><strong>告警及通知</strong></p>
<pre><code>1:
</code></pre>
<h2 id="Hive插入数据的几种方式"><a href="#Hive插入数据的几种方式" class="headerlink" title="Hive插入数据的几种方式"></a><strong>Hive插入数据的几种方式</strong></h2><pre><code class="python">##1 将查询结果保存到一张新的hive表中
create table t_tmp
as
select * from t_p;

##2 将查询结果保存到一张已经存在的hive表中 (overwrite 覆盖 into 追加)
insert into/overwrite table t_tmp            
select * from t_p;

##3 将查询结果保存到指定的文件目录
insert overwrite local directory &#39;/home/hadoop/test&#39;
select * from t_p;

##4 多重插入
from xxx_table_name PARTITION (ds = &#39;20190827&#39;)
insert overwrite table table1 PARTITION (ds = &#39;0&#39;) select name where crc32(cust_id)%4 == 0
insert overwrite table table1 PARTITION (ds = &#39;1&#39;) select name where crc32(cust_id)%4 == 1
insert overwrite table table1 PARTITION (ds = &#39;2&#39;) select name where crc32(cust_id)%4 == 2
insert overwrite table table1 PARTITION (ds = &#39;3&#39;) select name where crc32(cust_id)%4 == 3

##5 with创建临时表 CTE后面必须直接跟使用CTE的SQL语句（如select、insert、update等），否则，CTE将失效
with 
cte1 as 
( 
    select * from table1 where name like &#39;abc%&#39; 
), 
cte2 as 
( 
    select * from table2 where id &gt; 20 
), 
cte3 as 
( 
    select * from table3 where price &lt; 100 
) 
select a.* from cte1 a, cte2 b, cte3 c where a.id = b.id and a.id = c.id 

或者：
WITH Generation (ID) AS
(
-- First anchor member returns Bonnie&#39;s mother.
    SELECT Mother 
    FROM Person
    WHERE Name = &#39;Bonnie&#39;
UNION
-- Second anchor member returns Bonnie&#39;s father.
    SELECT Father 
    FROM Person
    WHERE Name = &#39;Bonnie&#39;
UNION ALL
-- First recursive member returns male ancestors of the previous generation.
    SELECT Person.Father
    FROM Generation, Person
    WHERE Generation.ID=Person.ID
UNION ALL
-- Second recursive member returns female ancestors of the previous generation.
    SELECT Person.Mother
    FROM Generation, Person
    WHERE Generation.ID=Person.ID
)
SELECT Person.ID, Person.Name, Person.Mother, Person.Father
FROM Generation, Person
WHERE Generation.ID = Person.ID;
</code></pre>
<h2 id="hive对于连续多天未出现的时间节点补零操作"><a href="#hive对于连续多天未出现的时间节点补零操作" class="headerlink" title="hive对于连续多天未出现的时间节点补零操作"></a>hive对于连续多天未出现的时间节点补零操作</h2><h2 id="postgresql与hive的炸裂函数"><a href="#postgresql与hive的炸裂函数" class="headerlink" title="postgresql与hive的炸裂函数"></a>postgresql与hive的炸裂函数</h2><pre><code class="sql">------------------------------------------------ |
                                             |
            postgresql炸裂详解				   |
                                             |
------------------------------------------------ |

select regexp_split_to_table(tr,&#39;,&#39;) field from 
(select 
array_to_string(tiff_ids,&#39;,&#39;) tr
from 
order_table_test)  o

炸裂函数regexp_split_to_table
select regexp_split_to_table(&#39;飞机，火车，地铁，汽车&#39;,  &#39;，&#39; )                    以逗号切分，转为数据集
select regexp_split_to_array(&#39;飞机，火车，地铁，汽车&#39;, &#39;，&#39; )                     转为数组

select (regexp_split_to_array(&#39;飞机，火车，地铁，汽车&#39;,  &#39;，&#39; ))[1]               取数组的第二个元素
select regexp_split_to_table(&#39;F:\QH本部文件\一套表部署相关\test.sh&#39;,&#39;\\&#39;)         正则匹配


array_agg(expression)       把表达式变成一个数组 一般配合 array_to_string() 函数使用
select nameid, array_agg(traffic ) from dbscheme.test0001 group by nameid order by nameid ;   变为数组
string_agg(expression, delimiter)    直接把一个表达式变成字符串
select n
ameid, string_agg(traffic,&#39;,&#39;) , update_time from dbscheme.test0001   //相同id 的连接到一起，逗号分隔
group by nameid,update_time order by  nameid,update_time;    


select nameid, array_to_string(array_agg(traffic),&#39;,&#39;) from dbscheme.test0001 group by nameid order by nameid ; .数组转字符串

------------------------------------------------ |
                                             |
            hive炸裂详解					   |
                                             |
------------------------------------------------ |
+----------+----------------------+--+
|   a.id   |        a.tim         |
+----------+----------------------+--+
| a,b,c,d  | 2:00,3:00,4:00,5:00  |
| f,b,c,d  | 1:10,2:20,3:30,4:40  |
+----------+----------------------+--+
explode 炸裂函数，一列变多行。
select id,tim,single_tim 
from atlasdemo.a 
lateral view explode(split(tim,&#39;,&#39;)) t as single_tim

+----------+----------------------+-------------+--+
|    id    |         tim          | single_tim  |
+----------+----------------------+-------------+--+
| a,b,c,d  | 2:00,3:00,4:00,5:00  | 2:00        |
| a,b,c,d  | 2:00,3:00,4:00,5:00  | 3:00        |
| a,b,c,d  | 2:00,3:00,4:00,5:00  | 4:00        |
| a,b,c,d  | 2:00,3:00,4:00,5:00  | 5:00        |
| f,b,c,d  | 1:10,2:20,3:30,4:40  | 1:10        |
| f,b,c,d  | 1:10,2:20,3:30,4:40  | 2:20        |
| f,b,c,d  | 1:10,2:20,3:30,4:40  | 3:30        |
| f,b,c,d  | 1:10,2:20,3:30,4:40  | 4:40        |
+----------+----------------------+-------------+--+
select id,tim,single_id_index,single_id 
from atlasdemo.a 
lateral view posexplode(split(id,&#39;,&#39;)) t as single_id_index, single_id;

posexplode炸裂除了会炸开数组/map，还会对应生成索引下标。

+----------+----------------------+------------------+------------+--+
|    id    |         tim          | single_id_index  | single_id  |
+----------+----------------------+------------------+------------+--+
| a,b,c,d  | 2:00,3:00,4:00,5:00  | 0                | a          |
| a,b,c,d  | 2:00,3:00,4:00,5:00  | 1                | b          |
| a,b,c,d  | 2:00,3:00,4:00,5:00  | 2                | c          |
| a,b,c,d  | 2:00,3:00,4:00,5:00  | 3                | d          |
| f,b,c,d  | 1:10,2:20,3:30,4:40  | 0                | f          |
| f,b,c,d  | 1:10,2:20,3:30,4:40  | 1                | b          |
| f,b,c,d  | 1:10,2:20,3:30,4:40  | 2                | c          |
| f,b,c,d  | 1:10,2:20,3:30,4:40  | 3                | d          |
+----------+----------------------+------------------+------------+--+

如果想实现对两列听同事进行多行转换，那么用explode()函数就不能实现了，
但可以用posexplode()函数，因为该函数可以将index和数据都取出来，
使用两次posexplode并令两次取到的index相等就行了。

select id,tim,single_id,single_tim from atlasdemo.a 
lateral view posexplode(split(id,&#39;,&#39;)) t as single_id_index, single_id
lateral view posexplode(split(tim,&#39;,&#39;)) t as single_yim_index, single_tim
where single_id_index = single_yim_index;
+----------+----------------------+------------+-------------+--+
|    id    |         tim          | single_id  | single_tim  |
+----------+----------------------+------------+-------------+--+
| a,b,c,d  | 2:00,3:00,4:00,5:00  | a          | 2:00        |
| a,b,c,d  | 2:00,3:00,4:00,5:00  | b          | 3:00        |
| a,b,c,d  | 2:00,3:00,4:00,5:00  | c          | 4:00        |
| a,b,c,d  | 2:00,3:00,4:00,5:00  | d          | 5:00        |
| f,b,c,d  | 1:10,2:20,3:30,4:40  | f          | 1:10        |
| f,b,c,d  | 1:10,2:20,3:30,4:40  | b          | 2:20        |
| f,b,c,d  | 1:10,2:20,3:30,4:40  | c          | 3:30        |
| f,b,c,d  | 1:10,2:20,3:30,4:40  | d          | 4:40        |
+----------+----------------------+------------+-------------+--+
</code></pre>
<h2 id="美团-数据质量平台-设计与实践"><a href="#美团-数据质量平台-设计与实践" class="headerlink" title="美团 数据质量平台 设计与实践"></a>美团 数据质量平台 设计与实践</h2><p><em>*<em>*</em>*挑战****</em>*</p>
<p>美旅数据中心日均处理的离线和实时作业高达数万量级， 如何更加合理、高效的监控每类作业的运行状态，并将原本分散、孤岛式的监控日志信息通过规则引擎集中共享、关联、处理；洞察关键信息，形成事前预判、事中监控、事后跟踪的质量管理闭环流程；沉淀故障问题，搭建解决方案的知识库体系。在数据质量监管平台的规划建设中，面临如下挑战：</p>
<ul>
<li>缺乏统一监控视图，离线和实时作业监控分散，影响性、关联性不足。</li>
<li>数据质量的衡量标准缺失，数据校验滞后，数据口径不统一。</li>
<li>问题故障处理流程未闭环，“点”式解决现象常在；缺乏统一归档，没有形成体系的知识库。</li>
<li>数据模型质量监控缺失，模型重复，基础模型与应用模型的关联度不足，形成信息孤岛。</li>
<li>数据存储资源增长过快，不能监控细粒度资源内容。</li>
</ul>
<p>DataMan质量监管平台研发正基于此，以下为具体建设方案</p>
<p><em>*<em>*</em>*解决思路****</em>*</p>
<p><strong>整体框架</strong></p>
<p>构建美旅大数据质量监控平台，从可实践运用的视角出发，整合平台资源、技术流程核心要点，重点着力平台支持、技术控制、流程制度、知识体系形成等方向建设，确保质量监控平台敏捷推进落地的可行性。数据质量监控平台整体框架如图1所示：</p>
<img src="/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/image\微信图片_20200927223431.jpg" alt="微信图片_20200927223431" style="zoom:50%;">
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">三山</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://star-light-star-bright.github.io/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/">https://star-light-star-bright.github.io/si-fang-cai-zi-liao-zheng-li/mian-shi-ti/ge-ren-zheng-li/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">三山</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/">
                                    <span class="chip bg-color">面试题</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/cdh/hadoop/hadoop/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/3.jpg" class="responsive-img" alt="Hadoop学习笔记">
                        
                        <span class="card-title">Hadoop学习笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-11-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Hadoop/" class="post-category">
                                    Hadoop
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Hadopp/">
                        <span class="chip bg-color">Hadopp</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/da-shu-ju/fu-wu-qi-zu-jian-ban-ben/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/21.jpg" class="responsive-img" alt="大数据组件版本">
                        
                        <span class="card-title">大数据组件版本</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-10-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="post-category">
                                    大数据
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">
                        <span class="chip bg-color">大数据</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2025</span>
            
            <span id="year">2019</span>
            <a href="/about" target="_blank">三山</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/blinkfox" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:1181062873@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2234607886" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2234607886" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

	
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
